{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pandas scikit-learn matplotlib tables pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed4a09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, matthews_corrcoef\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecbed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore('../ukdale.h5') #需要按照自己文件位置修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db77ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_meter(store=None, building=1, meter=1, period='1min', cutoff=1000.):\n",
    "    key = '/building{}/elec/meter{}'.format(building,meter)\n",
    "    m = store[key]\n",
    "    v = m.values.flatten()\n",
    "    t = m.index\n",
    "    s = pd.Series(v, index=t).clip(0.,cutoff)\n",
    "    s[s<10.] = 0.\n",
    "    return s.resample('1s').ffill(limit=300).fillna(0.).resample(period).mean().tz_convert('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc823cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series(datastore, house, label, cutoff):\n",
    "    filename = './house_%1d_labels.dat' %house   #需要按照自己文件位置修改\n",
    "    print(filename)\n",
    "    labels = pd.read_csv(filename, delimiter=' ', header=None, index_col=0).to_dict()[1]\n",
    "    \n",
    "    for i in labels:\n",
    "        if labels[i] == label:\n",
    "            print(i, labels[i])\n",
    "            #s = resample_meter(store, house, i, '1min', cutoff)\n",
    "            s = resample_meter(store, house, i, '6s', cutoff)\n",
    "    \n",
    "    s.index.name = 'datetime'\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40363bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "10 kettle\n",
      "./house_1_labels.dat\n",
      "12 fridge\n",
      "./house_1_labels.dat\n",
      "5 washing_machine\n",
      "./house_1_labels.dat\n",
      "13 microwave\n",
      "./house_1_labels.dat\n",
      "6 dishwasher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_15152\\3829807372.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\3829807372.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\3829807372.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_1_valid = ds_1[pd.datetime(2014,12,15):]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\3829807372.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_1_valid = ds_1[pd.datetime(2014,12,15):]\n"
     ]
    }
   ],
   "source": [
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_1 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_1.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_1_train = ds_1[pd.datetime(2013,4,12):pd.datetime(2014,12,15)]\n",
    "ds_1_valid = ds_1[pd.datetime(2014,12,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3df49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_2_labels.dat\n",
      "1 aggregate\n",
      "./house_2_labels.dat\n",
      "8 kettle\n",
      "./house_2_labels.dat\n",
      "14 fridge\n",
      "./house_2_labels.dat\n",
      "12 washing_machine\n",
      "./house_2_labels.dat\n",
      "15 microwave\n",
      "./house_2_labels.dat\n",
      "13 dish_washer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_15152\\845194013.py:19: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_2_train = ds_2[pd.Timestamp(2013,5,22):pd.Timestamp(2013,10,3,6,16)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\845194013.py:20: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_2_valid = ds_2[pd.Timestamp(2013,10,3,6,16):]\n"
     ]
    }
   ],
   "source": [
    "house = 2\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washing_machine', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dish_washer', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_2 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_2.fillna(method='pad', inplace=True)\n",
    "\n",
    "#ds_2_train = ds_2[pd.datetime(2013,5,22):pd.datetime(2013,10,3,6,16)]\n",
    "#ds_2_valid = ds_2[pd.datetime(2013,10,3,6,16):]\n",
    "ds_2_train = ds_2[pd.Timestamp(2013,5,22):pd.Timestamp(2013,10,3,6,16)]\n",
    "ds_2_valid = ds_2[pd.Timestamp(2013,10,3,6,16):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9d27e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_3_labels.dat\n",
      "1 aggregate\n",
      "./house_3_labels.dat\n",
      "2 kettle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_15152\\2982668517.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\2982668517.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\2982668517.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\2982668517.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]\n"
     ]
    }
   ],
   "source": [
    "house = 3\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = 0.*m\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_3 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_3.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_3_train = ds_3[pd.datetime(2013,2,27):pd.datetime(2013,4,1,6,15)]\n",
    "ds_3_valid = ds_3[pd.datetime(2013,4,1,6,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7511fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_4_labels.dat\n",
      "1 aggregate\n",
      "./house_4_labels.dat\n",
      "3 kettle_radio\n",
      "./house_4_labels.dat\n",
      "5 freezer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_15152\\1825372461.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_4_train = ds_4[pd.Timestamp(2013,3,9):pd.Timestamp(2013,9,24,6,15)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\1825372461.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_4_valid = ds_4[pd.Timestamp(2013,9,24,6,15):]\n"
     ]
    }
   ],
   "source": [
    "house = 4\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle_radio', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = 0.*m\n",
    "a3.name = 'washing_machine'\n",
    "a4 = 0.*m\n",
    "a4.name = 'microwave'\n",
    "a5 = 0.*m\n",
    "a5.name = 'dish_washer'\n",
    "ds_4 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_4.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_4_train = ds_4[pd.Timestamp(2013,3,9):pd.Timestamp(2013,9,24,6,15)]\n",
    "ds_4_valid = ds_4[pd.Timestamp(2013,9,24,6,15):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f9838c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_5_labels.dat\n",
      "1 aggregate\n",
      "./house_5_labels.dat\n",
      "18 kettle\n",
      "./house_5_labels.dat\n",
      "19 fridge_freezer\n",
      "./house_5_labels.dat\n",
      "24 washer_dryer\n",
      "./house_5_labels.dat\n",
      "23 microwave\n",
      "./house_5_labels.dat\n",
      "22 dishwasher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\usertemp\\xundao\\ipykernel_15152\\2498256263.py:17: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\2498256263.py:17: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\2498256263.py:18: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_5_valid = ds_5[pd.datetime(2014,9,1):]\n",
      "C:\\usertemp\\xundao\\ipykernel_15152\\2498256263.py:18: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version.  Use a timezone-aware object instead.\n",
      "  ds_5_valid = ds_5[pd.datetime(2014,9,1):]\n"
     ]
    }
   ],
   "source": [
    "house = 5\n",
    "m = get_series(store, house, 'aggregate', 10000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'kettle', 3100.)\n",
    "a1.name = 'kettle'\n",
    "a2 = get_series(store, house, 'fridge_freezer', 300.)\n",
    "a2.name = 'fridge'\n",
    "a3 = get_series(store, house, 'washer_dryer', 2500.)\n",
    "a3.name = 'washing_machine'\n",
    "a4 = get_series(store, house, 'microwave', 3000.)\n",
    "a4.name = 'microwave'\n",
    "a5 = get_series(store, house, 'dishwasher', 2500.)\n",
    "a5.name = 'dish_washer'\n",
    "ds_5 = pd.concat([m, a1, a2, a3, a4, a5], axis=1)\n",
    "ds_5.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_5_train = ds_5[pd.datetime(2014,6,29):pd.datetime(2014,9,1)]\n",
    "ds_5_valid = ds_5[pd.datetime(2014,9,1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba7a3669",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_1_train.reset_index().to_feather('./UKDALE_01_train.feather')\n",
    "ds_2_train.reset_index().to_feather('./UKDALE_02_train.feather')\n",
    "ds_3_train.reset_index().to_feather('./UKDALE_03_train.feather')\n",
    "ds_4_train.reset_index().to_feather('./UKDALE_04_train.feather')\n",
    "ds_5_train.reset_index().to_feather('./UKDALE_05_train.feather')\n",
    "\n",
    "ds_1_valid.reset_index().to_feather('./UKDALE_01_valid.feather')\n",
    "ds_2_valid.reset_index().to_feather('./UKDALE_02_valid.feather')\n",
    "ds_3_valid.reset_index().to_feather('./UKDALE_03_valid.feather')\n",
    "ds_4_valid.reset_index().to_feather('./UKDALE_04_valid.feather')\n",
    "ds_5_valid.reset_index().to_feather('./UKDALE_05_valid.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f245a",
   "metadata": {},
   "source": [
    "# Read the feather dataframe resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b3fae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status(app, threshold, min_off, min_on):\n",
    "    condition = app > threshold\n",
    "    # Find the indicies of changes in \"condition\"\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "\n",
    "    # We need to start things after the change in \"condition\". Therefore, \n",
    "    # we'll shift the index by 1 to the right.\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        # If the start of condition is True prepend a 0\n",
    "        idx = np.r_[0, idx]\n",
    "\n",
    "    if condition[-1]:\n",
    "        # If the end of condition is True, append the length of the array\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "\n",
    "    # Reshape the result into two columns\n",
    "    idx.shape = (-1,2)\n",
    "    on_events = idx[:,0].copy()\n",
    "    off_events = idx[:,1].copy()\n",
    "    assert len(on_events) == len(off_events)\n",
    "\n",
    "    if len(on_events) > 0:\n",
    "        off_duration = on_events[1:] - off_events[:-1]\n",
    "        off_duration = np.insert(off_duration, 0, 1000.)\n",
    "        on_events = on_events[off_duration > min_off]\n",
    "        off_events = off_events[np.roll(off_duration, -1) > min_off]\n",
    "        assert len(on_events) == len(off_events)\n",
    "\n",
    "        on_duration = off_events - on_events\n",
    "        on_events = on_events[on_duration > min_on]\n",
    "        off_events = off_events[on_duration > min_on]\n",
    "\n",
    "    s = app.copy()\n",
    "    #s.iloc[:] = 0.\n",
    "    s[:] = 0.\n",
    "\n",
    "    for on, off in zip(on_events, off_events):\n",
    "        #s.iloc[on:off] = 1.\n",
    "        s[on:off] = 1.\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9c82c",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为get_status的函数，用于从一个电器的用电量数据中获取该电器的状态。该函数接受四个参数：app表示电器的用电量数据，threshold表示电器的开启阈值，min_off表示电器关闭持续时间的最小值，min_on表示电器开启持续时间的最小值。\n",
    "\n",
    "该函数首先根据阈值将电器的用电量数据转换为布尔型数组condition。然后，它使用numpy库中的diff函数获取condition中相邻元素的差值，并使用nonzero方法获取差值不为0的元素的索引。接着，它将索引向右移动一位，并在开头和结尾处分别添加0和数组长度。最后，它将索引按每两个元素为一组进行划分，并计算出每个组对应的电器开启和关闭时间点，并将其存储在on_events和off_events数组中。\n",
    "\n",
    "接下来，该函数使用on_events和off_events数组计算出电器关闭持续时间和开启持续时间，并筛选出持续时间大于最小值的时间段。然后，它将每个时间段内的用电量数据标记为1，其余部分标记为0，并将标记后的用电量数据存储在s数组中。\n",
    "\n",
    "最后，该函数返回标记后的用电量数据s。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d4c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Power(data.Dataset):\n",
    "    def __init__(self, meter=None, appliance=None, status=None, \n",
    "                 length=256, border=680, max_power=1., train=False):\n",
    "        self.length = length\n",
    "        self.border = border\n",
    "        self.max_power = max_power\n",
    "        self.train = train\n",
    "\n",
    "        self.meter = meter.copy()/self.max_power\n",
    "        self.appliance = appliance.copy()/self.max_power\n",
    "        self.status = status.copy()\n",
    "\n",
    "        self.epochs = (len(self.meter) - 2*self.border) // self.length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        i = index * self.length + self.border\n",
    "        if self.train:\n",
    "            i = np.random.randint(self.border, len(self.meter) - self.length - self.border)\n",
    "\n",
    "        x = self.meter.iloc[i-self.border:i+self.length+self.border].values.astype('float32')\n",
    "        y = self.appliance.iloc[i:i+self.length].values.astype('float32')\n",
    "        s = self.status.iloc[i:i+self.length].values.astype('float32')\n",
    "        x -= x.mean()\n",
    "        \n",
    "        return x, y, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b548701",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为`Power`的数据集类，用于从电器的用电量数据中生成训练和测试数据。该类的初始化函数接受五个参数：`meter`表示电表的用电量数据，`appliance`表示电器的用电量数据，`status`表示电器的状态数据，`length`表示每个样本的长度，`border`表示每个样本的边界长度，`max_power`表示用电量数据的最大值，`train`表示是否为训练数据。\n",
    "\n",
    "该类实现了两个方法：`__getitem__`和`__len__`。其中，`__getitem__`方法用于获取指定索引的样本数据，它首先计算出样本的起始位置`i`，如果是训练数据，则随机生成一个起始位置。然后，它从电表和电器的用电量数据中获取指定位置和长度的数据，并将其存储在`x`和`y`数组中。同时，它还从电器的状态数据中获取指定位置和长度的状态数据，并将其存储在`s`数组中。最后，它将`x`数组减去平均值，并返回三个数组作为样本数据。\n",
    "\n",
    "x：表示电表数据（meter data）。它是一个时间序列，包含了电表的读数。在代码中，x被定义为一个Numpy数组，包含了从(i-self.border)到(i+self.length+self.border)的电表数据。\n",
    "y：表示电器数据（appliance data）。它也是一个时间序列，包含了特定电器的能耗数据。在代码中，y被定义为一个Numpy数组，包含了从i到(i+self.length)的电器数据。\n",
    "s：表示状态数据（status data）。它也是一个时间序列，包含了与电器状态相关的数据。在代码中，s被定义为一个Numpy数组，包含了从i到(i+self.length)的状态数据。\n",
    "\n",
    "`__len__`方法用于返回整个数据集中样本的数量，它计算出整个数据集中可以生成的样本数，并返回该数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6bd73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#引入EMAttention模块\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, channels, factor=8):\n",
    "        super(EMA, self).__init__()\n",
    "        self.groups = factor\n",
    "        assert channels // self.groups > 0\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.agp = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "        self.gn = nn.GroupNorm(channels // self.groups, channels // self.groups)\n",
    "        self.conv1x1 = nn.Conv2d(channels // self.groups, channels // self.groups, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3x3 = nn.Conv2d(channels // self.groups, channels // self.groups, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()\n",
    "        group_x = x.reshape(b * self.groups, -1, h, w)  # b*g,c//g,h,w\n",
    "        x_h = self.pool_h(group_x)\n",
    "        x_w = self.pool_w(group_x).permute(0, 1, 3, 2)\n",
    "        hw = self.conv1x1(torch.cat([x_h, x_w], dim=2))\n",
    "        x_h, x_w = torch.split(hw, [h, w], dim=2)\n",
    "        x1 = self.gn(group_x * x_h.sigmoid() * x_w.permute(0, 1, 3, 2).sigmoid())\n",
    "        x2 = self.conv3x3(group_x)\n",
    "        x11 = self.softmax(self.agp(x1).reshape(b * self.groups, -1, 1).permute(0, 2, 1))\n",
    "        x12 = x2.reshape(b * self.groups, c // self.groups, -1)  # b*g, c//g, hw\n",
    "        x21 = self.softmax(self.agp(x2).reshape(b * self.groups, -1, 1).permute(0, 2, 1))\n",
    "        x22 = x1.reshape(b * self.groups, c // self.groups, -1)  # b*g, c//g, hw\n",
    "        weights = (torch.matmul(x11, x12) + torch.matmul(x21, x22)).reshape(b * self.groups, 1, h, w)\n",
    "        return (group_x * weights.sigmoid()).reshape(b, c, h, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bf3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.einops import rearrange\n",
    "\n",
    "def to_3d(x):\n",
    "    return rearrange(x,'b c h w -> b (h w) c')\n",
    "\n",
    "def to_4d(x,h,w):\n",
    "    return rearrange(x,' b (h w) c -> b c h w',h=h,w=w)\n",
    "#x=x.permute(0,2,3,1)  #[B,C,H,W] -> [B,H,W,C]\n",
    "#x=x.permute(0,3,1,2)  #[B,H,W,C] -> [B,C,H,W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d59fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StripPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Reference:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, pool_size, norm_layer, up_kwargs):\n",
    "        super(StripPooling, self).__init__()\n",
    "        self.pool1 = nn.AdaptiveAvgPool2d(pool_size[0])\n",
    "        self.pool2 = nn.AdaptiveAvgPool2d(pool_size[1])\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((1, None))\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((None, 1))\n",
    "\n",
    "        inter_channels = int(in_channels / 4)\n",
    "        self.conv1_1 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n",
    "                                     norm_layer(inter_channels),\n",
    "                                     nn.ReLU(True))\n",
    "        self.conv1_2 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),\n",
    "                                     norm_layer(inter_channels),\n",
    "                                     nn.ReLU(True))\n",
    "        self.conv2_0 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),\n",
    "                                     norm_layer(inter_channels))\n",
    "        self.conv2_1 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),\n",
    "                                     norm_layer(inter_channels))\n",
    "        self.conv2_2 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),\n",
    "                                     norm_layer(inter_channels))\n",
    "        self.conv2_3 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (1, 3), 1, (0, 1), bias=False),\n",
    "                                     norm_layer(inter_channels))\n",
    "        self.conv2_4 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 1), 1, (1, 0), bias=False),\n",
    "                                     norm_layer(inter_channels))\n",
    "        self.conv2_5 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),\n",
    "                                     norm_layer(inter_channels),\n",
    "                                     nn.ReLU(True))\n",
    "        self.conv2_6 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),\n",
    "                                     norm_layer(inter_channels),\n",
    "                                     nn.ReLU(True))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(inter_channels * 2, in_channels, 1, bias=False),\n",
    "                                   norm_layer(in_channels))\n",
    "        # bilinear interpolate options\n",
    "        self._up_kwargs = up_kwargs\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.size()\n",
    "        x1 = self.conv1_1(x)\n",
    "        x2 = self.conv1_2(x)\n",
    "        x2_1 = self.conv2_0(x1)\n",
    "        x2_2 = F.interpolate(self.conv2_1(self.pool1(x1)), (h, w), **self._up_kwargs)\n",
    "        x2_3 = F.interpolate(self.conv2_2(self.pool2(x1)), (h, w), **self._up_kwargs)\n",
    "        x2_4 = F.interpolate(self.conv2_3(self.pool3(x2)), (h, w), **self._up_kwargs)\n",
    "        x2_5 = F.interpolate(self.conv2_4(self.pool4(x2)), (h, w), **self._up_kwargs)\n",
    "        x1 = self.conv2_5(F.relu_(x2_1 + x2_2 + x2_3))\n",
    "        x2 = self.conv2_6(F.relu_(x2_5 + x2_4))\n",
    "        out = self.conv3(torch.cat([x1, x2], dim=1))\n",
    "        return F.relu_(x + out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f626f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class SKAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, channel=512, kernels=[1, 3, 5, 7], reduction=16, group=1, L=32):\n",
    "        super().__init__()\n",
    "        self.d = max(L, channel // reduction)\n",
    "        self.convs = nn.ModuleList([])\n",
    "        for k in kernels:\n",
    "            self.convs.append(\n",
    "                nn.Sequential(OrderedDict([\n",
    "                    ('conv', nn.Conv2d(channel, channel, kernel_size=k, padding=k // 2, groups=group)),\n",
    "                    ('bn', nn.BatchNorm2d(channel)),\n",
    "                    ('relu', nn.ReLU())\n",
    "                ]))\n",
    "            )\n",
    "        self.fc = nn.Linear(channel, self.d)\n",
    "        self.fcs = nn.ModuleList([])\n",
    "        for i in range(len(kernels)):\n",
    "            self.fcs.append(nn.Linear(self.d, channel))\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.size()\n",
    "        conv_outs = []\n",
    "        ### split\n",
    "        for conv in self.convs:\n",
    "            conv_outs.append(conv(x))\n",
    "        feats = torch.stack(conv_outs, 0)  # k,bs,channel,h,w\n",
    "\n",
    "        ### fuse\n",
    "        U = sum(conv_outs)  # bs,c,h,w\n",
    "\n",
    "        ### reduction channel\n",
    "        S = U.mean(-1).mean(-1)  # bs,c\n",
    "        Z = self.fc(S)  # bs,d\n",
    "\n",
    "        ### calculate attention weight\n",
    "        weights = []\n",
    "        for fc in self.fcs:\n",
    "            weight = fc(Z)\n",
    "            weights.append(weight.view(bs, c, 1, 1))  # bs,channel\n",
    "        attention_weughts = torch.stack(weights, 0)  # k,bs,channel,1,1\n",
    "        attention_weughts = self.softmax(attention_weughts)  # k,bs,channel,1,1\n",
    "\n",
    "        ### fuse\n",
    "        V = (attention_weughts * feats).sum(0)\n",
    "        return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ea870",
   "metadata": {},
   "source": [
    "### AMSAIMNILM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb6d6f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc4: torch.Size([32, 320, 15])\n",
      "ema1: torch.Size([32, 16, 20, 15])\n",
      "tp1: torch.Size([32, 128, 15])\n",
      "torch.Size([32, 3, 120])\n",
      "607475\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride,\n",
    "                              bias=False)\n",
    "        # self.conv=LSKblock(out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        # self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        # return self.upsample(x)\n",
    "        # return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='nearest', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='bicubic', align_corners=True))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool1_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1 ** k, features * 2 ** k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool2_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2 ** k, features * 4 ** k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool3_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4 ** k, features * 10 ** k, kernel_size=3, padding=0)\n",
    "        #self.MUSEA = MUSEAttention(d_model=15, d_k=15, d_v=15, h=8)\n",
    "        self.EMA = EMA(16)\n",
    "        \n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=15)\n",
    "        self.tpool3 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=15)\n",
    "        self.tpool4 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=15)\n",
    "        self.tpool5 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=5)\n",
    "        #self.tpool6 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=10)\n",
    "        #self.tpool7 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "\n",
    "\n",
    "        self.decoder = Decoder(2 * features * 15 ** k, features * 1 ** k, kernel_size=p ** 3, stride=p ** 3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1 ** k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('x:',x.shape)\n",
    "        enc1 = self.encoder1(x)\n",
    "        #print('enc1:',enc1.shape)\n",
    "        enc2 = self.encoder2(self.pool1(enc1)+self.pool1_1(enc1))\n",
    "        #print('enc2:',enc2.shape)\n",
    "        enc3 = self.encoder3(self.pool2(enc2)+self.pool2_1(enc2))\n",
    "        #print('enc3:',enc3.shape)\n",
    "        enc4 = self.encoder4(self.pool3(enc3)+self.pool3_1(enc3))\n",
    "        print('enc4:',enc4.shape)\n",
    "        #mu1 = self.MUSEA(enc4, enc4, enc4)\n",
    "        bx1= to_4d(enc4,20,16).permute(0,3,2,1)\n",
    "        #print('bx1:',bx1.shape)\n",
    "        ema1 = self.EMA(bx1)\n",
    "        print('ema1:',ema1.shape)\n",
    "        ema2= to_3d(ema1.permute(0,3,2,1))\n",
    "        #print('ema2:',ema2.shape)\n",
    "        \n",
    "        tp1 = self.tpool1(ema2)\n",
    "        print('tp1:',tp1.shape)\n",
    "        tp2 = self.tpool2(ema2)\n",
    "        #print('tp2:',tp2.shape)\n",
    "        tp3 = self.tpool3(ema2)\n",
    "        #print('tp3:',tp3.shape)\n",
    "        tp4 = self.tpool4(ema2)\n",
    "        #print('tp4:',tp4.shape)\n",
    "        tp5 = self.tpool5(ema2)\n",
    "        #print('tp5:',tp5.shape)\n",
    "        #tp6 = self.tpool6(ema2)\n",
    "        #print('tp6:',tp6.shape)\n",
    "        #tp7 = self.tpool7(ema2)\n",
    "        #print('tp7:',tp7.shape)\n",
    "\n",
    "        \n",
    "        dec = self.decoder(torch.cat([ema2, tp1, tp2, tp3, tp4, tp5], dim=1))\n",
    "        #print('dec:',dec.shape)\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        #print('act:',act.shape)\n",
    "        return act\n",
    "\n",
    "\n",
    "x = torch.randn(32, 1, 150)\n",
    "x = x.to('cuda')\n",
    "model = PTPNet(1,3,32).cuda()\n",
    "#print(model)\n",
    "print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd4ed64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 120])\n",
      "633003\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride,\n",
    "                              bias=False)\n",
    "        # self.conv=LSKblock(out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        # self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        # return self.upsample(x)\n",
    "        # return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='nearest', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='bicubic', align_corners=True))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool1_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1 ** k, features * 2 ** k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool2_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2 ** k, features * 4 ** k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool3_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4 ** k, features * 10 ** k, kernel_size=3, padding=0)\n",
    "        #self.MUSEA = MUSEAttention(d_model=15, d_k=15, d_v=15, h=8)\n",
    "        self.EMA = EMA(16)\n",
    "        self.se = SKAttention(channel=16, reduction=8)\n",
    "        \n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=15)\n",
    "        self.tpool3 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=15)\n",
    "        self.tpool4 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=15)\n",
    "        self.tpool5 = TemporalPooling(features * 10 ** k, features * 4 ** k, kernel_size=5)\n",
    "        #self.tpool6 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=10)\n",
    "        #self.tpool7 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "        self.sp1 = StripPooling(\n",
    "        16, (20, 12), nn.BatchNorm2d, {'mode': 'bilinear', 'align_corners': True})\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = Decoder(2 * features * 15 ** k, features * 1 ** k, kernel_size=p ** 3, stride=p ** 3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1 ** k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print('x:',x.shape)\n",
    "        enc1 = self.encoder1(x)\n",
    "        #print('enc1:',enc1.shape)\n",
    "        enc2 = self.encoder2(self.pool1(enc1)+self.pool1_1(enc1))\n",
    "        #print('enc2:',enc2.shape)\n",
    "        enc3 = self.encoder3(self.pool2(enc2)+self.pool2_1(enc2))\n",
    "        #print('enc3:',enc3.shape)\n",
    "        enc4 = self.encoder4(self.pool3(enc3)+self.pool3_1(enc3))\n",
    "        #print('enc4:',enc4.shape)\n",
    "        #mu1 = self.MUSEA(enc4, enc4, enc4)\n",
    "        bx1= to_4d(enc4,20,16).permute(0,3,2,1)\n",
    "        #print('bx1:',bx1.shape)\n",
    "        ema1 = self.EMA(bx1)\n",
    "        #ema2= to_3d(ema1.permute(0,3,2,1))\n",
    "        #print('ema1:',ema1.shape)\n",
    "        #spa1 = to_4d(ema1,8,16).permute(0,3,2,1)\n",
    "        spa2 = self.se(ema1)\n",
    "        #print('spa2:',spa2.shape)\n",
    "        #spa2 = self.se(self.sp1(ema1))\n",
    "        ema2 = to_3d(spa2.permute(0,3,2,1))\n",
    "        #print('ema2:',ema2.shape)\n",
    "        \n",
    "        tp1 = self.tpool1(ema2)\n",
    "        #print('tp1:',tp1.shape)\n",
    "        tp2 = self.tpool2(ema2)\n",
    "        #print('tp2:',tp2.shape)\n",
    "        tp3 = self.tpool3(ema2)\n",
    "        #print('tp3:',tp3.shape)\n",
    "        tp4 = self.tpool4(ema2)\n",
    "        #print('tp4:',tp4.shape)\n",
    "        tp5 = self.tpool5(ema2)\n",
    "        #print('tp5:',tp5.shape)\n",
    "        #tp6 = self.tpool6(ema2)\n",
    "        #print('tp6:',tp6.shape)\n",
    "        #tp7 = self.tpool7(ema2)\n",
    "        #print('tp7:',tp7.shape)\n",
    "        #spa1 =(torch.cat([tp1, tp2, tp3, tp4, tp5], dim=1)\n",
    "        \n",
    "        \n",
    "        dec = self.decoder(torch.cat([ema2, tp1,tp2,tp3,tp4,tp5], dim=1))\n",
    "        #print('dec:',dec.shape)\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        #print('act:',act.shape)\n",
    "        return act\n",
    "\n",
    "\n",
    "x = torch.randn(32, 1, 150)\n",
    "x = x.to('cuda')\n",
    "model = PTPNet(1,3,32).cuda()\n",
    "#print(model)\n",
    "print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48115d6",
   "metadata": {},
   "source": [
    "### UNAMSAIMNILM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed99360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496259\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride,\n",
    "                              bias=False)\n",
    "        # self.conv=LSKblock(out_features)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        # self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        # return self.upsample(x)\n",
    "        # return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='nearest', align_corners=True))\n",
    "        #return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='bicubic', align_corners=True))\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "\n",
    "class PTPNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(PTPNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool1_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1 ** k, features * 2 ** k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool2_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2 ** k, features * 4 ** k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.pool3_1 = nn.AvgPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4 ** k, features * 10 ** k, kernel_size=3, padding=0)\n",
    "        #self.MUSEA = MUSEAttention(d_model=30, d_k=30, d_v=30, h=8)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=15)\n",
    "        self.tpool4 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=30)\n",
    "        self.tpool5 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=25)\n",
    "        self.tpool6 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=15)\n",
    "        self.tpool7 = TemporalPooling(features * 10 ** k, features * 2 ** k, kernel_size=5)\n",
    "\n",
    "        self.decoder = Decoder(2 * features * 12 ** k, features * 1 ** k, kernel_size=p ** 3, stride=p ** 3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1 ** k, out_channels, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "       # print('x:',x.shape)\n",
    "        enc1 = self.encoder1(x)\n",
    "        #print('enc1:',enc1.shape)\n",
    "        enc2 = self.encoder2(self.pool1(enc1)+self.pool1_1(enc1))\n",
    "       # print('enc2:',enc2.shape)\n",
    "        enc3 = self.encoder3(self.pool2(enc2)+self.pool2_1(enc2))\n",
    "       # print('enc3:',enc3.shape)\n",
    "        enc4 = self.encoder4(self.pool3(enc3)+self.pool3_1(enc3))\n",
    "       # print('enc4:',enc4.shape)\n",
    "        #mu1 = self.MUSEA(enc4, enc4, enc4)\n",
    "        \n",
    "        tp1 = self.tpool1(enc4)\n",
    "       # print('tp1:',tp1.shape)\n",
    "        tp2 = self.tpool2(enc4)\n",
    "        #print('tp2:',tp2.shape)\n",
    "        tp3 = self.tpool3(enc4)\n",
    "       # print('tp3:',tp3.shape)\n",
    "        tp4 = self.tpool4(enc4)\n",
    "        #print('tp4:',tp4.shape)\n",
    "        tp5 = self.tpool6(enc4)\n",
    "        #print('tp5:',tp5.shape)\n",
    "        tp6 = self.tpool6(enc4)\n",
    "        #print('tp6:',tp6.shape)\n",
    "        tp7 = self.tpool7(enc4)\n",
    "        #print('tp7:',tp7.shape)\n",
    "        \n",
    "        dec = self.decoder(torch.cat([enc4, tp1, tp2, tp3, tp4, tp5, tp6, tp7], dim=1))\n",
    "        #print('dec:',dec.shape)\n",
    "\n",
    "        act = self.activation(dec)\n",
    "        #print('act:',act.shape)\n",
    "        return act\n",
    "\n",
    "\n",
    "x = torch.randn(32, 1, 270)\n",
    "x = x.to('cuda')\n",
    "model = PTPNet(1,3,32).cuda()\n",
    "#print(model)\n",
    "#print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880701ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_visualization_1121.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'F:/ruanjian/Graphviz/bin/'\n",
    "\n",
    "dot = make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "# 保存为图片到当前文件夹\n",
    "dot.render('model_visualization_1121', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3a0543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"2654pt\" height=\"2536pt\"\n",
       " viewBox=\"0.00 0.00 2654.00 2536.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 2532)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-2532 2650,-2532 2650,4 -4,4\"/>\n",
       "<!-- 1759433427280 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1759433427280</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"1824,-32 1730,-32 1730,0 1824,0 1824,-32\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-6.5\" font-family=\"monospace\" font-size=\"10.00\"> (32, 3, 480)</text>\n",
       "</g>\n",
       "<!-- 1759433524176 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1759433524176</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1845,-88 1709,-88 1709,-68 1845,-68 1845,-88\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-74.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524176&#45;&gt;1759433427280 -->\n",
       "<g id=\"edge189\" class=\"edge\">\n",
       "<title>1759433524176&#45;&gt;1759433427280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777,-67.62C1777,-61.1 1777,-52.05 1777,-43.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1780.5,-43.65 1777,-33.65 1773.5,-43.65 1780.5,-43.65\"/>\n",
       "</g>\n",
       "<!-- 1759433524224 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1759433524224</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1694,-144 1600,-144 1600,-124 1694,-124 1694,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"1647\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524224&#45;&gt;1759433524176 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1759433524224&#45;&gt;1759433524176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1669.64,-123.59C1690.12,-115.09 1720.52,-102.46 1743.75,-92.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1744.89,-96.13 1752.78,-89.06 1742.2,-89.66 1744.89,-96.13\"/>\n",
       "</g>\n",
       "<!-- 1759433524320 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1759433524320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1700,-206 1564,-206 1564,-186 1700,-186 1700,-206\"/>\n",
       "<text text-anchor=\"middle\" x=\"1632\" y=\"-192.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524320&#45;&gt;1759433524224 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1759433524320&#45;&gt;1759433524224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1634.34,-185.62C1636.38,-177.48 1639.4,-165.39 1641.98,-155.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1645.32,-156.13 1644.35,-145.58 1638.53,-154.43 1645.32,-156.13\"/>\n",
       "</g>\n",
       "<!-- 1759433524464 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1759433524464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1620,-268 1532,-268 1532,-248 1620,-248 1620,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\">CatBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524464&#45;&gt;1759433524320 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1759433524464&#45;&gt;1759433524320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1584.75,-247.62C1593.02,-238.77 1605.62,-225.26 1615.74,-214.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1618.2,-216.92 1622.46,-207.22 1613.08,-212.14 1618.2,-216.92\"/>\n",
       "</g>\n",
       "<!-- 1759433524608 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1759433524608</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1485,-808 1397,-808 1397,-788 1485,-788 1485,-808\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-794.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1485.27,-792.45C1582.88,-781.2 1808,-747.99 1808,-681 1808,-681 1808,-681 1808,-381 1808,-301.57 1700.16,-273.74 1631.53,-264.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1632.33,-260.65 1621.96,-262.82 1631.42,-267.59 1632.33,-260.65\"/>\n",
       "</g>\n",
       "<!-- 1759433526912 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>1759433526912</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2025,-752 1901,-752 1901,-732 2025,-732 2025,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1963\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433526912 -->\n",
       "<g id=\"edge76\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433526912</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1485.06,-792.44C1574.67,-783.17 1779.02,-762.03 1889.25,-750.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1889.5,-754.12 1899.09,-749.61 1888.78,-747.16 1889.5,-754.12\"/>\n",
       "</g>\n",
       "<!-- 1759433539696 -->\n",
       "<g id=\"node90\" class=\"node\">\n",
       "<title>1759433539696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2335,-752 2211,-752 2211,-732 2335,-732 2335,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"2273\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433539696 -->\n",
       "<g id=\"edge93\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433539696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1485.44,-794.12C1621.07,-785.31 2030.72,-758.73 2199.3,-747.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2199.51,-751.28 2209.27,-747.14 2199.06,-744.29 2199.51,-751.28\"/>\n",
       "</g>\n",
       "<!-- 1759433540320 -->\n",
       "<g id=\"node106\" class=\"node\">\n",
       "<title>1759433540320</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"280,-752 156,-752 156,-732 280,-732 280,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"218\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433540320 -->\n",
       "<g id=\"edge110\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433540320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1396.75,-795.05C1214.4,-786.99 520.06,-756.34 291.46,-746.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"291.94,-742.76 281.8,-745.82 291.63,-749.75 291.94,-742.76\"/>\n",
       "</g>\n",
       "<!-- 1759433540656 -->\n",
       "<g id=\"node122\" class=\"node\">\n",
       "<title>1759433540656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"830,-752 706,-752 706,-732 830,-732 830,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"768\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433540656 -->\n",
       "<g id=\"edge127\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433540656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1396.58,-793.44C1283.16,-784.33 981.91,-760.16 841.7,-748.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"842.02,-745.43 831.78,-748.12 841.46,-752.41 842.02,-745.43\"/>\n",
       "</g>\n",
       "<!-- 1759433541040 -->\n",
       "<g id=\"node138\" class=\"node\">\n",
       "<title>1759433541040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1176,-752 1052,-752 1052,-732 1176,-732 1176,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1114\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433541040 -->\n",
       "<g id=\"edge144\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433541040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1396.56,-789.66C1341.46,-780.56 1246.95,-764.95 1182.53,-754.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1183.2,-750.88 1172.77,-752.7 1182.06,-757.79 1183.2,-750.88\"/>\n",
       "</g>\n",
       "<!-- 1759433541376 -->\n",
       "<g id=\"node154\" class=\"node\">\n",
       "<title>1759433541376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1420,-752 1296,-752 1296,-732 1420,-732 1420,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1358\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433541376 -->\n",
       "<g id=\"edge161\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433541376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1426.54,-787.59C1414.24,-779.59 1396.32,-767.93 1381.89,-758.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1384.13,-755.82 1373.84,-753.3 1380.31,-761.69 1384.13,-755.82\"/>\n",
       "</g>\n",
       "<!-- 1759433541712 -->\n",
       "<g id=\"node162\" class=\"node\">\n",
       "<title>1759433541712</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1587,-752 1463,-752 1463,-732 1587,-732 1587,-752\"/>\n",
       "<text text-anchor=\"middle\" x=\"1525\" y=\"-738.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524608&#45;&gt;1759433541712 -->\n",
       "<g id=\"edge174\" class=\"edge\">\n",
       "<title>1759433524608&#45;&gt;1759433541712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1455.63,-787.59C1468.08,-779.59 1486.22,-767.93 1500.82,-758.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1502.46,-761.65 1508.98,-753.3 1498.68,-755.76 1502.46,-761.65\"/>\n",
       "</g>\n",
       "<!-- 1759433525040 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1759433525040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1521,-864 1361,-864 1361,-844 1521,-844 1521,-864\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-850.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525040&#45;&gt;1759433524608 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1759433525040&#45;&gt;1759433524608</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441,-843.59C1441,-837.01 1441,-827.96 1441,-819.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.5,-819.81 1441,-809.81 1437.5,-819.81 1444.5,-819.81\"/>\n",
       "</g>\n",
       "<!-- 1759433525184 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1759433525184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1356,-920 1262,-920 1262,-900 1356,-900 1356,-920\"/>\n",
       "<text text-anchor=\"middle\" x=\"1309\" y=\"-906.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525184&#45;&gt;1759433525040 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1759433525184&#45;&gt;1759433525040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1331.99,-899.59C1352.78,-891.09 1383.65,-878.46 1407.24,-868.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1408.5,-872.08 1416.43,-865.05 1405.85,-865.6 1408.5,-872.08\"/>\n",
       "</g>\n",
       "<!-- 1759433525232 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1759433525232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1361,-982 1225,-982 1225,-962 1361,-962 1361,-982\"/>\n",
       "<text text-anchor=\"middle\" x=\"1293\" y=\"-968.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525232&#45;&gt;1759433525184 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1759433525232&#45;&gt;1759433525184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1295.5,-961.62C1297.67,-953.48 1300.9,-941.39 1303.65,-931.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1306.99,-932.14 1306.18,-921.58 1300.22,-930.34 1306.99,-932.14\"/>\n",
       "</g>\n",
       "<!-- 1759433525328 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1759433525328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1281,-1044 1193,-1044 1193,-1024 1281,-1024 1281,-1044\"/>\n",
       "<text text-anchor=\"middle\" x=\"1237\" y=\"-1030.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525328&#45;&gt;1759433525232 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1759433525328&#45;&gt;1759433525232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1245.75,-1023.62C1254.02,-1014.77 1266.62,-1001.26 1276.74,-990.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1279.2,-992.92 1283.46,-983.22 1274.08,-988.14 1279.2,-992.92\"/>\n",
       "</g>\n",
       "<!-- 1759433525424 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>1759433525424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1293,-1106 1181,-1106 1181,-1086 1293,-1086 1293,-1106\"/>\n",
       "<text text-anchor=\"middle\" x=\"1237\" y=\"-1092.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525424&#45;&gt;1759433525328 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1759433525424&#45;&gt;1759433525328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1237,-1085.62C1237,-1077.56 1237,-1065.65 1237,-1055.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1240.5,-1055.63 1237,-1045.63 1233.5,-1055.63 1240.5,-1055.63\"/>\n",
       "</g>\n",
       "<!-- 1759433525568 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>1759433525568</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1344,-1168 1154,-1168 1154,-1148 1344,-1148 1344,-1168\"/>\n",
       "<text text-anchor=\"middle\" x=\"1249\" y=\"-1154.5\" font-family=\"monospace\" font-size=\"10.00\">MaxPool2DWithIndicesBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525568&#45;&gt;1759433525424 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1759433525568&#45;&gt;1759433525424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1247.12,-1147.62C1245.5,-1139.48 1243.08,-1127.39 1241.01,-1117.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1244.51,-1116.72 1239.12,-1107.6 1237.65,-1118.09 1244.51,-1116.72\"/>\n",
       "</g>\n",
       "<!-- 1759433525664 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>1759433525664</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1302,-1224 1178,-1224 1178,-1204 1302,-1204 1302,-1224\"/>\n",
       "<text text-anchor=\"middle\" x=\"1240\" y=\"-1210.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525664&#45;&gt;1759433525568 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>1759433525664&#45;&gt;1759433525568</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1241.57,-1203.59C1242.68,-1196.93 1244.21,-1187.75 1245.59,-1179.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1249.01,-1180.23 1247.2,-1169.79 1242.11,-1179.08 1249.01,-1180.23\"/>\n",
       "</g>\n",
       "<!-- 1759433525712 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>1759433525712</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1205,-1280 1117,-1280 1117,-1260 1205,-1260 1205,-1280\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1266.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525712&#45;&gt;1759433525664 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>1759433525712&#45;&gt;1759433525664</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1174.76,-1259.59C1186.36,-1251.67 1203.2,-1240.16 1216.86,-1230.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1218.6,-1233.87 1224.88,-1225.33 1214.65,-1228.09 1218.6,-1233.87\"/>\n",
       "</g>\n",
       "<!-- 1759433525904 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>1759433525904</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1152,-1224 1028,-1224 1028,-1204 1152,-1204 1152,-1224\"/>\n",
       "<text text-anchor=\"middle\" x=\"1090\" y=\"-1210.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525712&#45;&gt;1759433525904 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>1759433525712&#45;&gt;1759433525904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1148.63,-1259.59C1138.35,-1251.77 1123.48,-1240.46 1111.29,-1231.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1113.51,-1228.48 1103.43,-1225.21 1109.27,-1234.05 1113.51,-1228.48\"/>\n",
       "</g>\n",
       "<!-- 1759433525808 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>1759433525808</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1241,-1336 1081,-1336 1081,-1316 1241,-1316 1241,-1336\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1322.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525808&#45;&gt;1759433525712 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1759433525808&#45;&gt;1759433525712</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1161,-1315.59C1161,-1309.01 1161,-1299.96 1161,-1291.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.5,-1291.81 1161,-1281.81 1157.5,-1291.81 1164.5,-1291.81\"/>\n",
       "</g>\n",
       "<!-- 1759433525952 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>1759433525952</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1077,-1392 983,-1392 983,-1372 1077,-1372 1077,-1392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1030\" y=\"-1378.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525952&#45;&gt;1759433525808 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>1759433525952&#45;&gt;1759433525808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1052.82,-1371.59C1073.45,-1363.09 1104.08,-1350.46 1127.49,-1340.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1128.69,-1344.1 1136.6,-1337.06 1126.02,-1337.63 1128.69,-1344.1\"/>\n",
       "</g>\n",
       "<!-- 1759433526048 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>1759433526048</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1081,-1454 945,-1454 945,-1434 1081,-1434 1081,-1454\"/>\n",
       "<text text-anchor=\"middle\" x=\"1013\" y=\"-1440.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526048&#45;&gt;1759433525952 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>1759433526048&#45;&gt;1759433525952</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1015.66,-1433.62C1017.97,-1425.48 1021.39,-1413.39 1024.31,-1403.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1027.65,-1404.14 1027.01,-1393.57 1020.91,-1402.24 1027.65,-1404.14\"/>\n",
       "</g>\n",
       "<!-- 1759433526144 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>1759433526144</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1001,-1516 913,-1516 913,-1496 1001,-1496 1001,-1516\"/>\n",
       "<text text-anchor=\"middle\" x=\"957\" y=\"-1502.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526144&#45;&gt;1759433526048 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>1759433526144&#45;&gt;1759433526048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M965.75,-1495.62C974.02,-1486.77 986.62,-1473.26 996.74,-1462.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"999.2,-1464.92 1003.46,-1455.22 994.08,-1460.14 999.2,-1464.92\"/>\n",
       "</g>\n",
       "<!-- 1759433526240 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>1759433526240</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1013,-1578 901,-1578 901,-1558 1013,-1558 1013,-1578\"/>\n",
       "<text text-anchor=\"middle\" x=\"957\" y=\"-1564.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526240&#45;&gt;1759433526144 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>1759433526240&#45;&gt;1759433526144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M957,-1557.62C957,-1549.56 957,-1537.65 957,-1527.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"960.5,-1527.63 957,-1517.63 953.5,-1527.63 960.5,-1527.63\"/>\n",
       "</g>\n",
       "<!-- 1759433526432 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>1759433526432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1064,-1640 874,-1640 874,-1620 1064,-1620 1064,-1640\"/>\n",
       "<text text-anchor=\"middle\" x=\"969\" y=\"-1626.5\" font-family=\"monospace\" font-size=\"10.00\">MaxPool2DWithIndicesBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526432&#45;&gt;1759433526240 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>1759433526432&#45;&gt;1759433526240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M967.12,-1619.62C965.5,-1611.48 963.08,-1599.39 961.01,-1589.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"964.51,-1588.72 959.12,-1579.6 957.65,-1590.09 964.51,-1588.72\"/>\n",
       "</g>\n",
       "<!-- 1759433526528 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>1759433526528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1022,-1696 898,-1696 898,-1676 1022,-1676 1022,-1696\"/>\n",
       "<text text-anchor=\"middle\" x=\"960\" y=\"-1682.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526528&#45;&gt;1759433526432 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>1759433526528&#45;&gt;1759433526432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M961.57,-1675.59C962.68,-1668.93 964.21,-1659.75 965.59,-1651.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"969.01,-1652.23 967.2,-1641.79 962.11,-1651.08 969.01,-1652.23\"/>\n",
       "</g>\n",
       "<!-- 1759433526624 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>1759433526624</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"925,-1752 837,-1752 837,-1732 925,-1732 925,-1752\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1738.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526624&#45;&gt;1759433526528 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>1759433526624&#45;&gt;1759433526528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M894.76,-1731.59C906.36,-1723.67 923.2,-1712.16 936.86,-1702.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"938.6,-1705.87 944.88,-1697.33 934.65,-1700.09 938.6,-1705.87\"/>\n",
       "</g>\n",
       "<!-- 1759433526816 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>1759433526816</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"872,-1696 748,-1696 748,-1676 872,-1676 872,-1696\"/>\n",
       "<text text-anchor=\"middle\" x=\"810\" y=\"-1682.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526624&#45;&gt;1759433526816 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>1759433526624&#45;&gt;1759433526816</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M868.63,-1731.59C858.35,-1723.77 843.48,-1712.46 831.29,-1703.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"833.51,-1700.48 823.43,-1697.21 829.27,-1706.05 833.51,-1700.48\"/>\n",
       "</g>\n",
       "<!-- 1759433526720 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>1759433526720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"961,-1808 801,-1808 801,-1788 961,-1788 961,-1808\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1794.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526720&#45;&gt;1759433526624 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>1759433526720&#45;&gt;1759433526624</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881,-1787.59C881,-1781.01 881,-1771.96 881,-1763.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.5,-1763.81 881,-1753.81 877.5,-1763.81 884.5,-1763.81\"/>\n",
       "</g>\n",
       "<!-- 1759433526864 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>1759433526864</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"797,-1864 703,-1864 703,-1844 797,-1844 797,-1864\"/>\n",
       "<text text-anchor=\"middle\" x=\"750\" y=\"-1850.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526864&#45;&gt;1759433526720 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>1759433526864&#45;&gt;1759433526720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M772.82,-1843.59C793.45,-1835.09 824.08,-1822.46 847.49,-1812.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"848.69,-1816.1 856.6,-1809.06 846.02,-1809.63 848.69,-1816.1\"/>\n",
       "</g>\n",
       "<!-- 1759433526960 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>1759433526960</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"801,-1926 665,-1926 665,-1906 801,-1906 801,-1926\"/>\n",
       "<text text-anchor=\"middle\" x=\"733\" y=\"-1912.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526960&#45;&gt;1759433526864 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>1759433526960&#45;&gt;1759433526864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M735.66,-1905.62C737.97,-1897.48 741.39,-1885.39 744.31,-1875.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"747.65,-1876.14 747.01,-1865.57 740.91,-1874.24 747.65,-1876.14\"/>\n",
       "</g>\n",
       "<!-- 1759433527056 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>1759433527056</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"721,-1988 633,-1988 633,-1968 721,-1968 721,-1988\"/>\n",
       "<text text-anchor=\"middle\" x=\"677\" y=\"-1974.5\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527056&#45;&gt;1759433526960 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>1759433527056&#45;&gt;1759433526960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M685.75,-1967.62C694.02,-1958.77 706.62,-1945.26 716.74,-1934.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"719.2,-1936.92 723.46,-1927.22 714.08,-1932.14 719.2,-1936.92\"/>\n",
       "</g>\n",
       "<!-- 1759433527104 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>1759433527104</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"733,-2050 621,-2050 621,-2030 733,-2030 733,-2050\"/>\n",
       "<text text-anchor=\"middle\" x=\"677\" y=\"-2036.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433527104&#45;&gt;1759433527056 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>1759433527104&#45;&gt;1759433527056</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M677,-2029.62C677,-2021.56 677,-2009.65 677,-1999.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"680.5,-1999.63 677,-1989.63 673.5,-1999.63 680.5,-1999.63\"/>\n",
       "</g>\n",
       "<!-- 1759433526336 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>1759433526336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"784,-2112 594,-2112 594,-2092 784,-2092 784,-2112\"/>\n",
       "<text text-anchor=\"middle\" x=\"689\" y=\"-2098.5\" font-family=\"monospace\" font-size=\"10.00\">MaxPool2DWithIndicesBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526336&#45;&gt;1759433527104 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>1759433526336&#45;&gt;1759433527104</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M687.12,-2091.62C685.5,-2083.48 683.08,-2071.39 681.01,-2061.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"684.51,-2060.72 679.12,-2051.6 677.65,-2062.09 684.51,-2060.72\"/>\n",
       "</g>\n",
       "<!-- 1759433526768 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>1759433526768</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"742,-2168 618,-2168 618,-2148 742,-2148 742,-2168\"/>\n",
       "<text text-anchor=\"middle\" x=\"680\" y=\"-2154.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526768&#45;&gt;1759433526336 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>1759433526768&#45;&gt;1759433526336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M681.57,-2147.59C682.68,-2140.93 684.21,-2131.75 685.59,-2123.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"689.01,-2124.23 687.2,-2113.79 682.11,-2123.08 689.01,-2124.23\"/>\n",
       "</g>\n",
       "<!-- 1759433527200 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>1759433527200</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"645,-2224 557,-2224 557,-2204 645,-2204 645,-2224\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2210.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527200&#45;&gt;1759433526768 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>1759433527200&#45;&gt;1759433526768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M614.76,-2203.59C626.36,-2195.67 643.2,-2184.16 656.86,-2174.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"658.6,-2177.87 664.88,-2169.33 654.65,-2172.09 658.6,-2177.87\"/>\n",
       "</g>\n",
       "<!-- 1759433539840 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>1759433539840</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"592,-2168 468,-2168 468,-2148 592,-2148 592,-2168\"/>\n",
       "<text text-anchor=\"middle\" x=\"530\" y=\"-2154.5\" font-family=\"monospace\" font-size=\"10.00\">UnsqueezeBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527200&#45;&gt;1759433539840 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>1759433527200&#45;&gt;1759433539840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M588.63,-2203.59C578.35,-2195.77 563.48,-2184.46 551.29,-2175.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"553.51,-2172.48 543.43,-2169.21 549.27,-2178.05 553.51,-2172.48\"/>\n",
       "</g>\n",
       "<!-- 1759433539792 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>1759433539792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"681,-2280 521,-2280 521,-2260 681,-2260 681,-2280\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2266.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539792&#45;&gt;1759433527200 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>1759433539792&#45;&gt;1759433527200</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-2259.59C601,-2253.01 601,-2243.96 601,-2235.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-2235.81 601,-2225.81 597.5,-2235.81 604.5,-2235.81\"/>\n",
       "</g>\n",
       "<!-- 1759433539888 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>1759433539888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"517,-2336 423,-2336 423,-2316 517,-2316 517,-2336\"/>\n",
       "<text text-anchor=\"middle\" x=\"470\" y=\"-2322.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539888&#45;&gt;1759433539792 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>1759433539888&#45;&gt;1759433539792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M492.82,-2315.59C513.45,-2307.09 544.08,-2294.46 567.49,-2284.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"568.69,-2288.1 576.6,-2281.06 566.02,-2281.63 568.69,-2288.1\"/>\n",
       "</g>\n",
       "<!-- 1759433539984 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>1759433539984</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"521,-2398 385,-2398 385,-2378 521,-2378 521,-2398\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2384.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539984&#45;&gt;1759433539888 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>1759433539984&#45;&gt;1759433539888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M455.66,-2377.62C457.97,-2369.48 461.39,-2357.39 464.31,-2347.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.65,-2348.14 467.01,-2337.57 460.91,-2346.24 467.65,-2348.14\"/>\n",
       "</g>\n",
       "<!-- 1759433464800 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>1759433464800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"503,-2460 403,-2460 403,-2440 503,-2440 503,-2460\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2446.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464800&#45;&gt;1759433539984 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>1759433464800&#45;&gt;1759433539984</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-2439.62C453,-2431.56 453,-2419.65 453,-2409.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-2409.63 453,-2399.63 449.5,-2409.63 456.5,-2409.63\"/>\n",
       "</g>\n",
       "<!-- 1759421711920 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>1759421711920</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"521,-2528 385,-2528 385,-2496 521,-2496 521,-2528\"/>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2514.5\" font-family=\"monospace\" font-size=\"10.00\">encoder1.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"453\" y=\"-2502.5\" font-family=\"monospace\" font-size=\"10.00\"> (32, 1, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421711920&#45;&gt;1759433464800 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>1759421711920&#45;&gt;1759433464800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M453,-2495.55C453,-2488.34 453,-2479.66 453,-2471.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"456.5,-2471.92 453,-2461.92 449.5,-2471.92 456.5,-2471.92\"/>\n",
       "</g>\n",
       "<!-- 1759433464464 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>1759433464464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"651,-2336 551,-2336 551,-2316 651,-2316 651,-2336\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2322.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464464&#45;&gt;1759433539792 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>1759433464464&#45;&gt;1759433539792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-2315.59C601,-2309.01 601,-2299.96 601,-2291.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-2291.81 601,-2281.81 597.5,-2291.81 604.5,-2291.81\"/>\n",
       "</g>\n",
       "<!-- 1759421712000 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>1759421712000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"663,-2404 539,-2404 539,-2372 663,-2372 663,-2404\"/>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2390.5\" font-family=\"monospace\" font-size=\"10.00\">encoder1.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"601\" y=\"-2378.5\" font-family=\"monospace\" font-size=\"10.00\"> (32)</text>\n",
       "</g>\n",
       "<!-- 1759421712000&#45;&gt;1759433464464 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>1759421712000&#45;&gt;1759433464464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M601,-2371.55C601,-2364.34 601,-2355.66 601,-2347.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"604.5,-2347.92 601,-2337.92 597.5,-2347.92 604.5,-2347.92\"/>\n",
       "</g>\n",
       "<!-- 1759433464032 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>1759433464032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"778,-2336 678,-2336 678,-2316 778,-2316 778,-2336\"/>\n",
       "<text text-anchor=\"middle\" x=\"728\" y=\"-2322.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464032&#45;&gt;1759433539792 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>1759433464032&#45;&gt;1759433539792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M705.88,-2315.59C685.97,-2307.13 656.45,-2294.58 633.79,-2284.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"635.26,-2281.76 624.69,-2281.07 632.52,-2288.21 635.26,-2281.76\"/>\n",
       "</g>\n",
       "<!-- 1759421712080 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>1759421712080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"793,-2404 681,-2404 681,-2372 793,-2372 793,-2404\"/>\n",
       "<text text-anchor=\"middle\" x=\"737\" y=\"-2390.5\" font-family=\"monospace\" font-size=\"10.00\">encoder1.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"737\" y=\"-2378.5\" font-family=\"monospace\" font-size=\"10.00\"> (32)</text>\n",
       "</g>\n",
       "<!-- 1759421712080&#45;&gt;1759433464032 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>1759421712080&#45;&gt;1759433464032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M734.68,-2371.55C733.59,-2364.26 732.27,-2355.45 731.09,-2347.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"734.58,-2347.27 729.64,-2337.9 727.66,-2348.31 734.58,-2347.27\"/>\n",
       "</g>\n",
       "<!-- 1759433527152 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>1759433527152</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"592,-2050 480,-2050 480,-2030 592,-2030 592,-2050\"/>\n",
       "<text text-anchor=\"middle\" x=\"536\" y=\"-2036.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433527152&#45;&gt;1759433527056 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>1759433527152&#45;&gt;1759433527056</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M558.03,-2029.62C581.31,-2019.72 618.26,-2003.99 644.81,-1992.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"645.9,-1996.04 653.73,-1988.9 643.16,-1989.6 645.9,-1996.04\"/>\n",
       "</g>\n",
       "<!-- 1759433524800 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>1759433524800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"576,-2112 452,-2112 452,-2092 576,-2092 576,-2112\"/>\n",
       "<text text-anchor=\"middle\" x=\"514\" y=\"-2098.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524800&#45;&gt;1759433527152 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>1759433524800&#45;&gt;1759433527152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M517.44,-2091.62C520.46,-2083.39 524.95,-2071.13 528.76,-2060.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"531.98,-2062.13 532.14,-2051.53 525.41,-2059.72 531.98,-2062.13\"/>\n",
       "</g>\n",
       "<!-- 1759433539840&#45;&gt;1759433524800 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>1759433539840&#45;&gt;1759433524800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M527.21,-2147.59C525.22,-2140.86 522.45,-2131.53 519.97,-2123.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"523.38,-2122.34 517.19,-2113.75 516.67,-2124.33 523.38,-2122.34\"/>\n",
       "</g>\n",
       "<!-- 1759433463792 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>1759433463792</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"854,-1988 754,-1988 754,-1968 854,-1968 854,-1988\"/>\n",
       "<text text-anchor=\"middle\" x=\"804\" y=\"-1974.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463792&#45;&gt;1759433526960 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>1759433463792&#45;&gt;1759433526960</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M792.91,-1967.62C782.11,-1958.5 765.48,-1944.45 752.47,-1933.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"754.84,-1930.87 744.94,-1927.09 750.32,-1936.22 754.84,-1930.87\"/>\n",
       "</g>\n",
       "<!-- 1759421712480 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>1759421712480</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"887,-2056 751,-2056 751,-2024 887,-2024 887,-2056\"/>\n",
       "<text text-anchor=\"middle\" x=\"819\" y=\"-2042.5\" font-family=\"monospace\" font-size=\"10.00\">encoder2.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"819\" y=\"-2030.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 32, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421712480&#45;&gt;1759433463792 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>1759421712480&#45;&gt;1759433463792</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M815.14,-2023.55C813.29,-2016.17 811.06,-2007.24 809.08,-1999.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"812.54,-1998.73 806.72,-1989.88 805.75,-2000.43 812.54,-1998.73\"/>\n",
       "</g>\n",
       "<!-- 1759433463504 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>1759433463504</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"931,-1864 831,-1864 831,-1844 931,-1844 931,-1864\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1850.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463504&#45;&gt;1759433526720 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>1759433463504&#45;&gt;1759433526720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881,-1843.59C881,-1837.01 881,-1827.96 881,-1819.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.5,-1819.81 881,-1809.81 877.5,-1819.81 884.5,-1819.81\"/>\n",
       "</g>\n",
       "<!-- 1759421712560 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>1759421712560</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"943,-1932 819,-1932 819,-1900 943,-1900 943,-1932\"/>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1918.5\" font-family=\"monospace\" font-size=\"10.00\">encoder2.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"881\" y=\"-1906.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421712560&#45;&gt;1759433463504 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>1759421712560&#45;&gt;1759433463504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M881,-1899.55C881,-1892.34 881,-1883.66 881,-1875.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"884.5,-1875.92 881,-1865.92 877.5,-1875.92 884.5,-1875.92\"/>\n",
       "</g>\n",
       "<!-- 1759433463072 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>1759433463072</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1058,-1864 958,-1864 958,-1844 1058,-1844 1058,-1864\"/>\n",
       "<text text-anchor=\"middle\" x=\"1008\" y=\"-1850.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463072&#45;&gt;1759433526720 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>1759433463072&#45;&gt;1759433526720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M985.88,-1843.59C965.97,-1835.13 936.45,-1822.58 913.79,-1812.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"915.26,-1809.76 904.69,-1809.07 912.52,-1816.21 915.26,-1809.76\"/>\n",
       "</g>\n",
       "<!-- 1759421712640 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>1759421712640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1073,-1932 961,-1932 961,-1900 1073,-1900 1073,-1932\"/>\n",
       "<text text-anchor=\"middle\" x=\"1017\" y=\"-1918.5\" font-family=\"monospace\" font-size=\"10.00\">encoder2.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1017\" y=\"-1906.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421712640&#45;&gt;1759433463072 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>1759421712640&#45;&gt;1759433463072</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1014.68,-1899.55C1013.59,-1892.26 1012.27,-1883.45 1011.09,-1875.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1014.58,-1875.27 1009.64,-1865.9 1007.66,-1876.31 1014.58,-1875.27\"/>\n",
       "</g>\n",
       "<!-- 1759433526192 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>1759433526192</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"872,-1578 760,-1578 760,-1558 872,-1558 872,-1578\"/>\n",
       "<text text-anchor=\"middle\" x=\"816\" y=\"-1564.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526192&#45;&gt;1759433526144 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>1759433526192&#45;&gt;1759433526144</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M838.03,-1557.62C861.31,-1547.72 898.26,-1531.99 924.81,-1520.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"925.9,-1524.04 933.73,-1516.9 923.16,-1517.6 925.9,-1524.04\"/>\n",
       "</g>\n",
       "<!-- 1759433526576 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>1759433526576</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"856,-1640 732,-1640 732,-1620 856,-1620 856,-1640\"/>\n",
       "<text text-anchor=\"middle\" x=\"794\" y=\"-1626.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526576&#45;&gt;1759433526192 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>1759433526576&#45;&gt;1759433526192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M797.44,-1619.62C800.46,-1611.39 804.95,-1599.13 808.76,-1588.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"811.98,-1590.13 812.14,-1579.53 805.41,-1587.72 811.98,-1590.13\"/>\n",
       "</g>\n",
       "<!-- 1759433526816&#45;&gt;1759433526576 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>1759433526816&#45;&gt;1759433526576</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M807.21,-1675.59C805.22,-1668.86 802.45,-1659.53 799.97,-1651.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"803.38,-1650.34 797.19,-1641.75 796.67,-1652.33 803.38,-1650.34\"/>\n",
       "</g>\n",
       "<!-- 1759433462832 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>1759433462832</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1134,-1516 1034,-1516 1034,-1496 1134,-1496 1134,-1516\"/>\n",
       "<text text-anchor=\"middle\" x=\"1084\" y=\"-1502.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462832&#45;&gt;1759433526048 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>1759433462832&#45;&gt;1759433526048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1072.91,-1495.62C1062.11,-1486.5 1045.48,-1472.45 1032.47,-1461.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1034.84,-1458.87 1024.94,-1455.09 1030.32,-1464.22 1034.84,-1458.87\"/>\n",
       "</g>\n",
       "<!-- 1759421712960 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>1759421712960</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1167,-1584 1031,-1584 1031,-1552 1167,-1552 1167,-1584\"/>\n",
       "<text text-anchor=\"middle\" x=\"1099\" y=\"-1570.5\" font-family=\"monospace\" font-size=\"10.00\">encoder3.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1099\" y=\"-1558.5\" font-family=\"monospace\" font-size=\"10.00\"> (128, 64, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421712960&#45;&gt;1759433462832 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>1759421712960&#45;&gt;1759433462832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1095.14,-1551.55C1093.29,-1544.17 1091.06,-1535.24 1089.08,-1527.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1092.54,-1526.73 1086.72,-1517.88 1085.75,-1528.43 1092.54,-1526.73\"/>\n",
       "</g>\n",
       "<!-- 1759433462544 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>1759433462544</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1211,-1392 1111,-1392 1111,-1372 1211,-1372 1211,-1392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1378.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462544&#45;&gt;1759433525808 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>1759433462544&#45;&gt;1759433525808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1161,-1371.59C1161,-1365.01 1161,-1355.96 1161,-1347.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.5,-1347.81 1161,-1337.81 1157.5,-1347.81 1164.5,-1347.81\"/>\n",
       "</g>\n",
       "<!-- 1759421713040 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>1759421713040</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1223,-1460 1099,-1460 1099,-1428 1223,-1428 1223,-1460\"/>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1446.5\" font-family=\"monospace\" font-size=\"10.00\">encoder3.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1161\" y=\"-1434.5\" font-family=\"monospace\" font-size=\"10.00\"> (128)</text>\n",
       "</g>\n",
       "<!-- 1759421713040&#45;&gt;1759433462544 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>1759421713040&#45;&gt;1759433462544</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1161,-1427.55C1161,-1420.34 1161,-1411.66 1161,-1403.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1164.5,-1403.92 1161,-1393.92 1157.5,-1403.92 1164.5,-1403.92\"/>\n",
       "</g>\n",
       "<!-- 1759433462160 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>1759433462160</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1338,-1392 1238,-1392 1238,-1372 1338,-1372 1338,-1392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1288\" y=\"-1378.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462160&#45;&gt;1759433525808 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>1759433462160&#45;&gt;1759433525808</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1265.88,-1371.59C1245.97,-1363.13 1216.45,-1350.58 1193.79,-1340.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1195.26,-1337.76 1184.69,-1337.07 1192.52,-1344.21 1195.26,-1337.76\"/>\n",
       "</g>\n",
       "<!-- 1759421713120 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>1759421713120</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1353,-1460 1241,-1460 1241,-1428 1353,-1428 1353,-1460\"/>\n",
       "<text text-anchor=\"middle\" x=\"1297\" y=\"-1446.5\" font-family=\"monospace\" font-size=\"10.00\">encoder3.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1297\" y=\"-1434.5\" font-family=\"monospace\" font-size=\"10.00\"> (128)</text>\n",
       "</g>\n",
       "<!-- 1759421713120&#45;&gt;1759433462160 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>1759421713120&#45;&gt;1759433462160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1294.68,-1427.55C1293.59,-1420.26 1292.27,-1411.45 1291.09,-1403.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1294.58,-1403.27 1289.64,-1393.9 1287.66,-1404.31 1294.58,-1403.27\"/>\n",
       "</g>\n",
       "<!-- 1759433525376 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>1759433525376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1152,-1106 1040,-1106 1040,-1086 1152,-1086 1152,-1106\"/>\n",
       "<text text-anchor=\"middle\" x=\"1096\" y=\"-1092.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525376&#45;&gt;1759433525328 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>1759433525376&#45;&gt;1759433525328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1118.03,-1085.62C1141.31,-1075.72 1178.26,-1059.99 1204.81,-1048.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1205.9,-1052.04 1213.73,-1044.9 1203.16,-1045.6 1205.9,-1052.04\"/>\n",
       "</g>\n",
       "<!-- 1759433524896 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>1759433524896</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1136,-1168 1012,-1168 1012,-1148 1136,-1148 1136,-1168\"/>\n",
       "<text text-anchor=\"middle\" x=\"1074\" y=\"-1154.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524896&#45;&gt;1759433525376 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>1759433524896&#45;&gt;1759433525376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1077.44,-1147.62C1080.46,-1139.39 1084.95,-1127.13 1088.76,-1116.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1091.98,-1118.13 1092.14,-1107.53 1085.41,-1115.72 1091.98,-1118.13\"/>\n",
       "</g>\n",
       "<!-- 1759433525904&#45;&gt;1759433524896 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>1759433525904&#45;&gt;1759433524896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1087.21,-1203.59C1085.22,-1196.86 1082.45,-1187.53 1079.97,-1179.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1083.38,-1178.34 1077.19,-1169.75 1076.67,-1180.33 1083.38,-1178.34\"/>\n",
       "</g>\n",
       "<!-- 1759433461872 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>1759433461872</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1414,-1044 1314,-1044 1314,-1024 1414,-1024 1414,-1044\"/>\n",
       "<text text-anchor=\"middle\" x=\"1364\" y=\"-1030.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433461872&#45;&gt;1759433525232 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>1759433461872&#45;&gt;1759433525232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1352.91,-1023.62C1342.11,-1014.5 1325.48,-1000.45 1312.47,-989.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1314.84,-986.87 1304.94,-983.09 1310.32,-992.22 1314.84,-986.87\"/>\n",
       "</g>\n",
       "<!-- 1759421713440 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>1759421713440</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1447,-1112 1311,-1112 1311,-1080 1447,-1080 1447,-1112\"/>\n",
       "<text text-anchor=\"middle\" x=\"1379\" y=\"-1098.5\" font-family=\"monospace\" font-size=\"10.00\">encoder4.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1379\" y=\"-1086.5\" font-family=\"monospace\" font-size=\"10.00\"> (320, 128, 3)</text>\n",
       "</g>\n",
       "<!-- 1759421713440&#45;&gt;1759433461872 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>1759421713440&#45;&gt;1759433461872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1375.14,-1079.55C1373.29,-1072.17 1371.06,-1063.24 1369.08,-1055.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1372.54,-1054.73 1366.72,-1045.88 1365.75,-1056.43 1372.54,-1054.73\"/>\n",
       "</g>\n",
       "<!-- 1759425755984 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>1759425755984</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1491,-920 1391,-920 1391,-900 1491,-900 1491,-920\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-906.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759425755984&#45;&gt;1759433525040 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>1759425755984&#45;&gt;1759433525040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441,-899.59C1441,-893.01 1441,-883.96 1441,-875.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.5,-875.81 1441,-865.81 1437.5,-875.81 1444.5,-875.81\"/>\n",
       "</g>\n",
       "<!-- 1759421713520 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>1759421713520</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1503,-988 1379,-988 1379,-956 1503,-956 1503,-988\"/>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-974.5\" font-family=\"monospace\" font-size=\"10.00\">encoder4.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1441\" y=\"-962.5\" font-family=\"monospace\" font-size=\"10.00\"> (320)</text>\n",
       "</g>\n",
       "<!-- 1759421713520&#45;&gt;1759425755984 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>1759421713520&#45;&gt;1759425755984</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441,-955.55C1441,-948.34 1441,-939.66 1441,-931.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1444.5,-931.92 1441,-921.92 1437.5,-931.92 1444.5,-931.92\"/>\n",
       "</g>\n",
       "<!-- 1759425753440 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>1759425753440</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1618,-920 1518,-920 1518,-900 1618,-900 1618,-920\"/>\n",
       "<text text-anchor=\"middle\" x=\"1568\" y=\"-906.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759425753440&#45;&gt;1759433525040 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>1759425753440&#45;&gt;1759433525040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1545.88,-899.59C1525.97,-891.13 1496.45,-878.58 1473.79,-868.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1475.26,-865.76 1464.69,-865.07 1472.52,-872.21 1475.26,-865.76\"/>\n",
       "</g>\n",
       "<!-- 1759421713600 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>1759421713600</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1633,-988 1521,-988 1521,-956 1633,-956 1633,-988\"/>\n",
       "<text text-anchor=\"middle\" x=\"1577\" y=\"-974.5\" font-family=\"monospace\" font-size=\"10.00\">encoder4.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1577\" y=\"-962.5\" font-family=\"monospace\" font-size=\"10.00\"> (320)</text>\n",
       "</g>\n",
       "<!-- 1759421713600&#45;&gt;1759425753440 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>1759421713600&#45;&gt;1759425753440</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1574.68,-955.55C1573.59,-948.26 1572.27,-939.45 1571.09,-931.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1574.58,-931.27 1569.64,-921.9 1567.66,-932.31 1574.58,-931.27\"/>\n",
       "</g>\n",
       "<!-- 1759433524512 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>1759433524512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1924,-330 1836,-330 1836,-310 1924,-310 1924,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1880\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524512&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>1759433524512&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1845.13,-309.55C1837.53,-307.61 1829.52,-305.66 1822,-304 1756.9,-289.65 1681.4,-276.32 1631.39,-267.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1632.12,-264.53 1621.68,-266.34 1630.97,-271.44 1632.12,-264.53\"/>\n",
       "</g>\n",
       "<!-- 1759433525136 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>1759433525136</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2025,-392 1859,-392 1859,-372 2025,-372 2025,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1942\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525136&#45;&gt;1759433524512 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>1759433525136&#45;&gt;1759433524512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1932.31,-371.62C1923.07,-362.68 1908.92,-348.99 1897.67,-338.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1900.12,-335.6 1890.5,-331.17 1895.26,-340.63 1900.12,-335.6\"/>\n",
       "</g>\n",
       "<!-- 1759433525760 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>1759433525760</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2047,-448 1887,-448 1887,-428 2047,-428 2047,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1967\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525760&#45;&gt;1759433525136 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>1759433525760&#45;&gt;1759433525136</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1962.65,-427.59C1959.45,-420.7 1955.01,-411.1 1951.06,-402.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1954.32,-401.28 1946.94,-393.67 1947.97,-404.22 1954.32,-401.28\"/>\n",
       "</g>\n",
       "<!-- 1759433524944 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>1759433524944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2060,-504 1966,-504 1966,-484 2060,-484 2060,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2013\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524944&#45;&gt;1759433525760 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>1759433524944&#45;&gt;1759433525760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2004.99,-483.59C1998.72,-476.24 1989.83,-465.8 1982.22,-456.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1985.05,-454.79 1975.9,-449.45 1979.72,-459.33 1985.05,-454.79\"/>\n",
       "</g>\n",
       "<!-- 1759433526000 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>1759433526000</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2091,-566 1955,-566 1955,-546 2091,-546 2091,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"2023\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526000&#45;&gt;1759433524944 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>1759433526000&#45;&gt;1759433524944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2021.44,-545.62C2020.09,-537.56 2018.11,-525.65 2016.4,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2019.86,-514.89 2014.77,-505.61 2012.96,-516.05 2019.86,-514.89\"/>\n",
       "</g>\n",
       "<!-- 1759433526672 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>1759433526672</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2081,-628 1969,-628 1969,-608 2081,-608 2081,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2025\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526672&#45;&gt;1759433526000 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>1759433526672&#45;&gt;1759433526000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2024.69,-607.62C2024.42,-599.56 2024.02,-587.65 2023.68,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2027.19,-577.5 2023.35,-567.63 2020.19,-577.74 2027.19,-577.5\"/>\n",
       "</g>\n",
       "<!-- 1759433525472 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>1759433525472</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2090,-690 1966,-690 1966,-670 2090,-670 2090,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"2028\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525472&#45;&gt;1759433526672 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>1759433525472&#45;&gt;1759433526672</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2027.53,-669.62C2027.13,-661.56 2026.53,-649.65 2026.02,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2029.53,-639.44 2025.53,-629.62 2022.54,-639.79 2029.53,-639.44\"/>\n",
       "</g>\n",
       "<!-- 1759433526912&#45;&gt;1759433525472 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>1759433526912&#45;&gt;1759433525472</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1973.16,-731.62C1982.94,-722.59 1997.97,-708.72 2009.82,-697.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2012.04,-700.49 2017.02,-691.14 2007.29,-695.35 2012.04,-700.49\"/>\n",
       "</g>\n",
       "<!-- 1759433463216 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>1759433463216</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2210,-628 2110,-628 2110,-608 2210,-608 2210,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2160\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463216&#45;&gt;1759433526000 -->\n",
       "<g id=\"edge77\" class=\"edge\">\n",
       "<title>1759433463216&#45;&gt;1759433526000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2138.59,-607.62C2116.07,-597.76 2080.39,-582.13 2054.62,-570.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2056.21,-567.72 2045.64,-566.92 2053.4,-574.13 2056.21,-567.72\"/>\n",
       "</g>\n",
       "<!-- 1759421713920 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>1759421713920</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2232,-696 2108,-696 2108,-664 2232,-664 2232,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421713920&#45;&gt;1759433463216 -->\n",
       "<g id=\"edge78\" class=\"edge\">\n",
       "<title>1759421713920&#45;&gt;1759433463216</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2167.42,-663.55C2166.21,-656.26 2164.74,-647.45 2163.43,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2166.91,-639.19 2161.82,-629.9 2160.01,-640.34 2166.91,-639.19\"/>\n",
       "</g>\n",
       "<!-- 1759433462736 -->\n",
       "<g id=\"node77\" class=\"node\">\n",
       "<title>1759433462736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1947,-628 1847,-628 1847,-608 1947,-608 1947,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462736&#45;&gt;1759433526000 -->\n",
       "<g id=\"edge79\" class=\"edge\">\n",
       "<title>1759433462736&#45;&gt;1759433526000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1916.69,-607.62C1937.21,-597.85 1969.63,-582.41 1993.28,-571.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1994.56,-574.42 2002.08,-566.96 1991.55,-568.1 1994.56,-574.42\"/>\n",
       "</g>\n",
       "<!-- 1759421714000 -->\n",
       "<g id=\"node78\" class=\"node\">\n",
       "<title>1759421714000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1948,-696 1836,-696 1836,-664 1948,-664 1948,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1892\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1892\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421714000&#45;&gt;1759433462736 -->\n",
       "<g id=\"edge80\" class=\"edge\">\n",
       "<title>1759421714000&#45;&gt;1759433462736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1893.29,-663.55C1893.9,-656.26 1894.63,-647.45 1895.28,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1898.75,-640.17 1896.09,-629.92 1891.77,-639.59 1898.75,-640.17\"/>\n",
       "</g>\n",
       "<!-- 1759433462256 -->\n",
       "<g id=\"node79\" class=\"node\">\n",
       "<title>1759433462256</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2197,-504 2097,-504 2097,-484 2197,-484 2197,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2147\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462256&#45;&gt;1759433525760 -->\n",
       "<g id=\"edge81\" class=\"edge\">\n",
       "<title>1759433462256&#45;&gt;1759433525760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2115.65,-483.59C2086.27,-474.78 2042.14,-461.54 2009.59,-451.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2010.63,-448.44 2000.05,-448.91 2008.62,-455.14 2010.63,-448.44\"/>\n",
       "</g>\n",
       "<!-- 1759421714080 -->\n",
       "<g id=\"node80\" class=\"node\">\n",
       "<title>1759421714080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2221,-572 2109,-572 2109,-540 2221,-540 2221,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"2165\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2165\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421714080&#45;&gt;1759433462256 -->\n",
       "<g id=\"edge82\" class=\"edge\">\n",
       "<title>1759421714080&#45;&gt;1759433462256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2160.36,-539.55C2158.15,-532.17 2155.47,-523.24 2153.1,-515.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2156.48,-514.43 2150.26,-505.86 2149.78,-516.44 2156.48,-514.43\"/>\n",
       "</g>\n",
       "<!-- 1759433461824 -->\n",
       "<g id=\"node81\" class=\"node\">\n",
       "<title>1759433461824</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1943,-504 1843,-504 1843,-484 1943,-484 1943,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1893\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433461824&#45;&gt;1759433525760 -->\n",
       "<g id=\"edge83\" class=\"edge\">\n",
       "<title>1759433461824&#45;&gt;1759433525760</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1905.89,-483.59C1916.71,-475.7 1932.41,-464.24 1945.19,-454.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1947.02,-457.91 1953.04,-449.19 1942.89,-452.26 1947.02,-457.91\"/>\n",
       "</g>\n",
       "<!-- 1759421714160 -->\n",
       "<g id=\"node82\" class=\"node\">\n",
       "<title>1759421714160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1937,-572 1837,-572 1837,-540 1937,-540 1937,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1887\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool1.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1887\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421714160&#45;&gt;1759433461824 -->\n",
       "<g id=\"edge84\" class=\"edge\">\n",
       "<title>1759421714160&#45;&gt;1759433461824</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1888.55,-539.55C1889.27,-532.26 1890.15,-523.45 1890.94,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1894.4,-516.21 1891.91,-505.91 1887.43,-515.52 1894.4,-516.21\"/>\n",
       "</g>\n",
       "<!-- 1759433524032 -->\n",
       "<g id=\"node83\" class=\"node\">\n",
       "<title>1759433524032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2030,-330 1942,-330 1942,-310 2030,-310 2030,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1986\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524032&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge85\" class=\"edge\">\n",
       "<title>1759433524032&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1955.64,-309.56C1948.28,-307.51 1940.4,-305.51 1933,-304 1827.46,-282.54 1702.16,-269.58 1631.58,-263.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1632.09,-259.93 1621.82,-262.56 1631.49,-266.9 1632.09,-259.93\"/>\n",
       "</g>\n",
       "<!-- 1759433525616 -->\n",
       "<g id=\"node84\" class=\"node\">\n",
       "<title>1759433525616</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2326,-392 2160,-392 2160,-372 2326,-372 2326,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"2243\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433525616&#45;&gt;1759433524032 -->\n",
       "<g id=\"edge86\" class=\"edge\">\n",
       "<title>1759433525616&#45;&gt;1759433524032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2196.64,-371.58C2155.17,-362.97 2092.81,-349.54 2039,-336 2035.72,-335.18 2032.34,-334.29 2028.94,-333.38\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2030.18,-330.09 2019.61,-330.83 2028.33,-336.84 2030.18,-330.09\"/>\n",
       "</g>\n",
       "<!-- 1759433525520 -->\n",
       "<g id=\"node85\" class=\"node\">\n",
       "<title>1759433525520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2442,-448 2282,-448 2282,-428 2442,-428 2442,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"2362\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525520&#45;&gt;1759433525616 -->\n",
       "<g id=\"edge87\" class=\"edge\">\n",
       "<title>1759433525520&#45;&gt;1759433525616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2341.27,-427.59C2322.79,-419.21 2295.46,-406.8 2274.31,-397.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2275.82,-394.05 2265.27,-393.1 2272.93,-400.42 2275.82,-394.05\"/>\n",
       "</g>\n",
       "<!-- 1759433527008 -->\n",
       "<g id=\"node86\" class=\"node\">\n",
       "<title>1759433527008</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2469,-504 2375,-504 2375,-484 2469,-484 2469,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2422\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433527008&#45;&gt;1759433525520 -->\n",
       "<g id=\"edge88\" class=\"edge\">\n",
       "<title>1759433527008&#45;&gt;1759433525520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2411.55,-483.59C2403.03,-475.93 2390.79,-464.91 2380.61,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2383.23,-453.4 2373.46,-449.31 2378.55,-458.6 2383.23,-453.4\"/>\n",
       "</g>\n",
       "<!-- 1759433526384 -->\n",
       "<g id=\"node87\" class=\"node\">\n",
       "<title>1759433526384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2496,-566 2360,-566 2360,-546 2496,-546 2496,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"2428\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433526384&#45;&gt;1759433527008 -->\n",
       "<g id=\"edge89\" class=\"edge\">\n",
       "<title>1759433526384&#45;&gt;1759433527008</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2427.06,-545.62C2426.26,-537.56 2425.07,-525.65 2424.04,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2427.54,-515.22 2423.06,-505.62 2420.57,-515.92 2427.54,-515.22\"/>\n",
       "</g>\n",
       "<!-- 1759433526288 -->\n",
       "<g id=\"node88\" class=\"node\">\n",
       "<title>1759433526288</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2495,-628 2383,-628 2383,-608 2495,-608 2495,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2439\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526288&#45;&gt;1759433526384 -->\n",
       "<g id=\"edge90\" class=\"edge\">\n",
       "<title>1759433526288&#45;&gt;1759433526384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2437.28,-607.62C2435.8,-599.56 2433.62,-587.65 2431.74,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2435.19,-576.81 2429.94,-567.6 2428.3,-578.07 2435.19,-576.81\"/>\n",
       "</g>\n",
       "<!-- 1759433525088 -->\n",
       "<g id=\"node89\" class=\"node\">\n",
       "<title>1759433525088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2504,-690 2380,-690 2380,-670 2504,-670 2504,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"2442\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525088&#45;&gt;1759433526288 -->\n",
       "<g id=\"edge91\" class=\"edge\">\n",
       "<title>1759433525088&#45;&gt;1759433526288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2441.53,-669.62C2441.13,-661.56 2440.53,-649.65 2440.02,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2443.53,-639.44 2439.53,-629.62 2436.54,-639.79 2443.53,-639.44\"/>\n",
       "</g>\n",
       "<!-- 1759433539696&#45;&gt;1759433525088 -->\n",
       "<g id=\"edge92\" class=\"edge\">\n",
       "<title>1759433539696&#45;&gt;1759433525088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2299.41,-731.62C2327.72,-721.57 2372.91,-705.53 2404.82,-694.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2405.82,-697.56 2414.07,-690.91 2403.48,-690.96 2405.82,-697.56\"/>\n",
       "</g>\n",
       "<!-- 1759433464848 -->\n",
       "<g id=\"node91\" class=\"node\">\n",
       "<title>1759433464848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2623,-628 2523,-628 2523,-608 2623,-608 2623,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2573\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464848&#45;&gt;1759433526384 -->\n",
       "<g id=\"edge94\" class=\"edge\">\n",
       "<title>1759433464848&#45;&gt;1759433526384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2550.34,-607.62C2526.4,-597.72 2488.4,-581.99 2461.11,-570.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2462.47,-567.48 2451.9,-566.89 2459.8,-573.95 2462.47,-567.48\"/>\n",
       "</g>\n",
       "<!-- 1759421853760 -->\n",
       "<g id=\"node92\" class=\"node\">\n",
       "<title>1759421853760</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2646,-696 2522,-696 2522,-664 2646,-664 2646,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"2584\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2584\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421853760&#45;&gt;1759433464848 -->\n",
       "<g id=\"edge95\" class=\"edge\">\n",
       "<title>1759421853760&#45;&gt;1759433464848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2581.17,-663.55C2579.83,-656.26 2578.22,-647.45 2576.78,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2580.24,-639.1 2575,-629.9 2573.36,-640.36 2580.24,-639.1\"/>\n",
       "</g>\n",
       "<!-- 1759433464656 -->\n",
       "<g id=\"node93\" class=\"node\">\n",
       "<title>1759433464656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2362,-628 2262,-628 2262,-608 2362,-608 2362,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"2312\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433464656&#45;&gt;1759433526384 -->\n",
       "<g id=\"edge96\" class=\"edge\">\n",
       "<title>1759433464656&#45;&gt;1759433526384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2330.12,-607.62C2348.85,-597.94 2378.33,-582.69 2400.05,-571.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2401.38,-574.71 2408.66,-567 2398.17,-568.49 2401.38,-574.71\"/>\n",
       "</g>\n",
       "<!-- 1759421853840 -->\n",
       "<g id=\"node94\" class=\"node\">\n",
       "<title>1759421853840</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2362,-696 2250,-696 2250,-664 2362,-664 2362,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"2306\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"2306\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421853840&#45;&gt;1759433464656 -->\n",
       "<g id=\"edge97\" class=\"edge\">\n",
       "<title>1759421853840&#45;&gt;1759433464656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2307.55,-663.55C2308.27,-656.26 2309.15,-647.45 2309.94,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2313.4,-640.21 2310.91,-629.91 2306.43,-639.52 2313.4,-640.21\"/>\n",
       "</g>\n",
       "<!-- 1759433463696 -->\n",
       "<g id=\"node95\" class=\"node\">\n",
       "<title>1759433463696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2603,-504 2503,-504 2503,-484 2603,-484 2603,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2553\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433463696&#45;&gt;1759433525520 -->\n",
       "<g id=\"edge98\" class=\"edge\">\n",
       "<title>1759433463696&#45;&gt;1759433525520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2519.73,-483.59C2488.29,-474.71 2440.91,-461.31 2406.29,-451.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2407.56,-448.24 2396.99,-448.89 2405.66,-454.98 2407.56,-448.24\"/>\n",
       "</g>\n",
       "<!-- 1759421854000 -->\n",
       "<g id=\"node96\" class=\"node\">\n",
       "<title>1759421854000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2626,-572 2514,-572 2514,-540 2626,-540 2626,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"2570\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2570\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854000&#45;&gt;1759433463696 -->\n",
       "<g id=\"edge99\" class=\"edge\">\n",
       "<title>1759421854000&#45;&gt;1759433463696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2565.62,-539.55C2563.53,-532.17 2561,-523.24 2558.76,-515.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2562.17,-514.53 2556.08,-505.86 2555.44,-516.44 2562.17,-514.53\"/>\n",
       "</g>\n",
       "<!-- 1759433462448 -->\n",
       "<g id=\"node97\" class=\"node\">\n",
       "<title>1759433462448</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"2352,-504 2252,-504 2252,-484 2352,-484 2352,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"2302\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462448&#45;&gt;1759433525520 -->\n",
       "<g id=\"edge100\" class=\"edge\">\n",
       "<title>1759433462448&#45;&gt;1759433525520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2312.45,-483.59C2320.97,-475.93 2333.21,-464.91 2343.39,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2345.45,-458.6 2350.54,-449.31 2340.77,-453.4 2345.45,-458.6\"/>\n",
       "</g>\n",
       "<!-- 1759421854080 -->\n",
       "<g id=\"node98\" class=\"node\">\n",
       "<title>1759421854080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2342,-572 2242,-572 2242,-540 2342,-540 2342,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"2292\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool2.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"2292\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854080&#45;&gt;1759433462448 -->\n",
       "<g id=\"edge101\" class=\"edge\">\n",
       "<title>1759421854080&#45;&gt;1759433462448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2294.58,-539.55C2295.79,-532.26 2297.26,-523.45 2298.57,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2301.99,-516.34 2300.18,-505.9 2295.09,-515.19 2301.99,-516.34\"/>\n",
       "</g>\n",
       "<!-- 1759433524368 -->\n",
       "<g id=\"node99\" class=\"node\">\n",
       "<title>1759433524368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"424,-330 336,-330 336,-310 424,-310 424,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"380\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524368&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge102\" class=\"edge\">\n",
       "<title>1759433524368&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M424.34,-316.78C608.89,-307.52 1315.9,-272.05 1520.33,-261.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1520.34,-265.3 1530.15,-261.3 1519.99,-258.31 1520.34,-265.3\"/>\n",
       "</g>\n",
       "<!-- 1759433526480 -->\n",
       "<g id=\"node100\" class=\"node\">\n",
       "<title>1759433526480</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"411,-392 245,-392 245,-372 411,-372 411,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"328\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526480&#45;&gt;1759433524368 -->\n",
       "<g id=\"edge103\" class=\"edge\">\n",
       "<title>1759433526480&#45;&gt;1759433524368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.12,-371.62C343.72,-362.86 355.27,-349.53 364.62,-338.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"367.21,-341.11 371.11,-331.26 361.92,-336.52 367.21,-341.11\"/>\n",
       "</g>\n",
       "<!-- 1759433524992 -->\n",
       "<g id=\"node101\" class=\"node\">\n",
       "<title>1759433524992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"362,-448 202,-448 202,-428 362,-428 362,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"282\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524992&#45;&gt;1759433526480 -->\n",
       "<g id=\"edge104\" class=\"edge\">\n",
       "<title>1759433524992&#45;&gt;1759433526480</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M290.01,-427.59C296.28,-420.24 305.17,-409.8 312.78,-400.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"315.28,-403.33 319.1,-393.45 309.95,-398.79 315.28,-403.33\"/>\n",
       "</g>\n",
       "<!-- 1759433525280 -->\n",
       "<g id=\"node102\" class=\"node\">\n",
       "<title>1759433525280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"268,-504 174,-504 174,-484 268,-484 268,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"221\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433525280&#45;&gt;1759433524992 -->\n",
       "<g id=\"edge105\" class=\"edge\">\n",
       "<title>1759433525280&#45;&gt;1759433524992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M231.63,-483.59C240.28,-475.93 252.73,-464.91 263.08,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"265.2,-458.55 270.36,-449.3 260.56,-453.31 265.2,-458.55\"/>\n",
       "</g>\n",
       "<!-- 1759433540128 -->\n",
       "<g id=\"node103\" class=\"node\">\n",
       "<title>1759433540128</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"280,-566 144,-566 144,-546 280,-546 280,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540128&#45;&gt;1759433525280 -->\n",
       "<g id=\"edge106\" class=\"edge\">\n",
       "<title>1759433540128&#45;&gt;1759433525280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.41,-545.62C214.62,-537.56 216.4,-525.65 217.94,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"221.39,-516.02 219.41,-505.61 214.46,-514.98 221.39,-516.02\"/>\n",
       "</g>\n",
       "<!-- 1759433540032 -->\n",
       "<g id=\"node104\" class=\"node\">\n",
       "<title>1759433540032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"253,-628 141,-628 141,-608 253,-608 253,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540032&#45;&gt;1759433540128 -->\n",
       "<g id=\"edge107\" class=\"edge\">\n",
       "<title>1759433540032&#45;&gt;1759433540128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M199.34,-607.62C201.38,-599.48 204.4,-587.39 206.98,-577.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"210.32,-578.13 209.35,-567.58 203.53,-576.43 210.32,-578.13\"/>\n",
       "</g>\n",
       "<!-- 1759433540224 -->\n",
       "<g id=\"node105\" class=\"node\">\n",
       "<title>1759433540224</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"254,-690 130,-690 130,-670 254,-670 254,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540224&#45;&gt;1759433540032 -->\n",
       "<g id=\"edge108\" class=\"edge\">\n",
       "<title>1759433540224&#45;&gt;1759433540032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192.78,-669.62C193.45,-661.56 194.45,-649.65 195.3,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.77,-639.88 196.11,-629.62 191.8,-639.3 198.77,-639.88\"/>\n",
       "</g>\n",
       "<!-- 1759433540320&#45;&gt;1759433540224 -->\n",
       "<g id=\"edge109\" class=\"edge\">\n",
       "<title>1759433540320&#45;&gt;1759433540224</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M213.94,-731.62C210.33,-723.3 204.94,-710.86 200.41,-700.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"203.74,-699.29 196.55,-691.5 197.32,-702.07 203.74,-699.29\"/>\n",
       "</g>\n",
       "<!-- 1759433465376 -->\n",
       "<g id=\"node107\" class=\"node\">\n",
       "<title>1759433465376</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"375,-628 275,-628 275,-608 375,-608 375,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"325\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465376&#45;&gt;1759433540128 -->\n",
       "<g id=\"edge111\" class=\"edge\">\n",
       "<title>1759433465376&#45;&gt;1759433540128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M307.34,-607.62C289.18,-597.98 260.64,-582.83 239.52,-571.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"241.34,-568.62 230.87,-567.02 238.06,-574.8 241.34,-568.62\"/>\n",
       "</g>\n",
       "<!-- 1759421854400 -->\n",
       "<g id=\"node108\" class=\"node\">\n",
       "<title>1759421854400</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"396,-696 272,-696 272,-664 396,-664 396,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"334\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"334\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421854400&#45;&gt;1759433465376 -->\n",
       "<g id=\"edge112\" class=\"edge\">\n",
       "<title>1759421854400&#45;&gt;1759433465376</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.68,-663.55C330.59,-656.26 329.27,-647.45 328.09,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.58,-639.27 326.64,-629.9 324.66,-640.31 331.58,-639.27\"/>\n",
       "</g>\n",
       "<!-- 1759433465232 -->\n",
       "<g id=\"node109\" class=\"node\">\n",
       "<title>1759433465232</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"114,-628 14,-628 14,-608 114,-608 114,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"64\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465232&#45;&gt;1759433540128 -->\n",
       "<g id=\"edge113\" class=\"edge\">\n",
       "<title>1759433465232&#45;&gt;1759433540128</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M87.12,-607.62C111.67,-597.67 150.69,-581.85 178.58,-570.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179.68,-573.88 187.63,-566.88 177.05,-567.39 179.68,-573.88\"/>\n",
       "</g>\n",
       "<!-- 1759421854480 -->\n",
       "<g id=\"node110\" class=\"node\">\n",
       "<title>1759421854480</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"112,-696 0,-696 0,-664 112,-664 112,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"56\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854480&#45;&gt;1759433465232 -->\n",
       "<g id=\"edge114\" class=\"edge\">\n",
       "<title>1759421854480&#45;&gt;1759433465232</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58.06,-663.55C59.03,-656.26 60.21,-647.45 61.25,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"64.69,-640.28 62.55,-629.91 57.75,-639.36 64.69,-640.28\"/>\n",
       "</g>\n",
       "<!-- 1759433465040 -->\n",
       "<g id=\"node111\" class=\"node\">\n",
       "<title>1759433465040</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"392,-504 292,-504 292,-484 392,-484 392,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"342\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465040&#45;&gt;1759433524992 -->\n",
       "<g id=\"edge115\" class=\"edge\">\n",
       "<title>1759433465040&#45;&gt;1759433524992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.55,-483.59C323.03,-475.93 310.79,-464.91 300.61,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"303.23,-453.4 293.46,-449.31 298.55,-458.6 303.23,-453.4\"/>\n",
       "</g>\n",
       "<!-- 1759421854560 -->\n",
       "<g id=\"node112\" class=\"node\">\n",
       "<title>1759421854560</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"410,-572 298,-572 298,-540 410,-540 410,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"354\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"354\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854560&#45;&gt;1759433465040 -->\n",
       "<g id=\"edge116\" class=\"edge\">\n",
       "<title>1759421854560&#45;&gt;1759433465040</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M350.91,-539.55C349.45,-532.26 347.69,-523.45 346.12,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"349.57,-515.01 344.18,-505.89 342.71,-516.38 349.57,-515.01\"/>\n",
       "</g>\n",
       "<!-- 1759433462688 -->\n",
       "<g id=\"node113\" class=\"node\">\n",
       "<title>1759433462688</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"150,-504 50,-504 50,-484 150,-484 150,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"100\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462688&#45;&gt;1759433524992 -->\n",
       "<g id=\"edge117\" class=\"edge\">\n",
       "<title>1759433462688&#45;&gt;1759433524992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M131.7,-483.59C161.53,-474.74 206.42,-461.43 239.37,-451.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.01,-455.11 248.6,-448.91 238.02,-448.4 240.01,-455.11\"/>\n",
       "</g>\n",
       "<!-- 1759421854640 -->\n",
       "<g id=\"node114\" class=\"node\">\n",
       "<title>1759421854640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"126,-572 26,-572 26,-540 126,-540 126,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"76\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool3.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"76\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421854640&#45;&gt;1759433462688 -->\n",
       "<g id=\"edge118\" class=\"edge\">\n",
       "<title>1759421854640&#45;&gt;1759433462688</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.18,-539.55C85.17,-532.08 88.79,-523.03 91.98,-515.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.21,-516.4 95.67,-505.81 88.71,-513.8 95.21,-516.4\"/>\n",
       "</g>\n",
       "<!-- 1759433524560 -->\n",
       "<g id=\"node115\" class=\"node\">\n",
       "<title>1759433524560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"942,-330 854,-330 854,-310 942,-310 942,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"898\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524560&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge119\" class=\"edge\">\n",
       "<title>1759433524560&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M942.49,-315.06C1061.88,-304.5 1389.18,-275.53 1520.45,-263.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1520.63,-267.41 1530.28,-263.05 1520.02,-260.44 1520.63,-267.41\"/>\n",
       "</g>\n",
       "<!-- 1759433527248 -->\n",
       "<g id=\"node116\" class=\"node\">\n",
       "<title>1759433527248</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"874,-392 708,-392 708,-372 874,-372 874,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"791\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433527248&#45;&gt;1759433524560 -->\n",
       "<g id=\"edge120\" class=\"edge\">\n",
       "<title>1759433527248&#45;&gt;1759433524560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M807.72,-371.62C824.76,-362.07 851.45,-347.11 871.39,-335.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"873.07,-338.99 880.08,-331.05 869.64,-332.89 873.07,-338.99\"/>\n",
       "</g>\n",
       "<!-- 1759433539744 -->\n",
       "<g id=\"node117\" class=\"node\">\n",
       "<title>1759433539744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"766,-448 606,-448 606,-428 766,-428 766,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"686\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539744&#45;&gt;1759433527248 -->\n",
       "<g id=\"edge121\" class=\"edge\">\n",
       "<title>1759433539744&#45;&gt;1759433527248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M704.29,-427.59C720.3,-419.36 743.83,-407.26 762.34,-397.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"763.93,-400.86 771.22,-393.17 760.73,-394.63 763.93,-400.86\"/>\n",
       "</g>\n",
       "<!-- 1759433540272 -->\n",
       "<g id=\"node118\" class=\"node\">\n",
       "<title>1759433540272</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"673,-504 579,-504 579,-484 673,-484 673,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"626\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540272&#45;&gt;1759433539744 -->\n",
       "<g id=\"edge122\" class=\"edge\">\n",
       "<title>1759433540272&#45;&gt;1759433539744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M636.45,-483.59C644.97,-475.93 657.21,-464.91 667.39,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"669.45,-458.6 674.54,-449.31 664.77,-453.4 669.45,-458.6\"/>\n",
       "</g>\n",
       "<!-- 1759433540368 -->\n",
       "<g id=\"node119\" class=\"node\">\n",
       "<title>1759433540368</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"685,-566 549,-566 549,-546 685,-546 685,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"617\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540368&#45;&gt;1759433540272 -->\n",
       "<g id=\"edge123\" class=\"edge\">\n",
       "<title>1759433540368&#45;&gt;1759433540272</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M618.41,-545.62C619.62,-537.56 621.4,-525.65 622.94,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"626.39,-516.02 624.41,-505.61 619.46,-514.98 626.39,-516.02\"/>\n",
       "</g>\n",
       "<!-- 1759433540464 -->\n",
       "<g id=\"node120\" class=\"node\">\n",
       "<title>1759433540464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"668,-628 556,-628 556,-608 668,-608 668,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"612\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540464&#45;&gt;1759433540368 -->\n",
       "<g id=\"edge124\" class=\"edge\">\n",
       "<title>1759433540464&#45;&gt;1759433540368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M612.78,-607.62C613.45,-599.56 614.45,-587.65 615.3,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"618.77,-577.88 616.11,-567.62 611.8,-577.3 618.77,-577.88\"/>\n",
       "</g>\n",
       "<!-- 1759433540560 -->\n",
       "<g id=\"node121\" class=\"node\">\n",
       "<title>1759433540560</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"668,-690 544,-690 544,-670 668,-670 668,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"606\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540560&#45;&gt;1759433540464 -->\n",
       "<g id=\"edge125\" class=\"edge\">\n",
       "<title>1759433540560&#45;&gt;1759433540464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M606.94,-669.62C607.74,-661.56 608.93,-649.65 609.96,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"613.43,-639.92 610.94,-629.62 606.46,-639.22 613.43,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433540656&#45;&gt;1759433540560 -->\n",
       "<g id=\"edge126\" class=\"edge\">\n",
       "<title>1759433540656&#45;&gt;1759433540560</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M742.69,-731.62C715.66,-721.62 672.6,-705.67 642.04,-694.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"643.41,-691.12 632.82,-690.93 640.98,-697.69 643.41,-691.12\"/>\n",
       "</g>\n",
       "<!-- 1759433494640 -->\n",
       "<g id=\"node123\" class=\"node\">\n",
       "<title>1759433494640</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"792,-628 692,-628 692,-608 792,-608 792,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"742\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494640&#45;&gt;1759433540368 -->\n",
       "<g id=\"edge128\" class=\"edge\">\n",
       "<title>1759433494640&#45;&gt;1759433540368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M722.47,-607.62C702.1,-597.85 669.94,-582.41 646.49,-571.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"648.29,-568.14 637.76,-566.96 645.26,-574.45 648.29,-568.14\"/>\n",
       "</g>\n",
       "<!-- 1759421854960 -->\n",
       "<g id=\"node124\" class=\"node\">\n",
       "<title>1759421854960</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"810,-696 686,-696 686,-664 810,-664 810,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"748\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"748\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421854960&#45;&gt;1759433494640 -->\n",
       "<g id=\"edge129\" class=\"edge\">\n",
       "<title>1759421854960&#45;&gt;1759433494640</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M746.45,-663.55C745.73,-656.26 744.85,-647.45 744.06,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"747.57,-639.52 743.09,-629.91 740.6,-640.21 747.57,-639.52\"/>\n",
       "</g>\n",
       "<!-- 1759433494592 -->\n",
       "<g id=\"node125\" class=\"node\">\n",
       "<title>1759433494592</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"529,-628 429,-628 429,-608 529,-608 529,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"479\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494592&#45;&gt;1759433540368 -->\n",
       "<g id=\"edge130\" class=\"edge\">\n",
       "<title>1759433494592&#45;&gt;1759433540368</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M500.56,-607.62C523.25,-597.76 559.19,-582.13 585.15,-570.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"586.43,-574.11 594.2,-566.91 583.63,-567.69 586.43,-574.11\"/>\n",
       "</g>\n",
       "<!-- 1759421855040 -->\n",
       "<g id=\"node126\" class=\"node\">\n",
       "<title>1759421855040</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"526,-696 414,-696 414,-664 526,-664 526,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"470\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"470\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421855040&#45;&gt;1759433494592 -->\n",
       "<g id=\"edge131\" class=\"edge\">\n",
       "<title>1759421855040&#45;&gt;1759433494592</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.32,-663.55C473.41,-656.26 474.73,-647.45 475.91,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"479.34,-640.31 477.36,-629.9 472.42,-639.27 479.34,-640.31\"/>\n",
       "</g>\n",
       "<!-- 1759433465568 -->\n",
       "<g id=\"node127\" class=\"node\">\n",
       "<title>1759433465568</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"797,-504 697,-504 697,-484 797,-484 797,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"747\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433465568&#45;&gt;1759433539744 -->\n",
       "<g id=\"edge132\" class=\"edge\">\n",
       "<title>1759433465568&#45;&gt;1759433539744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M736.37,-483.59C727.72,-475.93 715.27,-464.91 704.92,-455.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"707.44,-453.31 697.64,-449.3 702.8,-458.55 707.44,-453.31\"/>\n",
       "</g>\n",
       "<!-- 1759421855120 -->\n",
       "<g id=\"node128\" class=\"node\">\n",
       "<title>1759421855120</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"815,-572 703,-572 703,-540 815,-540 815,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"759\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"759\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421855120&#45;&gt;1759433465568 -->\n",
       "<g id=\"edge133\" class=\"edge\">\n",
       "<title>1759421855120&#45;&gt;1759433465568</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M755.91,-539.55C754.45,-532.26 752.69,-523.45 751.12,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"754.57,-515.01 749.18,-505.89 747.71,-516.38 754.57,-515.01\"/>\n",
       "</g>\n",
       "<!-- 1759433462112 -->\n",
       "<g id=\"node129\" class=\"node\">\n",
       "<title>1759433462112</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"537,-504 437,-504 437,-484 537,-484 537,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"487\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433462112&#45;&gt;1759433539744 -->\n",
       "<g id=\"edge134\" class=\"edge\">\n",
       "<title>1759433462112&#45;&gt;1759433539744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M521.66,-483.59C554.56,-474.67 604.21,-461.19 640.33,-451.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"640.87,-454.87 649.61,-448.88 639.04,-448.12 640.87,-454.87\"/>\n",
       "</g>\n",
       "<!-- 1759421855200 -->\n",
       "<g id=\"node130\" class=\"node\">\n",
       "<title>1759421855200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"531,-572 431,-572 431,-540 531,-540 531,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"481\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool4.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"481\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421855200&#45;&gt;1759433462112 -->\n",
       "<g id=\"edge135\" class=\"edge\">\n",
       "<title>1759421855200&#45;&gt;1759433462112</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M482.55,-539.55C483.27,-532.26 484.15,-523.45 484.94,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"488.4,-516.21 485.91,-505.91 481.43,-515.52 488.4,-516.21\"/>\n",
       "</g>\n",
       "<!-- 1759433524656 -->\n",
       "<g id=\"node131\" class=\"node\">\n",
       "<title>1759433524656</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1254,-330 1166,-330 1166,-310 1254,-310 1254,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1210\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524656&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge136\" class=\"edge\">\n",
       "<title>1759433524656&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1254.15,-311.76C1320.92,-300.82 1447.53,-280.06 1520.47,-268.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1520.9,-271.58 1530.2,-266.51 1519.77,-264.67 1520.9,-271.58\"/>\n",
       "</g>\n",
       "<!-- 1759433526096 -->\n",
       "<g id=\"node132\" class=\"node\">\n",
       "<title>1759433526096</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1197,-392 1031,-392 1031,-372 1197,-372 1197,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1114\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433526096&#45;&gt;1759433524656 -->\n",
       "<g id=\"edge137\" class=\"edge\">\n",
       "<title>1759433526096&#45;&gt;1759433524656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1129,-371.62C1144.08,-362.2 1167.57,-347.52 1185.38,-336.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1187.2,-339.38 1193.82,-331.11 1183.49,-333.44 1187.2,-339.38\"/>\n",
       "</g>\n",
       "<!-- 1759433540416 -->\n",
       "<g id=\"node133\" class=\"node\">\n",
       "<title>1759433540416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1162,-448 1002,-448 1002,-428 1162,-428 1162,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1082\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540416&#45;&gt;1759433526096 -->\n",
       "<g id=\"edge138\" class=\"edge\">\n",
       "<title>1759433540416&#45;&gt;1759433526096</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1087.57,-427.59C1091.75,-420.55 1097.6,-410.67 1102.74,-402\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1105.63,-403.99 1107.72,-393.6 1099.61,-400.42 1105.63,-403.99\"/>\n",
       "</g>\n",
       "<!-- 1759433540752 -->\n",
       "<g id=\"node134\" class=\"node\">\n",
       "<title>1759433540752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"956,-504 862,-504 862,-484 956,-484 956,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"909\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540752&#45;&gt;1759433540416 -->\n",
       "<g id=\"edge139\" class=\"edge\">\n",
       "<title>1759433540752&#45;&gt;1759433540416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M939.13,-483.59C967.24,-474.82 1009.41,-461.66 1040.66,-451.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1041.68,-455.25 1050.19,-448.93 1039.6,-448.57 1041.68,-455.25\"/>\n",
       "</g>\n",
       "<!-- 1759433539936 -->\n",
       "<g id=\"node135\" class=\"node\">\n",
       "<title>1759433539936</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"970,-566 834,-566 834,-546 970,-546 970,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"902\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539936&#45;&gt;1759433540752 -->\n",
       "<g id=\"edge140\" class=\"edge\">\n",
       "<title>1759433539936&#45;&gt;1759433540752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M903.09,-545.62C904.03,-537.56 905.42,-525.65 906.62,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"910.08,-515.95 907.76,-505.62 903.13,-515.14 910.08,-515.95\"/>\n",
       "</g>\n",
       "<!-- 1759433540848 -->\n",
       "<g id=\"node136\" class=\"node\">\n",
       "<title>1759433540848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"952,-628 840,-628 840,-608 952,-608 952,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"896\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540848&#45;&gt;1759433539936 -->\n",
       "<g id=\"edge141\" class=\"edge\">\n",
       "<title>1759433540848&#45;&gt;1759433539936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M896.94,-607.62C897.74,-599.56 898.93,-587.65 899.96,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"903.43,-577.92 900.94,-567.62 896.46,-577.22 903.43,-577.92\"/>\n",
       "</g>\n",
       "<!-- 1759433540944 -->\n",
       "<g id=\"node137\" class=\"node\">\n",
       "<title>1759433540944</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"952,-690 828,-690 828,-670 952,-670 952,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"890\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540944&#45;&gt;1759433540848 -->\n",
       "<g id=\"edge142\" class=\"edge\">\n",
       "<title>1759433540944&#45;&gt;1759433540848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890.94,-669.62C891.74,-661.56 892.93,-649.65 893.96,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"897.43,-639.92 894.94,-629.62 890.46,-639.22 897.43,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433541040&#45;&gt;1759433540944 -->\n",
       "<g id=\"edge143\" class=\"edge\">\n",
       "<title>1759433541040&#45;&gt;1759433540944</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1079,-731.62C1040.48,-721.31 978.42,-704.68 935.99,-693.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"937.15,-690.01 926.59,-690.8 935.34,-696.77 937.15,-690.01\"/>\n",
       "</g>\n",
       "<!-- 1759433495168 -->\n",
       "<g id=\"node139\" class=\"node\">\n",
       "<title>1759433495168</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1212,-628 1112,-628 1112,-608 1212,-608 1212,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495168&#45;&gt;1759433539936 -->\n",
       "<g id=\"edge145\" class=\"edge\">\n",
       "<title>1759433495168&#45;&gt;1759433539936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1121.09,-607.56C1075.98,-597.15 1003.3,-580.38 954.14,-569.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"955.11,-565.66 944.58,-566.83 953.53,-572.48 955.11,-565.66\"/>\n",
       "</g>\n",
       "<!-- 1759433541088 -->\n",
       "<g id=\"node151\" class=\"node\">\n",
       "<title>1759433541088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1372,-566 1236,-566 1236,-546 1372,-546 1372,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"1304\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433495168&#45;&gt;1759433541088 -->\n",
       "<g id=\"edge162\" class=\"edge\">\n",
       "<title>1759433495168&#45;&gt;1759433541088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1184.19,-607.62C1207.63,-597.72 1244.85,-581.99 1271.58,-570.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1272.72,-574.01 1280.57,-566.9 1270,-567.57 1272.72,-574.01\"/>\n",
       "</g>\n",
       "<!-- 1759421856080 -->\n",
       "<g id=\"node140\" class=\"node\">\n",
       "<title>1759421856080</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1224,-696 1100,-696 1100,-664 1224,-664 1224,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421856080&#45;&gt;1759433495168 -->\n",
       "<g id=\"edge146\" class=\"edge\">\n",
       "<title>1759421856080&#45;&gt;1759433495168</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1162,-663.55C1162,-656.34 1162,-647.66 1162,-639.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1165.5,-639.92 1162,-629.92 1158.5,-639.92 1165.5,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433495024 -->\n",
       "<g id=\"node141\" class=\"node\">\n",
       "<title>1759433495024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1076,-628 976,-628 976,-608 1076,-608 1076,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495024&#45;&gt;1759433539936 -->\n",
       "<g id=\"edge147\" class=\"edge\">\n",
       "<title>1759433495024&#45;&gt;1759433539936</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1006.62,-607.62C986.51,-597.89 954.81,-582.55 931.57,-571.31\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"933.13,-568.17 922.6,-566.97 930.08,-574.47 933.13,-568.17\"/>\n",
       "</g>\n",
       "<!-- 1759433495024&#45;&gt;1759433541088 -->\n",
       "<g id=\"edge163\" class=\"edge\">\n",
       "<title>1759433495024&#45;&gt;1759433541088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1069.74,-607.56C1118.18,-597.11 1196.34,-580.24 1248.92,-568.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1249.54,-572.34 1258.57,-566.8 1248.06,-565.49 1249.54,-572.34\"/>\n",
       "</g>\n",
       "<!-- 1759421856160 -->\n",
       "<g id=\"node142\" class=\"node\">\n",
       "<title>1759421856160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1082,-696 970,-696 970,-664 1082,-664 1082,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1026\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856160&#45;&gt;1759433495024 -->\n",
       "<g id=\"edge148\" class=\"edge\">\n",
       "<title>1759421856160&#45;&gt;1759433495024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1026,-663.55C1026,-656.34 1026,-647.66 1026,-639.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1029.5,-639.92 1026,-629.92 1022.5,-639.92 1029.5,-639.92\"/>\n",
       "</g>\n",
       "<!-- 1759433494832 -->\n",
       "<g id=\"node143\" class=\"node\">\n",
       "<title>1759433494832</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1219,-504 1119,-504 1119,-484 1219,-484 1219,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1169\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494832&#45;&gt;1759433540416 -->\n",
       "<g id=\"edge149\" class=\"edge\">\n",
       "<title>1759433494832&#45;&gt;1759433540416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1153.85,-483.59C1140.95,-475.59 1122.17,-463.93 1107.04,-454.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1108.9,-451.58 1098.56,-449.28 1105.21,-457.53 1108.9,-451.58\"/>\n",
       "</g>\n",
       "<!-- 1759433540800 -->\n",
       "<g id=\"node149\" class=\"node\">\n",
       "<title>1759433540800</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1355,-448 1195,-448 1195,-428 1355,-428 1355,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1275\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433494832&#45;&gt;1759433540800 -->\n",
       "<g id=\"edge164\" class=\"edge\">\n",
       "<title>1759433494832&#45;&gt;1759433540800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1187.46,-483.59C1203.7,-475.32 1227.61,-463.14 1246.33,-453.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1247.72,-456.82 1255.04,-449.17 1244.54,-450.59 1247.72,-456.82\"/>\n",
       "</g>\n",
       "<!-- 1759421856240 -->\n",
       "<g id=\"node144\" class=\"node\">\n",
       "<title>1759421856240</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1218,-572 1106,-572 1106,-540 1218,-540 1218,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1162\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856240&#45;&gt;1759433494832 -->\n",
       "<g id=\"edge150\" class=\"edge\">\n",
       "<title>1759421856240&#45;&gt;1759433494832</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1163.8,-539.55C1164.65,-532.26 1165.68,-523.45 1166.6,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1170.04,-516.25 1167.73,-505.91 1163.09,-515.44 1170.04,-516.25\"/>\n",
       "</g>\n",
       "<!-- 1759433494736 -->\n",
       "<g id=\"node145\" class=\"node\">\n",
       "<title>1759433494736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1101,-504 1001,-504 1001,-484 1101,-484 1101,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1051\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494736&#45;&gt;1759433540416 -->\n",
       "<g id=\"edge151\" class=\"edge\">\n",
       "<title>1759433494736&#45;&gt;1759433540416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1056.4,-483.59C1060.44,-476.55 1066.12,-466.67 1071.09,-458\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1073.97,-460.03 1075.91,-449.61 1067.89,-456.54 1073.97,-460.03\"/>\n",
       "</g>\n",
       "<!-- 1759433494736&#45;&gt;1759433540800 -->\n",
       "<g id=\"edge165\" class=\"edge\">\n",
       "<title>1759433494736&#45;&gt;1759433540800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1090.27,-483.53C1127.66,-474.52 1184.1,-460.91 1224.75,-451.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1225.4,-454.56 1234.3,-448.81 1223.76,-447.75 1225.4,-454.56\"/>\n",
       "</g>\n",
       "<!-- 1759421856320 -->\n",
       "<g id=\"node146\" class=\"node\">\n",
       "<title>1759421856320</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1088,-572 988,-572 988,-540 1088,-540 1088,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1038\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool6.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1038\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856320&#45;&gt;1759433494736 -->\n",
       "<g id=\"edge152\" class=\"edge\">\n",
       "<title>1759421856320&#45;&gt;1759433494736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1041.35,-539.55C1042.93,-532.26 1044.84,-523.45 1046.54,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1049.94,-516.4 1048.64,-505.89 1043.1,-514.92 1049.94,-516.4\"/>\n",
       "</g>\n",
       "<!-- 1759433524704 -->\n",
       "<g id=\"node147\" class=\"node\">\n",
       "<title>1759433524704</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1463,-330 1375,-330 1375,-310 1463,-310 1463,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1419\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524704&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge153\" class=\"edge\">\n",
       "<title>1759433524704&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1443.53,-309.62C1469.61,-299.66 1511.09,-283.81 1540.68,-272.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1541.88,-275.79 1549.97,-268.95 1539.38,-269.25 1541.88,-275.79\"/>\n",
       "</g>\n",
       "<!-- 1759433540080 -->\n",
       "<g id=\"node148\" class=\"node\">\n",
       "<title>1759433540080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1444,-392 1278,-392 1278,-372 1444,-372 1444,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1361\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540080&#45;&gt;1759433524704 -->\n",
       "<g id=\"edge154\" class=\"edge\">\n",
       "<title>1759433540080&#45;&gt;1759433524704</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1370.06,-371.62C1378.71,-362.68 1391.94,-348.99 1402.47,-338.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1404.71,-340.82 1409.14,-331.2 1399.67,-335.96 1404.71,-340.82\"/>\n",
       "</g>\n",
       "<!-- 1759433540800&#45;&gt;1759433540080 -->\n",
       "<g id=\"edge155\" class=\"edge\">\n",
       "<title>1759433540800&#45;&gt;1759433540080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1289.98,-427.59C1302.73,-419.59 1321.29,-407.93 1336.24,-398.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1338.01,-401.57 1344.62,-393.28 1334.29,-395.64 1338.01,-401.57\"/>\n",
       "</g>\n",
       "<!-- 1759433540992 -->\n",
       "<g id=\"node150\" class=\"node\">\n",
       "<title>1759433540992</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1338,-504 1244,-504 1244,-484 1338,-484 1338,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1291\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433540992&#45;&gt;1759433540800 -->\n",
       "<g id=\"edge156\" class=\"edge\">\n",
       "<title>1759433540992&#45;&gt;1759433540800</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1288.21,-483.59C1286.22,-476.86 1283.45,-467.53 1280.97,-459.15\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1284.38,-458.34 1278.19,-449.75 1277.67,-460.33 1284.38,-458.34\"/>\n",
       "</g>\n",
       "<!-- 1759433541088&#45;&gt;1759433540992 -->\n",
       "<g id=\"edge157\" class=\"edge\">\n",
       "<title>1759433541088&#45;&gt;1759433540992</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1301.97,-545.62C1300.2,-537.48 1297.58,-525.39 1295.35,-515.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1298.83,-514.62 1293.29,-505.59 1291.99,-516.11 1298.83,-514.62\"/>\n",
       "</g>\n",
       "<!-- 1759433541184 -->\n",
       "<g id=\"node152\" class=\"node\">\n",
       "<title>1759433541184</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1360,-628 1248,-628 1248,-608 1360,-608 1360,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1304\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433541184&#45;&gt;1759433541088 -->\n",
       "<g id=\"edge158\" class=\"edge\">\n",
       "<title>1759433541184&#45;&gt;1759433541088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1304,-607.62C1304,-599.56 1304,-587.65 1304,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1307.5,-577.63 1304,-567.63 1300.5,-577.63 1307.5,-577.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541280 -->\n",
       "<g id=\"node153\" class=\"node\">\n",
       "<title>1759433541280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1366,-690 1242,-690 1242,-670 1366,-670 1366,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"1304\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541280&#45;&gt;1759433541184 -->\n",
       "<g id=\"edge159\" class=\"edge\">\n",
       "<title>1759433541280&#45;&gt;1759433541184</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1304,-669.62C1304,-661.56 1304,-649.65 1304,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1307.5,-639.63 1304,-629.63 1300.5,-639.63 1307.5,-639.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541376&#45;&gt;1759433541280 -->\n",
       "<g id=\"edge160\" class=\"edge\">\n",
       "<title>1759433541376&#45;&gt;1759433541280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1349.56,-731.62C1341.59,-722.77 1329.43,-709.26 1319.68,-698.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1322.51,-696.33 1313.21,-691.24 1317.3,-701.01 1322.51,-696.33\"/>\n",
       "</g>\n",
       "<!-- 1759433524752 -->\n",
       "<g id=\"node155\" class=\"node\">\n",
       "<title>1759433524752</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1620,-330 1532,-330 1532,-310 1620,-310 1620,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-316.5\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433524752&#45;&gt;1759433524464 -->\n",
       "<g id=\"edge166\" class=\"edge\">\n",
       "<title>1759433524752&#45;&gt;1759433524464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-309.62C1576,-301.56 1576,-289.65 1576,-279.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-279.63 1576,-269.63 1572.5,-279.63 1579.5,-279.63\"/>\n",
       "</g>\n",
       "<!-- 1759433540896 -->\n",
       "<g id=\"node156\" class=\"node\">\n",
       "<title>1759433540896</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1659,-392 1493,-392 1493,-372 1659,-372 1659,-392\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-378.5\" font-family=\"monospace\" font-size=\"10.00\">UpsampleLinear1DBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433540896&#45;&gt;1759433524752 -->\n",
       "<g id=\"edge167\" class=\"edge\">\n",
       "<title>1759433540896&#45;&gt;1759433524752</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-371.62C1576,-363.56 1576,-351.65 1576,-341.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-341.63 1576,-331.63 1572.5,-341.63 1579.5,-341.63\"/>\n",
       "</g>\n",
       "<!-- 1759433539648 -->\n",
       "<g id=\"node157\" class=\"node\">\n",
       "<title>1759433539648</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1656,-448 1496,-448 1496,-428 1656,-428 1656,-448\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">NativeBatchNormBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433539648&#45;&gt;1759433540896 -->\n",
       "<g id=\"edge168\" class=\"edge\">\n",
       "<title>1759433539648&#45;&gt;1759433540896</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-427.59C1576,-421.01 1576,-411.96 1576,-403.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-403.81 1576,-393.81 1572.5,-403.81 1579.5,-403.81\"/>\n",
       "</g>\n",
       "<!-- 1759433541328 -->\n",
       "<g id=\"node158\" class=\"node\">\n",
       "<title>1759433541328</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1623,-504 1529,-504 1529,-484 1623,-484 1623,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">ReluBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541328&#45;&gt;1759433539648 -->\n",
       "<g id=\"edge169\" class=\"edge\">\n",
       "<title>1759433541328&#45;&gt;1759433539648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-483.59C1576,-477.01 1576,-467.96 1576,-459.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-459.81 1576,-449.81 1572.5,-459.81 1579.5,-459.81\"/>\n",
       "</g>\n",
       "<!-- 1759433541424 -->\n",
       "<g id=\"node159\" class=\"node\">\n",
       "<title>1759433541424</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1644,-566 1508,-566 1508,-546 1644,-546 1644,-566\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-552.5\" font-family=\"monospace\" font-size=\"10.00\">ConvolutionBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541424&#45;&gt;1759433541328 -->\n",
       "<g id=\"edge170\" class=\"edge\">\n",
       "<title>1759433541424&#45;&gt;1759433541328</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-545.62C1576,-537.56 1576,-525.65 1576,-515.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-515.63 1576,-505.63 1572.5,-515.63 1579.5,-515.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541520 -->\n",
       "<g id=\"node160\" class=\"node\">\n",
       "<title>1759433541520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1632,-628 1520,-628 1520,-608 1632,-608 1632,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward1</text>\n",
       "</g>\n",
       "<!-- 1759433541520&#45;&gt;1759433541424 -->\n",
       "<g id=\"edge171\" class=\"edge\">\n",
       "<title>1759433541520&#45;&gt;1759433541424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-607.62C1576,-599.56 1576,-587.65 1576,-577.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-577.63 1576,-567.63 1572.5,-577.63 1579.5,-577.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541616 -->\n",
       "<g id=\"node161\" class=\"node\">\n",
       "<title>1759433541616</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1638,-690 1514,-690 1514,-670 1638,-670 1638,-690\"/>\n",
       "<text text-anchor=\"middle\" x=\"1576\" y=\"-676.5\" font-family=\"monospace\" font-size=\"10.00\">AvgPool2DBackward0</text>\n",
       "</g>\n",
       "<!-- 1759433541616&#45;&gt;1759433541520 -->\n",
       "<g id=\"edge172\" class=\"edge\">\n",
       "<title>1759433541616&#45;&gt;1759433541520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1576,-669.62C1576,-661.56 1576,-649.65 1576,-639.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1579.5,-639.63 1576,-629.63 1572.5,-639.63 1579.5,-639.63\"/>\n",
       "</g>\n",
       "<!-- 1759433541712&#45;&gt;1759433541616 -->\n",
       "<g id=\"edge173\" class=\"edge\">\n",
       "<title>1759433541712&#45;&gt;1759433541616</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1532.97,-731.62C1540.42,-722.86 1551.75,-709.53 1560.91,-698.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1563.46,-701.15 1567.27,-691.27 1558.13,-696.62 1563.46,-701.15\"/>\n",
       "</g>\n",
       "<!-- 1759433496032 -->\n",
       "<g id=\"node163\" class=\"node\">\n",
       "<title>1759433496032</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1759,-628 1659,-628 1659,-608 1759,-608 1759,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1709\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433496032&#45;&gt;1759433541424 -->\n",
       "<g id=\"edge175\" class=\"edge\">\n",
       "<title>1759433496032&#45;&gt;1759433541424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1688.22,-607.62C1666.45,-597.81 1632.02,-582.27 1607.04,-571\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1608.57,-567.85 1598.01,-566.93 1605.69,-574.23 1608.57,-567.85\"/>\n",
       "</g>\n",
       "<!-- 1759421856640 -->\n",
       "<g id=\"node164\" class=\"node\">\n",
       "<title>1759421856640</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1780,-696 1656,-696 1656,-664 1780,-664 1780,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64, 320, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421856640&#45;&gt;1759433496032 -->\n",
       "<g id=\"edge176\" class=\"edge\">\n",
       "<title>1759421856640&#45;&gt;1759433496032</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1715.68,-663.55C1714.59,-656.26 1713.27,-647.45 1712.09,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1715.58,-639.27 1710.64,-629.9 1708.66,-640.31 1715.58,-639.27\"/>\n",
       "</g>\n",
       "<!-- 1759433495888 -->\n",
       "<g id=\"node165\" class=\"node\">\n",
       "<title>1759433495888</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1496,-628 1396,-628 1396,-608 1496,-608 1496,-628\"/>\n",
       "<text text-anchor=\"middle\" x=\"1446\" y=\"-614.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495888&#45;&gt;1759433541424 -->\n",
       "<g id=\"edge177\" class=\"edge\">\n",
       "<title>1759433495888&#45;&gt;1759433541424</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1466.31,-607.62C1487.59,-597.81 1521.24,-582.27 1545.66,-571\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1546.84,-574.31 1554.46,-566.94 1543.91,-567.96 1546.84,-574.31\"/>\n",
       "</g>\n",
       "<!-- 1759421856720 -->\n",
       "<g id=\"node166\" class=\"node\">\n",
       "<title>1759421856720</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1496,-696 1384,-696 1384,-664 1496,-664 1496,-696\"/>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-682.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.conv.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-670.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856720&#45;&gt;1759433495888 -->\n",
       "<g id=\"edge178\" class=\"edge\">\n",
       "<title>1759421856720&#45;&gt;1759433495888</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1441.55,-663.55C1442.27,-656.26 1443.15,-647.45 1443.94,-639.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1447.4,-640.21 1444.91,-629.91 1440.43,-639.52 1447.4,-640.21\"/>\n",
       "</g>\n",
       "<!-- 1759433495696 -->\n",
       "<g id=\"node167\" class=\"node\">\n",
       "<title>1759433495696</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1754,-504 1654,-504 1654,-484 1754,-484 1754,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1704\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433495696&#45;&gt;1759433539648 -->\n",
       "<g id=\"edge179\" class=\"edge\">\n",
       "<title>1759433495696&#45;&gt;1759433539648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1681.7,-483.59C1661.64,-475.13 1631.88,-462.58 1609.05,-452.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1610.44,-449.73 1599.86,-449.07 1607.72,-456.18 1610.44,-449.73\"/>\n",
       "</g>\n",
       "<!-- 1759421856800 -->\n",
       "<g id=\"node168\" class=\"node\">\n",
       "<title>1759421856800</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1774,-572 1662,-572 1662,-540 1774,-540 1774,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.bn.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1718\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856800&#45;&gt;1759433495696 -->\n",
       "<g id=\"edge180\" class=\"edge\">\n",
       "<title>1759421856800&#45;&gt;1759433495696</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1714.39,-539.55C1712.69,-532.26 1710.64,-523.45 1708.81,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1712.22,-514.83 1706.54,-505.88 1705.4,-516.42 1712.22,-514.83\"/>\n",
       "</g>\n",
       "<!-- 1759433494784 -->\n",
       "<g id=\"node169\" class=\"node\">\n",
       "<title>1759433494784</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1501,-504 1401,-504 1401,-484 1501,-484 1501,-504\"/>\n",
       "<text text-anchor=\"middle\" x=\"1451\" y=\"-490.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433494784&#45;&gt;1759433539648 -->\n",
       "<g id=\"edge181\" class=\"edge\">\n",
       "<title>1759433494784&#45;&gt;1759433539648</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1472.77,-483.59C1492.37,-475.13 1521.43,-462.58 1543.72,-452.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1544.88,-456.26 1552.67,-449.08 1542.1,-449.83 1544.88,-456.26\"/>\n",
       "</g>\n",
       "<!-- 1759421856880 -->\n",
       "<g id=\"node170\" class=\"node\">\n",
       "<title>1759421856880</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1490,-572 1390,-572 1390,-540 1490,-540 1490,-572\"/>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-558.5\" font-family=\"monospace\" font-size=\"10.00\">tpool7.bn.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1440\" y=\"-546.5\" font-family=\"monospace\" font-size=\"10.00\"> (64)</text>\n",
       "</g>\n",
       "<!-- 1759421856880&#45;&gt;1759433494784 -->\n",
       "<g id=\"edge182\" class=\"edge\">\n",
       "<title>1759421856880&#45;&gt;1759433494784</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1442.83,-539.55C1444.17,-532.26 1445.78,-523.45 1447.22,-515.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1450.64,-516.36 1449,-505.9 1443.76,-515.1 1450.64,-516.36\"/>\n",
       "</g>\n",
       "<!-- 1759433293680 -->\n",
       "<g id=\"node171\" class=\"node\">\n",
       "<title>1759433293680</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1950,-268 1850,-268 1850,-248 1950,-248 1950,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"1900\" y=\"-254.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433293680&#45;&gt;1759433524320 -->\n",
       "<g id=\"edge183\" class=\"edge\">\n",
       "<title>1759433293680&#45;&gt;1759433524320</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1857.83,-247.56C1811.24,-237.13 1736.1,-220.31 1685.42,-208.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1686.37,-205.58 1675.84,-206.82 1684.84,-212.42 1686.37,-205.58\"/>\n",
       "</g>\n",
       "<!-- 1759421857200 -->\n",
       "<g id=\"node172\" class=\"node\">\n",
       "<title>1759421857200</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"2178,-336 2048,-336 2048,-304 2178,-304 2178,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"2113\" y=\"-322.5\" font-family=\"monospace\" font-size=\"10.00\">decoder.conv.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"2113\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\"> (768, 32, 8)</text>\n",
       "</g>\n",
       "<!-- 1759421857200&#45;&gt;1759433293680 -->\n",
       "<g id=\"edge184\" class=\"edge\">\n",
       "<title>1759421857200&#45;&gt;1759433293680</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2058.15,-303.55C2022.82,-293.6 1977.49,-280.83 1944.42,-271.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1945.74,-268.25 1935.17,-268.91 1943.85,-274.99 1945.74,-268.25\"/>\n",
       "</g>\n",
       "<!-- 1759433293776 -->\n",
       "<g id=\"node173\" class=\"node\">\n",
       "<title>1759433293776</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1827,-144 1727,-144 1727,-124 1827,-124 1827,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433293776&#45;&gt;1759433524176 -->\n",
       "<g id=\"edge185\" class=\"edge\">\n",
       "<title>1759433293776&#45;&gt;1759433524176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777,-123.59C1777,-117.01 1777,-107.96 1777,-99.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1780.5,-99.81 1777,-89.81 1773.5,-99.81 1780.5,-99.81\"/>\n",
       "</g>\n",
       "<!-- 1759421857680 -->\n",
       "<g id=\"node174\" class=\"node\">\n",
       "<title>1759421857680</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1836,-212 1718,-212 1718,-180 1836,-180 1836,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-198.5\" font-family=\"monospace\" font-size=\"10.00\">activation.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"1777\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\"> (3, 32, 1)</text>\n",
       "</g>\n",
       "<!-- 1759421857680&#45;&gt;1759433293776 -->\n",
       "<g id=\"edge186\" class=\"edge\">\n",
       "<title>1759421857680&#45;&gt;1759433293776</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777,-179.55C1777,-172.34 1777,-163.66 1777,-155.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1780.5,-155.92 1777,-145.92 1773.5,-155.92 1780.5,-155.92\"/>\n",
       "</g>\n",
       "<!-- 1759433292768 -->\n",
       "<g id=\"node175\" class=\"node\">\n",
       "<title>1759433292768</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"1951,-144 1851,-144 1851,-124 1951,-124 1951,-144\"/>\n",
       "<text text-anchor=\"middle\" x=\"1901\" y=\"-130.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1759433292768&#45;&gt;1759433524176 -->\n",
       "<g id=\"edge187\" class=\"edge\">\n",
       "<title>1759433292768&#45;&gt;1759433524176</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1879.4,-123.59C1860.05,-115.17 1831.4,-102.69 1809.32,-93.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1810.72,-89.87 1800.16,-89.08 1807.93,-96.29 1810.72,-89.87\"/>\n",
       "</g>\n",
       "<!-- 1759421956160 -->\n",
       "<g id=\"node176\" class=\"node\">\n",
       "<title>1759421956160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"1960,-212 1854,-212 1854,-180 1960,-180 1960,-212\"/>\n",
       "<text text-anchor=\"middle\" x=\"1907\" y=\"-198.5\" font-family=\"monospace\" font-size=\"10.00\">activation.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"1907\" y=\"-186.5\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n",
       "</g>\n",
       "<!-- 1759421956160&#45;&gt;1759433292768 -->\n",
       "<g id=\"edge188\" class=\"edge\">\n",
       "<title>1759421956160&#45;&gt;1759433292768</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1905.45,-179.55C1904.73,-172.26 1903.85,-163.45 1903.06,-155.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1906.57,-155.52 1902.09,-145.91 1899.6,-156.21 1906.57,-155.52\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x199a66903a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15ed3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # 用于跟踪训练过程中的训练损失\n",
    "    train_losses = []\n",
    "    # 用于跟踪训练过程中的验证损失\n",
    "    valid_losses = []\n",
    "    # 用于跟踪训练过程中的测试损失\n",
    "    test_losses = []\n",
    "    # 用于跟踪每个 epoch 的平均训练损失\n",
    "    avg_train_losses = []\n",
    "    # 用于跟踪每个 epoch 的平均验证损失\n",
    "    avg_valid_losses = [] \n",
    "    # 用于跟踪每个 epoch 的平均测试损失\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    "    # 初始化 early_stopping 对象\n",
    "    #patience = 10\n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # 训练模型 #\n",
    "        ###################\n",
    "        model.train() # 准备模型进行训练\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 清除所有优化变量的梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 反向传播：计算损失相对于模型参数的梯度\n",
    "            loss.backward()\n",
    "            # 执行单次优化步骤（参数更新）\n",
    "            optimizer.step()\n",
    "            # 记录训练损失\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # 验证模型 #\n",
    "        ######################\n",
    "        model.eval() # 准备模型进行评估\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 记录验证损失\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        ##################    \n",
    "        # 测试模型 #\n",
    "        ##################\n",
    "        model.eval() # 准备模型进行评估\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 记录测试损失\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # 打印训练/验证统计信息\n",
    "        # 计算一个 epoch 的平均损失\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 清空列表以跟踪下一个 epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # early_stopping 需要验证损失来检查是否有减少，\n",
    "        # 如果有减少，它将创建当前模型的检查点\n",
    "        #early_stopping(valid_loss, model)\n",
    "        #if (early_stopping.early_stop and (epoch > 80)):\n",
    "        #    break\n",
    "        \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'验证损失减少 ({min_loss:.6f} --> {valid_loss:.6f}). 正在保存模型...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # 加载具有最佳模型的最后一个检查点\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d6187cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # 用于跟踪训练过程中的训练损失\n",
    "    train_losses = []\n",
    "    # 用于跟踪训练过程中的验证损失\n",
    "    valid_losses = []\n",
    "    # 用于跟踪训练过程中的测试损失\n",
    "    test_losses = []\n",
    "    # 用于跟踪每个 epoch 的平均训练损失\n",
    "    avg_train_losses = []\n",
    "    # 用于跟踪每个 epoch 的平均验证损失\n",
    "    avg_valid_losses = [] \n",
    "    # 用于跟踪每个 epoch 的平均测试损失\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    "    # 初始化 early_stopping 对象\n",
    "    #patience = 10\n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # 训练模型 #\n",
    "        ###################\n",
    "        model.train() # 准备模型进行训练\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 清除所有优化变量的梯度\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 反向传播：计算损失相对于模型参数的梯度\n",
    "            loss.backward()\n",
    "            # 执行单次优化步骤（参数更新）\n",
    "            optimizer.step()\n",
    "            # 记录训练损失\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # 验证模型 #\n",
    "        ######################\n",
    "        model.eval() # 准备模型进行评估\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 记录验证损失\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        ##################    \n",
    "        # 测试模型 #\n",
    "        ##################\n",
    "        model.eval() # 准备模型进行评估\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # 前向传播：通过将输入传递给模型来计算预测输出\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # 计算损失\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # 记录测试损失\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # 打印训练/验证统计信息\n",
    "        # 计算一个 epoch 的平均损失\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 清空列表以跟踪下一个 epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # early_stopping 需要验证损失来检查是否有减少，\n",
    "        # 如果有减少，它将创建当前模型的检查点\n",
    "        #early_stopping(valid_loss, model)\n",
    "        #if (early_stopping.early_stop and (epoch > 80)):\n",
    "        #    break\n",
    "        \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'验证损失减少 ({min_loss:.6f} --> {valid_loss:.6f}). 正在保存模型...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # 加载具有最佳模型的最后一个检查点\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bef96b",
   "metadata": {},
   "source": [
    "这段代码定义了一个训练模型的函数train_model，用于训练神经网络模型。在训练过程中，该函数会迭代多个epoch，每个epoch会对训练集、验证集和测试集进行一次前向传播和反向传播，并计算损失值。在每个epoch结束时，该函数会输出该epoch的平均训练损失、平均验证损失和平均测试损失，并将最好的模型保存下来。该函数的返回值包括训练好的模型和每个epoch的平均损失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef2c3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_activation(model, loader, a):\n",
    "    x_true = []\n",
    "    s_true = []\n",
    "    p_true = []\n",
    "    s_hat = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, p, s in loader:\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p = p.permute(0,2,1)[:,a,:]\n",
    "            s = s.permute(0,2,1)[:,a,:]\n",
    "            \n",
    "            sh = model(x)\n",
    "            sh = torch.sigmoid(sh[:,a,:])\n",
    "            \n",
    "            s_hat.append(sh.contiguous().view(-1).detach().cpu().numpy())\n",
    "            \n",
    "            x_true.append(x[:,:,BORDER:-BORDER].contiguous().view(-1).detach().cpu().numpy())\n",
    "            s_true.append(s.contiguous().view(-1).detach().cpu().numpy())\n",
    "            p_true.append(p.contiguous().view(-1).detach().cpu().numpy())\n",
    "    x_true = np.hstack(x_true)\n",
    "    s_true = np.hstack(s_true)\n",
    "    p_true = np.hstack(p_true)\n",
    "    s_hat = np.hstack(s_hat)\n",
    "\n",
    "    return x_true, p_true, s_true, s_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c4476",
   "metadata": {},
   "source": [
    "这段代码是一个函数，用于评估模型在给定数据集上的激活结果。\n",
    "\n",
    "首先，在函数内部定义了四个空列表：x_true、s_true、p_true和s_hat，用于存储计算结果。\n",
    "\n",
    "接下来，将模型设置为评估模式，即model.eval()。\n",
    "\n",
    "然后，通过一个循环遍历数据加载器loader中的每个批次。在每个批次中，执行以下操作：\n",
    "\n",
    "将输入数据x从(batch_size, sequence_length)的形状转换为(batch_size, 1, sequence_length)的形状，并将其移到GPU上（假设有可用的CUDA设备）。\n",
    "使用permute函数对位置编码p和状态s进行维度转换，将其从(batch_size, num_features, sequence_length)的形状转换为(batch_size, sequence_length)的形状，并选择第a个特征。\n",
    "将处理后的输入数据x输入到模型中，得到输出sh。使用sigmoid函数对输出进行激活，只选择第a个特征。\n",
    "将激活结果sh展平并转换为NumPy数组，然后将其添加到s_hat列表中。\n",
    "对输入数据x、位置编码p和状态s进行类似的处理，将它们展平并转换为NumPy数组，然后分别添加到x_true、s_true和p_true列表中。\n",
    "在循环结束后，将x_true、s_true、p_true和s_hat列表中的所有元素堆叠起来，得到最终的结果x_true、p_true、s_true和s_hat。\n",
    "\n",
    "最后，将这四个结果作为函数的返回值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020043a",
   "metadata": {},
   "source": [
    "x_true: 包含了模型输入数据经过处理后的真实值。在该代码中，x_true 是一个一维数组，保存了经过处理后的输入数据 x 的值。\n",
    "\n",
    "p_true: 包含了模型在数据加载器上对应位置的真实标签。在该代码中，p_true 是一个一维数组，保存了真实的标签值 p。\n",
    "\n",
    "s_true: 包含了模型在数据加载器上对应位置的真实输出值。在该代码中，s_true 是一个一维数组，保存了真实的输出值 s。\n",
    "\n",
    "s_hat: 包含了模型在数据加载器上对应位置的预测输出值。在该代码中，s_hat 是一个一维数组，保存了模型对应位置的预测输出值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442b12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLIANCE = ['fridge', 'dish_washer', 'washing_machine']\n",
    "THRESHOLD = [50., 10., 20.]\n",
    "MIN_ON = [1., 30., 30.]\n",
    "MIN_OFF = [1., 30., 3.]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*2   #输出时间序列长度1440\n",
    "BORDER = 16   #边界宽度\n",
    "#输入为SEQ_LEN+BORDER*2-2=1470\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "MAX_POWER = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "327f609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            aggregate  kettle     fridge  washing_machine  \\\n",
      "datetime                                                                    \n",
      "2013-04-12 00:00:00+00:00  166.416672     0.0   0.000000              0.0   \n",
      "2013-04-12 00:01:00+00:00  166.816666     0.0   0.000000              0.0   \n",
      "2013-04-12 00:02:00+00:00  168.083328     0.0   0.000000              0.0   \n",
      "2013-04-12 00:03:00+00:00  167.199997     0.0   0.000000              0.0   \n",
      "2013-04-12 00:04:00+00:00  167.383331     0.0   0.000000              0.0   \n",
      "...                               ...     ...        ...              ...   \n",
      "2014-12-14 23:56:00+00:00  210.449997     0.0  78.066666              0.0   \n",
      "2014-12-14 23:57:00+00:00  208.533340     0.0  79.699997              0.0   \n",
      "2014-12-14 23:58:00+00:00  209.333328     0.0  76.900002              0.0   \n",
      "2014-12-14 23:59:00+00:00  186.516663     0.0  50.049999              0.0   \n",
      "2014-12-15 00:00:00+00:00  165.399994     0.0   0.000000              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-04-12 00:00:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:01:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:02:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:03:00+00:00        0.0          0.0  \n",
      "2013-04-12 00:04:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2014-12-14 23:56:00+00:00        0.0          0.0  \n",
      "2014-12-14 23:57:00+00:00        0.0          0.0  \n",
      "2014-12-14 23:58:00+00:00        0.0          0.0  \n",
      "2014-12-14 23:59:00+00:00        0.0          0.0  \n",
      "2014-12-15 00:00:00+00:00        0.0          0.0  \n",
      "\n",
      "[881281 rows x 6 columns]\n",
      "                            aggregate  kettle     fridge  washing_machine  \\\n",
      "datetime                                                                    \n",
      "2013-05-22 00:00:00+00:00  198.866669     0.0  94.283333              0.0   \n",
      "2013-05-22 00:01:00+00:00  196.866669     0.0  91.216667              0.0   \n",
      "2013-05-22 00:02:00+00:00  195.016663     0.0  89.433334              0.0   \n",
      "2013-05-22 00:03:00+00:00  193.600006     0.0  88.250000              0.0   \n",
      "2013-05-22 00:04:00+00:00  192.983337     0.0  86.866669              0.0   \n",
      "...                               ...     ...        ...              ...   \n",
      "2013-10-03 06:12:00+00:00  131.350006     0.0  10.100000              0.0   \n",
      "2013-10-03 06:13:00+00:00  131.050003     0.0  10.000000              0.0   \n",
      "2013-10-03 06:14:00+00:00  130.050003     0.0  10.000000              0.0   \n",
      "2013-10-03 06:15:00+00:00  130.100006     0.0  10.000000              0.0   \n",
      "2013-10-03 06:16:00+00:00  129.616669     0.0  10.000000              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-05-22 00:00:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:01:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:02:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:03:00+00:00        0.0          0.0  \n",
      "2013-05-22 00:04:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2013-10-03 06:12:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:13:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:14:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:15:00+00:00        0.0          0.0  \n",
      "2013-10-03 06:16:00+00:00        0.0          0.0  \n",
      "\n",
      "[193337 rows x 6 columns]\n",
      "                            aggregate  kettle  fridge  washing_machine  \\\n",
      "datetime                                                                 \n",
      "2013-02-27 20:35:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:36:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:37:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:38:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "2013-02-27 20:39:00+00:00    0.000000     0.0     0.0              0.0   \n",
      "...                               ...     ...     ...              ...   \n",
      "2013-04-01 06:11:00+00:00  137.100006     0.0     0.0              0.0   \n",
      "2013-04-01 06:12:00+00:00  136.800003     0.0     0.0              0.0   \n",
      "2013-04-01 06:13:00+00:00  136.766663     0.0     0.0              0.0   \n",
      "2013-04-01 06:14:00+00:00  136.866669     0.0     0.0              0.0   \n",
      "2013-04-01 06:15:00+00:00  136.899994     0.0     0.0              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-02-27 20:35:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:36:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:37:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:38:00+00:00        0.0          0.0  \n",
      "2013-02-27 20:39:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2013-04-01 06:11:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:12:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:13:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:14:00+00:00        0.0          0.0  \n",
      "2013-04-01 06:15:00+00:00        0.0          0.0  \n",
      "\n",
      "[46661 rows x 6 columns]\n",
      "                             aggregate  kettle     fridge  washing_machine  \\\n",
      "datetime                                                                     \n",
      "2013-03-09 14:40:00+00:00   630.150940     0.0  98.000000              0.0   \n",
      "2013-03-09 14:41:00+00:00   628.833313     0.0  96.933334              0.0   \n",
      "2013-03-09 14:42:00+00:00  2171.550049     0.0  95.900002              0.0   \n",
      "2013-03-09 14:43:00+00:00  3000.566650     0.0  94.716667              0.0   \n",
      "2013-03-09 14:44:00+00:00  1205.650024     0.0  94.716667              0.0   \n",
      "...                                ...     ...        ...              ...   \n",
      "2013-09-24 06:11:00+00:00   572.299988     0.0   0.000000              0.0   \n",
      "2013-09-24 06:12:00+00:00   570.099976     0.0   0.000000              0.0   \n",
      "2013-09-24 06:13:00+00:00   626.700012     0.0   0.000000              0.0   \n",
      "2013-09-24 06:14:00+00:00   579.233337     0.0   0.000000              0.0   \n",
      "2013-09-24 06:15:00+00:00   589.366638     0.0   0.000000              0.0   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2013-03-09 14:40:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:41:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:42:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:43:00+00:00        0.0          0.0  \n",
      "2013-03-09 14:44:00+00:00        0.0          0.0  \n",
      "...                              ...          ...  \n",
      "2013-09-24 06:11:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:12:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:13:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:14:00+00:00        0.0          0.0  \n",
      "2013-09-24 06:15:00+00:00        0.0          0.0  \n",
      "\n",
      "[286056 rows x 6 columns]\n",
      "                             aggregate  kettle      fridge  washing_machine  \\\n",
      "datetime                                                                      \n",
      "2014-06-29 16:23:00+00:00   769.000000     0.0  106.800003       225.000000   \n",
      "2014-06-29 16:24:00+00:00  1026.633301     0.0  106.300003       389.633331   \n",
      "2014-06-29 16:25:00+00:00  2263.383301     0.0  106.000000      1312.466675   \n",
      "2014-06-29 16:26:00+00:00   957.500000     0.0  106.500000       235.699997   \n",
      "2014-06-29 16:27:00+00:00  1905.300049     0.0  105.300003      1058.266724   \n",
      "...                                ...     ...         ...              ...   \n",
      "2014-08-31 23:56:00+00:00   490.000000     0.0    0.000000        15.000000   \n",
      "2014-08-31 23:57:00+00:00   487.133331     0.0    0.000000        14.800000   \n",
      "2014-08-31 23:58:00+00:00   487.399994     0.0    0.000000        14.800000   \n",
      "2014-08-31 23:59:00+00:00   488.399994     0.0    0.000000        15.000000   \n",
      "2014-09-01 00:00:00+00:00   491.483337     0.0    0.000000        15.000000   \n",
      "\n",
      "                           microwave  dish_washer  \n",
      "datetime                                           \n",
      "2014-06-29 16:23:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:24:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:25:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:26:00+00:00   0.000000          0.0  \n",
      "2014-06-29 16:27:00+00:00   0.000000          0.0  \n",
      "...                              ...          ...  \n",
      "2014-08-31 23:56:00+00:00  50.700001          0.0  \n",
      "2014-08-31 23:57:00+00:00  50.400002          0.0  \n",
      "2014-08-31 23:58:00+00:00  50.299999          0.0  \n",
      "2014-08-31 23:59:00+00:00  50.299999          0.0  \n",
      "2014-09-01 00:00:00+00:00  50.000000          0.0  \n",
      "\n",
      "[91178 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather('./UKDALE_%d_train.feather' %(i+1))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    print(ds)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33033f6",
   "metadata": {},
   "source": [
    "这段代码首先定义了一些常量，包括家电的名称、阈值、最小开启时间和最小关闭时间，以及一些数据处理相关的参数。然后，通过循环读取5个训练数据集，并将其存储在ds_meter、ds_appliance和ds_status列表中。其中，ds_meter存储了总用电量数据，ds_appliance存储了各个家电的用电量数据，ds_status存储了家电的状态数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b421800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2013-04-12 00:00:00+00:00    166.416672\n",
       "2013-04-12 00:01:00+00:00    166.816666\n",
       "2013-04-12 00:02:00+00:00    168.083328\n",
       "2013-04-12 00:03:00+00:00    167.199997\n",
       "2013-04-12 00:04:00+00:00    167.383331\n",
       "                                ...    \n",
       "2014-12-14 23:56:00+00:00    210.449997\n",
       "2014-12-14 23:57:00+00:00    208.533340\n",
       "2014-12-14 23:58:00+00:00    209.333328\n",
       "2014-12-14 23:59:00+00:00    186.516663\n",
       "2014-12-15 00:00:00+00:00    165.399994\n",
       "Name: aggregate, Length: 881281, dtype: float32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_meter[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86da5191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = [\n",
    "    (i, value) for i, value in enumerate(ds_meter[0][:8000])\n",
    "]\n",
    "\n",
    "# 提取序列号和值\n",
    "indices = [d[0] for d in data]\n",
    "values = [d[1] for d in data]\n",
    "\n",
    "# 设置字体和大小\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 用于正确显示中文标签\n",
    "matplotlib.rcParams['font.size'] = 8  # 设置字体大小为8磅\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 用于正确显示负号\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'  # 设置英文字体为Times New Roman\n",
    "# 设置图表尺寸，以适应半栏A4宽度\n",
    "plt.figure(figsize=(4.13, 3.15))  # A4纸张半栏宽度约为4.13英寸\n",
    "\n",
    "# 创建折线图\n",
    "plt.plot(indices, values)\n",
    "\n",
    "\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('aggregate')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Power(W)')\n",
    "\n",
    "# 显示图形\n",
    "#plt.show()\n",
    "\n",
    "# 设置分辨率为600ppi\n",
    "dpi = 600\n",
    "\n",
    "# 设置图表尺寸，以英寸为单位\n",
    "#fig = plt.figure(figsize=(8, 6))  # 这里将宽度设置为8英寸，高度设置为6英寸，你可以根据需要进行调整\n",
    "\n",
    "# 选择文件路径和文件名，保存为PNG格式\n",
    "save_path = r'D:\\NILM\\小论文\\SKNILM\\绘图\\状态图\\image.png'\n",
    "\n",
    "\n",
    "# 导出图片\n",
    "plt.savefig(save_path, dpi=dpi)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b2ba155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 设置字体和大小\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 用于正确显示中文标签\n",
    "matplotlib.rcParams['font.size'] = 8  # 设置字体大小为8磅\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 用于正确显示负号\n",
    "matplotlib.rcParams['font.family'] = 'Times New Roman'  # 设置英文字体为Times New Roman\n",
    "\n",
    "# 设置图表尺寸，以适应半栏A4宽度\n",
    "plt.figure(figsize=(4.13, 3.15))  # A4纸张半栏宽度约为4.13英寸\n",
    "\n",
    "# 假设 ds_meter[0][:2000] 是您的数据源\n",
    "data = [(i, value) for i, value in enumerate(ds_meter[0][:2000])]\n",
    "\n",
    "# 提取序列号和值\n",
    "indices = [d[0] for d in data]\n",
    "values = [d[1] for d in data]\n",
    "\n",
    "# 创建黑白折线图，将颜色设置为黑色\n",
    "plt.plot(indices, values, color='black')\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('aggregate')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Power(W)')\n",
    "\n",
    "# 设置分辨率为600ppi\n",
    "dpi = 600\n",
    "\n",
    "# 选择文件路径和文件名，保存为PNG格式\n",
    "save_path = r'D:\\NILM\\小论文\\中文小论文\\投稿\\灰度图\\image.png'\n",
    "\n",
    "# 导出图片，设置为灰度格式\n",
    "plt.savefig(save_path, dpi=dpi, format='png', cmap='gray')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22735713",
   "metadata": {},
   "source": [
    "接下来，代码统计了ds_status[1]中状态变化为1的次数，并对ds_status[1]进行了描述性统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "846d93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_house_train = [Power(ds_meter[i][:int(0.8*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.8*ds_len[i])], \n",
    "                        ds_status[i][:int(0.8*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(5+0)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        ds_appliance[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])],\n",
    "                        ds_status[i][int(0.8*ds_len[i]):int(0.9*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_test  = [Power(ds_meter[i][int(0.9*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.9*ds_len[i]):],\n",
    "                        ds_status[i][int(0.9*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_train_seen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                ds_house_train[1], \n",
    "                                                #ds_house_train[2], \n",
    "                                                #ds_house_train[3],\n",
    "                                                ds_house_train[4]\n",
    "                                                ])\n",
    "ds_valid_seen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                #ds_house_valid[1], \n",
    "                                                #ds_house_valid[2], \n",
    "                                                #ds_house_valid[3], \n",
    "                                                #ds_house_valid[4]\n",
    "                                                ])\n",
    "\n",
    "dl_train_seen = DataLoader(dataset = ds_train_seen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_seen = DataLoader(dataset = ds_valid_seen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_seen = DataLoader(dataset = ds_house_test[0], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "ds_train_unseen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                  #ds_house_train[1], \n",
    "                                                  #ds_house_train[2], \n",
    "                                                  #ds_house_train[3], \n",
    "                                                  ds_house_train[4]\n",
    "                                                  ])\n",
    "ds_valid_unseen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                  #ds_house_valid[1], \n",
    "                                                  #ds_house_valid[2], \n",
    "                                                  #ds_house_valid[3], \n",
    "                                                  ds_house_valid[4]\n",
    "                                                  ])\n",
    "dl_train_unseen = DataLoader(dataset = ds_train_unseen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_unseen = DataLoader(dataset = ds_valid_unseen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_unseen = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_test[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b184433",
   "metadata": {},
   "source": [
    "这段代码定义了多个数据集和数据加载器，用于训练和测试神经网络模型。首先，代码根据数据集的长度将数据集分为训练集、验证集和测试集，并将每个家庭的数据存储在不同的数据集中。然后，代码使用`ConcatDataset`将不同家庭的数据集合并成一个数据集，并将其传递给`DataLoader`进行批量加载。其中，`dl_train_seen`和`dl_valid_seen`是已知家电的训练集和验证集数据加载器，`dl_test_seen`是已知家电的测试集数据加载器，`dl_train_unseen`和`dl_valid_unseen`是未知家电的训练集和验证集数据加载器，`dl_test_unseen`是未知家电的测试集数据加载器。\n",
    "\n",
    "最后，代码定义了多个用于单独加载每个家庭数据的数据加载器`dl_house_test`、`dl_house_valid`和`dl_house_total`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ff768b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/500] train_loss: 0.32598 valid_loss: 0.22979 test_loss: 0.23192 \n",
      "验证损失减少 (inf --> 0.229790). 正在保存模型...\n",
      "[  2/500] train_loss: 0.17772 valid_loss: 0.17494 test_loss: 0.17699 \n",
      "验证损失减少 (0.229790 --> 0.174944). 正在保存模型...\n",
      "[  3/500] train_loss: 0.14843 valid_loss: 0.15311 test_loss: 0.15612 \n",
      "验证损失减少 (0.174944 --> 0.153107). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13414 valid_loss: 0.13490 test_loss: 0.14057 \n",
      "验证损失减少 (0.153107 --> 0.134903). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12663 valid_loss: 0.12641 test_loss: 0.13319 \n",
      "验证损失减少 (0.134903 --> 0.126412). 正在保存模型...\n",
      "[  6/500] train_loss: 0.11950 valid_loss: 0.12356 test_loss: 0.13088 \n",
      "验证损失减少 (0.126412 --> 0.123557). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11629 valid_loss: 0.12083 test_loss: 0.12677 \n",
      "验证损失减少 (0.123557 --> 0.120835). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11430 valid_loss: 0.11659 test_loss: 0.12419 \n",
      "验证损失减少 (0.120835 --> 0.116589). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11037 valid_loss: 0.11376 test_loss: 0.12155 \n",
      "验证损失减少 (0.116589 --> 0.113756). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10950 valid_loss: 0.10944 test_loss: 0.11895 \n",
      "验证损失减少 (0.113756 --> 0.109444). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10399 valid_loss: 0.10727 test_loss: 0.11785 \n",
      "验证损失减少 (0.109444 --> 0.107275). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10722 valid_loss: 0.10895 test_loss: 0.11502 \n",
      "[ 13/500] train_loss: 0.10028 valid_loss: 0.10676 test_loss: 0.11406 \n",
      "验证损失减少 (0.107275 --> 0.106758). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10171 valid_loss: 0.10327 test_loss: 0.11350 \n",
      "验证损失减少 (0.106758 --> 0.103268). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10027 valid_loss: 0.10284 test_loss: 0.11096 \n",
      "验证损失减少 (0.103268 --> 0.102837). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09761 valid_loss: 0.10369 test_loss: 0.10946 \n",
      "[ 17/500] train_loss: 0.09392 valid_loss: 0.10018 test_loss: 0.10870 \n",
      "验证损失减少 (0.102837 --> 0.100181). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09524 valid_loss: 0.09730 test_loss: 0.10721 \n",
      "验证损失减少 (0.100181 --> 0.097299). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09440 valid_loss: 0.09790 test_loss: 0.10469 \n",
      "[ 20/500] train_loss: 0.09261 valid_loss: 0.09647 test_loss: 0.10471 \n",
      "验证损失减少 (0.097299 --> 0.096471). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09284 valid_loss: 0.09559 test_loss: 0.10554 \n",
      "验证损失减少 (0.096471 --> 0.095589). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09188 valid_loss: 0.10007 test_loss: 0.10295 \n",
      "[ 23/500] train_loss: 0.08834 valid_loss: 0.09483 test_loss: 0.10272 \n",
      "验证损失减少 (0.095589 --> 0.094829). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.08956 valid_loss: 0.09245 test_loss: 0.10051 \n",
      "验证损失减少 (0.094829 --> 0.092445). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.08865 valid_loss: 0.09358 test_loss: 0.09998 \n",
      "[ 26/500] train_loss: 0.08756 valid_loss: 0.09361 test_loss: 0.09879 \n",
      "[ 27/500] train_loss: 0.08889 valid_loss: 0.08952 test_loss: 0.09875 \n",
      "验证损失减少 (0.092445 --> 0.089522). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.09002 valid_loss: 0.08937 test_loss: 0.09782 \n",
      "验证损失减少 (0.089522 --> 0.089374). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08704 valid_loss: 0.08877 test_loss: 0.09671 \n",
      "验证损失减少 (0.089374 --> 0.088772). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08590 valid_loss: 0.08978 test_loss: 0.09664 \n",
      "[ 31/500] train_loss: 0.08434 valid_loss: 0.08829 test_loss: 0.09679 \n",
      "验证损失减少 (0.088772 --> 0.088294). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08672 valid_loss: 0.09039 test_loss: 0.09590 \n",
      "[ 33/500] train_loss: 0.08336 valid_loss: 0.08812 test_loss: 0.09513 \n",
      "验证损失减少 (0.088294 --> 0.088123). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08119 valid_loss: 0.08661 test_loss: 0.09424 \n",
      "验证损失减少 (0.088123 --> 0.086614). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08312 valid_loss: 0.08719 test_loss: 0.09470 \n",
      "[ 36/500] train_loss: 0.08069 valid_loss: 0.08651 test_loss: 0.09307 \n",
      "验证损失减少 (0.086614 --> 0.086511). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08114 valid_loss: 0.08497 test_loss: 0.09264 \n",
      "验证损失减少 (0.086511 --> 0.084973). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.08303 valid_loss: 0.08512 test_loss: 0.09146 \n",
      "[ 39/500] train_loss: 0.07984 valid_loss: 0.08434 test_loss: 0.09144 \n",
      "验证损失减少 (0.084973 --> 0.084340). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.08290 valid_loss: 0.08529 test_loss: 0.09275 \n",
      "[ 41/500] train_loss: 0.07983 valid_loss: 0.08419 test_loss: 0.09227 \n",
      "验证损失减少 (0.084340 --> 0.084194). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.08056 valid_loss: 0.08458 test_loss: 0.09222 \n",
      "[ 43/500] train_loss: 0.07829 valid_loss: 0.08333 test_loss: 0.09056 \n",
      "验证损失减少 (0.084194 --> 0.083330). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.07843 valid_loss: 0.08481 test_loss: 0.09026 \n",
      "[ 45/500] train_loss: 0.07885 valid_loss: 0.08573 test_loss: 0.09110 \n",
      "[ 46/500] train_loss: 0.07859 valid_loss: 0.08416 test_loss: 0.09094 \n",
      "[ 47/500] train_loss: 0.07885 valid_loss: 0.08385 test_loss: 0.08872 \n",
      "[ 48/500] train_loss: 0.07846 valid_loss: 0.08257 test_loss: 0.08946 \n",
      "验证损失减少 (0.083330 --> 0.082572). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07674 valid_loss: 0.08221 test_loss: 0.08876 \n",
      "验证损失减少 (0.082572 --> 0.082213). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07873 valid_loss: 0.08397 test_loss: 0.09026 \n",
      "[ 51/500] train_loss: 0.07609 valid_loss: 0.08175 test_loss: 0.08831 \n",
      "验证损失减少 (0.082213 --> 0.081747). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07602 valid_loss: 0.08230 test_loss: 0.08860 \n",
      "[ 53/500] train_loss: 0.07834 valid_loss: 0.08082 test_loss: 0.08884 \n",
      "验证损失减少 (0.081747 --> 0.080825). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07681 valid_loss: 0.08106 test_loss: 0.08941 \n",
      "[ 55/500] train_loss: 0.07694 valid_loss: 0.08184 test_loss: 0.08816 \n",
      "[ 56/500] train_loss: 0.07246 valid_loss: 0.08122 test_loss: 0.08840 \n",
      "[ 57/500] train_loss: 0.07413 valid_loss: 0.07997 test_loss: 0.08861 \n",
      "验证损失减少 (0.080825 --> 0.079971). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.07520 valid_loss: 0.08072 test_loss: 0.08684 \n",
      "[ 59/500] train_loss: 0.07431 valid_loss: 0.08192 test_loss: 0.08788 \n",
      "[ 60/500] train_loss: 0.07516 valid_loss: 0.07981 test_loss: 0.08899 \n",
      "验证损失减少 (0.079971 --> 0.079812). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.07363 valid_loss: 0.08045 test_loss: 0.08754 \n",
      "[ 62/500] train_loss: 0.07222 valid_loss: 0.08042 test_loss: 0.08699 \n",
      "[ 63/500] train_loss: 0.07424 valid_loss: 0.08014 test_loss: 0.08549 \n",
      "[ 64/500] train_loss: 0.07425 valid_loss: 0.07881 test_loss: 0.08618 \n",
      "验证损失减少 (0.079812 --> 0.078806). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07249 valid_loss: 0.07846 test_loss: 0.08599 \n",
      "验证损失减少 (0.078806 --> 0.078461). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.07197 valid_loss: 0.08032 test_loss: 0.08707 \n",
      "[ 67/500] train_loss: 0.07237 valid_loss: 0.07916 test_loss: 0.08716 \n",
      "[ 68/500] train_loss: 0.07233 valid_loss: 0.07874 test_loss: 0.08587 \n",
      "[ 69/500] train_loss: 0.07212 valid_loss: 0.07746 test_loss: 0.08432 \n",
      "验证损失减少 (0.078461 --> 0.077464). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.07216 valid_loss: 0.08054 test_loss: 0.08670 \n",
      "[ 71/500] train_loss: 0.07007 valid_loss: 0.08001 test_loss: 0.08526 \n",
      "[ 72/500] train_loss: 0.07134 valid_loss: 0.07975 test_loss: 0.08770 \n",
      "[ 73/500] train_loss: 0.07084 valid_loss: 0.07961 test_loss: 0.08798 \n",
      "[ 74/500] train_loss: 0.07123 valid_loss: 0.07800 test_loss: 0.08581 \n",
      "[ 75/500] train_loss: 0.07101 valid_loss: 0.07712 test_loss: 0.08422 \n",
      "验证损失减少 (0.077464 --> 0.077119). 正在保存模型...\n",
      "[ 76/500] train_loss: 0.06997 valid_loss: 0.07719 test_loss: 0.08499 \n",
      "[ 77/500] train_loss: 0.07126 valid_loss: 0.07625 test_loss: 0.08565 \n",
      "验证损失减少 (0.077119 --> 0.076253). 正在保存模型...\n",
      "[ 78/500] train_loss: 0.06827 valid_loss: 0.07735 test_loss: 0.08517 \n",
      "[ 79/500] train_loss: 0.07106 valid_loss: 0.07824 test_loss: 0.08534 \n",
      "[ 80/500] train_loss: 0.06786 valid_loss: 0.07674 test_loss: 0.08411 \n",
      "[ 81/500] train_loss: 0.06948 valid_loss: 0.07756 test_loss: 0.08414 \n",
      "[ 82/500] train_loss: 0.07040 valid_loss: 0.07809 test_loss: 0.08514 \n",
      "[ 83/500] train_loss: 0.06918 valid_loss: 0.07860 test_loss: 0.08580 \n",
      "[ 84/500] train_loss: 0.06788 valid_loss: 0.07874 test_loss: 0.08469 \n",
      "[ 85/500] train_loss: 0.06853 valid_loss: 0.07835 test_loss: 0.08404 \n",
      "[ 86/500] train_loss: 0.06914 valid_loss: 0.07830 test_loss: 0.08339 \n",
      "[ 87/500] train_loss: 0.06844 valid_loss: 0.07660 test_loss: 0.08471 \n",
      "[ 88/500] train_loss: 0.06791 valid_loss: 0.07727 test_loss: 0.08333 \n",
      "[ 89/500] train_loss: 0.06696 valid_loss: 0.07878 test_loss: 0.08417 \n",
      "[ 90/500] train_loss: 0.06771 valid_loss: 0.07914 test_loss: 0.08425 \n",
      "[ 91/500] train_loss: 0.06699 valid_loss: 0.07559 test_loss: 0.08380 \n",
      "验证损失减少 (0.076253 --> 0.075594). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.06864 valid_loss: 0.07637 test_loss: 0.08294 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 93/500] train_loss: 0.06840 valid_loss: 0.07623 test_loss: 0.08362 \n",
      "[ 94/500] train_loss: 0.06703 valid_loss: 0.07561 test_loss: 0.08320 \n",
      "[ 95/500] train_loss: 0.06617 valid_loss: 0.07824 test_loss: 0.08444 \n",
      "[ 96/500] train_loss: 0.06481 valid_loss: 0.07787 test_loss: 0.08443 \n",
      "[ 97/500] train_loss: 0.06540 valid_loss: 0.07629 test_loss: 0.08237 \n",
      "[ 98/500] train_loss: 0.06830 valid_loss: 0.07695 test_loss: 0.08205 \n",
      "[ 99/500] train_loss: 0.06736 valid_loss: 0.07583 test_loss: 0.08176 \n",
      "[100/500] train_loss: 0.06677 valid_loss: 0.07982 test_loss: 0.08602 \n",
      "[101/500] train_loss: 0.06430 valid_loss: 0.07760 test_loss: 0.08295 \n",
      "[102/500] train_loss: 0.06642 valid_loss: 0.07691 test_loss: 0.08282 \n",
      "[103/500] train_loss: 0.06529 valid_loss: 0.07516 test_loss: 0.08206 \n",
      "验证损失减少 (0.075594 --> 0.075162). 正在保存模型...\n",
      "[104/500] train_loss: 0.06550 valid_loss: 0.07723 test_loss: 0.08295 \n",
      "[105/500] train_loss: 0.06464 valid_loss: 0.07645 test_loss: 0.08314 \n",
      "[106/500] train_loss: 0.06653 valid_loss: 0.07652 test_loss: 0.08343 \n",
      "[107/500] train_loss: 0.06510 valid_loss: 0.07440 test_loss: 0.08210 \n",
      "验证损失减少 (0.075162 --> 0.074399). 正在保存模型...\n",
      "[108/500] train_loss: 0.06558 valid_loss: 0.07759 test_loss: 0.08277 \n",
      "[109/500] train_loss: 0.06491 valid_loss: 0.07571 test_loss: 0.08321 \n",
      "[110/500] train_loss: 0.06491 valid_loss: 0.07654 test_loss: 0.08262 \n",
      "[111/500] train_loss: 0.06441 valid_loss: 0.07678 test_loss: 0.08243 \n",
      "[112/500] train_loss: 0.06427 valid_loss: 0.07819 test_loss: 0.08359 \n",
      "[113/500] train_loss: 0.06469 valid_loss: 0.07559 test_loss: 0.08178 \n",
      "[114/500] train_loss: 0.06238 valid_loss: 0.07475 test_loss: 0.08157 \n",
      "[115/500] train_loss: 0.06464 valid_loss: 0.07465 test_loss: 0.08230 \n",
      "[116/500] train_loss: 0.06398 valid_loss: 0.07790 test_loss: 0.08259 \n",
      "[117/500] train_loss: 0.06441 valid_loss: 0.07507 test_loss: 0.08219 \n",
      "[118/500] train_loss: 0.06398 valid_loss: 0.07472 test_loss: 0.08155 \n",
      "[119/500] train_loss: 0.06543 valid_loss: 0.07366 test_loss: 0.08126 \n",
      "验证损失减少 (0.074399 --> 0.073660). 正在保存模型...\n",
      "[120/500] train_loss: 0.06292 valid_loss: 0.07587 test_loss: 0.08347 \n",
      "[121/500] train_loss: 0.06175 valid_loss: 0.07506 test_loss: 0.08365 \n",
      "[122/500] train_loss: 0.06502 valid_loss: 0.07653 test_loss: 0.08237 \n",
      "[123/500] train_loss: 0.06480 valid_loss: 0.07460 test_loss: 0.08155 \n",
      "[124/500] train_loss: 0.06311 valid_loss: 0.07487 test_loss: 0.08211 \n",
      "[125/500] train_loss: 0.06277 valid_loss: 0.07450 test_loss: 0.08190 \n",
      "[126/500] train_loss: 0.06221 valid_loss: 0.07482 test_loss: 0.08111 \n",
      "[127/500] train_loss: 0.06319 valid_loss: 0.07370 test_loss: 0.08250 \n",
      "[128/500] train_loss: 0.06198 valid_loss: 0.07395 test_loss: 0.08198 \n",
      "[129/500] train_loss: 0.05879 valid_loss: 0.07501 test_loss: 0.08079 \n",
      "[130/500] train_loss: 0.06315 valid_loss: 0.07479 test_loss: 0.08303 \n",
      "[131/500] train_loss: 0.06165 valid_loss: 0.07409 test_loss: 0.08138 \n",
      "[132/500] train_loss: 0.06281 valid_loss: 0.07373 test_loss: 0.08146 \n",
      "[133/500] train_loss: 0.06146 valid_loss: 0.07401 test_loss: 0.08158 \n",
      "[134/500] train_loss: 0.06034 valid_loss: 0.07417 test_loss: 0.08173 \n",
      "[135/500] train_loss: 0.06164 valid_loss: 0.07558 test_loss: 0.08121 \n",
      "[136/500] train_loss: 0.06110 valid_loss: 0.07517 test_loss: 0.08285 \n",
      "[137/500] train_loss: 0.06248 valid_loss: 0.07467 test_loss: 0.08137 \n",
      "[138/500] train_loss: 0.06152 valid_loss: 0.07281 test_loss: 0.08050 \n",
      "验证损失减少 (0.073660 --> 0.072811). 正在保存模型...\n",
      "[139/500] train_loss: 0.06162 valid_loss: 0.07348 test_loss: 0.08073 \n",
      "[140/500] train_loss: 0.06215 valid_loss: 0.07303 test_loss: 0.08024 \n",
      "[141/500] train_loss: 0.06009 valid_loss: 0.07412 test_loss: 0.08095 \n",
      "[142/500] train_loss: 0.06086 valid_loss: 0.07352 test_loss: 0.08181 \n",
      "[143/500] train_loss: 0.06297 valid_loss: 0.07462 test_loss: 0.08202 \n",
      "[144/500] train_loss: 0.05924 valid_loss: 0.07254 test_loss: 0.08089 \n",
      "验证损失减少 (0.072811 --> 0.072544). 正在保存模型...\n",
      "[145/500] train_loss: 0.05992 valid_loss: 0.07260 test_loss: 0.08120 \n",
      "[146/500] train_loss: 0.06016 valid_loss: 0.07282 test_loss: 0.08023 \n",
      "[147/500] train_loss: 0.05963 valid_loss: 0.07285 test_loss: 0.08161 \n",
      "[148/500] train_loss: 0.05952 valid_loss: 0.07225 test_loss: 0.08101 \n",
      "验证损失减少 (0.072544 --> 0.072247). 正在保存模型...\n",
      "[149/500] train_loss: 0.05997 valid_loss: 0.07385 test_loss: 0.08126 \n",
      "[150/500] train_loss: 0.06130 valid_loss: 0.07341 test_loss: 0.08156 \n",
      "[151/500] train_loss: 0.06092 valid_loss: 0.07307 test_loss: 0.08134 \n",
      "[152/500] train_loss: 0.05824 valid_loss: 0.07361 test_loss: 0.08107 \n",
      "[153/500] train_loss: 0.05956 valid_loss: 0.07625 test_loss: 0.08076 \n",
      "[154/500] train_loss: 0.06007 valid_loss: 0.07368 test_loss: 0.08221 \n",
      "[155/500] train_loss: 0.05949 valid_loss: 0.07319 test_loss: 0.08126 \n",
      "[156/500] train_loss: 0.05922 valid_loss: 0.07427 test_loss: 0.08130 \n",
      "[157/500] train_loss: 0.05847 valid_loss: 0.07394 test_loss: 0.07996 \n",
      "[158/500] train_loss: 0.06011 valid_loss: 0.07417 test_loss: 0.07993 \n",
      "[159/500] train_loss: 0.05957 valid_loss: 0.07196 test_loss: 0.08063 \n",
      "验证损失减少 (0.072247 --> 0.071962). 正在保存模型...\n",
      "[160/500] train_loss: 0.05803 valid_loss: 0.07350 test_loss: 0.08148 \n",
      "[161/500] train_loss: 0.05937 valid_loss: 0.07461 test_loss: 0.08167 \n",
      "[162/500] train_loss: 0.05762 valid_loss: 0.07215 test_loss: 0.08072 \n",
      "[163/500] train_loss: 0.05867 valid_loss: 0.07284 test_loss: 0.08121 \n",
      "[164/500] train_loss: 0.05797 valid_loss: 0.07256 test_loss: 0.08015 \n",
      "[165/500] train_loss: 0.05984 valid_loss: 0.07397 test_loss: 0.08042 \n",
      "[166/500] train_loss: 0.05853 valid_loss: 0.07225 test_loss: 0.08036 \n",
      "[167/500] train_loss: 0.05818 valid_loss: 0.07296 test_loss: 0.07969 \n",
      "[168/500] train_loss: 0.05699 valid_loss: 0.07235 test_loss: 0.07914 \n",
      "[169/500] train_loss: 0.05764 valid_loss: 0.07379 test_loss: 0.08108 \n",
      "[170/500] train_loss: 0.05631 valid_loss: 0.07388 test_loss: 0.08105 \n",
      "[171/500] train_loss: 0.05661 valid_loss: 0.07393 test_loss: 0.07989 \n",
      "[172/500] train_loss: 0.05783 valid_loss: 0.07325 test_loss: 0.08013 \n",
      "[173/500] train_loss: 0.05690 valid_loss: 0.07438 test_loss: 0.08008 \n",
      "[174/500] train_loss: 0.05677 valid_loss: 0.07274 test_loss: 0.08075 \n",
      "[175/500] train_loss: 0.05885 valid_loss: 0.07463 test_loss: 0.07945 \n",
      "[176/500] train_loss: 0.05672 valid_loss: 0.07304 test_loss: 0.08038 \n",
      "[177/500] train_loss: 0.05798 valid_loss: 0.07176 test_loss: 0.08082 \n",
      "验证损失减少 (0.071962 --> 0.071765). 正在保存模型...\n",
      "[178/500] train_loss: 0.05632 valid_loss: 0.07261 test_loss: 0.08067 \n",
      "[179/500] train_loss: 0.05628 valid_loss: 0.07336 test_loss: 0.07979 \n",
      "[180/500] train_loss: 0.05617 valid_loss: 0.07289 test_loss: 0.08005 \n",
      "[181/500] train_loss: 0.05706 valid_loss: 0.07249 test_loss: 0.08104 \n",
      "[182/500] train_loss: 0.05526 valid_loss: 0.07305 test_loss: 0.08079 \n",
      "[183/500] train_loss: 0.05470 valid_loss: 0.07380 test_loss: 0.08114 \n",
      "[184/500] train_loss: 0.05718 valid_loss: 0.07227 test_loss: 0.08098 \n",
      "[185/500] train_loss: 0.05617 valid_loss: 0.07337 test_loss: 0.08223 \n",
      "[186/500] train_loss: 0.05622 valid_loss: 0.07339 test_loss: 0.08160 \n",
      "[187/500] train_loss: 0.05502 valid_loss: 0.07241 test_loss: 0.08127 \n",
      "[188/500] train_loss: 0.05555 valid_loss: 0.07205 test_loss: 0.07985 \n",
      "[189/500] train_loss: 0.05614 valid_loss: 0.07204 test_loss: 0.07879 \n",
      "[190/500] train_loss: 0.05455 valid_loss: 0.07125 test_loss: 0.07979 \n",
      "验证损失减少 (0.071765 --> 0.071249). 正在保存模型...\n",
      "[191/500] train_loss: 0.05742 valid_loss: 0.07209 test_loss: 0.07912 \n",
      "[192/500] train_loss: 0.05610 valid_loss: 0.07311 test_loss: 0.07922 \n",
      "[193/500] train_loss: 0.05699 valid_loss: 0.07371 test_loss: 0.07982 \n",
      "[194/500] train_loss: 0.05609 valid_loss: 0.07358 test_loss: 0.07959 \n",
      "[195/500] train_loss: 0.05563 valid_loss: 0.07293 test_loss: 0.08052 \n",
      "[196/500] train_loss: 0.05507 valid_loss: 0.07143 test_loss: 0.08014 \n",
      "[197/500] train_loss: 0.05582 valid_loss: 0.07206 test_loss: 0.07890 \n",
      "[198/500] train_loss: 0.05631 valid_loss: 0.07176 test_loss: 0.07927 \n",
      "[199/500] train_loss: 0.05515 valid_loss: 0.07141 test_loss: 0.08055 \n",
      "[200/500] train_loss: 0.05461 valid_loss: 0.07228 test_loss: 0.08042 \n",
      "[201/500] train_loss: 0.05557 valid_loss: 0.07083 test_loss: 0.07859 \n",
      "验证损失减少 (0.071249 --> 0.070830). 正在保存模型...\n",
      "[202/500] train_loss: 0.05592 valid_loss: 0.07260 test_loss: 0.07989 \n",
      "[203/500] train_loss: 0.05547 valid_loss: 0.07124 test_loss: 0.07860 \n",
      "[204/500] train_loss: 0.05453 valid_loss: 0.07231 test_loss: 0.07929 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[205/500] train_loss: 0.05510 valid_loss: 0.07188 test_loss: 0.07984 \n",
      "[206/500] train_loss: 0.05606 valid_loss: 0.07248 test_loss: 0.08002 \n",
      "[207/500] train_loss: 0.05629 valid_loss: 0.07121 test_loss: 0.07978 \n",
      "[208/500] train_loss: 0.05489 valid_loss: 0.07207 test_loss: 0.08008 \n",
      "[209/500] train_loss: 0.05561 valid_loss: 0.07414 test_loss: 0.07982 \n",
      "[210/500] train_loss: 0.05464 valid_loss: 0.07053 test_loss: 0.07857 \n",
      "验证损失减少 (0.070830 --> 0.070531). 正在保存模型...\n",
      "[211/500] train_loss: 0.05378 valid_loss: 0.07181 test_loss: 0.07990 \n",
      "[212/500] train_loss: 0.05410 valid_loss: 0.07203 test_loss: 0.07931 \n",
      "[213/500] train_loss: 0.05351 valid_loss: 0.07148 test_loss: 0.07886 \n",
      "[214/500] train_loss: 0.05471 valid_loss: 0.07159 test_loss: 0.07915 \n",
      "[215/500] train_loss: 0.05294 valid_loss: 0.07058 test_loss: 0.07934 \n",
      "[216/500] train_loss: 0.05264 valid_loss: 0.07240 test_loss: 0.07864 \n",
      "[217/500] train_loss: 0.05613 valid_loss: 0.07203 test_loss: 0.07915 \n",
      "[218/500] train_loss: 0.05338 valid_loss: 0.07379 test_loss: 0.07975 \n",
      "[219/500] train_loss: 0.05306 valid_loss: 0.07151 test_loss: 0.08071 \n",
      "[220/500] train_loss: 0.05283 valid_loss: 0.07386 test_loss: 0.08004 \n",
      "[221/500] train_loss: 0.05358 valid_loss: 0.07224 test_loss: 0.07982 \n",
      "[222/500] train_loss: 0.05409 valid_loss: 0.07156 test_loss: 0.07933 \n",
      "[223/500] train_loss: 0.05297 valid_loss: 0.07234 test_loss: 0.08099 \n",
      "[224/500] train_loss: 0.05301 valid_loss: 0.07258 test_loss: 0.08046 \n",
      "[225/500] train_loss: 0.05409 valid_loss: 0.07249 test_loss: 0.07863 \n",
      "[226/500] train_loss: 0.05262 valid_loss: 0.07508 test_loss: 0.07898 \n",
      "[227/500] train_loss: 0.05299 valid_loss: 0.07387 test_loss: 0.07903 \n",
      "[228/500] train_loss: 0.05346 valid_loss: 0.07334 test_loss: 0.08007 \n",
      "[229/500] train_loss: 0.05379 valid_loss: 0.07085 test_loss: 0.07881 \n",
      "[230/500] train_loss: 0.05326 valid_loss: 0.07228 test_loss: 0.08031 \n",
      "[231/500] train_loss: 0.05361 valid_loss: 0.07304 test_loss: 0.07979 \n",
      "[232/500] train_loss: 0.05379 valid_loss: 0.07156 test_loss: 0.07994 \n",
      "[233/500] train_loss: 0.05198 valid_loss: 0.07149 test_loss: 0.08007 \n",
      "[234/500] train_loss: 0.05233 valid_loss: 0.07155 test_loss: 0.07950 \n",
      "[235/500] train_loss: 0.05366 valid_loss: 0.07144 test_loss: 0.08009 \n",
      "[236/500] train_loss: 0.05210 valid_loss: 0.07082 test_loss: 0.08017 \n",
      "[237/500] train_loss: 0.05265 valid_loss: 0.07165 test_loss: 0.08024 \n",
      "[238/500] train_loss: 0.05098 valid_loss: 0.07408 test_loss: 0.08154 \n",
      "[239/500] train_loss: 0.05313 valid_loss: 0.07512 test_loss: 0.08042 \n",
      "[240/500] train_loss: 0.05252 valid_loss: 0.07418 test_loss: 0.07968 \n",
      "[241/500] train_loss: 0.05300 valid_loss: 0.07281 test_loss: 0.07972 \n",
      "[242/500] train_loss: 0.05140 valid_loss: 0.07164 test_loss: 0.07907 \n",
      "[243/500] train_loss: 0.05158 valid_loss: 0.07220 test_loss: 0.08021 \n",
      "[244/500] train_loss: 0.05391 valid_loss: 0.07156 test_loss: 0.08018 \n",
      "[245/500] train_loss: 0.05075 valid_loss: 0.07206 test_loss: 0.08070 \n",
      "[246/500] train_loss: 0.05257 valid_loss: 0.07194 test_loss: 0.07979 \n",
      "[247/500] train_loss: 0.05150 valid_loss: 0.07243 test_loss: 0.08086 \n",
      "[248/500] train_loss: 0.05233 valid_loss: 0.07086 test_loss: 0.08065 \n",
      "[249/500] train_loss: 0.05175 valid_loss: 0.07145 test_loss: 0.07909 \n",
      "[250/500] train_loss: 0.05148 valid_loss: 0.07265 test_loss: 0.07958 \n",
      "[251/500] train_loss: 0.05162 valid_loss: 0.07240 test_loss: 0.07987 \n",
      "[252/500] train_loss: 0.05094 valid_loss: 0.07069 test_loss: 0.08042 \n",
      "[253/500] train_loss: 0.05109 valid_loss: 0.07120 test_loss: 0.08106 \n",
      "[254/500] train_loss: 0.05141 valid_loss: 0.07277 test_loss: 0.07995 \n",
      "[255/500] train_loss: 0.05185 valid_loss: 0.07237 test_loss: 0.08046 \n",
      "[256/500] train_loss: 0.05093 valid_loss: 0.07177 test_loss: 0.08017 \n",
      "[257/500] train_loss: 0.05189 valid_loss: 0.07521 test_loss: 0.08058 \n",
      "[258/500] train_loss: 0.05115 valid_loss: 0.07198 test_loss: 0.07899 \n",
      "[259/500] train_loss: 0.05176 valid_loss: 0.07500 test_loss: 0.07980 \n",
      "[260/500] train_loss: 0.05142 valid_loss: 0.07069 test_loss: 0.07858 \n",
      "[261/500] train_loss: 0.05061 valid_loss: 0.07097 test_loss: 0.07881 \n",
      "[262/500] train_loss: 0.05080 valid_loss: 0.07109 test_loss: 0.07940 \n",
      "[263/500] train_loss: 0.05176 valid_loss: 0.07016 test_loss: 0.07993 \n",
      "验证损失减少 (0.070531 --> 0.070158). 正在保存模型...\n",
      "[264/500] train_loss: 0.04997 valid_loss: 0.07169 test_loss: 0.07986 \n",
      "[265/500] train_loss: 0.05062 valid_loss: 0.07238 test_loss: 0.07979 \n",
      "[266/500] train_loss: 0.05134 valid_loss: 0.07019 test_loss: 0.07805 \n",
      "[267/500] train_loss: 0.04927 valid_loss: 0.07347 test_loss: 0.08052 \n",
      "[268/500] train_loss: 0.05078 valid_loss: 0.07196 test_loss: 0.08051 \n",
      "[269/500] train_loss: 0.05027 valid_loss: 0.07005 test_loss: 0.07942 \n",
      "验证损失减少 (0.070158 --> 0.070054). 正在保存模型...\n",
      "[270/500] train_loss: 0.05105 valid_loss: 0.07055 test_loss: 0.07883 \n",
      "[271/500] train_loss: 0.04980 valid_loss: 0.07297 test_loss: 0.08061 \n",
      "[272/500] train_loss: 0.04963 valid_loss: 0.07081 test_loss: 0.07977 \n",
      "[273/500] train_loss: 0.05058 valid_loss: 0.07071 test_loss: 0.07976 \n",
      "[274/500] train_loss: 0.05129 valid_loss: 0.07254 test_loss: 0.07975 \n",
      "[275/500] train_loss: 0.04923 valid_loss: 0.07222 test_loss: 0.08061 \n",
      "[276/500] train_loss: 0.04966 valid_loss: 0.07234 test_loss: 0.08003 \n",
      "[277/500] train_loss: 0.05137 valid_loss: 0.07228 test_loss: 0.08014 \n",
      "[278/500] train_loss: 0.04945 valid_loss: 0.07115 test_loss: 0.07960 \n",
      "[279/500] train_loss: 0.04947 valid_loss: 0.07184 test_loss: 0.08005 \n",
      "[280/500] train_loss: 0.04912 valid_loss: 0.07262 test_loss: 0.08112 \n",
      "[281/500] train_loss: 0.05001 valid_loss: 0.07231 test_loss: 0.08228 \n",
      "[282/500] train_loss: 0.05049 valid_loss: 0.07318 test_loss: 0.08216 \n",
      "[283/500] train_loss: 0.04922 valid_loss: 0.07094 test_loss: 0.08035 \n",
      "[284/500] train_loss: 0.04893 valid_loss: 0.07309 test_loss: 0.08113 \n",
      "[285/500] train_loss: 0.04955 valid_loss: 0.07314 test_loss: 0.08088 \n",
      "[286/500] train_loss: 0.04992 valid_loss: 0.07538 test_loss: 0.08047 \n",
      "[287/500] train_loss: 0.04805 valid_loss: 0.07225 test_loss: 0.08051 \n",
      "[288/500] train_loss: 0.04985 valid_loss: 0.07190 test_loss: 0.08080 \n",
      "[289/500] train_loss: 0.04822 valid_loss: 0.07165 test_loss: 0.07955 \n",
      "[290/500] train_loss: 0.04986 valid_loss: 0.07217 test_loss: 0.07985 \n",
      "[291/500] train_loss: 0.04956 valid_loss: 0.07292 test_loss: 0.07969 \n",
      "[292/500] train_loss: 0.04913 valid_loss: 0.07123 test_loss: 0.07925 \n",
      "[293/500] train_loss: 0.04894 valid_loss: 0.07265 test_loss: 0.08030 \n",
      "[294/500] train_loss: 0.04830 valid_loss: 0.07168 test_loss: 0.08108 \n",
      "[295/500] train_loss: 0.04929 valid_loss: 0.07324 test_loss: 0.08056 \n",
      "[296/500] train_loss: 0.04869 valid_loss: 0.07326 test_loss: 0.08178 \n",
      "[297/500] train_loss: 0.04752 valid_loss: 0.07202 test_loss: 0.07903 \n",
      "[298/500] train_loss: 0.04881 valid_loss: 0.07240 test_loss: 0.08040 \n",
      "[299/500] train_loss: 0.05029 valid_loss: 0.07201 test_loss: 0.08125 \n",
      "[300/500] train_loss: 0.04890 valid_loss: 0.07285 test_loss: 0.08103 \n",
      "[301/500] train_loss: 0.04925 valid_loss: 0.07362 test_loss: 0.08010 \n",
      "[302/500] train_loss: 0.04885 valid_loss: 0.07222 test_loss: 0.07984 \n",
      "[303/500] train_loss: 0.05018 valid_loss: 0.07323 test_loss: 0.08170 \n",
      "[304/500] train_loss: 0.04890 valid_loss: 0.07101 test_loss: 0.08057 \n",
      "[305/500] train_loss: 0.04948 valid_loss: 0.07272 test_loss: 0.08127 \n",
      "[306/500] train_loss: 0.04923 valid_loss: 0.07083 test_loss: 0.08074 \n",
      "[307/500] train_loss: 0.04820 valid_loss: 0.07140 test_loss: 0.08135 \n",
      "[308/500] train_loss: 0.04968 valid_loss: 0.07109 test_loss: 0.08068 \n",
      "[309/500] train_loss: 0.04786 valid_loss: 0.07237 test_loss: 0.08049 \n",
      "[310/500] train_loss: 0.04850 valid_loss: 0.07184 test_loss: 0.08179 \n",
      "[311/500] train_loss: 0.04957 valid_loss: 0.07329 test_loss: 0.08098 \n",
      "[312/500] train_loss: 0.04849 valid_loss: 0.07173 test_loss: 0.07998 \n",
      "[313/500] train_loss: 0.04848 valid_loss: 0.07318 test_loss: 0.08159 \n",
      "[314/500] train_loss: 0.04892 valid_loss: 0.07140 test_loss: 0.08040 \n",
      "[315/500] train_loss: 0.04854 valid_loss: 0.07212 test_loss: 0.08019 \n",
      "[316/500] train_loss: 0.04809 valid_loss: 0.07124 test_loss: 0.07975 \n",
      "[317/500] train_loss: 0.04781 valid_loss: 0.07302 test_loss: 0.07964 \n",
      "[318/500] train_loss: 0.04786 valid_loss: 0.07262 test_loss: 0.08181 \n",
      "[319/500] train_loss: 0.04993 valid_loss: 0.07565 test_loss: 0.08041 \n",
      "[320/500] train_loss: 0.04695 valid_loss: 0.07429 test_loss: 0.08153 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[321/500] train_loss: 0.04779 valid_loss: 0.07232 test_loss: 0.08176 \n",
      "[322/500] train_loss: 0.04774 valid_loss: 0.07262 test_loss: 0.08113 \n",
      "[323/500] train_loss: 0.04765 valid_loss: 0.07257 test_loss: 0.08127 \n",
      "[324/500] train_loss: 0.04729 valid_loss: 0.07239 test_loss: 0.08037 \n",
      "[325/500] train_loss: 0.04742 valid_loss: 0.07133 test_loss: 0.08068 \n",
      "[326/500] train_loss: 0.04728 valid_loss: 0.07019 test_loss: 0.07992 \n",
      "[327/500] train_loss: 0.04723 valid_loss: 0.07206 test_loss: 0.08068 \n",
      "[328/500] train_loss: 0.04830 valid_loss: 0.07230 test_loss: 0.08065 \n",
      "[329/500] train_loss: 0.04849 valid_loss: 0.07088 test_loss: 0.08111 \n",
      "[330/500] train_loss: 0.04647 valid_loss: 0.07199 test_loss: 0.08251 \n",
      "[331/500] train_loss: 0.04787 valid_loss: 0.07152 test_loss: 0.08210 \n",
      "[332/500] train_loss: 0.04779 valid_loss: 0.07284 test_loss: 0.08273 \n",
      "[333/500] train_loss: 0.04728 valid_loss: 0.07296 test_loss: 0.08151 \n",
      "[334/500] train_loss: 0.04646 valid_loss: 0.07337 test_loss: 0.08100 \n",
      "[335/500] train_loss: 0.04818 valid_loss: 0.07243 test_loss: 0.08155 \n",
      "[336/500] train_loss: 0.04713 valid_loss: 0.07413 test_loss: 0.08206 \n",
      "[337/500] train_loss: 0.04656 valid_loss: 0.07447 test_loss: 0.08205 \n",
      "[338/500] train_loss: 0.04833 valid_loss: 0.07214 test_loss: 0.08095 \n",
      "[339/500] train_loss: 0.04660 valid_loss: 0.07282 test_loss: 0.08096 \n",
      "[340/500] train_loss: 0.04625 valid_loss: 0.07215 test_loss: 0.08010 \n",
      "[341/500] train_loss: 0.04628 valid_loss: 0.07384 test_loss: 0.07930 \n",
      "[342/500] train_loss: 0.04647 valid_loss: 0.07214 test_loss: 0.08092 \n",
      "[343/500] train_loss: 0.04637 valid_loss: 0.07233 test_loss: 0.07901 \n",
      "[344/500] train_loss: 0.04790 valid_loss: 0.07696 test_loss: 0.08163 \n",
      "[345/500] train_loss: 0.04729 valid_loss: 0.07151 test_loss: 0.07928 \n",
      "[346/500] train_loss: 0.04643 valid_loss: 0.07187 test_loss: 0.08142 \n",
      "[347/500] train_loss: 0.04618 valid_loss: 0.07138 test_loss: 0.08052 \n",
      "[348/500] train_loss: 0.04526 valid_loss: 0.07146 test_loss: 0.08021 \n",
      "[349/500] train_loss: 0.04696 valid_loss: 0.07348 test_loss: 0.08098 \n",
      "[350/500] train_loss: 0.04634 valid_loss: 0.07061 test_loss: 0.08161 \n",
      "[351/500] train_loss: 0.04703 valid_loss: 0.07116 test_loss: 0.08135 \n",
      "[352/500] train_loss: 0.04612 valid_loss: 0.07279 test_loss: 0.08122 \n",
      "[353/500] train_loss: 0.04788 valid_loss: 0.07139 test_loss: 0.08216 \n",
      "[354/500] train_loss: 0.04524 valid_loss: 0.07301 test_loss: 0.08050 \n",
      "[355/500] train_loss: 0.04633 valid_loss: 0.07184 test_loss: 0.08199 \n",
      "[356/500] train_loss: 0.04597 valid_loss: 0.07126 test_loss: 0.08267 \n",
      "[357/500] train_loss: 0.04661 valid_loss: 0.07189 test_loss: 0.08191 \n",
      "[358/500] train_loss: 0.04658 valid_loss: 0.07261 test_loss: 0.08284 \n",
      "[359/500] train_loss: 0.04484 valid_loss: 0.07366 test_loss: 0.08128 \n",
      "[360/500] train_loss: 0.04588 valid_loss: 0.07335 test_loss: 0.08340 \n",
      "[361/500] train_loss: 0.04610 valid_loss: 0.07267 test_loss: 0.08127 \n",
      "[362/500] train_loss: 0.04638 valid_loss: 0.07231 test_loss: 0.08073 \n",
      "[363/500] train_loss: 0.04599 valid_loss: 0.07252 test_loss: 0.08235 \n",
      "[364/500] train_loss: 0.04636 valid_loss: 0.07110 test_loss: 0.08033 \n",
      "[365/500] train_loss: 0.04754 valid_loss: 0.07224 test_loss: 0.08135 \n",
      "[366/500] train_loss: 0.04548 valid_loss: 0.07218 test_loss: 0.08091 \n",
      "[367/500] train_loss: 0.04521 valid_loss: 0.07367 test_loss: 0.08218 \n",
      "[368/500] train_loss: 0.04541 valid_loss: 0.07184 test_loss: 0.08155 \n",
      "[369/500] train_loss: 0.04527 valid_loss: 0.07233 test_loss: 0.08096 \n",
      "[370/500] train_loss: 0.04581 valid_loss: 0.07322 test_loss: 0.08116 \n",
      "[371/500] train_loss: 0.04619 valid_loss: 0.07059 test_loss: 0.07996 \n",
      "[372/500] train_loss: 0.04670 valid_loss: 0.07372 test_loss: 0.08278 \n",
      "[373/500] train_loss: 0.04614 valid_loss: 0.07224 test_loss: 0.08162 \n",
      "[374/500] train_loss: 0.04588 valid_loss: 0.07282 test_loss: 0.08277 \n",
      "[375/500] train_loss: 0.04627 valid_loss: 0.07099 test_loss: 0.08124 \n",
      "[376/500] train_loss: 0.04540 valid_loss: 0.07375 test_loss: 0.08168 \n",
      "[377/500] train_loss: 0.04577 valid_loss: 0.07420 test_loss: 0.08255 \n",
      "[378/500] train_loss: 0.04572 valid_loss: 0.07382 test_loss: 0.08338 \n",
      "[379/500] train_loss: 0.04713 valid_loss: 0.07332 test_loss: 0.08157 \n",
      "[380/500] train_loss: 0.04463 valid_loss: 0.07364 test_loss: 0.08144 \n",
      "[381/500] train_loss: 0.04432 valid_loss: 0.07489 test_loss: 0.08360 \n",
      "[382/500] train_loss: 0.04468 valid_loss: 0.07401 test_loss: 0.08336 \n",
      "[383/500] train_loss: 0.04524 valid_loss: 0.07158 test_loss: 0.08180 \n",
      "[384/500] train_loss: 0.04502 valid_loss: 0.07208 test_loss: 0.08253 \n",
      "[385/500] train_loss: 0.04465 valid_loss: 0.07272 test_loss: 0.08042 \n",
      "[386/500] train_loss: 0.04542 valid_loss: 0.07345 test_loss: 0.07985 \n",
      "[387/500] train_loss: 0.04570 valid_loss: 0.07290 test_loss: 0.08162 \n",
      "[388/500] train_loss: 0.04554 valid_loss: 0.07040 test_loss: 0.08049 \n",
      "[389/500] train_loss: 0.04645 valid_loss: 0.07394 test_loss: 0.08112 \n",
      "[390/500] train_loss: 0.04602 valid_loss: 0.07237 test_loss: 0.08041 \n",
      "[391/500] train_loss: 0.04388 valid_loss: 0.07164 test_loss: 0.07966 \n",
      "[392/500] train_loss: 0.04488 valid_loss: 0.07408 test_loss: 0.08128 \n",
      "[393/500] train_loss: 0.04574 valid_loss: 0.07409 test_loss: 0.08083 \n",
      "[394/500] train_loss: 0.04417 valid_loss: 0.07300 test_loss: 0.08136 \n",
      "[395/500] train_loss: 0.04424 valid_loss: 0.07249 test_loss: 0.08017 \n",
      "[396/500] train_loss: 0.04391 valid_loss: 0.07163 test_loss: 0.08026 \n",
      "[397/500] train_loss: 0.04467 valid_loss: 0.07179 test_loss: 0.07961 \n",
      "[398/500] train_loss: 0.04473 valid_loss: 0.07343 test_loss: 0.08172 \n",
      "[399/500] train_loss: 0.04497 valid_loss: 0.07170 test_loss: 0.08094 \n",
      "[400/500] train_loss: 0.04448 valid_loss: 0.07561 test_loss: 0.08164 \n",
      "[401/500] train_loss: 0.04502 valid_loss: 0.07203 test_loss: 0.08042 \n",
      "[402/500] train_loss: 0.04636 valid_loss: 0.07405 test_loss: 0.08061 \n",
      "[403/500] train_loss: 0.04505 valid_loss: 0.07332 test_loss: 0.08063 \n",
      "[404/500] train_loss: 0.04346 valid_loss: 0.07520 test_loss: 0.08134 \n",
      "[405/500] train_loss: 0.04363 valid_loss: 0.07365 test_loss: 0.08179 \n",
      "[406/500] train_loss: 0.04575 valid_loss: 0.07332 test_loss: 0.08004 \n",
      "[407/500] train_loss: 0.04485 valid_loss: 0.07464 test_loss: 0.08055 \n",
      "[408/500] train_loss: 0.04438 valid_loss: 0.07184 test_loss: 0.08039 \n",
      "[409/500] train_loss: 0.04446 valid_loss: 0.07398 test_loss: 0.08216 \n",
      "[410/500] train_loss: 0.04384 valid_loss: 0.07381 test_loss: 0.08041 \n",
      "[411/500] train_loss: 0.04517 valid_loss: 0.07194 test_loss: 0.08081 \n",
      "[412/500] train_loss: 0.04520 valid_loss: 0.07363 test_loss: 0.07991 \n",
      "[413/500] train_loss: 0.04400 valid_loss: 0.07262 test_loss: 0.08049 \n",
      "[414/500] train_loss: 0.04399 valid_loss: 0.07125 test_loss: 0.07993 \n",
      "[415/500] train_loss: 0.04466 valid_loss: 0.07337 test_loss: 0.08241 \n",
      "[416/500] train_loss: 0.04414 valid_loss: 0.07290 test_loss: 0.08095 \n",
      "[417/500] train_loss: 0.04414 valid_loss: 0.07307 test_loss: 0.08034 \n",
      "[418/500] train_loss: 0.04418 valid_loss: 0.07284 test_loss: 0.08000 \n",
      "[419/500] train_loss: 0.04301 valid_loss: 0.07487 test_loss: 0.08127 \n",
      "[420/500] train_loss: 0.04407 valid_loss: 0.07437 test_loss: 0.08084 \n",
      "[421/500] train_loss: 0.04491 valid_loss: 0.07405 test_loss: 0.08137 \n",
      "[422/500] train_loss: 0.04575 valid_loss: 0.07321 test_loss: 0.07982 \n",
      "[423/500] train_loss: 0.04429 valid_loss: 0.07331 test_loss: 0.08044 \n",
      "[424/500] train_loss: 0.04416 valid_loss: 0.07346 test_loss: 0.08069 \n",
      "[425/500] train_loss: 0.04478 valid_loss: 0.07338 test_loss: 0.08025 \n",
      "[426/500] train_loss: 0.04418 valid_loss: 0.07320 test_loss: 0.08061 \n",
      "[427/500] train_loss: 0.04346 valid_loss: 0.07493 test_loss: 0.08235 \n",
      "[428/500] train_loss: 0.04433 valid_loss: 0.07640 test_loss: 0.08031 \n",
      "[429/500] train_loss: 0.04388 valid_loss: 0.07408 test_loss: 0.08076 \n",
      "[430/500] train_loss: 0.04237 valid_loss: 0.07478 test_loss: 0.08219 \n",
      "[431/500] train_loss: 0.04327 valid_loss: 0.07449 test_loss: 0.08062 \n",
      "[432/500] train_loss: 0.04345 valid_loss: 0.07400 test_loss: 0.08098 \n",
      "[433/500] train_loss: 0.04398 valid_loss: 0.07393 test_loss: 0.08073 \n",
      "[434/500] train_loss: 0.04320 valid_loss: 0.07694 test_loss: 0.08089 \n",
      "[435/500] train_loss: 0.04348 valid_loss: 0.07422 test_loss: 0.08022 \n",
      "[436/500] train_loss: 0.04311 valid_loss: 0.07434 test_loss: 0.08130 \n",
      "[437/500] train_loss: 0.04420 valid_loss: 0.07495 test_loss: 0.08050 \n",
      "[438/500] train_loss: 0.04337 valid_loss: 0.07264 test_loss: 0.08038 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[439/500] train_loss: 0.04382 valid_loss: 0.07398 test_loss: 0.08168 \n",
      "[440/500] train_loss: 0.04331 valid_loss: 0.07411 test_loss: 0.07989 \n",
      "[441/500] train_loss: 0.04291 valid_loss: 0.07417 test_loss: 0.08047 \n",
      "[442/500] train_loss: 0.04373 valid_loss: 0.07312 test_loss: 0.08086 \n",
      "[443/500] train_loss: 0.04487 valid_loss: 0.07262 test_loss: 0.08119 \n",
      "[444/500] train_loss: 0.04294 valid_loss: 0.07442 test_loss: 0.08073 \n",
      "[445/500] train_loss: 0.04250 valid_loss: 0.07423 test_loss: 0.08115 \n",
      "[446/500] train_loss: 0.04381 valid_loss: 0.07474 test_loss: 0.08264 \n",
      "[447/500] train_loss: 0.04288 valid_loss: 0.07291 test_loss: 0.08060 \n",
      "[448/500] train_loss: 0.04336 valid_loss: 0.07364 test_loss: 0.08200 \n",
      "[449/500] train_loss: 0.04268 valid_loss: 0.07485 test_loss: 0.08093 \n",
      "[450/500] train_loss: 0.04377 valid_loss: 0.07404 test_loss: 0.08159 \n",
      "[451/500] train_loss: 0.04297 valid_loss: 0.07595 test_loss: 0.08372 \n",
      "[452/500] train_loss: 0.04330 valid_loss: 0.07302 test_loss: 0.08014 \n",
      "[453/500] train_loss: 0.04396 valid_loss: 0.07269 test_loss: 0.08040 \n",
      "[454/500] train_loss: 0.04270 valid_loss: 0.07468 test_loss: 0.08135 \n",
      "[455/500] train_loss: 0.04278 valid_loss: 0.07196 test_loss: 0.08138 \n",
      "[456/500] train_loss: 0.04349 valid_loss: 0.07244 test_loss: 0.08115 \n",
      "[457/500] train_loss: 0.04153 valid_loss: 0.07204 test_loss: 0.08046 \n",
      "[458/500] train_loss: 0.04298 valid_loss: 0.07295 test_loss: 0.08220 \n",
      "[459/500] train_loss: 0.04175 valid_loss: 0.07243 test_loss: 0.07982 \n",
      "[460/500] train_loss: 0.04327 valid_loss: 0.07419 test_loss: 0.08231 \n",
      "[461/500] train_loss: 0.04166 valid_loss: 0.07329 test_loss: 0.08177 \n",
      "[462/500] train_loss: 0.04364 valid_loss: 0.07333 test_loss: 0.08206 \n",
      "[463/500] train_loss: 0.04289 valid_loss: 0.07474 test_loss: 0.08083 \n",
      "[464/500] train_loss: 0.04296 valid_loss: 0.07445 test_loss: 0.08151 \n",
      "[465/500] train_loss: 0.04348 valid_loss: 0.07225 test_loss: 0.08144 \n",
      "[466/500] train_loss: 0.04265 valid_loss: 0.07286 test_loss: 0.08171 \n",
      "[467/500] train_loss: 0.04343 valid_loss: 0.07372 test_loss: 0.08309 \n",
      "[468/500] train_loss: 0.04316 valid_loss: 0.07370 test_loss: 0.08246 \n",
      "[469/500] train_loss: 0.04471 valid_loss: 0.07790 test_loss: 0.08261 \n",
      "[470/500] train_loss: 0.04237 valid_loss: 0.07210 test_loss: 0.08032 \n",
      "[471/500] train_loss: 0.04209 valid_loss: 0.07322 test_loss: 0.08128 \n",
      "[472/500] train_loss: 0.04247 valid_loss: 0.07366 test_loss: 0.08221 \n",
      "[473/500] train_loss: 0.04265 valid_loss: 0.07224 test_loss: 0.08131 \n",
      "[474/500] train_loss: 0.04220 valid_loss: 0.07345 test_loss: 0.08158 \n",
      "[475/500] train_loss: 0.04191 valid_loss: 0.07308 test_loss: 0.08222 \n",
      "[476/500] train_loss: 0.04151 valid_loss: 0.07395 test_loss: 0.08104 \n",
      "[477/500] train_loss: 0.04378 valid_loss: 0.07221 test_loss: 0.08019 \n",
      "[478/500] train_loss: 0.04254 valid_loss: 0.07258 test_loss: 0.08141 \n",
      "[479/500] train_loss: 0.04275 valid_loss: 0.07228 test_loss: 0.08131 \n",
      "[480/500] train_loss: 0.04367 valid_loss: 0.07430 test_loss: 0.08154 \n",
      "[481/500] train_loss: 0.04400 valid_loss: 0.07387 test_loss: 0.08032 \n",
      "[482/500] train_loss: 0.04199 valid_loss: 0.07488 test_loss: 0.08156 \n",
      "[483/500] train_loss: 0.04303 valid_loss: 0.07343 test_loss: 0.08113 \n",
      "[484/500] train_loss: 0.04297 valid_loss: 0.07227 test_loss: 0.08144 \n",
      "[485/500] train_loss: 0.04199 valid_loss: 0.07343 test_loss: 0.08110 \n",
      "[486/500] train_loss: 0.04319 valid_loss: 0.07542 test_loss: 0.08332 \n",
      "[487/500] train_loss: 0.04148 valid_loss: 0.07325 test_loss: 0.08084 \n",
      "[488/500] train_loss: 0.04181 valid_loss: 0.07464 test_loss: 0.08169 \n",
      "[489/500] train_loss: 0.04177 valid_loss: 0.07642 test_loss: 0.08211 \n",
      "[490/500] train_loss: 0.04083 valid_loss: 0.07487 test_loss: 0.08079 \n",
      "[491/500] train_loss: 0.04153 valid_loss: 0.07190 test_loss: 0.08128 \n",
      "[492/500] train_loss: 0.04357 valid_loss: 0.07437 test_loss: 0.08078 \n",
      "[493/500] train_loss: 0.04192 valid_loss: 0.07430 test_loss: 0.08070 \n",
      "[494/500] train_loss: 0.04224 valid_loss: 0.07438 test_loss: 0.08057 \n",
      "[495/500] train_loss: 0.04142 valid_loss: 0.07397 test_loss: 0.08034 \n",
      "[496/500] train_loss: 0.04104 valid_loss: 0.07439 test_loss: 0.08044 \n",
      "[497/500] train_loss: 0.04270 valid_loss: 0.07527 test_loss: 0.08085 \n",
      "[498/500] train_loss: 0.04087 valid_loss: 0.07381 test_loss: 0.08148 \n",
      "[499/500] train_loss: 0.04166 valid_loss: 0.07388 test_loss: 0.08016 \n",
      "[500/500] train_loss: 0.04295 valid_loss: 0.07539 test_loss: 0.07985 \n",
      "TRAINING MODEL 1\n",
      "[  1/500] train_loss: 0.33019 valid_loss: 0.24400 test_loss: 0.25145 \n",
      "验证损失减少 (inf --> 0.244003). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18811 valid_loss: 0.17887 test_loss: 0.18338 \n",
      "验证损失减少 (0.244003 --> 0.178869). 正在保存模型...\n",
      "[  3/500] train_loss: 0.14799 valid_loss: 0.15170 test_loss: 0.15828 \n",
      "验证损失减少 (0.178869 --> 0.151697). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13860 valid_loss: 0.14191 test_loss: 0.14900 \n",
      "验证损失减少 (0.151697 --> 0.141909). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12845 valid_loss: 0.13590 test_loss: 0.14354 \n",
      "验证损失减少 (0.141909 --> 0.135904). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12442 valid_loss: 0.12823 test_loss: 0.13362 \n",
      "验证损失减少 (0.135904 --> 0.128234). 正在保存模型...\n",
      "[  7/500] train_loss: 0.12068 valid_loss: 0.12691 test_loss: 0.13388 \n",
      "验证损失减少 (0.128234 --> 0.126914). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11946 valid_loss: 0.11883 test_loss: 0.12901 \n",
      "验证损失减少 (0.126914 --> 0.118829). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11329 valid_loss: 0.11755 test_loss: 0.12790 \n",
      "验证损失减少 (0.118829 --> 0.117546). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11047 valid_loss: 0.11555 test_loss: 0.12131 \n",
      "验证损失减少 (0.117546 --> 0.115546). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10836 valid_loss: 0.11328 test_loss: 0.12207 \n",
      "验证损失减少 (0.115546 --> 0.113276). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10487 valid_loss: 0.11241 test_loss: 0.12067 \n",
      "验证损失减少 (0.113276 --> 0.112413). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10500 valid_loss: 0.10912 test_loss: 0.11734 \n",
      "验证损失减少 (0.112413 --> 0.109120). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10186 valid_loss: 0.10974 test_loss: 0.11929 \n",
      "[ 15/500] train_loss: 0.10269 valid_loss: 0.10329 test_loss: 0.11323 \n",
      "验证损失减少 (0.109120 --> 0.103291). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10162 valid_loss: 0.10486 test_loss: 0.11378 \n",
      "[ 17/500] train_loss: 0.09706 valid_loss: 0.10257 test_loss: 0.11234 \n",
      "验证损失减少 (0.103291 --> 0.102569). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09585 valid_loss: 0.10463 test_loss: 0.11251 \n",
      "[ 19/500] train_loss: 0.09674 valid_loss: 0.09964 test_loss: 0.10873 \n",
      "验证损失减少 (0.102569 --> 0.099642). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09667 valid_loss: 0.10246 test_loss: 0.11267 \n",
      "[ 21/500] train_loss: 0.09315 valid_loss: 0.09645 test_loss: 0.10603 \n",
      "验证损失减少 (0.099642 --> 0.096447). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09292 valid_loss: 0.09527 test_loss: 0.10501 \n",
      "验证损失减少 (0.096447 --> 0.095270). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09379 valid_loss: 0.09583 test_loss: 0.10482 \n",
      "[ 24/500] train_loss: 0.09123 valid_loss: 0.09311 test_loss: 0.10405 \n",
      "验证损失减少 (0.095270 --> 0.093113). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.08813 valid_loss: 0.09328 test_loss: 0.10260 \n",
      "[ 26/500] train_loss: 0.09024 valid_loss: 0.09477 test_loss: 0.10738 \n",
      "[ 27/500] train_loss: 0.08895 valid_loss: 0.09553 test_loss: 0.10491 \n",
      "[ 28/500] train_loss: 0.08926 valid_loss: 0.09088 test_loss: 0.10019 \n",
      "验证损失减少 (0.093113 --> 0.090879). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08746 valid_loss: 0.09174 test_loss: 0.10172 \n",
      "[ 30/500] train_loss: 0.08830 valid_loss: 0.08896 test_loss: 0.10228 \n",
      "验证损失减少 (0.090879 --> 0.088955). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08766 valid_loss: 0.08938 test_loss: 0.09837 \n",
      "[ 32/500] train_loss: 0.08458 valid_loss: 0.08926 test_loss: 0.09959 \n",
      "[ 33/500] train_loss: 0.08305 valid_loss: 0.08799 test_loss: 0.09799 \n",
      "验证损失减少 (0.088955 --> 0.087993). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08628 valid_loss: 0.08894 test_loss: 0.09893 \n",
      "[ 35/500] train_loss: 0.08409 valid_loss: 0.08776 test_loss: 0.09764 \n",
      "验证损失减少 (0.087993 --> 0.087755). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08279 valid_loss: 0.08616 test_loss: 0.09728 \n",
      "验证损失减少 (0.087755 --> 0.086158). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08376 valid_loss: 0.08776 test_loss: 0.09500 \n",
      "[ 38/500] train_loss: 0.08340 valid_loss: 0.08746 test_loss: 0.09540 \n",
      "[ 39/500] train_loss: 0.08234 valid_loss: 0.08643 test_loss: 0.09401 \n",
      "[ 40/500] train_loss: 0.08329 valid_loss: 0.08598 test_loss: 0.09481 \n",
      "验证损失减少 (0.086158 --> 0.085977). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 41/500] train_loss: 0.08289 valid_loss: 0.08518 test_loss: 0.09447 \n",
      "验证损失减少 (0.085977 --> 0.085176). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.07921 valid_loss: 0.08628 test_loss: 0.09418 \n",
      "[ 43/500] train_loss: 0.07959 valid_loss: 0.08594 test_loss: 0.09406 \n",
      "[ 44/500] train_loss: 0.08098 valid_loss: 0.08397 test_loss: 0.09206 \n",
      "验证损失减少 (0.085176 --> 0.083965). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.08084 valid_loss: 0.08455 test_loss: 0.09271 \n",
      "[ 46/500] train_loss: 0.07846 valid_loss: 0.08349 test_loss: 0.09167 \n",
      "验证损失减少 (0.083965 --> 0.083491). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07968 valid_loss: 0.08467 test_loss: 0.09234 \n",
      "[ 48/500] train_loss: 0.08030 valid_loss: 0.08378 test_loss: 0.09062 \n",
      "[ 49/500] train_loss: 0.07998 valid_loss: 0.08416 test_loss: 0.09082 \n",
      "[ 50/500] train_loss: 0.07886 valid_loss: 0.08414 test_loss: 0.09202 \n",
      "[ 51/500] train_loss: 0.07885 valid_loss: 0.08253 test_loss: 0.09103 \n",
      "验证损失减少 (0.083491 --> 0.082527). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07909 valid_loss: 0.08363 test_loss: 0.09191 \n",
      "[ 53/500] train_loss: 0.07817 valid_loss: 0.08350 test_loss: 0.09151 \n",
      "[ 54/500] train_loss: 0.07645 valid_loss: 0.08376 test_loss: 0.09029 \n",
      "[ 55/500] train_loss: 0.07690 valid_loss: 0.08154 test_loss: 0.08945 \n",
      "验证损失减少 (0.082527 --> 0.081542). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07527 valid_loss: 0.08190 test_loss: 0.09005 \n",
      "[ 57/500] train_loss: 0.07751 valid_loss: 0.08220 test_loss: 0.08953 \n",
      "[ 58/500] train_loss: 0.07660 valid_loss: 0.08103 test_loss: 0.08891 \n",
      "验证损失减少 (0.081542 --> 0.081030). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07503 valid_loss: 0.08403 test_loss: 0.08978 \n",
      "[ 60/500] train_loss: 0.07341 valid_loss: 0.08220 test_loss: 0.08875 \n",
      "[ 61/500] train_loss: 0.07520 valid_loss: 0.08132 test_loss: 0.08874 \n",
      "[ 62/500] train_loss: 0.07576 valid_loss: 0.08004 test_loss: 0.08816 \n",
      "验证损失减少 (0.081030 --> 0.080035). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.07244 valid_loss: 0.07972 test_loss: 0.08871 \n",
      "验证损失减少 (0.080035 --> 0.079724). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.07369 valid_loss: 0.08116 test_loss: 0.08747 \n",
      "[ 65/500] train_loss: 0.07272 valid_loss: 0.08010 test_loss: 0.08925 \n",
      "[ 66/500] train_loss: 0.07295 valid_loss: 0.08039 test_loss: 0.08602 \n",
      "[ 67/500] train_loss: 0.07407 valid_loss: 0.08145 test_loss: 0.08618 \n",
      "[ 68/500] train_loss: 0.07378 valid_loss: 0.08106 test_loss: 0.08703 \n",
      "[ 69/500] train_loss: 0.07351 valid_loss: 0.08217 test_loss: 0.08896 \n",
      "[ 70/500] train_loss: 0.07188 valid_loss: 0.07980 test_loss: 0.08537 \n",
      "[ 71/500] train_loss: 0.07420 valid_loss: 0.07927 test_loss: 0.08495 \n",
      "验证损失减少 (0.079724 --> 0.079271). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.07354 valid_loss: 0.07894 test_loss: 0.08529 \n",
      "验证损失减少 (0.079271 --> 0.078938). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.06958 valid_loss: 0.07804 test_loss: 0.08583 \n",
      "验证损失减少 (0.078938 --> 0.078036). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.07265 valid_loss: 0.07852 test_loss: 0.08533 \n",
      "[ 75/500] train_loss: 0.07207 valid_loss: 0.08008 test_loss: 0.08675 \n",
      "[ 76/500] train_loss: 0.07239 valid_loss: 0.07872 test_loss: 0.08646 \n",
      "[ 77/500] train_loss: 0.06964 valid_loss: 0.07873 test_loss: 0.08627 \n",
      "[ 78/500] train_loss: 0.06994 valid_loss: 0.07819 test_loss: 0.08704 \n",
      "[ 79/500] train_loss: 0.07302 valid_loss: 0.07861 test_loss: 0.08414 \n",
      "[ 80/500] train_loss: 0.06939 valid_loss: 0.07917 test_loss: 0.08543 \n",
      "[ 81/500] train_loss: 0.06932 valid_loss: 0.07815 test_loss: 0.08378 \n",
      "[ 82/500] train_loss: 0.06974 valid_loss: 0.07812 test_loss: 0.08543 \n",
      "[ 83/500] train_loss: 0.06947 valid_loss: 0.07691 test_loss: 0.08431 \n",
      "验证损失减少 (0.078036 --> 0.076910). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.06833 valid_loss: 0.07906 test_loss: 0.08400 \n",
      "[ 85/500] train_loss: 0.06785 valid_loss: 0.07733 test_loss: 0.08375 \n",
      "[ 86/500] train_loss: 0.06967 valid_loss: 0.07585 test_loss: 0.08353 \n",
      "验证损失减少 (0.076910 --> 0.075854). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.06933 valid_loss: 0.07780 test_loss: 0.08445 \n",
      "[ 88/500] train_loss: 0.06867 valid_loss: 0.07668 test_loss: 0.08339 \n",
      "[ 89/500] train_loss: 0.06804 valid_loss: 0.07778 test_loss: 0.08484 \n",
      "[ 90/500] train_loss: 0.06937 valid_loss: 0.07623 test_loss: 0.08442 \n",
      "[ 91/500] train_loss: 0.06983 valid_loss: 0.07666 test_loss: 0.08431 \n",
      "[ 92/500] train_loss: 0.06758 valid_loss: 0.07641 test_loss: 0.08318 \n",
      "[ 93/500] train_loss: 0.06816 valid_loss: 0.07607 test_loss: 0.08283 \n",
      "[ 94/500] train_loss: 0.06687 valid_loss: 0.07651 test_loss: 0.08340 \n",
      "[ 95/500] train_loss: 0.06588 valid_loss: 0.07830 test_loss: 0.08413 \n",
      "[ 96/500] train_loss: 0.06776 valid_loss: 0.07802 test_loss: 0.08479 \n",
      "[ 97/500] train_loss: 0.06628 valid_loss: 0.07666 test_loss: 0.08353 \n",
      "[ 98/500] train_loss: 0.06634 valid_loss: 0.07613 test_loss: 0.08242 \n",
      "[ 99/500] train_loss: 0.06651 valid_loss: 0.07727 test_loss: 0.08234 \n",
      "[100/500] train_loss: 0.06780 valid_loss: 0.07611 test_loss: 0.08400 \n",
      "[101/500] train_loss: 0.06862 valid_loss: 0.07968 test_loss: 0.08484 \n",
      "[102/500] train_loss: 0.06673 valid_loss: 0.07541 test_loss: 0.08234 \n",
      "验证损失减少 (0.075854 --> 0.075409). 正在保存模型...\n",
      "[103/500] train_loss: 0.06737 valid_loss: 0.07618 test_loss: 0.08404 \n",
      "[104/500] train_loss: 0.06624 valid_loss: 0.07725 test_loss: 0.08311 \n",
      "[105/500] train_loss: 0.06637 valid_loss: 0.07819 test_loss: 0.08289 \n",
      "[106/500] train_loss: 0.06413 valid_loss: 0.07567 test_loss: 0.08358 \n",
      "[107/500] train_loss: 0.06487 valid_loss: 0.07609 test_loss: 0.08243 \n",
      "[108/500] train_loss: 0.06254 valid_loss: 0.07733 test_loss: 0.08391 \n",
      "[109/500] train_loss: 0.06470 valid_loss: 0.07606 test_loss: 0.08545 \n",
      "[110/500] train_loss: 0.06499 valid_loss: 0.07577 test_loss: 0.08375 \n",
      "[111/500] train_loss: 0.06523 valid_loss: 0.07458 test_loss: 0.08255 \n",
      "验证损失减少 (0.075409 --> 0.074583). 正在保存模型...\n",
      "[112/500] train_loss: 0.06493 valid_loss: 0.07535 test_loss: 0.08262 \n",
      "[113/500] train_loss: 0.06487 valid_loss: 0.07628 test_loss: 0.08234 \n",
      "[114/500] train_loss: 0.06440 valid_loss: 0.07569 test_loss: 0.08270 \n",
      "[115/500] train_loss: 0.06417 valid_loss: 0.07658 test_loss: 0.08214 \n",
      "[116/500] train_loss: 0.06655 valid_loss: 0.07710 test_loss: 0.08241 \n",
      "[117/500] train_loss: 0.06378 valid_loss: 0.07644 test_loss: 0.08388 \n",
      "[118/500] train_loss: 0.06249 valid_loss: 0.07779 test_loss: 0.08280 \n",
      "[119/500] train_loss: 0.06219 valid_loss: 0.07593 test_loss: 0.08296 \n",
      "[120/500] train_loss: 0.06659 valid_loss: 0.07585 test_loss: 0.08289 \n",
      "[121/500] train_loss: 0.06475 valid_loss: 0.07457 test_loss: 0.08190 \n",
      "验证损失减少 (0.074583 --> 0.074569). 正在保存模型...\n",
      "[122/500] train_loss: 0.06173 valid_loss: 0.07545 test_loss: 0.08190 \n",
      "[123/500] train_loss: 0.06379 valid_loss: 0.07685 test_loss: 0.08219 \n",
      "[124/500] train_loss: 0.06446 valid_loss: 0.07549 test_loss: 0.08155 \n",
      "[125/500] train_loss: 0.06185 valid_loss: 0.07481 test_loss: 0.08114 \n",
      "[126/500] train_loss: 0.06216 valid_loss: 0.07437 test_loss: 0.08060 \n",
      "验证损失减少 (0.074569 --> 0.074372). 正在保存模型...\n",
      "[127/500] train_loss: 0.06297 valid_loss: 0.07478 test_loss: 0.08060 \n",
      "[128/500] train_loss: 0.06361 valid_loss: 0.07732 test_loss: 0.08239 \n",
      "[129/500] train_loss: 0.06274 valid_loss: 0.07558 test_loss: 0.08094 \n",
      "[130/500] train_loss: 0.06172 valid_loss: 0.07683 test_loss: 0.08154 \n",
      "[131/500] train_loss: 0.06241 valid_loss: 0.07649 test_loss: 0.08066 \n",
      "[132/500] train_loss: 0.06479 valid_loss: 0.07558 test_loss: 0.08116 \n",
      "[133/500] train_loss: 0.06045 valid_loss: 0.07403 test_loss: 0.08093 \n",
      "验证损失减少 (0.074372 --> 0.074032). 正在保存模型...\n",
      "[134/500] train_loss: 0.06161 valid_loss: 0.07464 test_loss: 0.08224 \n",
      "[135/500] train_loss: 0.06126 valid_loss: 0.07433 test_loss: 0.08364 \n",
      "[136/500] train_loss: 0.06125 valid_loss: 0.07453 test_loss: 0.08190 \n",
      "[137/500] train_loss: 0.06165 valid_loss: 0.07559 test_loss: 0.08430 \n",
      "[138/500] train_loss: 0.06130 valid_loss: 0.07521 test_loss: 0.08182 \n",
      "[139/500] train_loss: 0.06116 valid_loss: 0.07491 test_loss: 0.08294 \n",
      "[140/500] train_loss: 0.05981 valid_loss: 0.07425 test_loss: 0.08169 \n",
      "[141/500] train_loss: 0.06107 valid_loss: 0.07574 test_loss: 0.08117 \n",
      "[142/500] train_loss: 0.06173 valid_loss: 0.07363 test_loss: 0.08167 \n",
      "验证损失减少 (0.074032 --> 0.073625). 正在保存模型...\n",
      "[143/500] train_loss: 0.05999 valid_loss: 0.07549 test_loss: 0.08060 \n",
      "[144/500] train_loss: 0.05957 valid_loss: 0.07604 test_loss: 0.08035 \n",
      "[145/500] train_loss: 0.05979 valid_loss: 0.07599 test_loss: 0.08362 \n",
      "[146/500] train_loss: 0.05965 valid_loss: 0.07539 test_loss: 0.08065 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147/500] train_loss: 0.06032 valid_loss: 0.07488 test_loss: 0.08168 \n",
      "[148/500] train_loss: 0.06033 valid_loss: 0.07485 test_loss: 0.08155 \n",
      "[149/500] train_loss: 0.06093 valid_loss: 0.07609 test_loss: 0.08258 \n",
      "[150/500] train_loss: 0.05797 valid_loss: 0.07586 test_loss: 0.08135 \n",
      "[151/500] train_loss: 0.05974 valid_loss: 0.07578 test_loss: 0.08167 \n",
      "[152/500] train_loss: 0.06050 valid_loss: 0.07843 test_loss: 0.08156 \n",
      "[153/500] train_loss: 0.05953 valid_loss: 0.07492 test_loss: 0.08151 \n",
      "[154/500] train_loss: 0.05932 valid_loss: 0.07542 test_loss: 0.08186 \n",
      "[155/500] train_loss: 0.06017 valid_loss: 0.07516 test_loss: 0.08302 \n",
      "[156/500] train_loss: 0.05945 valid_loss: 0.07594 test_loss: 0.08104 \n",
      "[157/500] train_loss: 0.05936 valid_loss: 0.07523 test_loss: 0.08238 \n",
      "[158/500] train_loss: 0.05921 valid_loss: 0.07591 test_loss: 0.08235 \n",
      "[159/500] train_loss: 0.05857 valid_loss: 0.07510 test_loss: 0.08141 \n",
      "[160/500] train_loss: 0.05761 valid_loss: 0.07376 test_loss: 0.08022 \n",
      "[161/500] train_loss: 0.05850 valid_loss: 0.07491 test_loss: 0.08381 \n",
      "[162/500] train_loss: 0.05937 valid_loss: 0.07489 test_loss: 0.08135 \n",
      "[163/500] train_loss: 0.05784 valid_loss: 0.07599 test_loss: 0.08201 \n",
      "[164/500] train_loss: 0.05901 valid_loss: 0.07643 test_loss: 0.08128 \n",
      "[165/500] train_loss: 0.05686 valid_loss: 0.07606 test_loss: 0.08145 \n",
      "[166/500] train_loss: 0.05719 valid_loss: 0.07429 test_loss: 0.08052 \n",
      "[167/500] train_loss: 0.05701 valid_loss: 0.07651 test_loss: 0.08010 \n",
      "[168/500] train_loss: 0.05823 valid_loss: 0.07536 test_loss: 0.08032 \n",
      "[169/500] train_loss: 0.05828 valid_loss: 0.07562 test_loss: 0.08140 \n",
      "[170/500] train_loss: 0.05839 valid_loss: 0.07447 test_loss: 0.08093 \n",
      "[171/500] train_loss: 0.05708 valid_loss: 0.07496 test_loss: 0.08000 \n",
      "[172/500] train_loss: 0.05766 valid_loss: 0.07286 test_loss: 0.08073 \n",
      "验证损失减少 (0.073625 --> 0.072856). 正在保存模型...\n",
      "[173/500] train_loss: 0.05727 valid_loss: 0.07389 test_loss: 0.08375 \n",
      "[174/500] train_loss: 0.05671 valid_loss: 0.07473 test_loss: 0.08035 \n",
      "[175/500] train_loss: 0.05578 valid_loss: 0.07342 test_loss: 0.08124 \n",
      "[176/500] train_loss: 0.05703 valid_loss: 0.07397 test_loss: 0.08033 \n",
      "[177/500] train_loss: 0.05773 valid_loss: 0.07378 test_loss: 0.08004 \n",
      "[178/500] train_loss: 0.05638 valid_loss: 0.07340 test_loss: 0.08168 \n",
      "[179/500] train_loss: 0.05743 valid_loss: 0.07653 test_loss: 0.08052 \n",
      "[180/500] train_loss: 0.05668 valid_loss: 0.07513 test_loss: 0.08084 \n",
      "[181/500] train_loss: 0.05639 valid_loss: 0.07449 test_loss: 0.08112 \n",
      "[182/500] train_loss: 0.05517 valid_loss: 0.07668 test_loss: 0.08135 \n",
      "[183/500] train_loss: 0.05598 valid_loss: 0.07498 test_loss: 0.08038 \n",
      "[184/500] train_loss: 0.05621 valid_loss: 0.07394 test_loss: 0.08221 \n",
      "[185/500] train_loss: 0.05475 valid_loss: 0.07670 test_loss: 0.08172 \n",
      "[186/500] train_loss: 0.05753 valid_loss: 0.07541 test_loss: 0.08200 \n",
      "[187/500] train_loss: 0.05432 valid_loss: 0.07434 test_loss: 0.08078 \n",
      "[188/500] train_loss: 0.05756 valid_loss: 0.07395 test_loss: 0.08204 \n",
      "[189/500] train_loss: 0.05574 valid_loss: 0.07293 test_loss: 0.07970 \n",
      "[190/500] train_loss: 0.05393 valid_loss: 0.07611 test_loss: 0.08046 \n",
      "[191/500] train_loss: 0.05543 valid_loss: 0.07301 test_loss: 0.08128 \n",
      "[192/500] train_loss: 0.05427 valid_loss: 0.07537 test_loss: 0.08049 \n",
      "[193/500] train_loss: 0.05812 valid_loss: 0.07371 test_loss: 0.08014 \n",
      "[194/500] train_loss: 0.05490 valid_loss: 0.07468 test_loss: 0.08220 \n",
      "[195/500] train_loss: 0.05508 valid_loss: 0.07632 test_loss: 0.08145 \n",
      "[196/500] train_loss: 0.05640 valid_loss: 0.07473 test_loss: 0.08068 \n",
      "[197/500] train_loss: 0.05502 valid_loss: 0.07182 test_loss: 0.07991 \n",
      "验证损失减少 (0.072856 --> 0.071822). 正在保存模型...\n",
      "[198/500] train_loss: 0.05431 valid_loss: 0.07322 test_loss: 0.08047 \n",
      "[199/500] train_loss: 0.05482 valid_loss: 0.07351 test_loss: 0.08070 \n",
      "[200/500] train_loss: 0.05554 valid_loss: 0.07425 test_loss: 0.08151 \n",
      "[201/500] train_loss: 0.05416 valid_loss: 0.07549 test_loss: 0.08158 \n",
      "[202/500] train_loss: 0.05375 valid_loss: 0.07330 test_loss: 0.08170 \n",
      "[203/500] train_loss: 0.05572 valid_loss: 0.07175 test_loss: 0.08063 \n",
      "验证损失减少 (0.071822 --> 0.071747). 正在保存模型...\n",
      "[204/500] train_loss: 0.05409 valid_loss: 0.07399 test_loss: 0.08061 \n",
      "[205/500] train_loss: 0.05454 valid_loss: 0.07488 test_loss: 0.08141 \n",
      "[206/500] train_loss: 0.05495 valid_loss: 0.07484 test_loss: 0.08243 \n",
      "[207/500] train_loss: 0.05378 valid_loss: 0.07220 test_loss: 0.08089 \n",
      "[208/500] train_loss: 0.05402 valid_loss: 0.07510 test_loss: 0.07951 \n",
      "[209/500] train_loss: 0.05355 valid_loss: 0.07427 test_loss: 0.08242 \n",
      "[210/500] train_loss: 0.05308 valid_loss: 0.07327 test_loss: 0.08105 \n",
      "[211/500] train_loss: 0.05471 valid_loss: 0.07498 test_loss: 0.08226 \n",
      "[212/500] train_loss: 0.05360 valid_loss: 0.07566 test_loss: 0.08155 \n",
      "[213/500] train_loss: 0.05467 valid_loss: 0.07394 test_loss: 0.08135 \n",
      "[214/500] train_loss: 0.05355 valid_loss: 0.07133 test_loss: 0.08045 \n",
      "验证损失减少 (0.071747 --> 0.071330). 正在保存模型...\n",
      "[215/500] train_loss: 0.05360 valid_loss: 0.07424 test_loss: 0.08208 \n",
      "[216/500] train_loss: 0.05426 valid_loss: 0.07462 test_loss: 0.08175 \n",
      "[217/500] train_loss: 0.05326 valid_loss: 0.07506 test_loss: 0.08092 \n",
      "[218/500] train_loss: 0.05363 valid_loss: 0.07407 test_loss: 0.07950 \n",
      "[219/500] train_loss: 0.05350 valid_loss: 0.07405 test_loss: 0.08031 \n",
      "[220/500] train_loss: 0.05392 valid_loss: 0.07280 test_loss: 0.08084 \n",
      "[221/500] train_loss: 0.05299 valid_loss: 0.07323 test_loss: 0.08106 \n",
      "[222/500] train_loss: 0.05352 valid_loss: 0.07392 test_loss: 0.08149 \n",
      "[223/500] train_loss: 0.05325 valid_loss: 0.07339 test_loss: 0.08177 \n",
      "[224/500] train_loss: 0.05227 valid_loss: 0.07505 test_loss: 0.08106 \n",
      "[225/500] train_loss: 0.05278 valid_loss: 0.07530 test_loss: 0.08259 \n",
      "[226/500] train_loss: 0.05484 valid_loss: 0.07613 test_loss: 0.08086 \n",
      "[227/500] train_loss: 0.05387 valid_loss: 0.07390 test_loss: 0.08052 \n",
      "[228/500] train_loss: 0.05355 valid_loss: 0.07284 test_loss: 0.08033 \n",
      "[229/500] train_loss: 0.05328 valid_loss: 0.07265 test_loss: 0.08201 \n",
      "[230/500] train_loss: 0.05181 valid_loss: 0.07462 test_loss: 0.08337 \n",
      "[231/500] train_loss: 0.05418 valid_loss: 0.07488 test_loss: 0.08015 \n",
      "[232/500] train_loss: 0.05212 valid_loss: 0.07512 test_loss: 0.08188 \n",
      "[233/500] train_loss: 0.05375 valid_loss: 0.07647 test_loss: 0.08299 \n",
      "[234/500] train_loss: 0.05323 valid_loss: 0.07320 test_loss: 0.08242 \n",
      "[235/500] train_loss: 0.05072 valid_loss: 0.07374 test_loss: 0.08129 \n",
      "[236/500] train_loss: 0.05223 valid_loss: 0.07555 test_loss: 0.08199 \n",
      "[237/500] train_loss: 0.05184 valid_loss: 0.07294 test_loss: 0.08136 \n",
      "[238/500] train_loss: 0.05161 valid_loss: 0.07464 test_loss: 0.08300 \n",
      "[239/500] train_loss: 0.05217 valid_loss: 0.07443 test_loss: 0.08139 \n",
      "[240/500] train_loss: 0.05265 valid_loss: 0.07339 test_loss: 0.08073 \n",
      "[241/500] train_loss: 0.05247 valid_loss: 0.07283 test_loss: 0.08139 \n",
      "[242/500] train_loss: 0.05136 valid_loss: 0.07435 test_loss: 0.08330 \n",
      "[243/500] train_loss: 0.05174 valid_loss: 0.07331 test_loss: 0.08194 \n",
      "[244/500] train_loss: 0.05081 valid_loss: 0.07363 test_loss: 0.08051 \n",
      "[245/500] train_loss: 0.05239 valid_loss: 0.07280 test_loss: 0.08072 \n",
      "[246/500] train_loss: 0.05313 valid_loss: 0.07417 test_loss: 0.08071 \n",
      "[247/500] train_loss: 0.05225 valid_loss: 0.07390 test_loss: 0.08181 \n",
      "[248/500] train_loss: 0.05229 valid_loss: 0.07369 test_loss: 0.08213 \n",
      "[249/500] train_loss: 0.05194 valid_loss: 0.07485 test_loss: 0.08040 \n",
      "[250/500] train_loss: 0.05109 valid_loss: 0.07168 test_loss: 0.08017 \n",
      "[251/500] train_loss: 0.05080 valid_loss: 0.07334 test_loss: 0.08033 \n",
      "[252/500] train_loss: 0.05167 valid_loss: 0.07368 test_loss: 0.08058 \n",
      "[253/500] train_loss: 0.05212 valid_loss: 0.07385 test_loss: 0.08080 \n",
      "[254/500] train_loss: 0.05027 valid_loss: 0.07351 test_loss: 0.08247 \n",
      "[255/500] train_loss: 0.05097 valid_loss: 0.07346 test_loss: 0.08213 \n",
      "[256/500] train_loss: 0.05164 valid_loss: 0.07313 test_loss: 0.08133 \n",
      "[257/500] train_loss: 0.05086 valid_loss: 0.07418 test_loss: 0.08072 \n",
      "[258/500] train_loss: 0.05072 valid_loss: 0.07432 test_loss: 0.08124 \n",
      "[259/500] train_loss: 0.05041 valid_loss: 0.07350 test_loss: 0.08004 \n",
      "[260/500] train_loss: 0.05041 valid_loss: 0.07579 test_loss: 0.08312 \n",
      "[261/500] train_loss: 0.04963 valid_loss: 0.07529 test_loss: 0.08098 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[262/500] train_loss: 0.05136 valid_loss: 0.07425 test_loss: 0.08118 \n",
      "[263/500] train_loss: 0.05041 valid_loss: 0.07427 test_loss: 0.08036 \n",
      "[264/500] train_loss: 0.05198 valid_loss: 0.07407 test_loss: 0.08016 \n",
      "[265/500] train_loss: 0.05097 valid_loss: 0.07449 test_loss: 0.08127 \n",
      "[266/500] train_loss: 0.04998 valid_loss: 0.07352 test_loss: 0.08092 \n",
      "[267/500] train_loss: 0.04961 valid_loss: 0.07478 test_loss: 0.08234 \n",
      "[268/500] train_loss: 0.04993 valid_loss: 0.07528 test_loss: 0.08129 \n",
      "[269/500] train_loss: 0.04997 valid_loss: 0.07372 test_loss: 0.08131 \n",
      "[270/500] train_loss: 0.04938 valid_loss: 0.07616 test_loss: 0.08183 \n",
      "[271/500] train_loss: 0.05163 valid_loss: 0.07491 test_loss: 0.08200 \n",
      "[272/500] train_loss: 0.05194 valid_loss: 0.07303 test_loss: 0.08003 \n",
      "[273/500] train_loss: 0.04947 valid_loss: 0.07266 test_loss: 0.08158 \n",
      "[274/500] train_loss: 0.05080 valid_loss: 0.07388 test_loss: 0.08188 \n",
      "[275/500] train_loss: 0.05049 valid_loss: 0.07588 test_loss: 0.08315 \n",
      "[276/500] train_loss: 0.04894 valid_loss: 0.07551 test_loss: 0.08229 \n",
      "[277/500] train_loss: 0.04862 valid_loss: 0.07422 test_loss: 0.08133 \n",
      "[278/500] train_loss: 0.04802 valid_loss: 0.07348 test_loss: 0.08276 \n",
      "[279/500] train_loss: 0.04875 valid_loss: 0.07513 test_loss: 0.08307 \n",
      "[280/500] train_loss: 0.04891 valid_loss: 0.07415 test_loss: 0.08113 \n",
      "[281/500] train_loss: 0.04948 valid_loss: 0.07719 test_loss: 0.08431 \n",
      "[282/500] train_loss: 0.04820 valid_loss: 0.07443 test_loss: 0.08188 \n",
      "[283/500] train_loss: 0.05036 valid_loss: 0.07469 test_loss: 0.08155 \n",
      "[284/500] train_loss: 0.05075 valid_loss: 0.07371 test_loss: 0.08108 \n",
      "[285/500] train_loss: 0.04989 valid_loss: 0.07368 test_loss: 0.07987 \n",
      "[286/500] train_loss: 0.04985 valid_loss: 0.07316 test_loss: 0.07997 \n",
      "[287/500] train_loss: 0.04997 valid_loss: 0.07267 test_loss: 0.08035 \n",
      "[288/500] train_loss: 0.04803 valid_loss: 0.07400 test_loss: 0.08165 \n",
      "[289/500] train_loss: 0.05015 valid_loss: 0.08338 test_loss: 0.08394 \n",
      "[290/500] train_loss: 0.04910 valid_loss: 0.07387 test_loss: 0.08021 \n",
      "[291/500] train_loss: 0.04950 valid_loss: 0.07504 test_loss: 0.07989 \n",
      "[292/500] train_loss: 0.04866 valid_loss: 0.07593 test_loss: 0.08086 \n",
      "[293/500] train_loss: 0.04880 valid_loss: 0.07377 test_loss: 0.08021 \n",
      "[294/500] train_loss: 0.04956 valid_loss: 0.07567 test_loss: 0.08046 \n",
      "[295/500] train_loss: 0.04939 valid_loss: 0.07568 test_loss: 0.08140 \n",
      "[296/500] train_loss: 0.04711 valid_loss: 0.07371 test_loss: 0.08065 \n",
      "[297/500] train_loss: 0.04875 valid_loss: 0.07415 test_loss: 0.08044 \n",
      "[298/500] train_loss: 0.04944 valid_loss: 0.07447 test_loss: 0.08203 \n",
      "[299/500] train_loss: 0.04958 valid_loss: 0.07791 test_loss: 0.08193 \n",
      "[300/500] train_loss: 0.04952 valid_loss: 0.07510 test_loss: 0.08224 \n",
      "[301/500] train_loss: 0.04895 valid_loss: 0.07405 test_loss: 0.08092 \n",
      "[302/500] train_loss: 0.04760 valid_loss: 0.07505 test_loss: 0.08141 \n",
      "[303/500] train_loss: 0.04912 valid_loss: 0.07471 test_loss: 0.08170 \n",
      "[304/500] train_loss: 0.04863 valid_loss: 0.07985 test_loss: 0.08333 \n",
      "[305/500] train_loss: 0.04777 valid_loss: 0.07602 test_loss: 0.08127 \n",
      "[306/500] train_loss: 0.04906 valid_loss: 0.07461 test_loss: 0.08200 \n",
      "[307/500] train_loss: 0.04754 valid_loss: 0.07491 test_loss: 0.08099 \n",
      "[308/500] train_loss: 0.04804 valid_loss: 0.07543 test_loss: 0.07959 \n",
      "[309/500] train_loss: 0.04882 valid_loss: 0.07345 test_loss: 0.07973 \n",
      "[310/500] train_loss: 0.04767 valid_loss: 0.07522 test_loss: 0.08078 \n",
      "[311/500] train_loss: 0.04930 valid_loss: 0.07845 test_loss: 0.08076 \n",
      "[312/500] train_loss: 0.04900 valid_loss: 0.07859 test_loss: 0.08150 \n",
      "[313/500] train_loss: 0.04828 valid_loss: 0.07750 test_loss: 0.08228 \n",
      "[314/500] train_loss: 0.04697 valid_loss: 0.07600 test_loss: 0.07959 \n",
      "[315/500] train_loss: 0.04737 valid_loss: 0.07485 test_loss: 0.08107 \n",
      "[316/500] train_loss: 0.04763 valid_loss: 0.07697 test_loss: 0.08119 \n",
      "[317/500] train_loss: 0.04708 valid_loss: 0.07893 test_loss: 0.08220 \n",
      "[318/500] train_loss: 0.04763 valid_loss: 0.07536 test_loss: 0.08184 \n",
      "[319/500] train_loss: 0.04733 valid_loss: 0.07670 test_loss: 0.08291 \n",
      "[320/500] train_loss: 0.04672 valid_loss: 0.07604 test_loss: 0.08247 \n",
      "[321/500] train_loss: 0.04892 valid_loss: 0.07536 test_loss: 0.08193 \n",
      "[322/500] train_loss: 0.04798 valid_loss: 0.07620 test_loss: 0.08076 \n",
      "[323/500] train_loss: 0.04697 valid_loss: 0.07664 test_loss: 0.08142 \n",
      "[324/500] train_loss: 0.04676 valid_loss: 0.07612 test_loss: 0.08128 \n",
      "[325/500] train_loss: 0.04783 valid_loss: 0.07711 test_loss: 0.08286 \n",
      "[326/500] train_loss: 0.04730 valid_loss: 0.07554 test_loss: 0.08180 \n",
      "[327/500] train_loss: 0.04900 valid_loss: 0.07636 test_loss: 0.08187 \n",
      "[328/500] train_loss: 0.04758 valid_loss: 0.07554 test_loss: 0.08004 \n",
      "[329/500] train_loss: 0.04806 valid_loss: 0.07889 test_loss: 0.08242 \n",
      "[330/500] train_loss: 0.04731 valid_loss: 0.07512 test_loss: 0.08207 \n",
      "[331/500] train_loss: 0.04790 valid_loss: 0.07602 test_loss: 0.08132 \n",
      "[332/500] train_loss: 0.04710 valid_loss: 0.07590 test_loss: 0.08165 \n",
      "[333/500] train_loss: 0.04797 valid_loss: 0.07487 test_loss: 0.08055 \n",
      "[334/500] train_loss: 0.04642 valid_loss: 0.07489 test_loss: 0.08246 \n",
      "[335/500] train_loss: 0.04703 valid_loss: 0.07422 test_loss: 0.08101 \n",
      "[336/500] train_loss: 0.04650 valid_loss: 0.07506 test_loss: 0.08249 \n",
      "[337/500] train_loss: 0.04727 valid_loss: 0.07620 test_loss: 0.08176 \n",
      "[338/500] train_loss: 0.04675 valid_loss: 0.07459 test_loss: 0.08128 \n",
      "[339/500] train_loss: 0.04689 valid_loss: 0.07615 test_loss: 0.08265 \n",
      "[340/500] train_loss: 0.04719 valid_loss: 0.07882 test_loss: 0.08295 \n",
      "[341/500] train_loss: 0.04519 valid_loss: 0.07593 test_loss: 0.08253 \n",
      "[342/500] train_loss: 0.04711 valid_loss: 0.08004 test_loss: 0.08326 \n",
      "[343/500] train_loss: 0.04876 valid_loss: 0.07576 test_loss: 0.08197 \n",
      "[344/500] train_loss: 0.04666 valid_loss: 0.07411 test_loss: 0.08108 \n",
      "[345/500] train_loss: 0.04734 valid_loss: 0.07435 test_loss: 0.08143 \n",
      "[346/500] train_loss: 0.04688 valid_loss: 0.07586 test_loss: 0.08109 \n",
      "[347/500] train_loss: 0.04669 valid_loss: 0.07589 test_loss: 0.08128 \n",
      "[348/500] train_loss: 0.04634 valid_loss: 0.07547 test_loss: 0.08067 \n",
      "[349/500] train_loss: 0.04726 valid_loss: 0.07499 test_loss: 0.08146 \n",
      "[350/500] train_loss: 0.04728 valid_loss: 0.07423 test_loss: 0.08177 \n",
      "[351/500] train_loss: 0.04585 valid_loss: 0.07358 test_loss: 0.08170 \n",
      "[352/500] train_loss: 0.04623 valid_loss: 0.07506 test_loss: 0.08149 \n",
      "[353/500] train_loss: 0.04660 valid_loss: 0.07516 test_loss: 0.08237 \n",
      "[354/500] train_loss: 0.04763 valid_loss: 0.07682 test_loss: 0.08098 \n",
      "[355/500] train_loss: 0.04654 valid_loss: 0.07678 test_loss: 0.08139 \n",
      "[356/500] train_loss: 0.04601 valid_loss: 0.07485 test_loss: 0.08121 \n",
      "[357/500] train_loss: 0.04643 valid_loss: 0.07393 test_loss: 0.08255 \n",
      "[358/500] train_loss: 0.04649 valid_loss: 0.07662 test_loss: 0.08157 \n",
      "[359/500] train_loss: 0.04545 valid_loss: 0.07376 test_loss: 0.08108 \n",
      "[360/500] train_loss: 0.04704 valid_loss: 0.07486 test_loss: 0.08065 \n",
      "[361/500] train_loss: 0.04529 valid_loss: 0.07518 test_loss: 0.08040 \n",
      "[362/500] train_loss: 0.04667 valid_loss: 0.07578 test_loss: 0.08043 \n",
      "[363/500] train_loss: 0.04596 valid_loss: 0.07696 test_loss: 0.08090 \n",
      "[364/500] train_loss: 0.04686 valid_loss: 0.07517 test_loss: 0.08086 \n",
      "[365/500] train_loss: 0.04684 valid_loss: 0.07413 test_loss: 0.08077 \n",
      "[366/500] train_loss: 0.04406 valid_loss: 0.07539 test_loss: 0.08056 \n",
      "[367/500] train_loss: 0.04729 valid_loss: 0.07575 test_loss: 0.08060 \n",
      "[368/500] train_loss: 0.04486 valid_loss: 0.07762 test_loss: 0.08151 \n",
      "[369/500] train_loss: 0.04410 valid_loss: 0.07824 test_loss: 0.08205 \n",
      "[370/500] train_loss: 0.04574 valid_loss: 0.07618 test_loss: 0.08115 \n",
      "[371/500] train_loss: 0.04605 valid_loss: 0.07485 test_loss: 0.08093 \n",
      "[372/500] train_loss: 0.04579 valid_loss: 0.07439 test_loss: 0.08150 \n",
      "[373/500] train_loss: 0.04561 valid_loss: 0.07653 test_loss: 0.08357 \n",
      "[374/500] train_loss: 0.04482 valid_loss: 0.07348 test_loss: 0.08194 \n",
      "[375/500] train_loss: 0.04543 valid_loss: 0.07496 test_loss: 0.08163 \n",
      "[376/500] train_loss: 0.04524 valid_loss: 0.07415 test_loss: 0.08197 \n",
      "[377/500] train_loss: 0.04470 valid_loss: 0.07418 test_loss: 0.08177 \n",
      "[378/500] train_loss: 0.04518 valid_loss: 0.07554 test_loss: 0.08136 \n",
      "[379/500] train_loss: 0.04519 valid_loss: 0.07509 test_loss: 0.08203 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380/500] train_loss: 0.04553 valid_loss: 0.07318 test_loss: 0.08234 \n",
      "[381/500] train_loss: 0.04486 valid_loss: 0.07324 test_loss: 0.08176 \n",
      "[382/500] train_loss: 0.04426 valid_loss: 0.07469 test_loss: 0.08150 \n",
      "[383/500] train_loss: 0.04447 valid_loss: 0.07515 test_loss: 0.08269 \n",
      "[384/500] train_loss: 0.04437 valid_loss: 0.07481 test_loss: 0.08175 \n",
      "[385/500] train_loss: 0.04565 valid_loss: 0.07443 test_loss: 0.08248 \n",
      "[386/500] train_loss: 0.04546 valid_loss: 0.07360 test_loss: 0.08419 \n",
      "[387/500] train_loss: 0.04491 valid_loss: 0.07412 test_loss: 0.08391 \n",
      "[388/500] train_loss: 0.04480 valid_loss: 0.07558 test_loss: 0.08402 \n",
      "[389/500] train_loss: 0.04392 valid_loss: 0.07956 test_loss: 0.08251 \n",
      "[390/500] train_loss: 0.04453 valid_loss: 0.07456 test_loss: 0.08377 \n",
      "[391/500] train_loss: 0.04462 valid_loss: 0.07534 test_loss: 0.08390 \n",
      "[392/500] train_loss: 0.04365 valid_loss: 0.07550 test_loss: 0.08361 \n",
      "[393/500] train_loss: 0.04565 valid_loss: 0.07428 test_loss: 0.08214 \n",
      "[394/500] train_loss: 0.04431 valid_loss: 0.07615 test_loss: 0.08267 \n",
      "[395/500] train_loss: 0.04395 valid_loss: 0.07505 test_loss: 0.08259 \n",
      "[396/500] train_loss: 0.04439 valid_loss: 0.07755 test_loss: 0.08359 \n",
      "[397/500] train_loss: 0.04305 valid_loss: 0.07754 test_loss: 0.08251 \n",
      "[398/500] train_loss: 0.04474 valid_loss: 0.07672 test_loss: 0.08147 \n",
      "[399/500] train_loss: 0.04552 valid_loss: 0.07658 test_loss: 0.08173 \n",
      "[400/500] train_loss: 0.04453 valid_loss: 0.07619 test_loss: 0.08115 \n",
      "[401/500] train_loss: 0.04498 valid_loss: 0.07735 test_loss: 0.08239 \n",
      "[402/500] train_loss: 0.04481 valid_loss: 0.07717 test_loss: 0.08267 \n",
      "[403/500] train_loss: 0.04476 valid_loss: 0.07872 test_loss: 0.08275 \n",
      "[404/500] train_loss: 0.04454 valid_loss: 0.07995 test_loss: 0.07984 \n",
      "[405/500] train_loss: 0.04381 valid_loss: 0.08131 test_loss: 0.08271 \n",
      "[406/500] train_loss: 0.04428 valid_loss: 0.07770 test_loss: 0.08262 \n",
      "[407/500] train_loss: 0.04432 valid_loss: 0.07784 test_loss: 0.08258 \n",
      "[408/500] train_loss: 0.04352 valid_loss: 0.07541 test_loss: 0.08061 \n",
      "[409/500] train_loss: 0.04543 valid_loss: 0.07874 test_loss: 0.07966 \n",
      "[410/500] train_loss: 0.04331 valid_loss: 0.07388 test_loss: 0.08121 \n",
      "[411/500] train_loss: 0.04375 valid_loss: 0.07466 test_loss: 0.08286 \n",
      "[412/500] train_loss: 0.04558 valid_loss: 0.07686 test_loss: 0.08333 \n",
      "[413/500] train_loss: 0.04324 valid_loss: 0.07383 test_loss: 0.08164 \n",
      "[414/500] train_loss: 0.04264 valid_loss: 0.07497 test_loss: 0.08182 \n",
      "[415/500] train_loss: 0.04472 valid_loss: 0.07708 test_loss: 0.08025 \n",
      "[416/500] train_loss: 0.04420 valid_loss: 0.07628 test_loss: 0.08222 \n",
      "[417/500] train_loss: 0.04309 valid_loss: 0.07476 test_loss: 0.08174 \n",
      "[418/500] train_loss: 0.04446 valid_loss: 0.07708 test_loss: 0.08180 \n",
      "[419/500] train_loss: 0.04385 valid_loss: 0.07757 test_loss: 0.08173 \n",
      "[420/500] train_loss: 0.04371 valid_loss: 0.07568 test_loss: 0.08138 \n",
      "[421/500] train_loss: 0.04316 valid_loss: 0.07370 test_loss: 0.08128 \n",
      "[422/500] train_loss: 0.04286 valid_loss: 0.07443 test_loss: 0.08235 \n",
      "[423/500] train_loss: 0.04475 valid_loss: 0.07562 test_loss: 0.08180 \n",
      "[424/500] train_loss: 0.04420 valid_loss: 0.07450 test_loss: 0.08217 \n",
      "[425/500] train_loss: 0.04295 valid_loss: 0.07457 test_loss: 0.08319 \n",
      "[426/500] train_loss: 0.04236 valid_loss: 0.07629 test_loss: 0.08309 \n",
      "[427/500] train_loss: 0.04326 valid_loss: 0.07644 test_loss: 0.08506 \n",
      "[428/500] train_loss: 0.04341 valid_loss: 0.07710 test_loss: 0.08318 \n",
      "[429/500] train_loss: 0.04476 valid_loss: 0.07525 test_loss: 0.08200 \n",
      "[430/500] train_loss: 0.04276 valid_loss: 0.07634 test_loss: 0.08106 \n",
      "[431/500] train_loss: 0.04343 valid_loss: 0.07945 test_loss: 0.08300 \n",
      "[432/500] train_loss: 0.04266 valid_loss: 0.07757 test_loss: 0.08113 \n",
      "[433/500] train_loss: 0.04299 valid_loss: 0.08087 test_loss: 0.08195 \n",
      "[434/500] train_loss: 0.04280 valid_loss: 0.07959 test_loss: 0.08311 \n",
      "[435/500] train_loss: 0.04312 valid_loss: 0.07979 test_loss: 0.08416 \n",
      "[436/500] train_loss: 0.04296 valid_loss: 0.07506 test_loss: 0.08226 \n",
      "[437/500] train_loss: 0.04183 valid_loss: 0.07684 test_loss: 0.08406 \n",
      "[438/500] train_loss: 0.04317 valid_loss: 0.07782 test_loss: 0.08237 \n",
      "[439/500] train_loss: 0.04346 valid_loss: 0.07978 test_loss: 0.08360 \n",
      "[440/500] train_loss: 0.04409 valid_loss: 0.07669 test_loss: 0.08333 \n",
      "[441/500] train_loss: 0.04328 valid_loss: 0.07748 test_loss: 0.08237 \n",
      "[442/500] train_loss: 0.04433 valid_loss: 0.07679 test_loss: 0.08191 \n",
      "[443/500] train_loss: 0.04283 valid_loss: 0.08111 test_loss: 0.08206 \n",
      "[444/500] train_loss: 0.04308 valid_loss: 0.07619 test_loss: 0.08141 \n",
      "[445/500] train_loss: 0.04325 valid_loss: 0.07409 test_loss: 0.08146 \n",
      "[446/500] train_loss: 0.04435 valid_loss: 0.07623 test_loss: 0.08041 \n",
      "[447/500] train_loss: 0.04345 valid_loss: 0.07815 test_loss: 0.08189 \n",
      "[448/500] train_loss: 0.04473 valid_loss: 0.07788 test_loss: 0.08121 \n",
      "[449/500] train_loss: 0.04190 valid_loss: 0.07594 test_loss: 0.08229 \n",
      "[450/500] train_loss: 0.04391 valid_loss: 0.07765 test_loss: 0.08028 \n",
      "[451/500] train_loss: 0.04395 valid_loss: 0.07798 test_loss: 0.08139 \n",
      "[452/500] train_loss: 0.04183 valid_loss: 0.07559 test_loss: 0.08176 \n",
      "[453/500] train_loss: 0.04348 valid_loss: 0.08185 test_loss: 0.08230 \n",
      "[454/500] train_loss: 0.04266 valid_loss: 0.07769 test_loss: 0.08059 \n",
      "[455/500] train_loss: 0.04312 valid_loss: 0.07802 test_loss: 0.08187 \n",
      "[456/500] train_loss: 0.04320 valid_loss: 0.07465 test_loss: 0.08179 \n",
      "[457/500] train_loss: 0.04195 valid_loss: 0.07595 test_loss: 0.08176 \n",
      "[458/500] train_loss: 0.04176 valid_loss: 0.07550 test_loss: 0.08045 \n",
      "[459/500] train_loss: 0.04415 valid_loss: 0.07394 test_loss: 0.08104 \n",
      "[460/500] train_loss: 0.04159 valid_loss: 0.07610 test_loss: 0.08242 \n",
      "[461/500] train_loss: 0.04272 valid_loss: 0.07475 test_loss: 0.08543 \n",
      "[462/500] train_loss: 0.04189 valid_loss: 0.07509 test_loss: 0.08211 \n",
      "[463/500] train_loss: 0.04219 valid_loss: 0.07662 test_loss: 0.08184 \n",
      "[464/500] train_loss: 0.04260 valid_loss: 0.07760 test_loss: 0.08174 \n",
      "[465/500] train_loss: 0.04230 valid_loss: 0.07523 test_loss: 0.08281 \n",
      "[466/500] train_loss: 0.04172 valid_loss: 0.07886 test_loss: 0.08161 \n",
      "[467/500] train_loss: 0.04246 valid_loss: 0.07677 test_loss: 0.08153 \n",
      "[468/500] train_loss: 0.04225 valid_loss: 0.07563 test_loss: 0.08132 \n",
      "[469/500] train_loss: 0.04250 valid_loss: 0.07897 test_loss: 0.08299 \n",
      "[470/500] train_loss: 0.04270 valid_loss: 0.07637 test_loss: 0.08172 \n",
      "[471/500] train_loss: 0.04160 valid_loss: 0.07612 test_loss: 0.08247 \n",
      "[472/500] train_loss: 0.04256 valid_loss: 0.07674 test_loss: 0.08276 \n",
      "[473/500] train_loss: 0.04180 valid_loss: 0.07527 test_loss: 0.08306 \n",
      "[474/500] train_loss: 0.04163 valid_loss: 0.07562 test_loss: 0.08321 \n",
      "[475/500] train_loss: 0.04202 valid_loss: 0.07604 test_loss: 0.08359 \n",
      "[476/500] train_loss: 0.04328 valid_loss: 0.07454 test_loss: 0.08311 \n",
      "[477/500] train_loss: 0.04258 valid_loss: 0.07262 test_loss: 0.08239 \n",
      "[478/500] train_loss: 0.04164 valid_loss: 0.07403 test_loss: 0.08164 \n",
      "[479/500] train_loss: 0.04245 valid_loss: 0.07557 test_loss: 0.08297 \n",
      "[480/500] train_loss: 0.04073 valid_loss: 0.07529 test_loss: 0.08243 \n",
      "[481/500] train_loss: 0.04210 valid_loss: 0.07501 test_loss: 0.08353 \n",
      "[482/500] train_loss: 0.04115 valid_loss: 0.07442 test_loss: 0.08115 \n",
      "[483/500] train_loss: 0.04312 valid_loss: 0.07572 test_loss: 0.08212 \n",
      "[484/500] train_loss: 0.04157 valid_loss: 0.07614 test_loss: 0.08292 \n",
      "[485/500] train_loss: 0.04047 valid_loss: 0.07573 test_loss: 0.08296 \n",
      "[486/500] train_loss: 0.04227 valid_loss: 0.07245 test_loss: 0.08082 \n",
      "[487/500] train_loss: 0.04344 valid_loss: 0.07504 test_loss: 0.08207 \n",
      "[488/500] train_loss: 0.04286 valid_loss: 0.07475 test_loss: 0.08150 \n",
      "[489/500] train_loss: 0.04209 valid_loss: 0.07530 test_loss: 0.08253 \n",
      "[490/500] train_loss: 0.04175 valid_loss: 0.07578 test_loss: 0.08235 \n",
      "[491/500] train_loss: 0.04089 valid_loss: 0.07476 test_loss: 0.08274 \n",
      "[492/500] train_loss: 0.04351 valid_loss: 0.07894 test_loss: 0.08230 \n",
      "[493/500] train_loss: 0.04278 valid_loss: 0.07417 test_loss: 0.08159 \n",
      "[494/500] train_loss: 0.04153 valid_loss: 0.07484 test_loss: 0.08219 \n",
      "[495/500] train_loss: 0.04085 valid_loss: 0.07713 test_loss: 0.08177 \n",
      "[496/500] train_loss: 0.04088 valid_loss: 0.07837 test_loss: 0.08208 \n",
      "[497/500] train_loss: 0.04126 valid_loss: 0.07616 test_loss: 0.08373 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[498/500] train_loss: 0.04164 valid_loss: 0.07619 test_loss: 0.08157 \n",
      "[499/500] train_loss: 0.04181 valid_loss: 0.07602 test_loss: 0.08207 \n",
      "[500/500] train_loss: 0.04178 valid_loss: 0.07632 test_loss: 0.08139 \n",
      "TRAINING MODEL 2\n",
      "[  1/500] train_loss: 0.35372 valid_loss: 0.25025 test_loss: 0.25564 \n",
      "验证损失减少 (inf --> 0.250249). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18991 valid_loss: 0.17937 test_loss: 0.18445 \n",
      "验证损失减少 (0.250249 --> 0.179373). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15233 valid_loss: 0.15891 test_loss: 0.16964 \n",
      "验证损失减少 (0.179373 --> 0.158914). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13802 valid_loss: 0.13874 test_loss: 0.14461 \n",
      "验证损失减少 (0.158914 --> 0.138741). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12907 valid_loss: 0.13111 test_loss: 0.13966 \n",
      "验证损失减少 (0.138741 --> 0.131113). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12381 valid_loss: 0.12599 test_loss: 0.13356 \n",
      "验证损失减少 (0.131113 --> 0.125989). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11829 valid_loss: 0.12085 test_loss: 0.12776 \n",
      "验证损失减少 (0.125989 --> 0.120849). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11571 valid_loss: 0.11789 test_loss: 0.12478 \n",
      "验证损失减少 (0.120849 --> 0.117894). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11552 valid_loss: 0.11501 test_loss: 0.12376 \n",
      "验证损失减少 (0.117894 --> 0.115012). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10955 valid_loss: 0.11304 test_loss: 0.11915 \n",
      "验证损失减少 (0.115012 --> 0.113041). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10958 valid_loss: 0.11117 test_loss: 0.11837 \n",
      "验证损失减少 (0.113041 --> 0.111170). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10506 valid_loss: 0.10835 test_loss: 0.11530 \n",
      "验证损失减少 (0.111170 --> 0.108347). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10550 valid_loss: 0.10522 test_loss: 0.11346 \n",
      "验证损失减少 (0.108347 --> 0.105216). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10421 valid_loss: 0.10661 test_loss: 0.11370 \n",
      "[ 15/500] train_loss: 0.09967 valid_loss: 0.10262 test_loss: 0.11005 \n",
      "验证损失减少 (0.105216 --> 0.102620). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09667 valid_loss: 0.10368 test_loss: 0.10944 \n",
      "[ 17/500] train_loss: 0.09839 valid_loss: 0.09795 test_loss: 0.10771 \n",
      "验证损失减少 (0.102620 --> 0.097953). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09655 valid_loss: 0.09947 test_loss: 0.10631 \n",
      "[ 19/500] train_loss: 0.09282 valid_loss: 0.09814 test_loss: 0.10758 \n",
      "[ 20/500] train_loss: 0.09507 valid_loss: 0.09979 test_loss: 0.10597 \n",
      "[ 21/500] train_loss: 0.09360 valid_loss: 0.09896 test_loss: 0.10723 \n",
      "[ 22/500] train_loss: 0.09161 valid_loss: 0.09585 test_loss: 0.10469 \n",
      "验证损失减少 (0.097953 --> 0.095846). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09325 valid_loss: 0.09405 test_loss: 0.10300 \n",
      "验证损失减少 (0.095846 --> 0.094051). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09346 valid_loss: 0.09659 test_loss: 0.10193 \n",
      "[ 25/500] train_loss: 0.08963 valid_loss: 0.09424 test_loss: 0.10001 \n",
      "[ 26/500] train_loss: 0.09058 valid_loss: 0.09361 test_loss: 0.09994 \n",
      "验证损失减少 (0.094051 --> 0.093608). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.08776 valid_loss: 0.09139 test_loss: 0.09965 \n",
      "验证损失减少 (0.093608 --> 0.091393). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08744 valid_loss: 0.09091 test_loss: 0.10019 \n",
      "验证损失减少 (0.091393 --> 0.090911). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08532 valid_loss: 0.09107 test_loss: 0.09810 \n",
      "[ 30/500] train_loss: 0.08701 valid_loss: 0.09141 test_loss: 0.09801 \n",
      "[ 31/500] train_loss: 0.08593 valid_loss: 0.09178 test_loss: 0.09953 \n",
      "[ 32/500] train_loss: 0.08513 valid_loss: 0.08841 test_loss: 0.09737 \n",
      "验证损失减少 (0.090911 --> 0.088405). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08314 valid_loss: 0.08867 test_loss: 0.09785 \n",
      "[ 34/500] train_loss: 0.08304 valid_loss: 0.08941 test_loss: 0.09831 \n",
      "[ 35/500] train_loss: 0.08457 valid_loss: 0.08947 test_loss: 0.09798 \n",
      "[ 36/500] train_loss: 0.08422 valid_loss: 0.08807 test_loss: 0.09539 \n",
      "验证损失减少 (0.088405 --> 0.088068). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08151 valid_loss: 0.08854 test_loss: 0.09515 \n",
      "[ 38/500] train_loss: 0.08266 valid_loss: 0.08671 test_loss: 0.09496 \n",
      "验证损失减少 (0.088068 --> 0.086708). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.08189 valid_loss: 0.08724 test_loss: 0.09520 \n",
      "[ 40/500] train_loss: 0.08153 valid_loss: 0.08481 test_loss: 0.09472 \n",
      "验证损失减少 (0.086708 --> 0.084815). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.08036 valid_loss: 0.08475 test_loss: 0.09494 \n",
      "验证损失减少 (0.084815 --> 0.084751). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.08118 valid_loss: 0.08425 test_loss: 0.09390 \n",
      "验证损失减少 (0.084751 --> 0.084251). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.08267 valid_loss: 0.08454 test_loss: 0.09250 \n",
      "[ 44/500] train_loss: 0.08139 valid_loss: 0.08410 test_loss: 0.09117 \n",
      "验证损失减少 (0.084251 --> 0.084103). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.08069 valid_loss: 0.08334 test_loss: 0.09338 \n",
      "验证损失减少 (0.084103 --> 0.083337). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07974 valid_loss: 0.08593 test_loss: 0.09390 \n",
      "[ 47/500] train_loss: 0.07945 valid_loss: 0.08444 test_loss: 0.09173 \n",
      "[ 48/500] train_loss: 0.07874 valid_loss: 0.08281 test_loss: 0.08963 \n",
      "验证损失减少 (0.083337 --> 0.082810). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07721 valid_loss: 0.08380 test_loss: 0.09170 \n",
      "[ 50/500] train_loss: 0.07837 valid_loss: 0.08346 test_loss: 0.09050 \n",
      "[ 51/500] train_loss: 0.07761 valid_loss: 0.08371 test_loss: 0.09097 \n",
      "[ 52/500] train_loss: 0.07671 valid_loss: 0.08268 test_loss: 0.09067 \n",
      "验证损失减少 (0.082810 --> 0.082679). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07677 valid_loss: 0.08042 test_loss: 0.08939 \n",
      "验证损失减少 (0.082679 --> 0.080422). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07672 valid_loss: 0.08166 test_loss: 0.08927 \n",
      "[ 55/500] train_loss: 0.07485 valid_loss: 0.08096 test_loss: 0.08923 \n",
      "[ 56/500] train_loss: 0.07628 valid_loss: 0.08056 test_loss: 0.08855 \n",
      "[ 57/500] train_loss: 0.07568 valid_loss: 0.08028 test_loss: 0.08993 \n",
      "验证损失减少 (0.080422 --> 0.080278). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.07468 valid_loss: 0.08027 test_loss: 0.08956 \n",
      "验证损失减少 (0.080278 --> 0.080269). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07265 valid_loss: 0.08110 test_loss: 0.08896 \n",
      "[ 60/500] train_loss: 0.07383 valid_loss: 0.08062 test_loss: 0.09027 \n",
      "[ 61/500] train_loss: 0.07656 valid_loss: 0.08156 test_loss: 0.09032 \n",
      "[ 62/500] train_loss: 0.07436 valid_loss: 0.07917 test_loss: 0.08681 \n",
      "验证损失减少 (0.080269 --> 0.079173). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.07432 valid_loss: 0.08248 test_loss: 0.08924 \n",
      "[ 64/500] train_loss: 0.07286 valid_loss: 0.07946 test_loss: 0.08773 \n",
      "[ 65/500] train_loss: 0.07360 valid_loss: 0.08155 test_loss: 0.08864 \n",
      "[ 66/500] train_loss: 0.07440 valid_loss: 0.08071 test_loss: 0.08743 \n",
      "[ 67/500] train_loss: 0.07174 valid_loss: 0.07976 test_loss: 0.08611 \n",
      "[ 68/500] train_loss: 0.07088 valid_loss: 0.07929 test_loss: 0.08549 \n",
      "[ 69/500] train_loss: 0.07331 valid_loss: 0.07921 test_loss: 0.08655 \n",
      "[ 70/500] train_loss: 0.07262 valid_loss: 0.08039 test_loss: 0.08608 \n",
      "[ 71/500] train_loss: 0.07037 valid_loss: 0.07999 test_loss: 0.08539 \n",
      "[ 72/500] train_loss: 0.07333 valid_loss: 0.08076 test_loss: 0.08720 \n",
      "[ 73/500] train_loss: 0.07213 valid_loss: 0.07968 test_loss: 0.08678 \n",
      "[ 74/500] train_loss: 0.07263 valid_loss: 0.07990 test_loss: 0.08595 \n",
      "[ 75/500] train_loss: 0.07168 valid_loss: 0.08020 test_loss: 0.08715 \n",
      "[ 76/500] train_loss: 0.07067 valid_loss: 0.07893 test_loss: 0.08765 \n",
      "验证损失减少 (0.079173 --> 0.078933). 正在保存模型...\n",
      "[ 77/500] train_loss: 0.07141 valid_loss: 0.07775 test_loss: 0.08483 \n",
      "验证损失减少 (0.078933 --> 0.077752). 正在保存模型...\n",
      "[ 78/500] train_loss: 0.06958 valid_loss: 0.07913 test_loss: 0.08657 \n",
      "[ 79/500] train_loss: 0.07244 valid_loss: 0.07752 test_loss: 0.08510 \n",
      "验证损失减少 (0.077752 --> 0.077519). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.07038 valid_loss: 0.07868 test_loss: 0.08508 \n",
      "[ 81/500] train_loss: 0.06948 valid_loss: 0.07929 test_loss: 0.08557 \n",
      "[ 82/500] train_loss: 0.06863 valid_loss: 0.07732 test_loss: 0.08570 \n",
      "验证损失减少 (0.077519 --> 0.077317). 正在保存模型...\n",
      "[ 83/500] train_loss: 0.07037 valid_loss: 0.07786 test_loss: 0.08518 \n",
      "[ 84/500] train_loss: 0.07033 valid_loss: 0.08213 test_loss: 0.08540 \n",
      "[ 85/500] train_loss: 0.07004 valid_loss: 0.07768 test_loss: 0.08439 \n",
      "[ 86/500] train_loss: 0.06878 valid_loss: 0.08309 test_loss: 0.08582 \n",
      "[ 87/500] train_loss: 0.06919 valid_loss: 0.07809 test_loss: 0.08472 \n",
      "[ 88/500] train_loss: 0.06749 valid_loss: 0.08265 test_loss: 0.08411 \n",
      "[ 89/500] train_loss: 0.06808 valid_loss: 0.07642 test_loss: 0.08519 \n",
      "验证损失减少 (0.077317 --> 0.076415). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.06819 valid_loss: 0.08003 test_loss: 0.08353 \n",
      "[ 91/500] train_loss: 0.06556 valid_loss: 0.07791 test_loss: 0.08249 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 92/500] train_loss: 0.06987 valid_loss: 0.08112 test_loss: 0.08451 \n",
      "[ 93/500] train_loss: 0.06772 valid_loss: 0.07678 test_loss: 0.08638 \n",
      "[ 94/500] train_loss: 0.06849 valid_loss: 0.07751 test_loss: 0.08381 \n",
      "[ 95/500] train_loss: 0.06701 valid_loss: 0.07832 test_loss: 0.08477 \n",
      "[ 96/500] train_loss: 0.06645 valid_loss: 0.07808 test_loss: 0.08503 \n",
      "[ 97/500] train_loss: 0.06754 valid_loss: 0.07634 test_loss: 0.08183 \n",
      "验证损失减少 (0.076415 --> 0.076342). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.06607 valid_loss: 0.07661 test_loss: 0.08375 \n",
      "[ 99/500] train_loss: 0.06564 valid_loss: 0.07589 test_loss: 0.08428 \n",
      "验证损失减少 (0.076342 --> 0.075894). 正在保存模型...\n",
      "[100/500] train_loss: 0.06646 valid_loss: 0.07561 test_loss: 0.08408 \n",
      "验证损失减少 (0.075894 --> 0.075607). 正在保存模型...\n",
      "[101/500] train_loss: 0.06681 valid_loss: 0.07683 test_loss: 0.08363 \n",
      "[102/500] train_loss: 0.06569 valid_loss: 0.07732 test_loss: 0.08263 \n",
      "[103/500] train_loss: 0.06622 valid_loss: 0.07624 test_loss: 0.08300 \n",
      "[104/500] train_loss: 0.06605 valid_loss: 0.07764 test_loss: 0.08343 \n",
      "[105/500] train_loss: 0.06651 valid_loss: 0.07720 test_loss: 0.08459 \n",
      "[106/500] train_loss: 0.06447 valid_loss: 0.07578 test_loss: 0.08254 \n",
      "[107/500] train_loss: 0.06589 valid_loss: 0.07509 test_loss: 0.08369 \n",
      "验证损失减少 (0.075607 --> 0.075089). 正在保存模型...\n",
      "[108/500] train_loss: 0.06563 valid_loss: 0.07552 test_loss: 0.08318 \n",
      "[109/500] train_loss: 0.06452 valid_loss: 0.07446 test_loss: 0.08279 \n",
      "验证损失减少 (0.075089 --> 0.074464). 正在保存模型...\n",
      "[110/500] train_loss: 0.06188 valid_loss: 0.07579 test_loss: 0.08614 \n",
      "[111/500] train_loss: 0.06351 valid_loss: 0.07527 test_loss: 0.08315 \n",
      "[112/500] train_loss: 0.06285 valid_loss: 0.07384 test_loss: 0.08232 \n",
      "验证损失减少 (0.074464 --> 0.073838). 正在保存模型...\n",
      "[113/500] train_loss: 0.06484 valid_loss: 0.07555 test_loss: 0.08328 \n",
      "[114/500] train_loss: 0.06533 valid_loss: 0.07578 test_loss: 0.08436 \n",
      "[115/500] train_loss: 0.06420 valid_loss: 0.07556 test_loss: 0.08313 \n",
      "[116/500] train_loss: 0.06329 valid_loss: 0.07518 test_loss: 0.08262 \n",
      "[117/500] train_loss: 0.06340 valid_loss: 0.07476 test_loss: 0.08286 \n",
      "[118/500] train_loss: 0.06401 valid_loss: 0.07600 test_loss: 0.08348 \n",
      "[119/500] train_loss: 0.06373 valid_loss: 0.07432 test_loss: 0.08271 \n",
      "[120/500] train_loss: 0.06363 valid_loss: 0.07687 test_loss: 0.08179 \n",
      "[121/500] train_loss: 0.06382 valid_loss: 0.07733 test_loss: 0.08425 \n",
      "[122/500] train_loss: 0.06157 valid_loss: 0.07582 test_loss: 0.08518 \n",
      "[123/500] train_loss: 0.06291 valid_loss: 0.07668 test_loss: 0.08300 \n",
      "[124/500] train_loss: 0.06239 valid_loss: 0.07565 test_loss: 0.08092 \n",
      "[125/500] train_loss: 0.06181 valid_loss: 0.07522 test_loss: 0.08151 \n",
      "[126/500] train_loss: 0.06286 valid_loss: 0.07504 test_loss: 0.08274 \n",
      "[127/500] train_loss: 0.06192 valid_loss: 0.07494 test_loss: 0.08254 \n",
      "[128/500] train_loss: 0.06188 valid_loss: 0.07593 test_loss: 0.08362 \n",
      "[129/500] train_loss: 0.06107 valid_loss: 0.07602 test_loss: 0.08146 \n",
      "[130/500] train_loss: 0.06198 valid_loss: 0.07581 test_loss: 0.08296 \n",
      "[131/500] train_loss: 0.06166 valid_loss: 0.07332 test_loss: 0.08099 \n",
      "验证损失减少 (0.073838 --> 0.073324). 正在保存模型...\n",
      "[132/500] train_loss: 0.06135 valid_loss: 0.07431 test_loss: 0.08181 \n",
      "[133/500] train_loss: 0.06081 valid_loss: 0.07523 test_loss: 0.08115 \n",
      "[134/500] train_loss: 0.05928 valid_loss: 0.07418 test_loss: 0.08258 \n",
      "[135/500] train_loss: 0.06165 valid_loss: 0.07495 test_loss: 0.08140 \n",
      "[136/500] train_loss: 0.06068 valid_loss: 0.07394 test_loss: 0.08104 \n",
      "[137/500] train_loss: 0.05898 valid_loss: 0.07598 test_loss: 0.08293 \n",
      "[138/500] train_loss: 0.06114 valid_loss: 0.07681 test_loss: 0.08125 \n",
      "[139/500] train_loss: 0.06208 valid_loss: 0.07875 test_loss: 0.08353 \n",
      "[140/500] train_loss: 0.06097 valid_loss: 0.07709 test_loss: 0.08168 \n",
      "[141/500] train_loss: 0.06035 valid_loss: 0.07658 test_loss: 0.08388 \n",
      "[142/500] train_loss: 0.06080 valid_loss: 0.08138 test_loss: 0.08375 \n",
      "[143/500] train_loss: 0.05893 valid_loss: 0.08107 test_loss: 0.08104 \n",
      "[144/500] train_loss: 0.06019 valid_loss: 0.07492 test_loss: 0.08165 \n",
      "[145/500] train_loss: 0.06119 valid_loss: 0.07800 test_loss: 0.08014 \n",
      "[146/500] train_loss: 0.05956 valid_loss: 0.07833 test_loss: 0.08178 \n",
      "[147/500] train_loss: 0.05989 valid_loss: 0.07384 test_loss: 0.08075 \n",
      "[148/500] train_loss: 0.06001 valid_loss: 0.08034 test_loss: 0.08155 \n",
      "[149/500] train_loss: 0.05923 valid_loss: 0.07565 test_loss: 0.08276 \n",
      "[150/500] train_loss: 0.05890 valid_loss: 0.07511 test_loss: 0.07960 \n",
      "[151/500] train_loss: 0.06025 valid_loss: 0.07442 test_loss: 0.08061 \n",
      "[152/500] train_loss: 0.05960 valid_loss: 0.07470 test_loss: 0.08028 \n",
      "[153/500] train_loss: 0.05887 valid_loss: 0.07562 test_loss: 0.08199 \n",
      "[154/500] train_loss: 0.06044 valid_loss: 0.07609 test_loss: 0.08355 \n",
      "[155/500] train_loss: 0.05749 valid_loss: 0.07536 test_loss: 0.08423 \n",
      "[156/500] train_loss: 0.05882 valid_loss: 0.07646 test_loss: 0.08253 \n",
      "[157/500] train_loss: 0.05798 valid_loss: 0.07450 test_loss: 0.08265 \n",
      "[158/500] train_loss: 0.05848 valid_loss: 0.07537 test_loss: 0.08198 \n",
      "[159/500] train_loss: 0.05747 valid_loss: 0.07524 test_loss: 0.08146 \n",
      "[160/500] train_loss: 0.05915 valid_loss: 0.07429 test_loss: 0.08184 \n",
      "[161/500] train_loss: 0.05906 valid_loss: 0.07505 test_loss: 0.08164 \n",
      "[162/500] train_loss: 0.05887 valid_loss: 0.07423 test_loss: 0.08062 \n",
      "[163/500] train_loss: 0.05745 valid_loss: 0.07592 test_loss: 0.08218 \n",
      "[164/500] train_loss: 0.05800 valid_loss: 0.07436 test_loss: 0.08060 \n",
      "[165/500] train_loss: 0.05697 valid_loss: 0.07744 test_loss: 0.07976 \n",
      "[166/500] train_loss: 0.05897 valid_loss: 0.07343 test_loss: 0.08039 \n",
      "[167/500] train_loss: 0.05717 valid_loss: 0.07329 test_loss: 0.08089 \n",
      "验证损失减少 (0.073324 --> 0.073289). 正在保存模型...\n",
      "[168/500] train_loss: 0.05795 valid_loss: 0.07474 test_loss: 0.08192 \n",
      "[169/500] train_loss: 0.05870 valid_loss: 0.07271 test_loss: 0.08000 \n",
      "验证损失减少 (0.073289 --> 0.072708). 正在保存模型...\n",
      "[170/500] train_loss: 0.05849 valid_loss: 0.07403 test_loss: 0.08119 \n",
      "[171/500] train_loss: 0.05859 valid_loss: 0.07303 test_loss: 0.07894 \n",
      "[172/500] train_loss: 0.05723 valid_loss: 0.07347 test_loss: 0.08016 \n",
      "[173/500] train_loss: 0.05671 valid_loss: 0.07529 test_loss: 0.08127 \n",
      "[174/500] train_loss: 0.05671 valid_loss: 0.07545 test_loss: 0.08213 \n",
      "[175/500] train_loss: 0.05689 valid_loss: 0.07399 test_loss: 0.08030 \n",
      "[176/500] train_loss: 0.05567 valid_loss: 0.07539 test_loss: 0.08209 \n",
      "[177/500] train_loss: 0.05714 valid_loss: 0.07306 test_loss: 0.08083 \n",
      "[178/500] train_loss: 0.05672 valid_loss: 0.07342 test_loss: 0.08024 \n",
      "[179/500] train_loss: 0.05581 valid_loss: 0.07408 test_loss: 0.07948 \n",
      "[180/500] train_loss: 0.05680 valid_loss: 0.07389 test_loss: 0.07978 \n",
      "[181/500] train_loss: 0.05664 valid_loss: 0.07345 test_loss: 0.07958 \n",
      "[182/500] train_loss: 0.05680 valid_loss: 0.07649 test_loss: 0.08084 \n",
      "[183/500] train_loss: 0.05552 valid_loss: 0.07554 test_loss: 0.07959 \n",
      "[184/500] train_loss: 0.05803 valid_loss: 0.07484 test_loss: 0.07922 \n",
      "[185/500] train_loss: 0.05615 valid_loss: 0.07470 test_loss: 0.08265 \n",
      "[186/500] train_loss: 0.05778 valid_loss: 0.07560 test_loss: 0.08118 \n",
      "[187/500] train_loss: 0.05694 valid_loss: 0.07585 test_loss: 0.08153 \n",
      "[188/500] train_loss: 0.05608 valid_loss: 0.07385 test_loss: 0.07984 \n",
      "[189/500] train_loss: 0.05453 valid_loss: 0.07437 test_loss: 0.07929 \n",
      "[190/500] train_loss: 0.05530 valid_loss: 0.07475 test_loss: 0.07997 \n",
      "[191/500] train_loss: 0.05450 valid_loss: 0.07454 test_loss: 0.08172 \n",
      "[192/500] train_loss: 0.05408 valid_loss: 0.07622 test_loss: 0.08150 \n",
      "[193/500] train_loss: 0.05506 valid_loss: 0.07459 test_loss: 0.08040 \n",
      "[194/500] train_loss: 0.05516 valid_loss: 0.07449 test_loss: 0.08194 \n",
      "[195/500] train_loss: 0.05499 valid_loss: 0.07562 test_loss: 0.08069 \n",
      "[196/500] train_loss: 0.05449 valid_loss: 0.07493 test_loss: 0.08221 \n",
      "[197/500] train_loss: 0.05553 valid_loss: 0.07508 test_loss: 0.08092 \n",
      "[198/500] train_loss: 0.05458 valid_loss: 0.07528 test_loss: 0.08060 \n",
      "[199/500] train_loss: 0.05454 valid_loss: 0.07399 test_loss: 0.08010 \n",
      "[200/500] train_loss: 0.05617 valid_loss: 0.07769 test_loss: 0.07888 \n",
      "[201/500] train_loss: 0.05571 valid_loss: 0.07418 test_loss: 0.07925 \n",
      "[202/500] train_loss: 0.05519 valid_loss: 0.07450 test_loss: 0.08125 \n",
      "[203/500] train_loss: 0.05351 valid_loss: 0.07462 test_loss: 0.08001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[204/500] train_loss: 0.05400 valid_loss: 0.07337 test_loss: 0.08073 \n",
      "[205/500] train_loss: 0.05369 valid_loss: 0.07467 test_loss: 0.07951 \n",
      "[206/500] train_loss: 0.05377 valid_loss: 0.07499 test_loss: 0.08187 \n",
      "[207/500] train_loss: 0.05495 valid_loss: 0.07788 test_loss: 0.07969 \n",
      "[208/500] train_loss: 0.05218 valid_loss: 0.07424 test_loss: 0.08248 \n",
      "[209/500] train_loss: 0.05430 valid_loss: 0.07556 test_loss: 0.08234 \n",
      "[210/500] train_loss: 0.05279 valid_loss: 0.07371 test_loss: 0.08076 \n",
      "[211/500] train_loss: 0.05319 valid_loss: 0.07513 test_loss: 0.08149 \n",
      "[212/500] train_loss: 0.05228 valid_loss: 0.07702 test_loss: 0.08103 \n",
      "[213/500] train_loss: 0.05297 valid_loss: 0.07644 test_loss: 0.08226 \n",
      "[214/500] train_loss: 0.05488 valid_loss: 0.07558 test_loss: 0.08025 \n",
      "[215/500] train_loss: 0.05290 valid_loss: 0.07435 test_loss: 0.08079 \n",
      "[216/500] train_loss: 0.05347 valid_loss: 0.07616 test_loss: 0.08045 \n",
      "[217/500] train_loss: 0.05340 valid_loss: 0.07661 test_loss: 0.08007 \n",
      "[218/500] train_loss: 0.05170 valid_loss: 0.07641 test_loss: 0.07931 \n",
      "[219/500] train_loss: 0.05315 valid_loss: 0.07485 test_loss: 0.07967 \n",
      "[220/500] train_loss: 0.05244 valid_loss: 0.07387 test_loss: 0.08032 \n",
      "[221/500] train_loss: 0.05420 valid_loss: 0.07509 test_loss: 0.08141 \n",
      "[222/500] train_loss: 0.05318 valid_loss: 0.07408 test_loss: 0.08073 \n",
      "[223/500] train_loss: 0.05361 valid_loss: 0.07336 test_loss: 0.08012 \n",
      "[224/500] train_loss: 0.05281 valid_loss: 0.07587 test_loss: 0.08182 \n",
      "[225/500] train_loss: 0.05342 valid_loss: 0.07328 test_loss: 0.08155 \n",
      "[226/500] train_loss: 0.05277 valid_loss: 0.07308 test_loss: 0.07944 \n",
      "[227/500] train_loss: 0.05083 valid_loss: 0.07523 test_loss: 0.08063 \n",
      "[228/500] train_loss: 0.05255 valid_loss: 0.07694 test_loss: 0.08017 \n",
      "[229/500] train_loss: 0.05081 valid_loss: 0.07459 test_loss: 0.08027 \n",
      "[230/500] train_loss: 0.05250 valid_loss: 0.07497 test_loss: 0.07996 \n",
      "[231/500] train_loss: 0.05293 valid_loss: 0.07369 test_loss: 0.08095 \n",
      "[232/500] train_loss: 0.05050 valid_loss: 0.07441 test_loss: 0.07846 \n",
      "[233/500] train_loss: 0.05160 valid_loss: 0.07752 test_loss: 0.08019 \n",
      "[234/500] train_loss: 0.05170 valid_loss: 0.07480 test_loss: 0.08086 \n",
      "[235/500] train_loss: 0.05133 valid_loss: 0.07477 test_loss: 0.07979 \n",
      "[236/500] train_loss: 0.05216 valid_loss: 0.07516 test_loss: 0.08027 \n",
      "[237/500] train_loss: 0.05177 valid_loss: 0.07395 test_loss: 0.07962 \n",
      "[238/500] train_loss: 0.05159 valid_loss: 0.07412 test_loss: 0.08088 \n",
      "[239/500] train_loss: 0.05203 valid_loss: 0.07474 test_loss: 0.08028 \n",
      "[240/500] train_loss: 0.05158 valid_loss: 0.07755 test_loss: 0.08145 \n",
      "[241/500] train_loss: 0.05096 valid_loss: 0.07527 test_loss: 0.08057 \n",
      "[242/500] train_loss: 0.05249 valid_loss: 0.07430 test_loss: 0.07984 \n",
      "[243/500] train_loss: 0.05072 valid_loss: 0.07581 test_loss: 0.08124 \n",
      "[244/500] train_loss: 0.05090 valid_loss: 0.07588 test_loss: 0.08213 \n",
      "[245/500] train_loss: 0.05256 valid_loss: 0.07514 test_loss: 0.08019 \n",
      "[246/500] train_loss: 0.04972 valid_loss: 0.07499 test_loss: 0.08086 \n",
      "[247/500] train_loss: 0.05112 valid_loss: 0.07445 test_loss: 0.08244 \n",
      "[248/500] train_loss: 0.05218 valid_loss: 0.07486 test_loss: 0.08110 \n",
      "[249/500] train_loss: 0.05088 valid_loss: 0.07587 test_loss: 0.08258 \n",
      "[250/500] train_loss: 0.04965 valid_loss: 0.07532 test_loss: 0.08090 \n",
      "[251/500] train_loss: 0.05106 valid_loss: 0.07414 test_loss: 0.07917 \n",
      "[252/500] train_loss: 0.05210 valid_loss: 0.07465 test_loss: 0.08083 \n",
      "[253/500] train_loss: 0.05079 valid_loss: 0.07550 test_loss: 0.08122 \n",
      "[254/500] train_loss: 0.04985 valid_loss: 0.07328 test_loss: 0.07995 \n",
      "[255/500] train_loss: 0.05011 valid_loss: 0.07490 test_loss: 0.07937 \n",
      "[256/500] train_loss: 0.04876 valid_loss: 0.08379 test_loss: 0.08185 \n",
      "[257/500] train_loss: 0.05112 valid_loss: 0.07799 test_loss: 0.08092 \n",
      "[258/500] train_loss: 0.05217 valid_loss: 0.07488 test_loss: 0.08142 \n",
      "[259/500] train_loss: 0.05126 valid_loss: 0.07282 test_loss: 0.08082 \n",
      "[260/500] train_loss: 0.05004 valid_loss: 0.07486 test_loss: 0.08038 \n",
      "[261/500] train_loss: 0.05010 valid_loss: 0.07545 test_loss: 0.08022 \n",
      "[262/500] train_loss: 0.05018 valid_loss: 0.07493 test_loss: 0.07942 \n",
      "[263/500] train_loss: 0.05159 valid_loss: 0.07249 test_loss: 0.07970 \n",
      "验证损失减少 (0.072708 --> 0.072490). 正在保存模型...\n",
      "[264/500] train_loss: 0.05003 valid_loss: 0.07485 test_loss: 0.08129 \n",
      "[265/500] train_loss: 0.05182 valid_loss: 0.07450 test_loss: 0.08092 \n",
      "[266/500] train_loss: 0.05086 valid_loss: 0.07421 test_loss: 0.08005 \n",
      "[267/500] train_loss: 0.04955 valid_loss: 0.07640 test_loss: 0.08047 \n",
      "[268/500] train_loss: 0.04895 valid_loss: 0.07519 test_loss: 0.08100 \n",
      "[269/500] train_loss: 0.04970 valid_loss: 0.07499 test_loss: 0.08113 \n",
      "[270/500] train_loss: 0.04930 valid_loss: 0.08211 test_loss: 0.08053 \n",
      "[271/500] train_loss: 0.04951 valid_loss: 0.07420 test_loss: 0.07924 \n",
      "[272/500] train_loss: 0.04977 valid_loss: 0.07542 test_loss: 0.08052 \n",
      "[273/500] train_loss: 0.04984 valid_loss: 0.07464 test_loss: 0.08014 \n",
      "[274/500] train_loss: 0.04960 valid_loss: 0.07689 test_loss: 0.07910 \n",
      "[275/500] train_loss: 0.04947 valid_loss: 0.07591 test_loss: 0.08010 \n",
      "[276/500] train_loss: 0.04852 valid_loss: 0.07656 test_loss: 0.08015 \n",
      "[277/500] train_loss: 0.04926 valid_loss: 0.07483 test_loss: 0.07951 \n",
      "[278/500] train_loss: 0.04850 valid_loss: 0.07727 test_loss: 0.08197 \n",
      "[279/500] train_loss: 0.05054 valid_loss: 0.07378 test_loss: 0.07995 \n",
      "[280/500] train_loss: 0.04926 valid_loss: 0.07513 test_loss: 0.08007 \n",
      "[281/500] train_loss: 0.04824 valid_loss: 0.07532 test_loss: 0.08065 \n",
      "[282/500] train_loss: 0.04962 valid_loss: 0.07568 test_loss: 0.08023 \n",
      "[283/500] train_loss: 0.04880 valid_loss: 0.07446 test_loss: 0.07934 \n",
      "[284/500] train_loss: 0.04815 valid_loss: 0.07676 test_loss: 0.08066 \n",
      "[285/500] train_loss: 0.04955 valid_loss: 0.07543 test_loss: 0.08092 \n",
      "[286/500] train_loss: 0.04896 valid_loss: 0.07708 test_loss: 0.07949 \n",
      "[287/500] train_loss: 0.04874 valid_loss: 0.07439 test_loss: 0.08071 \n",
      "[288/500] train_loss: 0.04789 valid_loss: 0.07498 test_loss: 0.08031 \n",
      "[289/500] train_loss: 0.04829 valid_loss: 0.07563 test_loss: 0.08030 \n",
      "[290/500] train_loss: 0.04878 valid_loss: 0.07562 test_loss: 0.08084 \n",
      "[291/500] train_loss: 0.04864 valid_loss: 0.07580 test_loss: 0.08139 \n",
      "[292/500] train_loss: 0.04856 valid_loss: 0.07503 test_loss: 0.08096 \n",
      "[293/500] train_loss: 0.04799 valid_loss: 0.07535 test_loss: 0.08431 \n",
      "[294/500] train_loss: 0.04818 valid_loss: 0.07893 test_loss: 0.08349 \n",
      "[295/500] train_loss: 0.04827 valid_loss: 0.07606 test_loss: 0.07911 \n",
      "[296/500] train_loss: 0.04782 valid_loss: 0.07524 test_loss: 0.08085 \n",
      "[297/500] train_loss: 0.04733 valid_loss: 0.07447 test_loss: 0.08097 \n",
      "[298/500] train_loss: 0.04839 valid_loss: 0.07514 test_loss: 0.08186 \n",
      "[299/500] train_loss: 0.04937 valid_loss: 0.07491 test_loss: 0.08040 \n",
      "[300/500] train_loss: 0.04692 valid_loss: 0.07399 test_loss: 0.08099 \n",
      "[301/500] train_loss: 0.04800 valid_loss: 0.07533 test_loss: 0.08142 \n",
      "[302/500] train_loss: 0.04835 valid_loss: 0.07418 test_loss: 0.08183 \n",
      "[303/500] train_loss: 0.04865 valid_loss: 0.07547 test_loss: 0.08080 \n",
      "[304/500] train_loss: 0.04735 valid_loss: 0.07532 test_loss: 0.08081 \n",
      "[305/500] train_loss: 0.04732 valid_loss: 0.07406 test_loss: 0.08136 \n",
      "[306/500] train_loss: 0.04749 valid_loss: 0.07424 test_loss: 0.08256 \n",
      "[307/500] train_loss: 0.04881 valid_loss: 0.07744 test_loss: 0.08248 \n",
      "[308/500] train_loss: 0.04752 valid_loss: 0.07579 test_loss: 0.08117 \n",
      "[309/500] train_loss: 0.04787 valid_loss: 0.07540 test_loss: 0.08285 \n",
      "[310/500] train_loss: 0.04773 valid_loss: 0.07446 test_loss: 0.08112 \n",
      "[311/500] train_loss: 0.04781 valid_loss: 0.07439 test_loss: 0.07967 \n",
      "[312/500] train_loss: 0.04766 valid_loss: 0.07254 test_loss: 0.08030 \n",
      "[313/500] train_loss: 0.04709 valid_loss: 0.07580 test_loss: 0.08019 \n",
      "[314/500] train_loss: 0.04759 valid_loss: 0.07422 test_loss: 0.08122 \n",
      "[315/500] train_loss: 0.04688 valid_loss: 0.07438 test_loss: 0.08067 \n",
      "[316/500] train_loss: 0.04690 valid_loss: 0.07371 test_loss: 0.08213 \n",
      "[317/500] train_loss: 0.04850 valid_loss: 0.07513 test_loss: 0.08087 \n",
      "[318/500] train_loss: 0.04776 valid_loss: 0.07427 test_loss: 0.08029 \n",
      "[319/500] train_loss: 0.04767 valid_loss: 0.07423 test_loss: 0.08007 \n",
      "[320/500] train_loss: 0.04668 valid_loss: 0.07449 test_loss: 0.07999 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[321/500] train_loss: 0.04788 valid_loss: 0.07553 test_loss: 0.08080 \n",
      "[322/500] train_loss: 0.04603 valid_loss: 0.07403 test_loss: 0.08072 \n",
      "[323/500] train_loss: 0.04695 valid_loss: 0.07549 test_loss: 0.08072 \n",
      "[324/500] train_loss: 0.04751 valid_loss: 0.07519 test_loss: 0.08116 \n",
      "[325/500] train_loss: 0.04806 valid_loss: 0.07366 test_loss: 0.08108 \n",
      "[326/500] train_loss: 0.04728 valid_loss: 0.07359 test_loss: 0.08105 \n",
      "[327/500] train_loss: 0.04763 valid_loss: 0.07498 test_loss: 0.08225 \n",
      "[328/500] train_loss: 0.04663 valid_loss: 0.07438 test_loss: 0.08091 \n",
      "[329/500] train_loss: 0.04620 valid_loss: 0.07550 test_loss: 0.08232 \n",
      "[330/500] train_loss: 0.04785 valid_loss: 0.07537 test_loss: 0.08096 \n",
      "[331/500] train_loss: 0.04790 valid_loss: 0.07565 test_loss: 0.08073 \n",
      "[332/500] train_loss: 0.04674 valid_loss: 0.07426 test_loss: 0.08353 \n",
      "[333/500] train_loss: 0.04605 valid_loss: 0.07458 test_loss: 0.08119 \n",
      "[334/500] train_loss: 0.04618 valid_loss: 0.07454 test_loss: 0.08085 \n",
      "[335/500] train_loss: 0.04706 valid_loss: 0.07381 test_loss: 0.08060 \n",
      "[336/500] train_loss: 0.04710 valid_loss: 0.07498 test_loss: 0.08140 \n",
      "[337/500] train_loss: 0.04674 valid_loss: 0.07395 test_loss: 0.08057 \n",
      "[338/500] train_loss: 0.04691 valid_loss: 0.07405 test_loss: 0.07962 \n",
      "[339/500] train_loss: 0.04667 valid_loss: 0.07273 test_loss: 0.08214 \n",
      "[340/500] train_loss: 0.04608 valid_loss: 0.07478 test_loss: 0.08117 \n",
      "[341/500] train_loss: 0.04647 valid_loss: 0.07430 test_loss: 0.08025 \n",
      "[342/500] train_loss: 0.04713 valid_loss: 0.07462 test_loss: 0.07965 \n",
      "[343/500] train_loss: 0.04637 valid_loss: 0.07531 test_loss: 0.08094 \n",
      "[344/500] train_loss: 0.04522 valid_loss: 0.07372 test_loss: 0.08059 \n",
      "[345/500] train_loss: 0.04598 valid_loss: 0.07501 test_loss: 0.08043 \n",
      "[346/500] train_loss: 0.04666 valid_loss: 0.07482 test_loss: 0.08071 \n",
      "[347/500] train_loss: 0.04693 valid_loss: 0.07464 test_loss: 0.08234 \n",
      "[348/500] train_loss: 0.04488 valid_loss: 0.07574 test_loss: 0.08308 \n",
      "[349/500] train_loss: 0.04576 valid_loss: 0.07422 test_loss: 0.08030 \n",
      "[350/500] train_loss: 0.04542 valid_loss: 0.07496 test_loss: 0.08196 \n",
      "[351/500] train_loss: 0.04718 valid_loss: 0.07402 test_loss: 0.08172 \n",
      "[352/500] train_loss: 0.04575 valid_loss: 0.07534 test_loss: 0.08198 \n",
      "[353/500] train_loss: 0.04536 valid_loss: 0.07507 test_loss: 0.08053 \n",
      "[354/500] train_loss: 0.04513 valid_loss: 0.07471 test_loss: 0.08049 \n",
      "[355/500] train_loss: 0.04565 valid_loss: 0.07521 test_loss: 0.08113 \n",
      "[356/500] train_loss: 0.04690 valid_loss: 0.07732 test_loss: 0.08173 \n",
      "[357/500] train_loss: 0.04755 valid_loss: 0.07528 test_loss: 0.08166 \n",
      "[358/500] train_loss: 0.04685 valid_loss: 0.07458 test_loss: 0.08147 \n",
      "[359/500] train_loss: 0.04483 valid_loss: 0.07513 test_loss: 0.08200 \n",
      "[360/500] train_loss: 0.04497 valid_loss: 0.07553 test_loss: 0.08287 \n",
      "[361/500] train_loss: 0.04447 valid_loss: 0.07405 test_loss: 0.08134 \n",
      "[362/500] train_loss: 0.04356 valid_loss: 0.07605 test_loss: 0.08362 \n",
      "[363/500] train_loss: 0.04560 valid_loss: 0.07385 test_loss: 0.08037 \n",
      "[364/500] train_loss: 0.04577 valid_loss: 0.07540 test_loss: 0.08176 \n",
      "[365/500] train_loss: 0.04525 valid_loss: 0.07496 test_loss: 0.08203 \n",
      "[366/500] train_loss: 0.04647 valid_loss: 0.07338 test_loss: 0.08042 \n",
      "[367/500] train_loss: 0.04490 valid_loss: 0.07355 test_loss: 0.08270 \n",
      "[368/500] train_loss: 0.04460 valid_loss: 0.07351 test_loss: 0.08123 \n",
      "[369/500] train_loss: 0.04501 valid_loss: 0.07362 test_loss: 0.08127 \n",
      "[370/500] train_loss: 0.04420 valid_loss: 0.07514 test_loss: 0.08126 \n",
      "[371/500] train_loss: 0.04523 valid_loss: 0.07316 test_loss: 0.07985 \n",
      "[372/500] train_loss: 0.04638 valid_loss: 0.07539 test_loss: 0.08024 \n",
      "[373/500] train_loss: 0.04484 valid_loss: 0.07317 test_loss: 0.08204 \n",
      "[374/500] train_loss: 0.04441 valid_loss: 0.07461 test_loss: 0.08035 \n",
      "[375/500] train_loss: 0.04634 valid_loss: 0.07350 test_loss: 0.08082 \n",
      "[376/500] train_loss: 0.04531 valid_loss: 0.07279 test_loss: 0.08009 \n",
      "[377/500] train_loss: 0.04458 valid_loss: 0.07588 test_loss: 0.08305 \n",
      "[378/500] train_loss: 0.04581 valid_loss: 0.07489 test_loss: 0.08152 \n",
      "[379/500] train_loss: 0.04379 valid_loss: 0.07495 test_loss: 0.08165 \n",
      "[380/500] train_loss: 0.04443 valid_loss: 0.07658 test_loss: 0.08113 \n",
      "[381/500] train_loss: 0.04361 valid_loss: 0.07501 test_loss: 0.08147 \n",
      "[382/500] train_loss: 0.04393 valid_loss: 0.07402 test_loss: 0.08130 \n",
      "[383/500] train_loss: 0.04453 valid_loss: 0.07474 test_loss: 0.08169 \n",
      "[384/500] train_loss: 0.04495 valid_loss: 0.07429 test_loss: 0.08115 \n",
      "[385/500] train_loss: 0.04558 valid_loss: 0.07552 test_loss: 0.08149 \n",
      "[386/500] train_loss: 0.04508 valid_loss: 0.07424 test_loss: 0.08147 \n",
      "[387/500] train_loss: 0.04405 valid_loss: 0.07497 test_loss: 0.08173 \n",
      "[388/500] train_loss: 0.04334 valid_loss: 0.07396 test_loss: 0.08252 \n",
      "[389/500] train_loss: 0.04407 valid_loss: 0.07311 test_loss: 0.08128 \n",
      "[390/500] train_loss: 0.04439 valid_loss: 0.07374 test_loss: 0.08244 \n",
      "[391/500] train_loss: 0.04423 valid_loss: 0.07316 test_loss: 0.08185 \n",
      "[392/500] train_loss: 0.04435 valid_loss: 0.07381 test_loss: 0.08091 \n",
      "[393/500] train_loss: 0.04305 valid_loss: 0.07440 test_loss: 0.08052 \n",
      "[394/500] train_loss: 0.04479 valid_loss: 0.07359 test_loss: 0.08143 \n",
      "[395/500] train_loss: 0.04354 valid_loss: 0.07425 test_loss: 0.08114 \n",
      "[396/500] train_loss: 0.04458 valid_loss: 0.07531 test_loss: 0.08200 \n",
      "[397/500] train_loss: 0.04265 valid_loss: 0.07744 test_loss: 0.08229 \n",
      "[398/500] train_loss: 0.04511 valid_loss: 0.07471 test_loss: 0.08311 \n",
      "[399/500] train_loss: 0.04347 valid_loss: 0.07515 test_loss: 0.08205 \n",
      "[400/500] train_loss: 0.04255 valid_loss: 0.07361 test_loss: 0.08093 \n",
      "[401/500] train_loss: 0.04262 valid_loss: 0.07564 test_loss: 0.08184 \n",
      "[402/500] train_loss: 0.04400 valid_loss: 0.07491 test_loss: 0.08197 \n",
      "[403/500] train_loss: 0.04385 valid_loss: 0.07446 test_loss: 0.08100 \n",
      "[404/500] train_loss: 0.04431 valid_loss: 0.07540 test_loss: 0.08155 \n",
      "[405/500] train_loss: 0.04278 valid_loss: 0.07420 test_loss: 0.08048 \n",
      "[406/500] train_loss: 0.04400 valid_loss: 0.07471 test_loss: 0.08146 \n",
      "[407/500] train_loss: 0.04303 valid_loss: 0.07588 test_loss: 0.08115 \n",
      "[408/500] train_loss: 0.04414 valid_loss: 0.07659 test_loss: 0.08164 \n",
      "[409/500] train_loss: 0.04422 valid_loss: 0.07484 test_loss: 0.08234 \n",
      "[410/500] train_loss: 0.04346 valid_loss: 0.07535 test_loss: 0.08172 \n",
      "[411/500] train_loss: 0.04375 valid_loss: 0.07363 test_loss: 0.08139 \n",
      "[412/500] train_loss: 0.04357 valid_loss: 0.07630 test_loss: 0.08210 \n",
      "[413/500] train_loss: 0.04406 valid_loss: 0.07482 test_loss: 0.07997 \n",
      "[414/500] train_loss: 0.04331 valid_loss: 0.07846 test_loss: 0.08309 \n",
      "[415/500] train_loss: 0.04330 valid_loss: 0.07680 test_loss: 0.08201 \n",
      "[416/500] train_loss: 0.04356 valid_loss: 0.07538 test_loss: 0.07963 \n",
      "[417/500] train_loss: 0.04288 valid_loss: 0.07633 test_loss: 0.08120 \n",
      "[418/500] train_loss: 0.04226 valid_loss: 0.07570 test_loss: 0.08203 \n",
      "[419/500] train_loss: 0.04282 valid_loss: 0.07700 test_loss: 0.08410 \n",
      "[420/500] train_loss: 0.04204 valid_loss: 0.07744 test_loss: 0.08280 \n",
      "[421/500] train_loss: 0.04329 valid_loss: 0.07718 test_loss: 0.08273 \n",
      "[422/500] train_loss: 0.04267 valid_loss: 0.07545 test_loss: 0.08271 \n",
      "[423/500] train_loss: 0.04214 valid_loss: 0.07594 test_loss: 0.08194 \n",
      "[424/500] train_loss: 0.04155 valid_loss: 0.07568 test_loss: 0.08323 \n",
      "[425/500] train_loss: 0.04262 valid_loss: 0.07610 test_loss: 0.08196 \n",
      "[426/500] train_loss: 0.04448 valid_loss: 0.07602 test_loss: 0.08272 \n",
      "[427/500] train_loss: 0.04319 valid_loss: 0.07520 test_loss: 0.08348 \n",
      "[428/500] train_loss: 0.04319 valid_loss: 0.07620 test_loss: 0.08233 \n",
      "[429/500] train_loss: 0.04234 valid_loss: 0.07423 test_loss: 0.08239 \n",
      "[430/500] train_loss: 0.04335 valid_loss: 0.07440 test_loss: 0.08307 \n",
      "[431/500] train_loss: 0.04236 valid_loss: 0.07524 test_loss: 0.08286 \n",
      "[432/500] train_loss: 0.04192 valid_loss: 0.07585 test_loss: 0.08306 \n",
      "[433/500] train_loss: 0.04194 valid_loss: 0.07633 test_loss: 0.08344 \n",
      "[434/500] train_loss: 0.04292 valid_loss: 0.07690 test_loss: 0.08096 \n",
      "[435/500] train_loss: 0.04160 valid_loss: 0.07540 test_loss: 0.08258 \n",
      "[436/500] train_loss: 0.04131 valid_loss: 0.07614 test_loss: 0.08412 \n",
      "[437/500] train_loss: 0.04213 valid_loss: 0.07458 test_loss: 0.08127 \n",
      "[438/500] train_loss: 0.04209 valid_loss: 0.07504 test_loss: 0.08324 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[439/500] train_loss: 0.04194 valid_loss: 0.07505 test_loss: 0.08296 \n",
      "[440/500] train_loss: 0.04336 valid_loss: 0.07680 test_loss: 0.08442 \n",
      "[441/500] train_loss: 0.04298 valid_loss: 0.07590 test_loss: 0.08382 \n",
      "[442/500] train_loss: 0.04200 valid_loss: 0.07512 test_loss: 0.08273 \n",
      "[443/500] train_loss: 0.04309 valid_loss: 0.07548 test_loss: 0.08204 \n",
      "[444/500] train_loss: 0.04230 valid_loss: 0.07564 test_loss: 0.08274 \n",
      "[445/500] train_loss: 0.04288 valid_loss: 0.07722 test_loss: 0.08400 \n",
      "[446/500] train_loss: 0.04232 valid_loss: 0.07708 test_loss: 0.08377 \n",
      "[447/500] train_loss: 0.04157 valid_loss: 0.07651 test_loss: 0.08349 \n",
      "[448/500] train_loss: 0.04289 valid_loss: 0.07533 test_loss: 0.08308 \n",
      "[449/500] train_loss: 0.04197 valid_loss: 0.07506 test_loss: 0.08437 \n",
      "[450/500] train_loss: 0.04174 valid_loss: 0.07654 test_loss: 0.08411 \n",
      "[451/500] train_loss: 0.04263 valid_loss: 0.07722 test_loss: 0.08562 \n",
      "[452/500] train_loss: 0.04491 valid_loss: 0.07629 test_loss: 0.08522 \n",
      "[453/500] train_loss: 0.04260 valid_loss: 0.07483 test_loss: 0.08200 \n",
      "[454/500] train_loss: 0.04216 valid_loss: 0.07646 test_loss: 0.08316 \n",
      "[455/500] train_loss: 0.04233 valid_loss: 0.07875 test_loss: 0.08621 \n",
      "[456/500] train_loss: 0.04244 valid_loss: 0.07592 test_loss: 0.08282 \n",
      "[457/500] train_loss: 0.04352 valid_loss: 0.07747 test_loss: 0.08437 \n",
      "[458/500] train_loss: 0.04264 valid_loss: 0.07586 test_loss: 0.08348 \n",
      "[459/500] train_loss: 0.04161 valid_loss: 0.07485 test_loss: 0.08283 \n",
      "[460/500] train_loss: 0.04177 valid_loss: 0.07436 test_loss: 0.08199 \n",
      "[461/500] train_loss: 0.04318 valid_loss: 0.07691 test_loss: 0.08337 \n",
      "[462/500] train_loss: 0.04198 valid_loss: 0.07503 test_loss: 0.08296 \n",
      "[463/500] train_loss: 0.04180 valid_loss: 0.07422 test_loss: 0.08302 \n",
      "[464/500] train_loss: 0.04136 valid_loss: 0.07463 test_loss: 0.08294 \n",
      "[465/500] train_loss: 0.04124 valid_loss: 0.07554 test_loss: 0.08335 \n",
      "[466/500] train_loss: 0.04234 valid_loss: 0.07622 test_loss: 0.08294 \n",
      "[467/500] train_loss: 0.04211 valid_loss: 0.07502 test_loss: 0.08212 \n",
      "[468/500] train_loss: 0.04065 valid_loss: 0.07612 test_loss: 0.08414 \n",
      "[469/500] train_loss: 0.04168 valid_loss: 0.07467 test_loss: 0.08188 \n",
      "[470/500] train_loss: 0.04216 valid_loss: 0.07561 test_loss: 0.08453 \n",
      "[471/500] train_loss: 0.04141 valid_loss: 0.07465 test_loss: 0.08251 \n",
      "[472/500] train_loss: 0.04212 valid_loss: 0.07592 test_loss: 0.08191 \n",
      "[473/500] train_loss: 0.04215 valid_loss: 0.07527 test_loss: 0.08264 \n",
      "[474/500] train_loss: 0.04175 valid_loss: 0.07650 test_loss: 0.08470 \n",
      "[475/500] train_loss: 0.04122 valid_loss: 0.07726 test_loss: 0.08376 \n",
      "[476/500] train_loss: 0.04013 valid_loss: 0.07547 test_loss: 0.08399 \n",
      "[477/500] train_loss: 0.04094 valid_loss: 0.07592 test_loss: 0.08240 \n",
      "[478/500] train_loss: 0.04143 valid_loss: 0.07536 test_loss: 0.08369 \n",
      "[479/500] train_loss: 0.04206 valid_loss: 0.07523 test_loss: 0.08227 \n",
      "[480/500] train_loss: 0.04170 valid_loss: 0.07493 test_loss: 0.08319 \n",
      "[481/500] train_loss: 0.04111 valid_loss: 0.07739 test_loss: 0.08481 \n",
      "[482/500] train_loss: 0.04157 valid_loss: 0.07662 test_loss: 0.08437 \n",
      "[483/500] train_loss: 0.04208 valid_loss: 0.07683 test_loss: 0.08493 \n",
      "[484/500] train_loss: 0.04072 valid_loss: 0.07585 test_loss: 0.08552 \n",
      "[485/500] train_loss: 0.04025 valid_loss: 0.07599 test_loss: 0.08526 \n",
      "[486/500] train_loss: 0.04046 valid_loss: 0.07571 test_loss: 0.08424 \n",
      "[487/500] train_loss: 0.04055 valid_loss: 0.07497 test_loss: 0.08283 \n",
      "[488/500] train_loss: 0.04029 valid_loss: 0.07607 test_loss: 0.08472 \n",
      "[489/500] train_loss: 0.04005 valid_loss: 0.07571 test_loss: 0.08409 \n",
      "[490/500] train_loss: 0.04147 valid_loss: 0.07569 test_loss: 0.08160 \n",
      "[491/500] train_loss: 0.04164 valid_loss: 0.07593 test_loss: 0.08253 \n",
      "[492/500] train_loss: 0.04049 valid_loss: 0.07620 test_loss: 0.08318 \n",
      "[493/500] train_loss: 0.04073 valid_loss: 0.07566 test_loss: 0.08239 \n",
      "[494/500] train_loss: 0.04157 valid_loss: 0.07475 test_loss: 0.08235 \n",
      "[495/500] train_loss: 0.04075 valid_loss: 0.07582 test_loss: 0.08139 \n",
      "[496/500] train_loss: 0.04072 valid_loss: 0.07715 test_loss: 0.08220 \n",
      "[497/500] train_loss: 0.04032 valid_loss: 0.07671 test_loss: 0.08294 \n",
      "[498/500] train_loss: 0.04214 valid_loss: 0.07454 test_loss: 0.08068 \n",
      "[499/500] train_loss: 0.04055 valid_loss: 0.07570 test_loss: 0.08402 \n",
      "[500/500] train_loss: 0.04197 valid_loss: 0.07509 test_loss: 0.08247 \n",
      "TRAINING MODEL 3\n",
      "[  1/500] train_loss: 0.33297 valid_loss: 0.23369 test_loss: 0.23842 \n",
      "验证损失减少 (inf --> 0.233689). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18296 valid_loss: 0.17387 test_loss: 0.17732 \n",
      "验证损失减少 (0.233689 --> 0.173869). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15019 valid_loss: 0.14790 test_loss: 0.15187 \n",
      "验证损失减少 (0.173869 --> 0.147897). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13484 valid_loss: 0.14095 test_loss: 0.14935 \n",
      "验证损失减少 (0.147897 --> 0.140953). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12718 valid_loss: 0.13021 test_loss: 0.13690 \n",
      "验证损失减少 (0.140953 --> 0.130210). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12346 valid_loss: 0.12543 test_loss: 0.13221 \n",
      "验证损失减少 (0.130210 --> 0.125435). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11528 valid_loss: 0.11689 test_loss: 0.12575 \n",
      "验证损失减少 (0.125435 --> 0.116891). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11121 valid_loss: 0.11768 test_loss: 0.12439 \n",
      "[  9/500] train_loss: 0.11066 valid_loss: 0.11393 test_loss: 0.12473 \n",
      "验证损失减少 (0.116891 --> 0.113934). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10913 valid_loss: 0.11296 test_loss: 0.11904 \n",
      "验证损失减少 (0.113934 --> 0.112964). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10646 valid_loss: 0.10712 test_loss: 0.11597 \n",
      "验证损失减少 (0.112964 --> 0.107117). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10391 valid_loss: 0.10645 test_loss: 0.11467 \n",
      "验证损失减少 (0.107117 --> 0.106447). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10319 valid_loss: 0.10775 test_loss: 0.11941 \n",
      "[ 14/500] train_loss: 0.10039 valid_loss: 0.10461 test_loss: 0.11467 \n",
      "验证损失减少 (0.106447 --> 0.104613). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.09924 valid_loss: 0.10022 test_loss: 0.11122 \n",
      "验证损失减少 (0.104613 --> 0.100221). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09844 valid_loss: 0.09992 test_loss: 0.10915 \n",
      "验证损失减少 (0.100221 --> 0.099917). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09809 valid_loss: 0.10006 test_loss: 0.10754 \n",
      "[ 18/500] train_loss: 0.09611 valid_loss: 0.09924 test_loss: 0.10946 \n",
      "验证损失减少 (0.099917 --> 0.099240). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09490 valid_loss: 0.09958 test_loss: 0.10634 \n",
      "[ 20/500] train_loss: 0.09507 valid_loss: 0.09679 test_loss: 0.10448 \n",
      "验证损失减少 (0.099240 --> 0.096786). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09245 valid_loss: 0.09503 test_loss: 0.10432 \n",
      "验证损失减少 (0.096786 --> 0.095034). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09107 valid_loss: 0.09487 test_loss: 0.10354 \n",
      "验证损失减少 (0.095034 --> 0.094865). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09036 valid_loss: 0.09540 test_loss: 0.10552 \n",
      "[ 24/500] train_loss: 0.09058 valid_loss: 0.09395 test_loss: 0.10343 \n",
      "验证损失减少 (0.094865 --> 0.093951). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.09101 valid_loss: 0.09261 test_loss: 0.10298 \n",
      "验证损失减少 (0.093951 --> 0.092612). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.08828 valid_loss: 0.09307 test_loss: 0.10133 \n",
      "[ 27/500] train_loss: 0.08815 valid_loss: 0.09247 test_loss: 0.10119 \n",
      "验证损失减少 (0.092612 --> 0.092469). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08668 valid_loss: 0.09001 test_loss: 0.09967 \n",
      "验证损失减少 (0.092469 --> 0.090013). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08804 valid_loss: 0.08821 test_loss: 0.09755 \n",
      "验证损失减少 (0.090013 --> 0.088214). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08696 valid_loss: 0.09318 test_loss: 0.10449 \n",
      "[ 31/500] train_loss: 0.08585 valid_loss: 0.08858 test_loss: 0.09816 \n",
      "[ 32/500] train_loss: 0.08443 valid_loss: 0.09098 test_loss: 0.10023 \n",
      "[ 33/500] train_loss: 0.08448 valid_loss: 0.08945 test_loss: 0.09623 \n",
      "[ 34/500] train_loss: 0.08353 valid_loss: 0.08872 test_loss: 0.09696 \n",
      "[ 35/500] train_loss: 0.08185 valid_loss: 0.08791 test_loss: 0.09840 \n",
      "验证损失减少 (0.088214 --> 0.087915). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08482 valid_loss: 0.08655 test_loss: 0.09747 \n",
      "验证损失减少 (0.087915 --> 0.086555). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08212 valid_loss: 0.08694 test_loss: 0.09633 \n",
      "[ 38/500] train_loss: 0.08228 valid_loss: 0.08577 test_loss: 0.09372 \n",
      "验证损失减少 (0.086555 --> 0.085774). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.07993 valid_loss: 0.08551 test_loss: 0.09430 \n",
      "验证损失减少 (0.085774 --> 0.085513). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 40/500] train_loss: 0.07941 valid_loss: 0.08708 test_loss: 0.09324 \n",
      "[ 41/500] train_loss: 0.08061 valid_loss: 0.08975 test_loss: 0.09303 \n",
      "[ 42/500] train_loss: 0.08069 valid_loss: 0.08896 test_loss: 0.09475 \n",
      "[ 43/500] train_loss: 0.07902 valid_loss: 0.08651 test_loss: 0.09445 \n",
      "[ 44/500] train_loss: 0.07938 valid_loss: 0.08776 test_loss: 0.09198 \n",
      "[ 45/500] train_loss: 0.07907 valid_loss: 0.08477 test_loss: 0.09143 \n",
      "验证损失减少 (0.085513 --> 0.084773). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07996 valid_loss: 0.08364 test_loss: 0.09368 \n",
      "验证损失减少 (0.084773 --> 0.083642). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07921 valid_loss: 0.08408 test_loss: 0.09215 \n",
      "[ 48/500] train_loss: 0.07807 valid_loss: 0.08213 test_loss: 0.09170 \n",
      "验证损失减少 (0.083642 --> 0.082131). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07698 valid_loss: 0.08283 test_loss: 0.09055 \n",
      "[ 50/500] train_loss: 0.07764 valid_loss: 0.08195 test_loss: 0.09093 \n",
      "验证损失减少 (0.082131 --> 0.081952). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.07748 valid_loss: 0.08473 test_loss: 0.08976 \n",
      "[ 52/500] train_loss: 0.07533 valid_loss: 0.08296 test_loss: 0.09009 \n",
      "[ 53/500] train_loss: 0.07532 valid_loss: 0.08094 test_loss: 0.08991 \n",
      "验证损失减少 (0.081952 --> 0.080942). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07730 valid_loss: 0.08402 test_loss: 0.09198 \n",
      "[ 55/500] train_loss: 0.07629 valid_loss: 0.08396 test_loss: 0.09110 \n",
      "[ 56/500] train_loss: 0.07465 valid_loss: 0.08391 test_loss: 0.09177 \n",
      "[ 57/500] train_loss: 0.07556 valid_loss: 0.08006 test_loss: 0.08991 \n",
      "验证损失减少 (0.080942 --> 0.080055). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.07480 valid_loss: 0.08033 test_loss: 0.08953 \n",
      "[ 59/500] train_loss: 0.07359 valid_loss: 0.08019 test_loss: 0.08917 \n",
      "[ 60/500] train_loss: 0.07316 valid_loss: 0.08258 test_loss: 0.08957 \n",
      "[ 61/500] train_loss: 0.07264 valid_loss: 0.08003 test_loss: 0.08803 \n",
      "验证损失减少 (0.080055 --> 0.080034). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.07535 valid_loss: 0.08090 test_loss: 0.09188 \n",
      "[ 63/500] train_loss: 0.07355 valid_loss: 0.07990 test_loss: 0.08827 \n",
      "验证损失减少 (0.080034 --> 0.079900). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.07464 valid_loss: 0.08133 test_loss: 0.08950 \n",
      "[ 65/500] train_loss: 0.07322 valid_loss: 0.08010 test_loss: 0.08909 \n",
      "[ 66/500] train_loss: 0.07385 valid_loss: 0.08121 test_loss: 0.08657 \n",
      "[ 67/500] train_loss: 0.07165 valid_loss: 0.08041 test_loss: 0.08851 \n",
      "[ 68/500] train_loss: 0.07157 valid_loss: 0.07992 test_loss: 0.08738 \n",
      "[ 69/500] train_loss: 0.07216 valid_loss: 0.08023 test_loss: 0.08865 \n",
      "[ 70/500] train_loss: 0.07176 valid_loss: 0.08131 test_loss: 0.08916 \n",
      "[ 71/500] train_loss: 0.07045 valid_loss: 0.08019 test_loss: 0.08896 \n",
      "[ 72/500] train_loss: 0.07195 valid_loss: 0.08082 test_loss: 0.08729 \n",
      "[ 73/500] train_loss: 0.07005 valid_loss: 0.08053 test_loss: 0.08917 \n",
      "[ 74/500] train_loss: 0.07134 valid_loss: 0.07931 test_loss: 0.08804 \n",
      "验证损失减少 (0.079900 --> 0.079313). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.07087 valid_loss: 0.08029 test_loss: 0.08759 \n",
      "[ 76/500] train_loss: 0.06880 valid_loss: 0.07937 test_loss: 0.08604 \n",
      "[ 77/500] train_loss: 0.07082 valid_loss: 0.08112 test_loss: 0.08689 \n",
      "[ 78/500] train_loss: 0.06999 valid_loss: 0.07888 test_loss: 0.08829 \n",
      "验证损失减少 (0.079313 --> 0.078879). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.07003 valid_loss: 0.07750 test_loss: 0.08736 \n",
      "验证损失减少 (0.078879 --> 0.077499). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06879 valid_loss: 0.07807 test_loss: 0.08669 \n",
      "[ 81/500] train_loss: 0.06824 valid_loss: 0.07764 test_loss: 0.08683 \n",
      "[ 82/500] train_loss: 0.06814 valid_loss: 0.07848 test_loss: 0.08604 \n",
      "[ 83/500] train_loss: 0.06969 valid_loss: 0.07635 test_loss: 0.08427 \n",
      "验证损失减少 (0.077499 --> 0.076347). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.06815 valid_loss: 0.08138 test_loss: 0.08959 \n",
      "[ 85/500] train_loss: 0.06944 valid_loss: 0.07697 test_loss: 0.08686 \n",
      "[ 86/500] train_loss: 0.06759 valid_loss: 0.07823 test_loss: 0.08571 \n",
      "[ 87/500] train_loss: 0.06885 valid_loss: 0.07852 test_loss: 0.08669 \n",
      "[ 88/500] train_loss: 0.06693 valid_loss: 0.07790 test_loss: 0.08328 \n",
      "[ 89/500] train_loss: 0.06840 valid_loss: 0.07994 test_loss: 0.08841 \n",
      "[ 90/500] train_loss: 0.06756 valid_loss: 0.07900 test_loss: 0.08610 \n",
      "[ 91/500] train_loss: 0.06625 valid_loss: 0.07887 test_loss: 0.08721 \n",
      "[ 92/500] train_loss: 0.06876 valid_loss: 0.08006 test_loss: 0.08356 \n",
      "[ 93/500] train_loss: 0.06688 valid_loss: 0.07748 test_loss: 0.08515 \n",
      "[ 94/500] train_loss: 0.06652 valid_loss: 0.07650 test_loss: 0.08485 \n",
      "[ 95/500] train_loss: 0.06711 valid_loss: 0.07599 test_loss: 0.08424 \n",
      "验证损失减少 (0.076347 --> 0.075992). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.06577 valid_loss: 0.07651 test_loss: 0.08571 \n",
      "[ 97/500] train_loss: 0.06641 valid_loss: 0.07618 test_loss: 0.08270 \n",
      "[ 98/500] train_loss: 0.06520 valid_loss: 0.07858 test_loss: 0.08486 \n",
      "[ 99/500] train_loss: 0.06568 valid_loss: 0.07674 test_loss: 0.08254 \n",
      "[100/500] train_loss: 0.06553 valid_loss: 0.07941 test_loss: 0.08307 \n",
      "[101/500] train_loss: 0.06450 valid_loss: 0.07681 test_loss: 0.08256 \n",
      "[102/500] train_loss: 0.06659 valid_loss: 0.07603 test_loss: 0.08314 \n",
      "[103/500] train_loss: 0.06490 valid_loss: 0.07842 test_loss: 0.08362 \n",
      "[104/500] train_loss: 0.06535 valid_loss: 0.07595 test_loss: 0.08508 \n",
      "验证损失减少 (0.075992 --> 0.075949). 正在保存模型...\n",
      "[105/500] train_loss: 0.06378 valid_loss: 0.07806 test_loss: 0.08320 \n",
      "[106/500] train_loss: 0.06570 valid_loss: 0.07602 test_loss: 0.08424 \n",
      "[107/500] train_loss: 0.06609 valid_loss: 0.07994 test_loss: 0.08455 \n",
      "[108/500] train_loss: 0.06358 valid_loss: 0.07791 test_loss: 0.08407 \n",
      "[109/500] train_loss: 0.06450 valid_loss: 0.07729 test_loss: 0.08504 \n",
      "[110/500] train_loss: 0.06631 valid_loss: 0.07630 test_loss: 0.08495 \n",
      "[111/500] train_loss: 0.06375 valid_loss: 0.07729 test_loss: 0.08351 \n",
      "[112/500] train_loss: 0.06280 valid_loss: 0.07668 test_loss: 0.08366 \n",
      "[113/500] train_loss: 0.06375 valid_loss: 0.07662 test_loss: 0.08408 \n",
      "[114/500] train_loss: 0.06399 valid_loss: 0.07837 test_loss: 0.08387 \n",
      "[115/500] train_loss: 0.06314 valid_loss: 0.07585 test_loss: 0.08428 \n",
      "验证损失减少 (0.075949 --> 0.075851). 正在保存模型...\n",
      "[116/500] train_loss: 0.06312 valid_loss: 0.07538 test_loss: 0.08250 \n",
      "验证损失减少 (0.075851 --> 0.075376). 正在保存模型...\n",
      "[117/500] train_loss: 0.06364 valid_loss: 0.07641 test_loss: 0.08354 \n",
      "[118/500] train_loss: 0.06215 valid_loss: 0.07508 test_loss: 0.08240 \n",
      "验证损失减少 (0.075376 --> 0.075084). 正在保存模型...\n",
      "[119/500] train_loss: 0.06255 valid_loss: 0.07755 test_loss: 0.08364 \n",
      "[120/500] train_loss: 0.06278 valid_loss: 0.07477 test_loss: 0.08214 \n",
      "验证损失减少 (0.075084 --> 0.074770). 正在保存模型...\n",
      "[121/500] train_loss: 0.06337 valid_loss: 0.07441 test_loss: 0.08211 \n",
      "验证损失减少 (0.074770 --> 0.074407). 正在保存模型...\n",
      "[122/500] train_loss: 0.06372 valid_loss: 0.07627 test_loss: 0.08470 \n",
      "[123/500] train_loss: 0.06074 valid_loss: 0.07568 test_loss: 0.08213 \n",
      "[124/500] train_loss: 0.06180 valid_loss: 0.07433 test_loss: 0.08165 \n",
      "验证损失减少 (0.074407 --> 0.074326). 正在保存模型...\n",
      "[125/500] train_loss: 0.06124 valid_loss: 0.07811 test_loss: 0.08146 \n",
      "[126/500] train_loss: 0.06156 valid_loss: 0.07562 test_loss: 0.08185 \n",
      "[127/500] train_loss: 0.06091 valid_loss: 0.07650 test_loss: 0.08200 \n",
      "[128/500] train_loss: 0.06250 valid_loss: 0.07653 test_loss: 0.08225 \n",
      "[129/500] train_loss: 0.06187 valid_loss: 0.07554 test_loss: 0.08309 \n",
      "[130/500] train_loss: 0.06105 valid_loss: 0.07358 test_loss: 0.08177 \n",
      "验证损失减少 (0.074326 --> 0.073580). 正在保存模型...\n",
      "[131/500] train_loss: 0.06006 valid_loss: 0.07521 test_loss: 0.08198 \n",
      "[132/500] train_loss: 0.06067 valid_loss: 0.07563 test_loss: 0.08215 \n",
      "[133/500] train_loss: 0.06136 valid_loss: 0.07406 test_loss: 0.08196 \n",
      "[134/500] train_loss: 0.06094 valid_loss: 0.07442 test_loss: 0.08160 \n",
      "[135/500] train_loss: 0.06098 valid_loss: 0.07562 test_loss: 0.08309 \n",
      "[136/500] train_loss: 0.06205 valid_loss: 0.07532 test_loss: 0.08332 \n",
      "[137/500] train_loss: 0.05988 valid_loss: 0.07523 test_loss: 0.08281 \n",
      "[138/500] train_loss: 0.05956 valid_loss: 0.07517 test_loss: 0.08527 \n",
      "[139/500] train_loss: 0.05953 valid_loss: 0.07582 test_loss: 0.08202 \n",
      "[140/500] train_loss: 0.06191 valid_loss: 0.07579 test_loss: 0.08312 \n",
      "[141/500] train_loss: 0.05985 valid_loss: 0.07543 test_loss: 0.08247 \n",
      "[142/500] train_loss: 0.05960 valid_loss: 0.07458 test_loss: 0.08290 \n",
      "[143/500] train_loss: 0.06154 valid_loss: 0.07383 test_loss: 0.08251 \n",
      "[144/500] train_loss: 0.05898 valid_loss: 0.07429 test_loss: 0.08226 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145/500] train_loss: 0.05934 valid_loss: 0.07608 test_loss: 0.08181 \n",
      "[146/500] train_loss: 0.05929 valid_loss: 0.07346 test_loss: 0.08157 \n",
      "验证损失减少 (0.073580 --> 0.073461). 正在保存模型...\n",
      "[147/500] train_loss: 0.05872 valid_loss: 0.07300 test_loss: 0.08293 \n",
      "验证损失减少 (0.073461 --> 0.072998). 正在保存模型...\n",
      "[148/500] train_loss: 0.05828 valid_loss: 0.07587 test_loss: 0.08479 \n",
      "[149/500] train_loss: 0.05940 valid_loss: 0.07733 test_loss: 0.08259 \n",
      "[150/500] train_loss: 0.05853 valid_loss: 0.07458 test_loss: 0.08188 \n",
      "[151/500] train_loss: 0.05913 valid_loss: 0.07542 test_loss: 0.08483 \n",
      "[152/500] train_loss: 0.06043 valid_loss: 0.07370 test_loss: 0.08237 \n",
      "[153/500] train_loss: 0.05714 valid_loss: 0.07477 test_loss: 0.08172 \n",
      "[154/500] train_loss: 0.05816 valid_loss: 0.07476 test_loss: 0.08278 \n",
      "[155/500] train_loss: 0.05762 valid_loss: 0.07433 test_loss: 0.08142 \n",
      "[156/500] train_loss: 0.05906 valid_loss: 0.07402 test_loss: 0.08106 \n",
      "[157/500] train_loss: 0.05801 valid_loss: 0.07367 test_loss: 0.08122 \n",
      "[158/500] train_loss: 0.05923 valid_loss: 0.07368 test_loss: 0.08078 \n",
      "[159/500] train_loss: 0.05712 valid_loss: 0.07472 test_loss: 0.08139 \n",
      "[160/500] train_loss: 0.05750 valid_loss: 0.07445 test_loss: 0.08194 \n",
      "[161/500] train_loss: 0.05730 valid_loss: 0.07581 test_loss: 0.08298 \n",
      "[162/500] train_loss: 0.05969 valid_loss: 0.07364 test_loss: 0.08213 \n",
      "[163/500] train_loss: 0.05659 valid_loss: 0.07302 test_loss: 0.08112 \n",
      "[164/500] train_loss: 0.05672 valid_loss: 0.07409 test_loss: 0.08245 \n",
      "[165/500] train_loss: 0.05689 valid_loss: 0.07475 test_loss: 0.08201 \n",
      "[166/500] train_loss: 0.05692 valid_loss: 0.07331 test_loss: 0.08168 \n",
      "[167/500] train_loss: 0.05642 valid_loss: 0.07292 test_loss: 0.08126 \n",
      "验证损失减少 (0.072998 --> 0.072919). 正在保存模型...\n",
      "[168/500] train_loss: 0.05642 valid_loss: 0.07317 test_loss: 0.08065 \n",
      "[169/500] train_loss: 0.05574 valid_loss: 0.07411 test_loss: 0.08203 \n",
      "[170/500] train_loss: 0.05712 valid_loss: 0.07384 test_loss: 0.08263 \n",
      "[171/500] train_loss: 0.05705 valid_loss: 0.07476 test_loss: 0.08333 \n",
      "[172/500] train_loss: 0.05752 valid_loss: 0.07385 test_loss: 0.08260 \n",
      "[173/500] train_loss: 0.05632 valid_loss: 0.07209 test_loss: 0.08175 \n",
      "验证损失减少 (0.072919 --> 0.072089). 正在保存模型...\n",
      "[174/500] train_loss: 0.05580 valid_loss: 0.07400 test_loss: 0.08200 \n",
      "[175/500] train_loss: 0.05629 valid_loss: 0.07355 test_loss: 0.08177 \n",
      "[176/500] train_loss: 0.05663 valid_loss: 0.07243 test_loss: 0.08127 \n",
      "[177/500] train_loss: 0.05622 valid_loss: 0.07572 test_loss: 0.08180 \n",
      "[178/500] train_loss: 0.05689 valid_loss: 0.07356 test_loss: 0.07968 \n",
      "[179/500] train_loss: 0.05667 valid_loss: 0.07251 test_loss: 0.08209 \n",
      "[180/500] train_loss: 0.05732 valid_loss: 0.07213 test_loss: 0.08083 \n",
      "[181/500] train_loss: 0.05609 valid_loss: 0.07337 test_loss: 0.08185 \n",
      "[182/500] train_loss: 0.05592 valid_loss: 0.07275 test_loss: 0.08087 \n",
      "[183/500] train_loss: 0.05582 valid_loss: 0.07412 test_loss: 0.08236 \n",
      "[184/500] train_loss: 0.05741 valid_loss: 0.07248 test_loss: 0.08024 \n",
      "[185/500] train_loss: 0.05542 valid_loss: 0.07592 test_loss: 0.08285 \n",
      "[186/500] train_loss: 0.05500 valid_loss: 0.07369 test_loss: 0.08231 \n",
      "[187/500] train_loss: 0.05574 valid_loss: 0.07354 test_loss: 0.08202 \n",
      "[188/500] train_loss: 0.05452 valid_loss: 0.07450 test_loss: 0.08077 \n",
      "[189/500] train_loss: 0.05441 valid_loss: 0.07325 test_loss: 0.08038 \n",
      "[190/500] train_loss: 0.05535 valid_loss: 0.07435 test_loss: 0.08275 \n",
      "[191/500] train_loss: 0.05509 valid_loss: 0.07520 test_loss: 0.08130 \n",
      "[192/500] train_loss: 0.05406 valid_loss: 0.07370 test_loss: 0.08249 \n",
      "[193/500] train_loss: 0.05302 valid_loss: 0.07446 test_loss: 0.08268 \n",
      "[194/500] train_loss: 0.05527 valid_loss: 0.07523 test_loss: 0.08164 \n",
      "[195/500] train_loss: 0.05350 valid_loss: 0.07541 test_loss: 0.08268 \n",
      "[196/500] train_loss: 0.05316 valid_loss: 0.07462 test_loss: 0.08160 \n",
      "[197/500] train_loss: 0.05575 valid_loss: 0.07125 test_loss: 0.08105 \n",
      "验证损失减少 (0.072089 --> 0.071253). 正在保存模型...\n",
      "[198/500] train_loss: 0.05434 valid_loss: 0.07331 test_loss: 0.08362 \n",
      "[199/500] train_loss: 0.05317 valid_loss: 0.07252 test_loss: 0.08083 \n",
      "[200/500] train_loss: 0.05384 valid_loss: 0.07201 test_loss: 0.08177 \n",
      "[201/500] train_loss: 0.05514 valid_loss: 0.07421 test_loss: 0.08192 \n",
      "[202/500] train_loss: 0.05493 valid_loss: 0.07365 test_loss: 0.08035 \n",
      "[203/500] train_loss: 0.05406 valid_loss: 0.07645 test_loss: 0.08229 \n",
      "[204/500] train_loss: 0.05286 valid_loss: 0.07451 test_loss: 0.08132 \n",
      "[205/500] train_loss: 0.05363 valid_loss: 0.07196 test_loss: 0.08021 \n",
      "[206/500] train_loss: 0.05356 valid_loss: 0.07280 test_loss: 0.08095 \n",
      "[207/500] train_loss: 0.05457 valid_loss: 0.07188 test_loss: 0.08154 \n",
      "[208/500] train_loss: 0.05382 valid_loss: 0.07386 test_loss: 0.08066 \n",
      "[209/500] train_loss: 0.05242 valid_loss: 0.07312 test_loss: 0.08036 \n",
      "[210/500] train_loss: 0.05257 valid_loss: 0.07374 test_loss: 0.08369 \n",
      "[211/500] train_loss: 0.05214 valid_loss: 0.07361 test_loss: 0.08155 \n",
      "[212/500] train_loss: 0.05327 valid_loss: 0.07389 test_loss: 0.08131 \n",
      "[213/500] train_loss: 0.05342 valid_loss: 0.07211 test_loss: 0.08029 \n",
      "[214/500] train_loss: 0.05354 valid_loss: 0.07135 test_loss: 0.08219 \n",
      "[215/500] train_loss: 0.05343 valid_loss: 0.07288 test_loss: 0.08117 \n",
      "[216/500] train_loss: 0.05336 valid_loss: 0.07310 test_loss: 0.08203 \n",
      "[217/500] train_loss: 0.05393 valid_loss: 0.07270 test_loss: 0.08054 \n",
      "[218/500] train_loss: 0.05296 valid_loss: 0.07300 test_loss: 0.08193 \n",
      "[219/500] train_loss: 0.05181 valid_loss: 0.07274 test_loss: 0.08232 \n",
      "[220/500] train_loss: 0.05432 valid_loss: 0.07398 test_loss: 0.08108 \n",
      "[221/500] train_loss: 0.05182 valid_loss: 0.07349 test_loss: 0.08291 \n",
      "[222/500] train_loss: 0.05271 valid_loss: 0.07211 test_loss: 0.08226 \n",
      "[223/500] train_loss: 0.05332 valid_loss: 0.07503 test_loss: 0.08264 \n",
      "[224/500] train_loss: 0.05192 valid_loss: 0.07410 test_loss: 0.08212 \n",
      "[225/500] train_loss: 0.05311 valid_loss: 0.07605 test_loss: 0.08269 \n",
      "[226/500] train_loss: 0.05233 valid_loss: 0.07441 test_loss: 0.08171 \n",
      "[227/500] train_loss: 0.05202 valid_loss: 0.07352 test_loss: 0.08259 \n",
      "[228/500] train_loss: 0.05286 valid_loss: 0.07245 test_loss: 0.08243 \n",
      "[229/500] train_loss: 0.05059 valid_loss: 0.07216 test_loss: 0.08241 \n",
      "[230/500] train_loss: 0.05227 valid_loss: 0.07261 test_loss: 0.08207 \n",
      "[231/500] train_loss: 0.05168 valid_loss: 0.07304 test_loss: 0.08040 \n",
      "[232/500] train_loss: 0.05205 valid_loss: 0.07191 test_loss: 0.08028 \n",
      "[233/500] train_loss: 0.05160 valid_loss: 0.07282 test_loss: 0.08170 \n",
      "[234/500] train_loss: 0.05135 valid_loss: 0.07253 test_loss: 0.08093 \n",
      "[235/500] train_loss: 0.05187 valid_loss: 0.07235 test_loss: 0.08091 \n",
      "[236/500] train_loss: 0.05298 valid_loss: 0.07256 test_loss: 0.08117 \n",
      "[237/500] train_loss: 0.05111 valid_loss: 0.07222 test_loss: 0.08182 \n",
      "[238/500] train_loss: 0.05065 valid_loss: 0.07359 test_loss: 0.08280 \n",
      "[239/500] train_loss: 0.05184 valid_loss: 0.07175 test_loss: 0.08077 \n",
      "[240/500] train_loss: 0.05160 valid_loss: 0.07275 test_loss: 0.08254 \n",
      "[241/500] train_loss: 0.05072 valid_loss: 0.07240 test_loss: 0.08146 \n",
      "[242/500] train_loss: 0.05073 valid_loss: 0.07351 test_loss: 0.08221 \n",
      "[243/500] train_loss: 0.05250 valid_loss: 0.07171 test_loss: 0.08181 \n",
      "[244/500] train_loss: 0.05165 valid_loss: 0.07262 test_loss: 0.08147 \n",
      "[245/500] train_loss: 0.05142 valid_loss: 0.07183 test_loss: 0.08107 \n",
      "[246/500] train_loss: 0.05005 valid_loss: 0.07231 test_loss: 0.08081 \n",
      "[247/500] train_loss: 0.05076 valid_loss: 0.07344 test_loss: 0.08161 \n",
      "[248/500] train_loss: 0.05137 valid_loss: 0.07276 test_loss: 0.08231 \n",
      "[249/500] train_loss: 0.05134 valid_loss: 0.07169 test_loss: 0.08060 \n",
      "[250/500] train_loss: 0.05222 valid_loss: 0.07149 test_loss: 0.08167 \n",
      "[251/500] train_loss: 0.05119 valid_loss: 0.07261 test_loss: 0.08389 \n",
      "[252/500] train_loss: 0.04882 valid_loss: 0.07307 test_loss: 0.08171 \n",
      "[253/500] train_loss: 0.05095 valid_loss: 0.07338 test_loss: 0.08270 \n",
      "[254/500] train_loss: 0.05019 valid_loss: 0.07307 test_loss: 0.08159 \n",
      "[255/500] train_loss: 0.05155 valid_loss: 0.07150 test_loss: 0.08279 \n",
      "[256/500] train_loss: 0.05001 valid_loss: 0.07277 test_loss: 0.08131 \n",
      "[257/500] train_loss: 0.04934 valid_loss: 0.07236 test_loss: 0.08158 \n",
      "[258/500] train_loss: 0.05070 valid_loss: 0.07284 test_loss: 0.08232 \n",
      "[259/500] train_loss: 0.04956 valid_loss: 0.07327 test_loss: 0.08185 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260/500] train_loss: 0.04998 valid_loss: 0.07200 test_loss: 0.08104 \n",
      "[261/500] train_loss: 0.05029 valid_loss: 0.07155 test_loss: 0.08126 \n",
      "[262/500] train_loss: 0.05074 valid_loss: 0.07614 test_loss: 0.08344 \n",
      "[263/500] train_loss: 0.05071 valid_loss: 0.07409 test_loss: 0.08080 \n",
      "[264/500] train_loss: 0.05142 valid_loss: 0.07179 test_loss: 0.08123 \n",
      "[265/500] train_loss: 0.04937 valid_loss: 0.07306 test_loss: 0.08188 \n",
      "[266/500] train_loss: 0.05023 valid_loss: 0.07137 test_loss: 0.08178 \n",
      "[267/500] train_loss: 0.05002 valid_loss: 0.07198 test_loss: 0.08188 \n",
      "[268/500] train_loss: 0.04871 valid_loss: 0.07292 test_loss: 0.08067 \n",
      "[269/500] train_loss: 0.04976 valid_loss: 0.07134 test_loss: 0.07970 \n",
      "[270/500] train_loss: 0.04992 valid_loss: 0.07246 test_loss: 0.08323 \n",
      "[271/500] train_loss: 0.04895 valid_loss: 0.07114 test_loss: 0.08371 \n",
      "验证损失减少 (0.071253 --> 0.071142). 正在保存模型...\n",
      "[272/500] train_loss: 0.04932 valid_loss: 0.07105 test_loss: 0.08099 \n",
      "验证损失减少 (0.071142 --> 0.071047). 正在保存模型...\n",
      "[273/500] train_loss: 0.05024 valid_loss: 0.07197 test_loss: 0.08154 \n",
      "[274/500] train_loss: 0.04957 valid_loss: 0.07086 test_loss: 0.08010 \n",
      "验证损失减少 (0.071047 --> 0.070864). 正在保存模型...\n",
      "[275/500] train_loss: 0.04777 valid_loss: 0.07386 test_loss: 0.08328 \n",
      "[276/500] train_loss: 0.04979 valid_loss: 0.07158 test_loss: 0.08035 \n",
      "[277/500] train_loss: 0.04846 valid_loss: 0.07179 test_loss: 0.08210 \n",
      "[278/500] train_loss: 0.04914 valid_loss: 0.07087 test_loss: 0.08082 \n",
      "[279/500] train_loss: 0.04990 valid_loss: 0.07334 test_loss: 0.07975 \n",
      "[280/500] train_loss: 0.04935 valid_loss: 0.07251 test_loss: 0.08079 \n",
      "[281/500] train_loss: 0.04835 valid_loss: 0.07255 test_loss: 0.08071 \n",
      "[282/500] train_loss: 0.04859 valid_loss: 0.07215 test_loss: 0.08095 \n",
      "[283/500] train_loss: 0.04958 valid_loss: 0.07377 test_loss: 0.08089 \n",
      "[284/500] train_loss: 0.04956 valid_loss: 0.07269 test_loss: 0.08251 \n",
      "[285/500] train_loss: 0.04820 valid_loss: 0.07260 test_loss: 0.08132 \n",
      "[286/500] train_loss: 0.04933 valid_loss: 0.07242 test_loss: 0.08303 \n",
      "[287/500] train_loss: 0.04780 valid_loss: 0.07162 test_loss: 0.08137 \n",
      "[288/500] train_loss: 0.04842 valid_loss: 0.07372 test_loss: 0.08166 \n",
      "[289/500] train_loss: 0.04760 valid_loss: 0.07228 test_loss: 0.08308 \n",
      "[290/500] train_loss: 0.04836 valid_loss: 0.07276 test_loss: 0.08132 \n",
      "[291/500] train_loss: 0.04814 valid_loss: 0.07280 test_loss: 0.08057 \n",
      "[292/500] train_loss: 0.04728 valid_loss: 0.07565 test_loss: 0.08113 \n",
      "[293/500] train_loss: 0.04819 valid_loss: 0.07284 test_loss: 0.08137 \n",
      "[294/500] train_loss: 0.04835 valid_loss: 0.07226 test_loss: 0.08134 \n",
      "[295/500] train_loss: 0.04781 valid_loss: 0.07271 test_loss: 0.08161 \n",
      "[296/500] train_loss: 0.04719 valid_loss: 0.07110 test_loss: 0.08045 \n",
      "[297/500] train_loss: 0.04769 valid_loss: 0.07214 test_loss: 0.08111 \n",
      "[298/500] train_loss: 0.04710 valid_loss: 0.07223 test_loss: 0.08048 \n",
      "[299/500] train_loss: 0.04797 valid_loss: 0.07299 test_loss: 0.08055 \n",
      "[300/500] train_loss: 0.04748 valid_loss: 0.07233 test_loss: 0.08171 \n",
      "[301/500] train_loss: 0.04727 valid_loss: 0.07608 test_loss: 0.08134 \n",
      "[302/500] train_loss: 0.04712 valid_loss: 0.07319 test_loss: 0.08120 \n",
      "[303/500] train_loss: 0.04651 valid_loss: 0.07459 test_loss: 0.08255 \n",
      "[304/500] train_loss: 0.04855 valid_loss: 0.07384 test_loss: 0.08170 \n",
      "[305/500] train_loss: 0.04788 valid_loss: 0.07283 test_loss: 0.08080 \n",
      "[306/500] train_loss: 0.04752 valid_loss: 0.07399 test_loss: 0.08052 \n",
      "[307/500] train_loss: 0.04765 valid_loss: 0.07449 test_loss: 0.08303 \n",
      "[308/500] train_loss: 0.04795 valid_loss: 0.07424 test_loss: 0.08393 \n",
      "[309/500] train_loss: 0.04783 valid_loss: 0.07432 test_loss: 0.08335 \n",
      "[310/500] train_loss: 0.04820 valid_loss: 0.07201 test_loss: 0.08141 \n",
      "[311/500] train_loss: 0.04699 valid_loss: 0.07213 test_loss: 0.08155 \n",
      "[312/500] train_loss: 0.04751 valid_loss: 0.07234 test_loss: 0.08206 \n",
      "[313/500] train_loss: 0.04841 valid_loss: 0.07218 test_loss: 0.08226 \n",
      "[314/500] train_loss: 0.04695 valid_loss: 0.07328 test_loss: 0.08193 \n",
      "[315/500] train_loss: 0.04779 valid_loss: 0.07278 test_loss: 0.08230 \n",
      "[316/500] train_loss: 0.04762 valid_loss: 0.07495 test_loss: 0.08321 \n",
      "[317/500] train_loss: 0.04567 valid_loss: 0.07347 test_loss: 0.08195 \n",
      "[318/500] train_loss: 0.04737 valid_loss: 0.07480 test_loss: 0.08241 \n",
      "[319/500] train_loss: 0.04864 valid_loss: 0.07385 test_loss: 0.08152 \n",
      "[320/500] train_loss: 0.04680 valid_loss: 0.07219 test_loss: 0.08118 \n",
      "[321/500] train_loss: 0.04678 valid_loss: 0.07481 test_loss: 0.08288 \n",
      "[322/500] train_loss: 0.04718 valid_loss: 0.07603 test_loss: 0.08358 \n",
      "[323/500] train_loss: 0.04498 valid_loss: 0.07603 test_loss: 0.08223 \n",
      "[324/500] train_loss: 0.04789 valid_loss: 0.07675 test_loss: 0.08393 \n",
      "[325/500] train_loss: 0.04825 valid_loss: 0.07408 test_loss: 0.08165 \n",
      "[326/500] train_loss: 0.04748 valid_loss: 0.07314 test_loss: 0.08275 \n",
      "[327/500] train_loss: 0.04586 valid_loss: 0.07425 test_loss: 0.08269 \n",
      "[328/500] train_loss: 0.04668 valid_loss: 0.07281 test_loss: 0.08110 \n",
      "[329/500] train_loss: 0.04562 valid_loss: 0.07458 test_loss: 0.08245 \n",
      "[330/500] train_loss: 0.04654 valid_loss: 0.07369 test_loss: 0.08311 \n",
      "[331/500] train_loss: 0.04572 valid_loss: 0.07434 test_loss: 0.08384 \n",
      "[332/500] train_loss: 0.04621 valid_loss: 0.07231 test_loss: 0.08276 \n",
      "[333/500] train_loss: 0.04703 valid_loss: 0.07375 test_loss: 0.08180 \n",
      "[334/500] train_loss: 0.04580 valid_loss: 0.07430 test_loss: 0.08109 \n",
      "[335/500] train_loss: 0.04561 valid_loss: 0.07725 test_loss: 0.08138 \n",
      "[336/500] train_loss: 0.04508 valid_loss: 0.07349 test_loss: 0.08098 \n",
      "[337/500] train_loss: 0.04756 valid_loss: 0.07458 test_loss: 0.08345 \n",
      "[338/500] train_loss: 0.04538 valid_loss: 0.07449 test_loss: 0.08165 \n",
      "[339/500] train_loss: 0.04708 valid_loss: 0.07377 test_loss: 0.08100 \n",
      "[340/500] train_loss: 0.04773 valid_loss: 0.07518 test_loss: 0.08253 \n",
      "[341/500] train_loss: 0.04549 valid_loss: 0.07600 test_loss: 0.08206 \n",
      "[342/500] train_loss: 0.04584 valid_loss: 0.07580 test_loss: 0.08362 \n",
      "[343/500] train_loss: 0.04576 valid_loss: 0.07597 test_loss: 0.08240 \n",
      "[344/500] train_loss: 0.04583 valid_loss: 0.07678 test_loss: 0.08263 \n",
      "[345/500] train_loss: 0.04525 valid_loss: 0.07549 test_loss: 0.08209 \n",
      "[346/500] train_loss: 0.04643 valid_loss: 0.07383 test_loss: 0.08215 \n",
      "[347/500] train_loss: 0.04591 valid_loss: 0.07354 test_loss: 0.08281 \n",
      "[348/500] train_loss: 0.04587 valid_loss: 0.07412 test_loss: 0.08240 \n",
      "[349/500] train_loss: 0.04647 valid_loss: 0.07277 test_loss: 0.08197 \n",
      "[350/500] train_loss: 0.04554 valid_loss: 0.07371 test_loss: 0.08156 \n",
      "[351/500] train_loss: 0.04638 valid_loss: 0.07655 test_loss: 0.08184 \n",
      "[352/500] train_loss: 0.04535 valid_loss: 0.07509 test_loss: 0.08120 \n",
      "[353/500] train_loss: 0.04707 valid_loss: 0.07570 test_loss: 0.08205 \n",
      "[354/500] train_loss: 0.04495 valid_loss: 0.07458 test_loss: 0.08220 \n",
      "[355/500] train_loss: 0.04571 valid_loss: 0.07543 test_loss: 0.08201 \n",
      "[356/500] train_loss: 0.04506 valid_loss: 0.07692 test_loss: 0.08187 \n",
      "[357/500] train_loss: 0.04636 valid_loss: 0.07646 test_loss: 0.08369 \n",
      "[358/500] train_loss: 0.04487 valid_loss: 0.07609 test_loss: 0.08292 \n",
      "[359/500] train_loss: 0.04473 valid_loss: 0.07383 test_loss: 0.08287 \n",
      "[360/500] train_loss: 0.04405 valid_loss: 0.07389 test_loss: 0.08455 \n",
      "[361/500] train_loss: 0.04496 valid_loss: 0.07274 test_loss: 0.08402 \n",
      "[362/500] train_loss: 0.04522 valid_loss: 0.07718 test_loss: 0.08477 \n",
      "[363/500] train_loss: 0.04434 valid_loss: 0.07459 test_loss: 0.08297 \n",
      "[364/500] train_loss: 0.04490 valid_loss: 0.07479 test_loss: 0.08268 \n",
      "[365/500] train_loss: 0.04507 valid_loss: 0.07712 test_loss: 0.08334 \n",
      "[366/500] train_loss: 0.04583 valid_loss: 0.07407 test_loss: 0.08346 \n",
      "[367/500] train_loss: 0.04606 valid_loss: 0.07373 test_loss: 0.08362 \n",
      "[368/500] train_loss: 0.04415 valid_loss: 0.07415 test_loss: 0.08248 \n",
      "[369/500] train_loss: 0.04467 valid_loss: 0.07358 test_loss: 0.08240 \n",
      "[370/500] train_loss: 0.04528 valid_loss: 0.07516 test_loss: 0.08203 \n",
      "[371/500] train_loss: 0.04463 valid_loss: 0.07802 test_loss: 0.08269 \n",
      "[372/500] train_loss: 0.04530 valid_loss: 0.07896 test_loss: 0.08284 \n",
      "[373/500] train_loss: 0.04355 valid_loss: 0.07530 test_loss: 0.08201 \n",
      "[374/500] train_loss: 0.04450 valid_loss: 0.07479 test_loss: 0.08212 \n",
      "[375/500] train_loss: 0.04464 valid_loss: 0.07258 test_loss: 0.08069 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[376/500] train_loss: 0.04477 valid_loss: 0.07441 test_loss: 0.08355 \n",
      "[377/500] train_loss: 0.04545 valid_loss: 0.07450 test_loss: 0.08149 \n",
      "[378/500] train_loss: 0.04429 valid_loss: 0.07240 test_loss: 0.08241 \n",
      "[379/500] train_loss: 0.04427 valid_loss: 0.07276 test_loss: 0.08382 \n",
      "[380/500] train_loss: 0.04371 valid_loss: 0.07741 test_loss: 0.08319 \n",
      "[381/500] train_loss: 0.04463 valid_loss: 0.07289 test_loss: 0.08322 \n",
      "[382/500] train_loss: 0.04339 valid_loss: 0.07476 test_loss: 0.08330 \n",
      "[383/500] train_loss: 0.04439 valid_loss: 0.07437 test_loss: 0.08455 \n",
      "[384/500] train_loss: 0.04545 valid_loss: 0.07283 test_loss: 0.08272 \n",
      "[385/500] train_loss: 0.04450 valid_loss: 0.07279 test_loss: 0.08352 \n",
      "[386/500] train_loss: 0.04411 valid_loss: 0.07377 test_loss: 0.08319 \n",
      "[387/500] train_loss: 0.04483 valid_loss: 0.07409 test_loss: 0.08390 \n",
      "[388/500] train_loss: 0.04425 valid_loss: 0.07376 test_loss: 0.08369 \n",
      "[389/500] train_loss: 0.04397 valid_loss: 0.07410 test_loss: 0.08432 \n",
      "[390/500] train_loss: 0.04477 valid_loss: 0.07341 test_loss: 0.08411 \n",
      "[391/500] train_loss: 0.04287 valid_loss: 0.07434 test_loss: 0.08251 \n",
      "[392/500] train_loss: 0.04411 valid_loss: 0.07554 test_loss: 0.08512 \n",
      "[393/500] train_loss: 0.04514 valid_loss: 0.07327 test_loss: 0.08467 \n",
      "[394/500] train_loss: 0.04426 valid_loss: 0.07371 test_loss: 0.08390 \n",
      "[395/500] train_loss: 0.04440 valid_loss: 0.07238 test_loss: 0.08305 \n",
      "[396/500] train_loss: 0.04452 valid_loss: 0.07390 test_loss: 0.08252 \n",
      "[397/500] train_loss: 0.04366 valid_loss: 0.07236 test_loss: 0.08323 \n",
      "[398/500] train_loss: 0.04468 valid_loss: 0.07389 test_loss: 0.08248 \n",
      "[399/500] train_loss: 0.04338 valid_loss: 0.07507 test_loss: 0.08618 \n",
      "[400/500] train_loss: 0.04349 valid_loss: 0.07345 test_loss: 0.08285 \n",
      "[401/500] train_loss: 0.04403 valid_loss: 0.07553 test_loss: 0.08330 \n",
      "[402/500] train_loss: 0.04264 valid_loss: 0.07547 test_loss: 0.08322 \n",
      "[403/500] train_loss: 0.04279 valid_loss: 0.07339 test_loss: 0.08336 \n",
      "[404/500] train_loss: 0.04301 valid_loss: 0.07306 test_loss: 0.08359 \n",
      "[405/500] train_loss: 0.04394 valid_loss: 0.07505 test_loss: 0.08507 \n",
      "[406/500] train_loss: 0.04352 valid_loss: 0.07429 test_loss: 0.08203 \n",
      "[407/500] train_loss: 0.04348 valid_loss: 0.07488 test_loss: 0.08311 \n",
      "[408/500] train_loss: 0.04348 valid_loss: 0.07416 test_loss: 0.08216 \n",
      "[409/500] train_loss: 0.04353 valid_loss: 0.07398 test_loss: 0.08287 \n",
      "[410/500] train_loss: 0.04293 valid_loss: 0.07422 test_loss: 0.08389 \n",
      "[411/500] train_loss: 0.04370 valid_loss: 0.07361 test_loss: 0.08302 \n",
      "[412/500] train_loss: 0.04291 valid_loss: 0.07346 test_loss: 0.08305 \n",
      "[413/500] train_loss: 0.04230 valid_loss: 0.07423 test_loss: 0.08184 \n",
      "[414/500] train_loss: 0.04120 valid_loss: 0.07470 test_loss: 0.08250 \n",
      "[415/500] train_loss: 0.04310 valid_loss: 0.07637 test_loss: 0.08362 \n",
      "[416/500] train_loss: 0.04330 valid_loss: 0.07471 test_loss: 0.08198 \n",
      "[417/500] train_loss: 0.04310 valid_loss: 0.07443 test_loss: 0.08282 \n",
      "[418/500] train_loss: 0.04224 valid_loss: 0.07497 test_loss: 0.08425 \n",
      "[419/500] train_loss: 0.04322 valid_loss: 0.07378 test_loss: 0.08375 \n",
      "[420/500] train_loss: 0.04220 valid_loss: 0.07542 test_loss: 0.08389 \n",
      "[421/500] train_loss: 0.04242 valid_loss: 0.07382 test_loss: 0.08373 \n",
      "[422/500] train_loss: 0.04234 valid_loss: 0.07450 test_loss: 0.08518 \n",
      "[423/500] train_loss: 0.04363 valid_loss: 0.07393 test_loss: 0.08279 \n",
      "[424/500] train_loss: 0.04172 valid_loss: 0.07518 test_loss: 0.08370 \n",
      "[425/500] train_loss: 0.04316 valid_loss: 0.07403 test_loss: 0.08544 \n",
      "[426/500] train_loss: 0.04119 valid_loss: 0.07342 test_loss: 0.08401 \n",
      "[427/500] train_loss: 0.04318 valid_loss: 0.07408 test_loss: 0.08406 \n",
      "[428/500] train_loss: 0.04158 valid_loss: 0.07485 test_loss: 0.08379 \n",
      "[429/500] train_loss: 0.04298 valid_loss: 0.07549 test_loss: 0.08406 \n",
      "[430/500] train_loss: 0.04284 valid_loss: 0.07461 test_loss: 0.08458 \n",
      "[431/500] train_loss: 0.04189 valid_loss: 0.07495 test_loss: 0.08557 \n",
      "[432/500] train_loss: 0.04132 valid_loss: 0.07515 test_loss: 0.08530 \n",
      "[433/500] train_loss: 0.04330 valid_loss: 0.07578 test_loss: 0.08341 \n",
      "[434/500] train_loss: 0.04228 valid_loss: 0.07409 test_loss: 0.08491 \n",
      "[435/500] train_loss: 0.04245 valid_loss: 0.07364 test_loss: 0.08239 \n",
      "[436/500] train_loss: 0.04323 valid_loss: 0.07513 test_loss: 0.08568 \n",
      "[437/500] train_loss: 0.04319 valid_loss: 0.07412 test_loss: 0.08444 \n",
      "[438/500] train_loss: 0.04194 valid_loss: 0.07620 test_loss: 0.08704 \n",
      "[439/500] train_loss: 0.04360 valid_loss: 0.07717 test_loss: 0.08466 \n",
      "[440/500] train_loss: 0.04226 valid_loss: 0.07428 test_loss: 0.08336 \n",
      "[441/500] train_loss: 0.04229 valid_loss: 0.07323 test_loss: 0.08358 \n",
      "[442/500] train_loss: 0.04181 valid_loss: 0.07411 test_loss: 0.08446 \n",
      "[443/500] train_loss: 0.04196 valid_loss: 0.07466 test_loss: 0.08503 \n",
      "[444/500] train_loss: 0.04138 valid_loss: 0.07655 test_loss: 0.08530 \n",
      "[445/500] train_loss: 0.04148 valid_loss: 0.07450 test_loss: 0.08484 \n",
      "[446/500] train_loss: 0.04214 valid_loss: 0.07601 test_loss: 0.08375 \n",
      "[447/500] train_loss: 0.04207 valid_loss: 0.07818 test_loss: 0.08578 \n",
      "[448/500] train_loss: 0.04261 valid_loss: 0.07335 test_loss: 0.08386 \n",
      "[449/500] train_loss: 0.04190 valid_loss: 0.07634 test_loss: 0.08305 \n",
      "[450/500] train_loss: 0.04184 valid_loss: 0.07365 test_loss: 0.08406 \n",
      "[451/500] train_loss: 0.04328 valid_loss: 0.07429 test_loss: 0.08462 \n",
      "[452/500] train_loss: 0.04275 valid_loss: 0.07491 test_loss: 0.08488 \n",
      "[453/500] train_loss: 0.04281 valid_loss: 0.07589 test_loss: 0.08585 \n",
      "[454/500] train_loss: 0.04196 valid_loss: 0.07297 test_loss: 0.08369 \n",
      "[455/500] train_loss: 0.04140 valid_loss: 0.07403 test_loss: 0.08393 \n",
      "[456/500] train_loss: 0.04166 valid_loss: 0.07308 test_loss: 0.08316 \n",
      "[457/500] train_loss: 0.04178 valid_loss: 0.07416 test_loss: 0.08393 \n",
      "[458/500] train_loss: 0.04225 valid_loss: 0.07466 test_loss: 0.08619 \n",
      "[459/500] train_loss: 0.04193 valid_loss: 0.07392 test_loss: 0.08466 \n",
      "[460/500] train_loss: 0.04178 valid_loss: 0.07482 test_loss: 0.08580 \n",
      "[461/500] train_loss: 0.04070 valid_loss: 0.07288 test_loss: 0.08285 \n",
      "[462/500] train_loss: 0.04182 valid_loss: 0.07631 test_loss: 0.08487 \n",
      "[463/500] train_loss: 0.04133 valid_loss: 0.07477 test_loss: 0.08464 \n",
      "[464/500] train_loss: 0.04118 valid_loss: 0.07338 test_loss: 0.08465 \n",
      "[465/500] train_loss: 0.04206 valid_loss: 0.07248 test_loss: 0.08423 \n",
      "[466/500] train_loss: 0.04207 valid_loss: 0.07338 test_loss: 0.08317 \n",
      "[467/500] train_loss: 0.04123 valid_loss: 0.07368 test_loss: 0.08414 \n",
      "[468/500] train_loss: 0.04114 valid_loss: 0.07485 test_loss: 0.08315 \n",
      "[469/500] train_loss: 0.04185 valid_loss: 0.08065 test_loss: 0.08447 \n",
      "[470/500] train_loss: 0.04091 valid_loss: 0.07710 test_loss: 0.08339 \n",
      "[471/500] train_loss: 0.04122 valid_loss: 0.07524 test_loss: 0.08478 \n",
      "[472/500] train_loss: 0.04118 valid_loss: 0.07751 test_loss: 0.08556 \n",
      "[473/500] train_loss: 0.04150 valid_loss: 0.07388 test_loss: 0.08572 \n",
      "[474/500] train_loss: 0.04186 valid_loss: 0.07692 test_loss: 0.08651 \n",
      "[475/500] train_loss: 0.04075 valid_loss: 0.07673 test_loss: 0.08493 \n",
      "[476/500] train_loss: 0.04152 valid_loss: 0.08240 test_loss: 0.08565 \n",
      "[477/500] train_loss: 0.04148 valid_loss: 0.07477 test_loss: 0.08426 \n",
      "[478/500] train_loss: 0.04072 valid_loss: 0.07525 test_loss: 0.08710 \n",
      "[479/500] train_loss: 0.04181 valid_loss: 0.07273 test_loss: 0.08594 \n",
      "[480/500] train_loss: 0.04068 valid_loss: 0.07418 test_loss: 0.08437 \n",
      "[481/500] train_loss: 0.04072 valid_loss: 0.07448 test_loss: 0.08476 \n",
      "[482/500] train_loss: 0.04101 valid_loss: 0.07613 test_loss: 0.08396 \n",
      "[483/500] train_loss: 0.04199 valid_loss: 0.07922 test_loss: 0.08606 \n",
      "[484/500] train_loss: 0.04189 valid_loss: 0.07643 test_loss: 0.08514 \n",
      "[485/500] train_loss: 0.04113 valid_loss: 0.07657 test_loss: 0.08421 \n",
      "[486/500] train_loss: 0.04094 valid_loss: 0.07520 test_loss: 0.08485 \n",
      "[487/500] train_loss: 0.03992 valid_loss: 0.07465 test_loss: 0.08564 \n",
      "[488/500] train_loss: 0.04106 valid_loss: 0.07309 test_loss: 0.08363 \n",
      "[489/500] train_loss: 0.04187 valid_loss: 0.07558 test_loss: 0.08332 \n",
      "[490/500] train_loss: 0.04078 valid_loss: 0.07606 test_loss: 0.08420 \n",
      "[491/500] train_loss: 0.04067 valid_loss: 0.07487 test_loss: 0.08431 \n",
      "[492/500] train_loss: 0.04074 valid_loss: 0.07580 test_loss: 0.08464 \n",
      "[493/500] train_loss: 0.04119 valid_loss: 0.07507 test_loss: 0.08430 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[494/500] train_loss: 0.04015 valid_loss: 0.07546 test_loss: 0.08469 \n",
      "[495/500] train_loss: 0.04044 valid_loss: 0.07460 test_loss: 0.08428 \n",
      "[496/500] train_loss: 0.04008 valid_loss: 0.07740 test_loss: 0.08466 \n",
      "[497/500] train_loss: 0.04056 valid_loss: 0.07481 test_loss: 0.08537 \n",
      "[498/500] train_loss: 0.04100 valid_loss: 0.07251 test_loss: 0.08423 \n",
      "[499/500] train_loss: 0.04081 valid_loss: 0.07394 test_loss: 0.08480 \n",
      "[500/500] train_loss: 0.04107 valid_loss: 0.07302 test_loss: 0.08608 \n",
      "TRAINING MODEL 4\n",
      "[  1/500] train_loss: 0.32799 valid_loss: 0.24929 test_loss: 0.25806 \n",
      "验证损失减少 (inf --> 0.249285). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19054 valid_loss: 0.17724 test_loss: 0.18638 \n",
      "验证损失减少 (0.249285 --> 0.177244). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15257 valid_loss: 0.14785 test_loss: 0.15743 \n",
      "验证损失减少 (0.177244 --> 0.147846). 正在保存模型...\n",
      "[  4/500] train_loss: 0.14018 valid_loss: 0.13579 test_loss: 0.14478 \n",
      "验证损失减少 (0.147846 --> 0.135792). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12803 valid_loss: 0.12884 test_loss: 0.13707 \n",
      "验证损失减少 (0.135792 --> 0.128841). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12450 valid_loss: 0.12386 test_loss: 0.13394 \n",
      "验证损失减少 (0.128841 --> 0.123859). 正在保存模型...\n",
      "[  7/500] train_loss: 0.12176 valid_loss: 0.12178 test_loss: 0.13259 \n",
      "验证损失减少 (0.123859 --> 0.121777). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11636 valid_loss: 0.12475 test_loss: 0.13208 \n",
      "[  9/500] train_loss: 0.11427 valid_loss: 0.11672 test_loss: 0.12554 \n",
      "验证损失减少 (0.121777 --> 0.116717). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10920 valid_loss: 0.11075 test_loss: 0.12152 \n",
      "验证损失减少 (0.116717 --> 0.110753). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.11069 valid_loss: 0.11321 test_loss: 0.12165 \n",
      "[ 12/500] train_loss: 0.10934 valid_loss: 0.10699 test_loss: 0.11796 \n",
      "验证损失减少 (0.110753 --> 0.106990). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10342 valid_loss: 0.10967 test_loss: 0.11631 \n",
      "[ 14/500] train_loss: 0.10337 valid_loss: 0.10461 test_loss: 0.11487 \n",
      "验证损失减少 (0.106990 --> 0.104614). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10151 valid_loss: 0.10492 test_loss: 0.11542 \n",
      "[ 16/500] train_loss: 0.10312 valid_loss: 0.10224 test_loss: 0.11397 \n",
      "验证损失减少 (0.104614 --> 0.102242). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09871 valid_loss: 0.10149 test_loss: 0.11368 \n",
      "验证损失减少 (0.102242 --> 0.101490). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09871 valid_loss: 0.09978 test_loss: 0.11089 \n",
      "验证损失减少 (0.101490 --> 0.099783). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09793 valid_loss: 0.09845 test_loss: 0.10854 \n",
      "验证损失减少 (0.099783 --> 0.098448). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09767 valid_loss: 0.09777 test_loss: 0.10874 \n",
      "验证损失减少 (0.098448 --> 0.097770). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09537 valid_loss: 0.09532 test_loss: 0.10742 \n",
      "验证损失减少 (0.097770 --> 0.095321). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09383 valid_loss: 0.09900 test_loss: 0.10751 \n",
      "[ 23/500] train_loss: 0.09224 valid_loss: 0.09442 test_loss: 0.10524 \n",
      "验证损失减少 (0.095321 --> 0.094424). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09002 valid_loss: 0.09472 test_loss: 0.10414 \n",
      "[ 25/500] train_loss: 0.09228 valid_loss: 0.09431 test_loss: 0.10536 \n",
      "验证损失减少 (0.094424 --> 0.094311). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.08943 valid_loss: 0.09126 test_loss: 0.10298 \n",
      "验证损失减少 (0.094311 --> 0.091261). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.08971 valid_loss: 0.09079 test_loss: 0.10323 \n",
      "验证损失减少 (0.091261 --> 0.090791). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.09033 valid_loss: 0.09029 test_loss: 0.10100 \n",
      "验证损失减少 (0.090791 --> 0.090287). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08647 valid_loss: 0.09181 test_loss: 0.10144 \n",
      "[ 30/500] train_loss: 0.08826 valid_loss: 0.09045 test_loss: 0.10128 \n",
      "[ 31/500] train_loss: 0.08679 valid_loss: 0.08923 test_loss: 0.10101 \n",
      "验证损失减少 (0.090287 --> 0.089230). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08712 valid_loss: 0.08848 test_loss: 0.09966 \n",
      "验证损失减少 (0.089230 --> 0.088483). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08634 valid_loss: 0.08878 test_loss: 0.10072 \n",
      "[ 34/500] train_loss: 0.08602 valid_loss: 0.08723 test_loss: 0.09840 \n",
      "验证损失减少 (0.088483 --> 0.087229). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08558 valid_loss: 0.08808 test_loss: 0.09766 \n",
      "[ 36/500] train_loss: 0.08460 valid_loss: 0.08775 test_loss: 0.09822 \n",
      "[ 37/500] train_loss: 0.08584 valid_loss: 0.08635 test_loss: 0.09698 \n",
      "验证损失减少 (0.087229 --> 0.086351). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.08213 valid_loss: 0.08687 test_loss: 0.09675 \n",
      "[ 39/500] train_loss: 0.08352 valid_loss: 0.08695 test_loss: 0.09579 \n",
      "[ 40/500] train_loss: 0.08460 valid_loss: 0.08673 test_loss: 0.09632 \n",
      "[ 41/500] train_loss: 0.08016 valid_loss: 0.08584 test_loss: 0.09603 \n",
      "验证损失减少 (0.086351 --> 0.085836). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.08092 valid_loss: 0.08446 test_loss: 0.09530 \n",
      "验证损失减少 (0.085836 --> 0.084459). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.07872 valid_loss: 0.08510 test_loss: 0.09542 \n",
      "[ 44/500] train_loss: 0.08137 valid_loss: 0.08427 test_loss: 0.09635 \n",
      "验证损失减少 (0.084459 --> 0.084273). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.07931 valid_loss: 0.08722 test_loss: 0.09273 \n",
      "[ 46/500] train_loss: 0.07994 valid_loss: 0.08327 test_loss: 0.09372 \n",
      "验证损失减少 (0.084273 --> 0.083267). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.08049 valid_loss: 0.08469 test_loss: 0.09503 \n",
      "[ 48/500] train_loss: 0.08015 valid_loss: 0.08413 test_loss: 0.09283 \n",
      "[ 49/500] train_loss: 0.07815 valid_loss: 0.08326 test_loss: 0.09157 \n",
      "验证损失减少 (0.083267 --> 0.083256). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07773 valid_loss: 0.08345 test_loss: 0.09267 \n",
      "[ 51/500] train_loss: 0.07772 valid_loss: 0.08290 test_loss: 0.09205 \n",
      "验证损失减少 (0.083256 --> 0.082899). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07823 valid_loss: 0.08244 test_loss: 0.09261 \n",
      "验证损失减少 (0.082899 --> 0.082438). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07762 valid_loss: 0.08269 test_loss: 0.09114 \n",
      "[ 54/500] train_loss: 0.07752 valid_loss: 0.08333 test_loss: 0.09124 \n",
      "[ 55/500] train_loss: 0.07623 valid_loss: 0.08123 test_loss: 0.08978 \n",
      "验证损失减少 (0.082438 --> 0.081231). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07678 valid_loss: 0.08181 test_loss: 0.08860 \n",
      "[ 57/500] train_loss: 0.07695 valid_loss: 0.08208 test_loss: 0.08928 \n",
      "[ 58/500] train_loss: 0.07601 valid_loss: 0.08148 test_loss: 0.08905 \n",
      "[ 59/500] train_loss: 0.07440 valid_loss: 0.08122 test_loss: 0.09001 \n",
      "验证损失减少 (0.081231 --> 0.081215). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07606 valid_loss: 0.08095 test_loss: 0.08887 \n",
      "验证损失减少 (0.081215 --> 0.080945). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.07510 valid_loss: 0.08224 test_loss: 0.08883 \n",
      "[ 62/500] train_loss: 0.07468 valid_loss: 0.08088 test_loss: 0.08953 \n",
      "验证损失减少 (0.080945 --> 0.080884). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.07442 valid_loss: 0.07965 test_loss: 0.08854 \n",
      "验证损失减少 (0.080884 --> 0.079653). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.07521 valid_loss: 0.08044 test_loss: 0.08875 \n",
      "[ 65/500] train_loss: 0.07514 valid_loss: 0.08027 test_loss: 0.08838 \n",
      "[ 66/500] train_loss: 0.07248 valid_loss: 0.08003 test_loss: 0.08850 \n",
      "[ 67/500] train_loss: 0.07353 valid_loss: 0.08061 test_loss: 0.08825 \n",
      "[ 68/500] train_loss: 0.07233 valid_loss: 0.08019 test_loss: 0.08757 \n",
      "[ 69/500] train_loss: 0.07210 valid_loss: 0.08036 test_loss: 0.08708 \n",
      "[ 70/500] train_loss: 0.07510 valid_loss: 0.07851 test_loss: 0.08688 \n",
      "验证损失减少 (0.079653 --> 0.078514). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.07267 valid_loss: 0.08012 test_loss: 0.08587 \n",
      "[ 72/500] train_loss: 0.07224 valid_loss: 0.07883 test_loss: 0.08705 \n",
      "[ 73/500] train_loss: 0.07269 valid_loss: 0.07815 test_loss: 0.08776 \n",
      "验证损失减少 (0.078514 --> 0.078152). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.07276 valid_loss: 0.07828 test_loss: 0.08668 \n",
      "[ 75/500] train_loss: 0.07057 valid_loss: 0.07923 test_loss: 0.08814 \n",
      "[ 76/500] train_loss: 0.07072 valid_loss: 0.07814 test_loss: 0.08819 \n",
      "验证损失减少 (0.078152 --> 0.078138). 正在保存模型...\n",
      "[ 77/500] train_loss: 0.06979 valid_loss: 0.07977 test_loss: 0.08659 \n",
      "[ 78/500] train_loss: 0.07102 valid_loss: 0.07702 test_loss: 0.08474 \n",
      "验证损失减少 (0.078138 --> 0.077022). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.07004 valid_loss: 0.07685 test_loss: 0.08580 \n",
      "验证损失减少 (0.077022 --> 0.076855). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06961 valid_loss: 0.07820 test_loss: 0.08770 \n",
      "[ 81/500] train_loss: 0.07032 valid_loss: 0.07831 test_loss: 0.08788 \n",
      "[ 82/500] train_loss: 0.07172 valid_loss: 0.07711 test_loss: 0.08691 \n",
      "[ 83/500] train_loss: 0.06907 valid_loss: 0.07818 test_loss: 0.08564 \n",
      "[ 84/500] train_loss: 0.06817 valid_loss: 0.07702 test_loss: 0.08471 \n",
      "[ 85/500] train_loss: 0.06841 valid_loss: 0.07777 test_loss: 0.08820 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 86/500] train_loss: 0.06872 valid_loss: 0.07902 test_loss: 0.08706 \n",
      "[ 87/500] train_loss: 0.06881 valid_loss: 0.07650 test_loss: 0.08489 \n",
      "验证损失减少 (0.076855 --> 0.076499). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.06835 valid_loss: 0.07656 test_loss: 0.08594 \n",
      "[ 89/500] train_loss: 0.06877 valid_loss: 0.07625 test_loss: 0.08502 \n",
      "验证损失减少 (0.076499 --> 0.076253). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.06674 valid_loss: 0.07614 test_loss: 0.08482 \n",
      "验证损失减少 (0.076253 --> 0.076145). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.06993 valid_loss: 0.07665 test_loss: 0.08422 \n",
      "[ 92/500] train_loss: 0.06734 valid_loss: 0.07723 test_loss: 0.08435 \n",
      "[ 93/500] train_loss: 0.06747 valid_loss: 0.07563 test_loss: 0.08453 \n",
      "验证损失减少 (0.076145 --> 0.075634). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.06797 valid_loss: 0.07720 test_loss: 0.08502 \n",
      "[ 95/500] train_loss: 0.06855 valid_loss: 0.07605 test_loss: 0.08601 \n",
      "[ 96/500] train_loss: 0.06790 valid_loss: 0.07602 test_loss: 0.08495 \n",
      "[ 97/500] train_loss: 0.06501 valid_loss: 0.07562 test_loss: 0.08487 \n",
      "验证损失减少 (0.075634 --> 0.075620). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.06609 valid_loss: 0.07562 test_loss: 0.08497 \n",
      "验证损失减少 (0.075620 --> 0.075615). 正在保存模型...\n",
      "[ 99/500] train_loss: 0.06644 valid_loss: 0.07518 test_loss: 0.08500 \n",
      "验证损失减少 (0.075615 --> 0.075176). 正在保存模型...\n",
      "[100/500] train_loss: 0.06601 valid_loss: 0.07689 test_loss: 0.08428 \n",
      "[101/500] train_loss: 0.06580 valid_loss: 0.07615 test_loss: 0.08540 \n",
      "[102/500] train_loss: 0.06703 valid_loss: 0.07543 test_loss: 0.08378 \n",
      "[103/500] train_loss: 0.06538 valid_loss: 0.07475 test_loss: 0.08304 \n",
      "验证损失减少 (0.075176 --> 0.074745). 正在保存模型...\n",
      "[104/500] train_loss: 0.06508 valid_loss: 0.07726 test_loss: 0.08366 \n",
      "[105/500] train_loss: 0.06476 valid_loss: 0.07604 test_loss: 0.08458 \n",
      "[106/500] train_loss: 0.06599 valid_loss: 0.07445 test_loss: 0.08327 \n",
      "验证损失减少 (0.074745 --> 0.074450). 正在保存模型...\n",
      "[107/500] train_loss: 0.06496 valid_loss: 0.07778 test_loss: 0.08501 \n",
      "[108/500] train_loss: 0.06478 valid_loss: 0.07534 test_loss: 0.08285 \n",
      "[109/500] train_loss: 0.06485 valid_loss: 0.07581 test_loss: 0.08210 \n",
      "[110/500] train_loss: 0.06669 valid_loss: 0.07651 test_loss: 0.08484 \n",
      "[111/500] train_loss: 0.06626 valid_loss: 0.07736 test_loss: 0.08156 \n",
      "[112/500] train_loss: 0.06526 valid_loss: 0.07692 test_loss: 0.08253 \n",
      "[113/500] train_loss: 0.06244 valid_loss: 0.07554 test_loss: 0.08233 \n",
      "[114/500] train_loss: 0.06412 valid_loss: 0.07768 test_loss: 0.08347 \n",
      "[115/500] train_loss: 0.06475 valid_loss: 0.07823 test_loss: 0.08263 \n",
      "[116/500] train_loss: 0.06393 valid_loss: 0.07449 test_loss: 0.08146 \n",
      "[117/500] train_loss: 0.06387 valid_loss: 0.07506 test_loss: 0.08225 \n",
      "[118/500] train_loss: 0.06437 valid_loss: 0.07518 test_loss: 0.08151 \n",
      "[119/500] train_loss: 0.06316 valid_loss: 0.07554 test_loss: 0.08265 \n",
      "[120/500] train_loss: 0.06267 valid_loss: 0.07708 test_loss: 0.08321 \n",
      "[121/500] train_loss: 0.06361 valid_loss: 0.07500 test_loss: 0.08232 \n",
      "[122/500] train_loss: 0.06267 valid_loss: 0.07716 test_loss: 0.08118 \n",
      "[123/500] train_loss: 0.06415 valid_loss: 0.07442 test_loss: 0.08226 \n",
      "验证损失减少 (0.074450 --> 0.074418). 正在保存模型...\n",
      "[124/500] train_loss: 0.06473 valid_loss: 0.07504 test_loss: 0.08276 \n",
      "[125/500] train_loss: 0.06391 valid_loss: 0.07503 test_loss: 0.08259 \n",
      "[126/500] train_loss: 0.06273 valid_loss: 0.07421 test_loss: 0.08079 \n",
      "验证损失减少 (0.074418 --> 0.074215). 正在保存模型...\n",
      "[127/500] train_loss: 0.06287 valid_loss: 0.07586 test_loss: 0.08220 \n",
      "[128/500] train_loss: 0.06280 valid_loss: 0.07370 test_loss: 0.08081 \n",
      "验证损失减少 (0.074215 --> 0.073703). 正在保存模型...\n",
      "[129/500] train_loss: 0.06294 valid_loss: 0.07533 test_loss: 0.08137 \n",
      "[130/500] train_loss: 0.06257 valid_loss: 0.07613 test_loss: 0.08142 \n",
      "[131/500] train_loss: 0.06235 valid_loss: 0.07519 test_loss: 0.08147 \n",
      "[132/500] train_loss: 0.06234 valid_loss: 0.07497 test_loss: 0.08068 \n",
      "[133/500] train_loss: 0.06267 valid_loss: 0.07442 test_loss: 0.08047 \n",
      "[134/500] train_loss: 0.06147 valid_loss: 0.07585 test_loss: 0.07993 \n",
      "[135/500] train_loss: 0.06185 valid_loss: 0.07348 test_loss: 0.07874 \n",
      "验证损失减少 (0.073703 --> 0.073477). 正在保存模型...\n",
      "[136/500] train_loss: 0.06007 valid_loss: 0.07339 test_loss: 0.08107 \n",
      "验证损失减少 (0.073477 --> 0.073388). 正在保存模型...\n",
      "[137/500] train_loss: 0.06143 valid_loss: 0.07399 test_loss: 0.08100 \n",
      "[138/500] train_loss: 0.06228 valid_loss: 0.07491 test_loss: 0.08115 \n",
      "[139/500] train_loss: 0.06163 valid_loss: 0.07681 test_loss: 0.08137 \n",
      "[140/500] train_loss: 0.06164 valid_loss: 0.07449 test_loss: 0.08083 \n",
      "[141/500] train_loss: 0.06213 valid_loss: 0.07579 test_loss: 0.08216 \n",
      "[142/500] train_loss: 0.06163 valid_loss: 0.07364 test_loss: 0.08070 \n",
      "[143/500] train_loss: 0.06012 valid_loss: 0.07436 test_loss: 0.08076 \n",
      "[144/500] train_loss: 0.06182 valid_loss: 0.07553 test_loss: 0.08097 \n",
      "[145/500] train_loss: 0.05848 valid_loss: 0.07397 test_loss: 0.08152 \n",
      "[146/500] train_loss: 0.06054 valid_loss: 0.07425 test_loss: 0.07969 \n",
      "[147/500] train_loss: 0.05881 valid_loss: 0.07421 test_loss: 0.08088 \n",
      "[148/500] train_loss: 0.06020 valid_loss: 0.07412 test_loss: 0.08173 \n",
      "[149/500] train_loss: 0.05986 valid_loss: 0.07250 test_loss: 0.08153 \n",
      "验证损失减少 (0.073388 --> 0.072502). 正在保存模型...\n",
      "[150/500] train_loss: 0.06103 valid_loss: 0.07280 test_loss: 0.07985 \n",
      "[151/500] train_loss: 0.05955 valid_loss: 0.07278 test_loss: 0.08020 \n",
      "[152/500] train_loss: 0.05865 valid_loss: 0.07391 test_loss: 0.08028 \n",
      "[153/500] train_loss: 0.05855 valid_loss: 0.07395 test_loss: 0.08337 \n",
      "[154/500] train_loss: 0.05987 valid_loss: 0.07256 test_loss: 0.08094 \n",
      "[155/500] train_loss: 0.05921 valid_loss: 0.07344 test_loss: 0.08097 \n",
      "[156/500] train_loss: 0.05961 valid_loss: 0.07362 test_loss: 0.08100 \n",
      "[157/500] train_loss: 0.06009 valid_loss: 0.07416 test_loss: 0.08058 \n",
      "[158/500] train_loss: 0.05770 valid_loss: 0.07442 test_loss: 0.08038 \n",
      "[159/500] train_loss: 0.05885 valid_loss: 0.07288 test_loss: 0.08060 \n",
      "[160/500] train_loss: 0.05721 valid_loss: 0.07445 test_loss: 0.08200 \n",
      "[161/500] train_loss: 0.05911 valid_loss: 0.07350 test_loss: 0.08078 \n",
      "[162/500] train_loss: 0.05805 valid_loss: 0.07230 test_loss: 0.07873 \n",
      "验证损失减少 (0.072502 --> 0.072296). 正在保存模型...\n",
      "[163/500] train_loss: 0.05896 valid_loss: 0.07415 test_loss: 0.07970 \n",
      "[164/500] train_loss: 0.05734 valid_loss: 0.07372 test_loss: 0.08017 \n",
      "[165/500] train_loss: 0.05738 valid_loss: 0.07622 test_loss: 0.08187 \n",
      "[166/500] train_loss: 0.05839 valid_loss: 0.07367 test_loss: 0.08109 \n",
      "[167/500] train_loss: 0.05669 valid_loss: 0.07454 test_loss: 0.08112 \n",
      "[168/500] train_loss: 0.05782 valid_loss: 0.07434 test_loss: 0.08094 \n",
      "[169/500] train_loss: 0.05729 valid_loss: 0.07319 test_loss: 0.07963 \n",
      "[170/500] train_loss: 0.05615 valid_loss: 0.07508 test_loss: 0.08042 \n",
      "[171/500] train_loss: 0.05768 valid_loss: 0.07363 test_loss: 0.08052 \n",
      "[172/500] train_loss: 0.05756 valid_loss: 0.07300 test_loss: 0.08039 \n",
      "[173/500] train_loss: 0.05597 valid_loss: 0.07371 test_loss: 0.08026 \n",
      "[174/500] train_loss: 0.05931 valid_loss: 0.07455 test_loss: 0.08010 \n",
      "[175/500] train_loss: 0.05584 valid_loss: 0.07308 test_loss: 0.07907 \n",
      "[176/500] train_loss: 0.05909 valid_loss: 0.07419 test_loss: 0.07938 \n",
      "[177/500] train_loss: 0.05672 valid_loss: 0.07450 test_loss: 0.08105 \n",
      "[178/500] train_loss: 0.05841 valid_loss: 0.07362 test_loss: 0.08097 \n",
      "[179/500] train_loss: 0.05580 valid_loss: 0.07360 test_loss: 0.08118 \n",
      "[180/500] train_loss: 0.05765 valid_loss: 0.07209 test_loss: 0.07923 \n",
      "验证损失减少 (0.072296 --> 0.072086). 正在保存模型...\n",
      "[181/500] train_loss: 0.05591 valid_loss: 0.07344 test_loss: 0.08044 \n",
      "[182/500] train_loss: 0.05561 valid_loss: 0.07394 test_loss: 0.08099 \n",
      "[183/500] train_loss: 0.05769 valid_loss: 0.07397 test_loss: 0.08098 \n",
      "[184/500] train_loss: 0.05693 valid_loss: 0.07287 test_loss: 0.07937 \n",
      "[185/500] train_loss: 0.05560 valid_loss: 0.07213 test_loss: 0.07905 \n",
      "[186/500] train_loss: 0.05708 valid_loss: 0.07396 test_loss: 0.07822 \n",
      "[187/500] train_loss: 0.05732 valid_loss: 0.07298 test_loss: 0.08004 \n",
      "[188/500] train_loss: 0.05560 valid_loss: 0.07248 test_loss: 0.07972 \n",
      "[189/500] train_loss: 0.05718 valid_loss: 0.07252 test_loss: 0.07963 \n",
      "[190/500] train_loss: 0.05714 valid_loss: 0.07175 test_loss: 0.07991 \n",
      "验证损失减少 (0.072086 --> 0.071753). 正在保存模型...\n",
      "[191/500] train_loss: 0.05582 valid_loss: 0.07105 test_loss: 0.07774 \n",
      "验证损失减少 (0.071753 --> 0.071050). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192/500] train_loss: 0.05548 valid_loss: 0.07273 test_loss: 0.07853 \n",
      "[193/500] train_loss: 0.05556 valid_loss: 0.07287 test_loss: 0.08000 \n",
      "[194/500] train_loss: 0.05620 valid_loss: 0.07271 test_loss: 0.07844 \n",
      "[195/500] train_loss: 0.05420 valid_loss: 0.07312 test_loss: 0.07931 \n",
      "[196/500] train_loss: 0.05393 valid_loss: 0.07299 test_loss: 0.08056 \n",
      "[197/500] train_loss: 0.05668 valid_loss: 0.07274 test_loss: 0.07916 \n",
      "[198/500] train_loss: 0.05500 valid_loss: 0.07198 test_loss: 0.07853 \n",
      "[199/500] train_loss: 0.05578 valid_loss: 0.07318 test_loss: 0.08153 \n",
      "[200/500] train_loss: 0.05539 valid_loss: 0.07339 test_loss: 0.07754 \n",
      "[201/500] train_loss: 0.05495 valid_loss: 0.07388 test_loss: 0.08090 \n",
      "[202/500] train_loss: 0.05595 valid_loss: 0.07458 test_loss: 0.07908 \n",
      "[203/500] train_loss: 0.05419 valid_loss: 0.07354 test_loss: 0.08011 \n",
      "[204/500] train_loss: 0.05410 valid_loss: 0.07492 test_loss: 0.07865 \n",
      "[205/500] train_loss: 0.05518 valid_loss: 0.07316 test_loss: 0.07905 \n",
      "[206/500] train_loss: 0.05407 valid_loss: 0.07388 test_loss: 0.07934 \n",
      "[207/500] train_loss: 0.05494 valid_loss: 0.07543 test_loss: 0.07951 \n",
      "[208/500] train_loss: 0.05308 valid_loss: 0.07270 test_loss: 0.07905 \n",
      "[209/500] train_loss: 0.05481 valid_loss: 0.07462 test_loss: 0.07844 \n",
      "[210/500] train_loss: 0.05410 valid_loss: 0.07326 test_loss: 0.07809 \n",
      "[211/500] train_loss: 0.05348 valid_loss: 0.07182 test_loss: 0.08152 \n",
      "[212/500] train_loss: 0.05499 valid_loss: 0.07327 test_loss: 0.08007 \n",
      "[213/500] train_loss: 0.05350 valid_loss: 0.07177 test_loss: 0.07977 \n",
      "[214/500] train_loss: 0.05461 valid_loss: 0.07193 test_loss: 0.08009 \n",
      "[215/500] train_loss: 0.05434 valid_loss: 0.07255 test_loss: 0.07862 \n",
      "[216/500] train_loss: 0.05361 valid_loss: 0.07292 test_loss: 0.07885 \n",
      "[217/500] train_loss: 0.05428 valid_loss: 0.07187 test_loss: 0.07860 \n",
      "[218/500] train_loss: 0.05435 valid_loss: 0.07349 test_loss: 0.07780 \n",
      "[219/500] train_loss: 0.05384 valid_loss: 0.07218 test_loss: 0.07945 \n",
      "[220/500] train_loss: 0.05430 valid_loss: 0.07344 test_loss: 0.08079 \n",
      "[221/500] train_loss: 0.05315 valid_loss: 0.07270 test_loss: 0.07975 \n",
      "[222/500] train_loss: 0.05326 valid_loss: 0.07162 test_loss: 0.07949 \n",
      "[223/500] train_loss: 0.05371 valid_loss: 0.07154 test_loss: 0.07809 \n",
      "[224/500] train_loss: 0.05259 valid_loss: 0.07263 test_loss: 0.07854 \n",
      "[225/500] train_loss: 0.05281 valid_loss: 0.07261 test_loss: 0.07887 \n",
      "[226/500] train_loss: 0.05114 valid_loss: 0.07073 test_loss: 0.07849 \n",
      "验证损失减少 (0.071050 --> 0.070734). 正在保存模型...\n",
      "[227/500] train_loss: 0.05304 valid_loss: 0.07166 test_loss: 0.07799 \n",
      "[228/500] train_loss: 0.05236 valid_loss: 0.07120 test_loss: 0.07913 \n",
      "[229/500] train_loss: 0.05426 valid_loss: 0.07549 test_loss: 0.07768 \n",
      "[230/500] train_loss: 0.05219 valid_loss: 0.07513 test_loss: 0.07879 \n",
      "[231/500] train_loss: 0.05204 valid_loss: 0.07552 test_loss: 0.07859 \n",
      "[232/500] train_loss: 0.05157 valid_loss: 0.07369 test_loss: 0.07840 \n",
      "[233/500] train_loss: 0.05168 valid_loss: 0.07374 test_loss: 0.07977 \n",
      "[234/500] train_loss: 0.05345 valid_loss: 0.07367 test_loss: 0.07869 \n",
      "[235/500] train_loss: 0.05208 valid_loss: 0.07451 test_loss: 0.08019 \n",
      "[236/500] train_loss: 0.05251 valid_loss: 0.07235 test_loss: 0.07728 \n",
      "[237/500] train_loss: 0.05162 valid_loss: 0.07388 test_loss: 0.07777 \n",
      "[238/500] train_loss: 0.05061 valid_loss: 0.07398 test_loss: 0.07854 \n",
      "[239/500] train_loss: 0.05125 valid_loss: 0.07240 test_loss: 0.07847 \n",
      "[240/500] train_loss: 0.05165 valid_loss: 0.07342 test_loss: 0.07891 \n",
      "[241/500] train_loss: 0.05135 valid_loss: 0.07697 test_loss: 0.07846 \n",
      "[242/500] train_loss: 0.05123 valid_loss: 0.07195 test_loss: 0.07851 \n",
      "[243/500] train_loss: 0.05160 valid_loss: 0.07314 test_loss: 0.07899 \n",
      "[244/500] train_loss: 0.05210 valid_loss: 0.07409 test_loss: 0.07957 \n",
      "[245/500] train_loss: 0.05161 valid_loss: 0.07184 test_loss: 0.07760 \n",
      "[246/500] train_loss: 0.05321 valid_loss: 0.07297 test_loss: 0.07695 \n",
      "[247/500] train_loss: 0.05129 valid_loss: 0.07294 test_loss: 0.07802 \n",
      "[248/500] train_loss: 0.05199 valid_loss: 0.07214 test_loss: 0.07844 \n",
      "[249/500] train_loss: 0.05174 valid_loss: 0.07420 test_loss: 0.07848 \n",
      "[250/500] train_loss: 0.05069 valid_loss: 0.07195 test_loss: 0.08019 \n",
      "[251/500] train_loss: 0.05186 valid_loss: 0.07298 test_loss: 0.07920 \n",
      "[252/500] train_loss: 0.05227 valid_loss: 0.07102 test_loss: 0.07750 \n",
      "[253/500] train_loss: 0.05173 valid_loss: 0.07068 test_loss: 0.07904 \n",
      "验证损失减少 (0.070734 --> 0.070680). 正在保存模型...\n",
      "[254/500] train_loss: 0.05209 valid_loss: 0.07136 test_loss: 0.07765 \n",
      "[255/500] train_loss: 0.05255 valid_loss: 0.07200 test_loss: 0.07683 \n",
      "[256/500] train_loss: 0.05199 valid_loss: 0.07155 test_loss: 0.07756 \n",
      "[257/500] train_loss: 0.05067 valid_loss: 0.07162 test_loss: 0.07878 \n",
      "[258/500] train_loss: 0.05092 valid_loss: 0.07325 test_loss: 0.07872 \n",
      "[259/500] train_loss: 0.05152 valid_loss: 0.07171 test_loss: 0.07858 \n",
      "[260/500] train_loss: 0.05027 valid_loss: 0.07154 test_loss: 0.07849 \n",
      "[261/500] train_loss: 0.05129 valid_loss: 0.07306 test_loss: 0.07906 \n",
      "[262/500] train_loss: 0.05039 valid_loss: 0.07255 test_loss: 0.07675 \n",
      "[263/500] train_loss: 0.05131 valid_loss: 0.07259 test_loss: 0.07782 \n",
      "[264/500] train_loss: 0.04938 valid_loss: 0.07240 test_loss: 0.07844 \n",
      "[265/500] train_loss: 0.05104 valid_loss: 0.07223 test_loss: 0.07911 \n",
      "[266/500] train_loss: 0.04983 valid_loss: 0.07198 test_loss: 0.07925 \n",
      "[267/500] train_loss: 0.05117 valid_loss: 0.07253 test_loss: 0.07941 \n",
      "[268/500] train_loss: 0.05007 valid_loss: 0.07127 test_loss: 0.07803 \n",
      "[269/500] train_loss: 0.05079 valid_loss: 0.07349 test_loss: 0.07963 \n",
      "[270/500] train_loss: 0.05058 valid_loss: 0.07370 test_loss: 0.07864 \n",
      "[271/500] train_loss: 0.04981 valid_loss: 0.07318 test_loss: 0.08078 \n",
      "[272/500] train_loss: 0.04962 valid_loss: 0.07456 test_loss: 0.07950 \n",
      "[273/500] train_loss: 0.04858 valid_loss: 0.07454 test_loss: 0.07889 \n",
      "[274/500] train_loss: 0.05042 valid_loss: 0.07383 test_loss: 0.07921 \n",
      "[275/500] train_loss: 0.05029 valid_loss: 0.07329 test_loss: 0.08028 \n",
      "[276/500] train_loss: 0.04876 valid_loss: 0.07318 test_loss: 0.07943 \n",
      "[277/500] train_loss: 0.05014 valid_loss: 0.07293 test_loss: 0.07782 \n",
      "[278/500] train_loss: 0.04980 valid_loss: 0.07259 test_loss: 0.07971 \n",
      "[279/500] train_loss: 0.04963 valid_loss: 0.07379 test_loss: 0.07923 \n",
      "[280/500] train_loss: 0.04940 valid_loss: 0.07242 test_loss: 0.07998 \n",
      "[281/500] train_loss: 0.04791 valid_loss: 0.07332 test_loss: 0.07933 \n",
      "[282/500] train_loss: 0.04917 valid_loss: 0.07135 test_loss: 0.08022 \n",
      "[283/500] train_loss: 0.04946 valid_loss: 0.07209 test_loss: 0.07901 \n",
      "[284/500] train_loss: 0.04935 valid_loss: 0.07322 test_loss: 0.07888 \n",
      "[285/500] train_loss: 0.04841 valid_loss: 0.07252 test_loss: 0.07881 \n",
      "[286/500] train_loss: 0.04963 valid_loss: 0.07521 test_loss: 0.08006 \n",
      "[287/500] train_loss: 0.05044 valid_loss: 0.07228 test_loss: 0.07871 \n",
      "[288/500] train_loss: 0.04929 valid_loss: 0.07273 test_loss: 0.08004 \n",
      "[289/500] train_loss: 0.04857 valid_loss: 0.07323 test_loss: 0.07960 \n",
      "[290/500] train_loss: 0.04827 valid_loss: 0.07469 test_loss: 0.08023 \n",
      "[291/500] train_loss: 0.05161 valid_loss: 0.07416 test_loss: 0.07861 \n",
      "[292/500] train_loss: 0.04996 valid_loss: 0.07332 test_loss: 0.07912 \n",
      "[293/500] train_loss: 0.04861 valid_loss: 0.07285 test_loss: 0.07943 \n",
      "[294/500] train_loss: 0.04900 valid_loss: 0.07489 test_loss: 0.07915 \n",
      "[295/500] train_loss: 0.04863 valid_loss: 0.07300 test_loss: 0.07887 \n",
      "[296/500] train_loss: 0.04874 valid_loss: 0.07398 test_loss: 0.08002 \n",
      "[297/500] train_loss: 0.04995 valid_loss: 0.07412 test_loss: 0.07856 \n",
      "[298/500] train_loss: 0.04840 valid_loss: 0.07292 test_loss: 0.07810 \n",
      "[299/500] train_loss: 0.04783 valid_loss: 0.07360 test_loss: 0.07945 \n",
      "[300/500] train_loss: 0.04768 valid_loss: 0.07273 test_loss: 0.07945 \n",
      "[301/500] train_loss: 0.04811 valid_loss: 0.07420 test_loss: 0.07933 \n",
      "[302/500] train_loss: 0.04900 valid_loss: 0.07269 test_loss: 0.07923 \n",
      "[303/500] train_loss: 0.04915 valid_loss: 0.07228 test_loss: 0.07759 \n",
      "[304/500] train_loss: 0.04945 valid_loss: 0.07310 test_loss: 0.07738 \n",
      "[305/500] train_loss: 0.04847 valid_loss: 0.07271 test_loss: 0.07761 \n",
      "[306/500] train_loss: 0.04804 valid_loss: 0.07425 test_loss: 0.07871 \n",
      "[307/500] train_loss: 0.04770 valid_loss: 0.07128 test_loss: 0.07752 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308/500] train_loss: 0.04779 valid_loss: 0.07267 test_loss: 0.07872 \n",
      "[309/500] train_loss: 0.04788 valid_loss: 0.07508 test_loss: 0.08003 \n",
      "[310/500] train_loss: 0.04810 valid_loss: 0.07353 test_loss: 0.07978 \n",
      "[311/500] train_loss: 0.04800 valid_loss: 0.07221 test_loss: 0.07843 \n",
      "[312/500] train_loss: 0.04742 valid_loss: 0.07458 test_loss: 0.07963 \n",
      "[313/500] train_loss: 0.04827 valid_loss: 0.07442 test_loss: 0.08124 \n",
      "[314/500] train_loss: 0.04785 valid_loss: 0.07718 test_loss: 0.07957 \n",
      "[315/500] train_loss: 0.04894 valid_loss: 0.07668 test_loss: 0.07944 \n",
      "[316/500] train_loss: 0.04897 valid_loss: 0.07171 test_loss: 0.07760 \n",
      "[317/500] train_loss: 0.04770 valid_loss: 0.07383 test_loss: 0.08011 \n",
      "[318/500] train_loss: 0.04858 valid_loss: 0.07343 test_loss: 0.07966 \n",
      "[319/500] train_loss: 0.04772 valid_loss: 0.07325 test_loss: 0.07928 \n",
      "[320/500] train_loss: 0.04875 valid_loss: 0.07278 test_loss: 0.08017 \n",
      "[321/500] train_loss: 0.04785 valid_loss: 0.07298 test_loss: 0.07881 \n",
      "[322/500] train_loss: 0.04765 valid_loss: 0.07326 test_loss: 0.07918 \n",
      "[323/500] train_loss: 0.04815 valid_loss: 0.07173 test_loss: 0.07899 \n",
      "[324/500] train_loss: 0.04701 valid_loss: 0.07193 test_loss: 0.07765 \n",
      "[325/500] train_loss: 0.04796 valid_loss: 0.07422 test_loss: 0.07921 \n",
      "[326/500] train_loss: 0.04713 valid_loss: 0.07247 test_loss: 0.07927 \n",
      "[327/500] train_loss: 0.04756 valid_loss: 0.07208 test_loss: 0.07781 \n",
      "[328/500] train_loss: 0.04748 valid_loss: 0.07248 test_loss: 0.07780 \n",
      "[329/500] train_loss: 0.04612 valid_loss: 0.07538 test_loss: 0.08012 \n",
      "[330/500] train_loss: 0.04785 valid_loss: 0.07494 test_loss: 0.07972 \n",
      "[331/500] train_loss: 0.04702 valid_loss: 0.07432 test_loss: 0.07983 \n",
      "[332/500] train_loss: 0.04690 valid_loss: 0.07253 test_loss: 0.07800 \n",
      "[333/500] train_loss: 0.04819 valid_loss: 0.07237 test_loss: 0.07975 \n",
      "[334/500] train_loss: 0.04659 valid_loss: 0.07223 test_loss: 0.07912 \n",
      "[335/500] train_loss: 0.04680 valid_loss: 0.07386 test_loss: 0.07909 \n",
      "[336/500] train_loss: 0.04790 valid_loss: 0.07213 test_loss: 0.07926 \n",
      "[337/500] train_loss: 0.04752 valid_loss: 0.07390 test_loss: 0.08127 \n",
      "[338/500] train_loss: 0.04620 valid_loss: 0.07251 test_loss: 0.07999 \n",
      "[339/500] train_loss: 0.04722 valid_loss: 0.07306 test_loss: 0.07950 \n",
      "[340/500] train_loss: 0.04822 valid_loss: 0.07385 test_loss: 0.07929 \n",
      "[341/500] train_loss: 0.04708 valid_loss: 0.07330 test_loss: 0.07906 \n",
      "[342/500] train_loss: 0.04716 valid_loss: 0.07306 test_loss: 0.07947 \n",
      "[343/500] train_loss: 0.04654 valid_loss: 0.07436 test_loss: 0.07953 \n",
      "[344/500] train_loss: 0.04668 valid_loss: 0.07363 test_loss: 0.07878 \n",
      "[345/500] train_loss: 0.04714 valid_loss: 0.07246 test_loss: 0.07673 \n",
      "[346/500] train_loss: 0.04705 valid_loss: 0.07491 test_loss: 0.07774 \n",
      "[347/500] train_loss: 0.04594 valid_loss: 0.07319 test_loss: 0.07725 \n",
      "[348/500] train_loss: 0.04772 valid_loss: 0.07346 test_loss: 0.07915 \n",
      "[349/500] train_loss: 0.04692 valid_loss: 0.07469 test_loss: 0.07930 \n",
      "[350/500] train_loss: 0.04635 valid_loss: 0.07370 test_loss: 0.07830 \n",
      "[351/500] train_loss: 0.04518 valid_loss: 0.07343 test_loss: 0.07754 \n",
      "[352/500] train_loss: 0.04639 valid_loss: 0.07558 test_loss: 0.07891 \n",
      "[353/500] train_loss: 0.04654 valid_loss: 0.07319 test_loss: 0.07858 \n",
      "[354/500] train_loss: 0.04651 valid_loss: 0.07343 test_loss: 0.07865 \n",
      "[355/500] train_loss: 0.04678 valid_loss: 0.07341 test_loss: 0.07673 \n",
      "[356/500] train_loss: 0.04610 valid_loss: 0.07263 test_loss: 0.07824 \n",
      "[357/500] train_loss: 0.04573 valid_loss: 0.07317 test_loss: 0.07831 \n",
      "[358/500] train_loss: 0.04640 valid_loss: 0.07390 test_loss: 0.07808 \n",
      "[359/500] train_loss: 0.04627 valid_loss: 0.07394 test_loss: 0.07931 \n",
      "[360/500] train_loss: 0.04604 valid_loss: 0.07412 test_loss: 0.07817 \n",
      "[361/500] train_loss: 0.04505 valid_loss: 0.07359 test_loss: 0.07816 \n",
      "[362/500] train_loss: 0.04489 valid_loss: 0.07499 test_loss: 0.07872 \n",
      "[363/500] train_loss: 0.04589 valid_loss: 0.07461 test_loss: 0.07745 \n",
      "[364/500] train_loss: 0.04640 valid_loss: 0.07652 test_loss: 0.07829 \n",
      "[365/500] train_loss: 0.04463 valid_loss: 0.07350 test_loss: 0.07834 \n",
      "[366/500] train_loss: 0.04652 valid_loss: 0.07352 test_loss: 0.07934 \n",
      "[367/500] train_loss: 0.04535 valid_loss: 0.07412 test_loss: 0.07851 \n",
      "[368/500] train_loss: 0.04436 valid_loss: 0.07297 test_loss: 0.07975 \n",
      "[369/500] train_loss: 0.04426 valid_loss: 0.07424 test_loss: 0.07892 \n",
      "[370/500] train_loss: 0.04492 valid_loss: 0.07402 test_loss: 0.07966 \n",
      "[371/500] train_loss: 0.04487 valid_loss: 0.07219 test_loss: 0.07850 \n",
      "[372/500] train_loss: 0.04494 valid_loss: 0.07319 test_loss: 0.07943 \n",
      "[373/500] train_loss: 0.04705 valid_loss: 0.07323 test_loss: 0.07944 \n",
      "[374/500] train_loss: 0.04523 valid_loss: 0.07523 test_loss: 0.07971 \n",
      "[375/500] train_loss: 0.04527 valid_loss: 0.07437 test_loss: 0.08002 \n",
      "[376/500] train_loss: 0.04670 valid_loss: 0.07254 test_loss: 0.07849 \n",
      "[377/500] train_loss: 0.04669 valid_loss: 0.07316 test_loss: 0.07929 \n",
      "[378/500] train_loss: 0.04422 valid_loss: 0.07323 test_loss: 0.07764 \n",
      "[379/500] train_loss: 0.04532 valid_loss: 0.07282 test_loss: 0.07697 \n",
      "[380/500] train_loss: 0.04615 valid_loss: 0.07262 test_loss: 0.07964 \n",
      "[381/500] train_loss: 0.04487 valid_loss: 0.07289 test_loss: 0.07915 \n",
      "[382/500] train_loss: 0.04632 valid_loss: 0.07135 test_loss: 0.07831 \n",
      "[383/500] train_loss: 0.04541 valid_loss: 0.07332 test_loss: 0.07978 \n",
      "[384/500] train_loss: 0.04497 valid_loss: 0.07112 test_loss: 0.07773 \n",
      "[385/500] train_loss: 0.04468 valid_loss: 0.07168 test_loss: 0.07887 \n",
      "[386/500] train_loss: 0.04595 valid_loss: 0.07311 test_loss: 0.07905 \n",
      "[387/500] train_loss: 0.04491 valid_loss: 0.07160 test_loss: 0.07749 \n",
      "[388/500] train_loss: 0.04363 valid_loss: 0.07342 test_loss: 0.07807 \n",
      "[389/500] train_loss: 0.04476 valid_loss: 0.07460 test_loss: 0.07905 \n",
      "[390/500] train_loss: 0.04433 valid_loss: 0.07290 test_loss: 0.07901 \n",
      "[391/500] train_loss: 0.04557 valid_loss: 0.07355 test_loss: 0.07912 \n",
      "[392/500] train_loss: 0.04425 valid_loss: 0.07244 test_loss: 0.07841 \n",
      "[393/500] train_loss: 0.04554 valid_loss: 0.07304 test_loss: 0.07807 \n",
      "[394/500] train_loss: 0.04374 valid_loss: 0.07595 test_loss: 0.07865 \n",
      "[395/500] train_loss: 0.04499 valid_loss: 0.07361 test_loss: 0.07912 \n",
      "[396/500] train_loss: 0.04447 valid_loss: 0.07256 test_loss: 0.07744 \n",
      "[397/500] train_loss: 0.04408 valid_loss: 0.07386 test_loss: 0.07926 \n",
      "[398/500] train_loss: 0.04471 valid_loss: 0.07278 test_loss: 0.07976 \n",
      "[399/500] train_loss: 0.04315 valid_loss: 0.07429 test_loss: 0.07946 \n",
      "[400/500] train_loss: 0.04454 valid_loss: 0.07340 test_loss: 0.07906 \n",
      "[401/500] train_loss: 0.04431 valid_loss: 0.07574 test_loss: 0.07952 \n",
      "[402/500] train_loss: 0.04438 valid_loss: 0.07448 test_loss: 0.08009 \n",
      "[403/500] train_loss: 0.04470 valid_loss: 0.07452 test_loss: 0.08078 \n",
      "[404/500] train_loss: 0.04432 valid_loss: 0.07440 test_loss: 0.07983 \n",
      "[405/500] train_loss: 0.04433 valid_loss: 0.07371 test_loss: 0.07991 \n",
      "[406/500] train_loss: 0.04484 valid_loss: 0.07445 test_loss: 0.07968 \n",
      "[407/500] train_loss: 0.04340 valid_loss: 0.07309 test_loss: 0.08019 \n",
      "[408/500] train_loss: 0.04384 valid_loss: 0.07269 test_loss: 0.07824 \n",
      "[409/500] train_loss: 0.04272 valid_loss: 0.07252 test_loss: 0.07744 \n",
      "[410/500] train_loss: 0.04287 valid_loss: 0.07282 test_loss: 0.07835 \n",
      "[411/500] train_loss: 0.04409 valid_loss: 0.07454 test_loss: 0.07922 \n",
      "[412/500] train_loss: 0.04481 valid_loss: 0.07206 test_loss: 0.07968 \n",
      "[413/500] train_loss: 0.04438 valid_loss: 0.07485 test_loss: 0.07663 \n",
      "[414/500] train_loss: 0.04256 valid_loss: 0.07394 test_loss: 0.07722 \n",
      "[415/500] train_loss: 0.04372 valid_loss: 0.07398 test_loss: 0.07887 \n",
      "[416/500] train_loss: 0.04353 valid_loss: 0.07520 test_loss: 0.07733 \n",
      "[417/500] train_loss: 0.04432 valid_loss: 0.07284 test_loss: 0.07766 \n",
      "[418/500] train_loss: 0.04350 valid_loss: 0.07422 test_loss: 0.07776 \n",
      "[419/500] train_loss: 0.04347 valid_loss: 0.07406 test_loss: 0.07880 \n",
      "[420/500] train_loss: 0.04352 valid_loss: 0.07325 test_loss: 0.08045 \n",
      "[421/500] train_loss: 0.04393 valid_loss: 0.07301 test_loss: 0.08048 \n",
      "[422/500] train_loss: 0.04412 valid_loss: 0.07160 test_loss: 0.08031 \n",
      "[423/500] train_loss: 0.04427 valid_loss: 0.07210 test_loss: 0.07740 \n",
      "[424/500] train_loss: 0.04314 valid_loss: 0.07312 test_loss: 0.08034 \n",
      "[425/500] train_loss: 0.04264 valid_loss: 0.07186 test_loss: 0.07858 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426/500] train_loss: 0.04345 valid_loss: 0.07195 test_loss: 0.07844 \n",
      "[427/500] train_loss: 0.04307 valid_loss: 0.07280 test_loss: 0.08113 \n",
      "[428/500] train_loss: 0.04345 valid_loss: 0.07221 test_loss: 0.07803 \n",
      "[429/500] train_loss: 0.04296 valid_loss: 0.07103 test_loss: 0.07892 \n",
      "[430/500] train_loss: 0.04386 valid_loss: 0.07199 test_loss: 0.07891 \n",
      "[431/500] train_loss: 0.04370 valid_loss: 0.07121 test_loss: 0.07775 \n",
      "[432/500] train_loss: 0.04240 valid_loss: 0.07400 test_loss: 0.07926 \n",
      "[433/500] train_loss: 0.04418 valid_loss: 0.07279 test_loss: 0.07868 \n",
      "[434/500] train_loss: 0.04272 valid_loss: 0.07400 test_loss: 0.07945 \n",
      "[435/500] train_loss: 0.04276 valid_loss: 0.07433 test_loss: 0.07975 \n",
      "[436/500] train_loss: 0.04216 valid_loss: 0.07398 test_loss: 0.07837 \n",
      "[437/500] train_loss: 0.04382 valid_loss: 0.07203 test_loss: 0.07865 \n",
      "[438/500] train_loss: 0.04317 valid_loss: 0.07379 test_loss: 0.07839 \n",
      "[439/500] train_loss: 0.04247 valid_loss: 0.07241 test_loss: 0.07877 \n",
      "[440/500] train_loss: 0.04348 valid_loss: 0.07252 test_loss: 0.07826 \n",
      "[441/500] train_loss: 0.04144 valid_loss: 0.07354 test_loss: 0.07925 \n",
      "[442/500] train_loss: 0.04262 valid_loss: 0.07460 test_loss: 0.07961 \n",
      "[443/500] train_loss: 0.04275 valid_loss: 0.07258 test_loss: 0.07988 \n",
      "[444/500] train_loss: 0.04287 valid_loss: 0.07200 test_loss: 0.07947 \n",
      "[445/500] train_loss: 0.04287 valid_loss: 0.07375 test_loss: 0.07882 \n",
      "[446/500] train_loss: 0.04295 valid_loss: 0.07225 test_loss: 0.07791 \n",
      "[447/500] train_loss: 0.04323 valid_loss: 0.07340 test_loss: 0.07976 \n",
      "[448/500] train_loss: 0.04190 valid_loss: 0.07314 test_loss: 0.07938 \n",
      "[449/500] train_loss: 0.04319 valid_loss: 0.07484 test_loss: 0.07959 \n",
      "[450/500] train_loss: 0.04171 valid_loss: 0.07260 test_loss: 0.07877 \n",
      "[451/500] train_loss: 0.04148 valid_loss: 0.07537 test_loss: 0.08025 \n",
      "[452/500] train_loss: 0.04301 valid_loss: 0.07347 test_loss: 0.07885 \n",
      "[453/500] train_loss: 0.04221 valid_loss: 0.07248 test_loss: 0.07906 \n",
      "[454/500] train_loss: 0.04316 valid_loss: 0.07329 test_loss: 0.07987 \n",
      "[455/500] train_loss: 0.04394 valid_loss: 0.07435 test_loss: 0.07876 \n",
      "[456/500] train_loss: 0.04226 valid_loss: 0.07369 test_loss: 0.07925 \n",
      "[457/500] train_loss: 0.04205 valid_loss: 0.07245 test_loss: 0.08058 \n",
      "[458/500] train_loss: 0.04136 valid_loss: 0.07417 test_loss: 0.07950 \n",
      "[459/500] train_loss: 0.04217 valid_loss: 0.07357 test_loss: 0.08095 \n",
      "[460/500] train_loss: 0.04222 valid_loss: 0.07461 test_loss: 0.07761 \n",
      "[461/500] train_loss: 0.04348 valid_loss: 0.07318 test_loss: 0.07764 \n",
      "[462/500] train_loss: 0.04236 valid_loss: 0.07775 test_loss: 0.07959 \n",
      "[463/500] train_loss: 0.04211 valid_loss: 0.07391 test_loss: 0.07964 \n",
      "[464/500] train_loss: 0.04293 valid_loss: 0.07320 test_loss: 0.07979 \n",
      "[465/500] train_loss: 0.04267 valid_loss: 0.07318 test_loss: 0.07820 \n",
      "[466/500] train_loss: 0.04107 valid_loss: 0.07203 test_loss: 0.07798 \n",
      "[467/500] train_loss: 0.04296 valid_loss: 0.07272 test_loss: 0.07779 \n",
      "[468/500] train_loss: 0.04115 valid_loss: 0.07349 test_loss: 0.07782 \n",
      "[469/500] train_loss: 0.04252 valid_loss: 0.07295 test_loss: 0.08088 \n",
      "[470/500] train_loss: 0.04326 valid_loss: 0.07445 test_loss: 0.07958 \n",
      "[471/500] train_loss: 0.04196 valid_loss: 0.07311 test_loss: 0.07969 \n",
      "[472/500] train_loss: 0.04190 valid_loss: 0.07126 test_loss: 0.07991 \n",
      "[473/500] train_loss: 0.04245 valid_loss: 0.07306 test_loss: 0.07864 \n",
      "[474/500] train_loss: 0.04287 valid_loss: 0.07269 test_loss: 0.07793 \n",
      "[475/500] train_loss: 0.04095 valid_loss: 0.07444 test_loss: 0.08120 \n",
      "[476/500] train_loss: 0.04162 valid_loss: 0.07382 test_loss: 0.07883 \n",
      "[477/500] train_loss: 0.04287 valid_loss: 0.07373 test_loss: 0.07891 \n",
      "[478/500] train_loss: 0.04216 valid_loss: 0.07507 test_loss: 0.07981 \n",
      "[479/500] train_loss: 0.04111 valid_loss: 0.07318 test_loss: 0.07936 \n",
      "[480/500] train_loss: 0.04240 valid_loss: 0.07258 test_loss: 0.07864 \n",
      "[481/500] train_loss: 0.04219 valid_loss: 0.07308 test_loss: 0.07724 \n",
      "[482/500] train_loss: 0.04202 valid_loss: 0.07597 test_loss: 0.07935 \n",
      "[483/500] train_loss: 0.04049 valid_loss: 0.07457 test_loss: 0.07904 \n",
      "[484/500] train_loss: 0.04198 valid_loss: 0.07421 test_loss: 0.07915 \n",
      "[485/500] train_loss: 0.04165 valid_loss: 0.07354 test_loss: 0.07915 \n",
      "[486/500] train_loss: 0.04156 valid_loss: 0.07294 test_loss: 0.07966 \n",
      "[487/500] train_loss: 0.04177 valid_loss: 0.07167 test_loss: 0.07906 \n",
      "[488/500] train_loss: 0.04184 valid_loss: 0.07392 test_loss: 0.07916 \n",
      "[489/500] train_loss: 0.04113 valid_loss: 0.07175 test_loss: 0.07881 \n",
      "[490/500] train_loss: 0.04239 valid_loss: 0.07220 test_loss: 0.07828 \n",
      "[491/500] train_loss: 0.04064 valid_loss: 0.07199 test_loss: 0.07797 \n",
      "[492/500] train_loss: 0.04168 valid_loss: 0.07335 test_loss: 0.07838 \n",
      "[493/500] train_loss: 0.04094 valid_loss: 0.07317 test_loss: 0.07968 \n",
      "[494/500] train_loss: 0.04201 valid_loss: 0.07391 test_loss: 0.08024 \n",
      "[495/500] train_loss: 0.04309 valid_loss: 0.07352 test_loss: 0.07997 \n",
      "[496/500] train_loss: 0.04109 valid_loss: 0.07151 test_loss: 0.07880 \n",
      "[497/500] train_loss: 0.04254 valid_loss: 0.07266 test_loss: 0.07968 \n",
      "[498/500] train_loss: 0.04187 valid_loss: 0.07330 test_loss: 0.07912 \n",
      "[499/500] train_loss: 0.04247 valid_loss: 0.07233 test_loss: 0.07871 \n",
      "[500/500] train_loss: 0.04112 valid_loss: 0.07325 test_loss: 0.07946 \n",
      "TRAINING MODEL 5\n",
      "[  1/500] train_loss: 0.37747 valid_loss: 0.27919 test_loss: 0.28378 \n",
      "验证损失减少 (inf --> 0.279195). 正在保存模型...\n",
      "[  2/500] train_loss: 0.20700 valid_loss: 0.19621 test_loss: 0.20037 \n",
      "验证损失减少 (0.279195 --> 0.196213). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15950 valid_loss: 0.16222 test_loss: 0.16594 \n",
      "验证损失减少 (0.196213 --> 0.162223). 正在保存模型...\n",
      "[  4/500] train_loss: 0.14262 valid_loss: 0.14244 test_loss: 0.14817 \n",
      "验证损失减少 (0.162223 --> 0.142438). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13223 valid_loss: 0.13547 test_loss: 0.14327 \n",
      "验证损失减少 (0.142438 --> 0.135467). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12651 valid_loss: 0.13033 test_loss: 0.13549 \n",
      "验证损失减少 (0.135467 --> 0.130328). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11993 valid_loss: 0.12578 test_loss: 0.13280 \n",
      "验证损失减少 (0.130328 --> 0.125782). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11578 valid_loss: 0.12262 test_loss: 0.13525 \n",
      "验证损失减少 (0.125782 --> 0.122620). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11593 valid_loss: 0.11884 test_loss: 0.12831 \n",
      "验证损失减少 (0.122620 --> 0.118838). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11072 valid_loss: 0.11489 test_loss: 0.12498 \n",
      "验证损失减少 (0.118838 --> 0.114895). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10822 valid_loss: 0.11249 test_loss: 0.12140 \n",
      "验证损失减少 (0.114895 --> 0.112489). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10773 valid_loss: 0.10907 test_loss: 0.12025 \n",
      "验证损失减少 (0.112489 --> 0.109070). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10536 valid_loss: 0.10958 test_loss: 0.11903 \n",
      "[ 14/500] train_loss: 0.10352 valid_loss: 0.10660 test_loss: 0.11700 \n",
      "验证损失减少 (0.109070 --> 0.106603). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10164 valid_loss: 0.10387 test_loss: 0.11438 \n",
      "验证损失减少 (0.106603 --> 0.103866). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10051 valid_loss: 0.10136 test_loss: 0.11199 \n",
      "验证损失减少 (0.103866 --> 0.101360). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09996 valid_loss: 0.10080 test_loss: 0.11414 \n",
      "验证损失减少 (0.101360 --> 0.100796). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09816 valid_loss: 0.09861 test_loss: 0.11103 \n",
      "验证损失减少 (0.100796 --> 0.098611). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09419 valid_loss: 0.09942 test_loss: 0.11072 \n",
      "[ 20/500] train_loss: 0.09622 valid_loss: 0.09941 test_loss: 0.11140 \n",
      "[ 21/500] train_loss: 0.09541 valid_loss: 0.09641 test_loss: 0.10800 \n",
      "验证损失减少 (0.098611 --> 0.096412). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09351 valid_loss: 0.09657 test_loss: 0.10693 \n",
      "[ 23/500] train_loss: 0.09255 valid_loss: 0.09681 test_loss: 0.10651 \n",
      "[ 24/500] train_loss: 0.09237 valid_loss: 0.09595 test_loss: 0.10423 \n",
      "验证损失减少 (0.096412 --> 0.095949). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.09064 valid_loss: 0.09223 test_loss: 0.10553 \n",
      "验证损失减少 (0.095949 --> 0.092233). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.08896 valid_loss: 0.09337 test_loss: 0.10229 \n",
      "[ 27/500] train_loss: 0.08964 valid_loss: 0.09391 test_loss: 0.10114 \n",
      "[ 28/500] train_loss: 0.08782 valid_loss: 0.09229 test_loss: 0.10083 \n",
      "[ 29/500] train_loss: 0.08673 valid_loss: 0.09052 test_loss: 0.09859 \n",
      "验证损失减少 (0.092233 --> 0.090524). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08602 valid_loss: 0.08992 test_loss: 0.09833 \n",
      "验证损失减少 (0.090524 --> 0.089919). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31/500] train_loss: 0.08484 valid_loss: 0.09236 test_loss: 0.09988 \n",
      "[ 32/500] train_loss: 0.08860 valid_loss: 0.09090 test_loss: 0.09991 \n",
      "[ 33/500] train_loss: 0.08372 valid_loss: 0.09125 test_loss: 0.09792 \n",
      "[ 34/500] train_loss: 0.08499 valid_loss: 0.08626 test_loss: 0.09752 \n",
      "验证损失减少 (0.089919 --> 0.086261). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08293 valid_loss: 0.08885 test_loss: 0.09813 \n",
      "[ 36/500] train_loss: 0.08491 valid_loss: 0.08747 test_loss: 0.09685 \n",
      "[ 37/500] train_loss: 0.08433 valid_loss: 0.08710 test_loss: 0.09558 \n",
      "[ 38/500] train_loss: 0.08372 valid_loss: 0.08668 test_loss: 0.09472 \n",
      "[ 39/500] train_loss: 0.08399 valid_loss: 0.08716 test_loss: 0.09444 \n",
      "[ 40/500] train_loss: 0.08026 valid_loss: 0.08717 test_loss: 0.09812 \n",
      "[ 41/500] train_loss: 0.08168 valid_loss: 0.08494 test_loss: 0.09538 \n",
      "验证损失减少 (0.086261 --> 0.084942). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.08135 valid_loss: 0.08534 test_loss: 0.09695 \n",
      "[ 43/500] train_loss: 0.07968 valid_loss: 0.08672 test_loss: 0.09761 \n",
      "[ 44/500] train_loss: 0.08010 valid_loss: 0.08795 test_loss: 0.09489 \n",
      "[ 45/500] train_loss: 0.08023 valid_loss: 0.08830 test_loss: 0.09364 \n",
      "[ 46/500] train_loss: 0.07892 valid_loss: 0.08477 test_loss: 0.09257 \n",
      "验证损失减少 (0.084942 --> 0.084767). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07917 valid_loss: 0.08426 test_loss: 0.09429 \n",
      "验证损失减少 (0.084767 --> 0.084257). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.07732 valid_loss: 0.08590 test_loss: 0.09394 \n",
      "[ 49/500] train_loss: 0.07797 valid_loss: 0.08460 test_loss: 0.09257 \n",
      "[ 50/500] train_loss: 0.07793 valid_loss: 0.08405 test_loss: 0.09099 \n",
      "验证损失减少 (0.084257 --> 0.084050). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.07758 valid_loss: 0.08527 test_loss: 0.09100 \n",
      "[ 52/500] train_loss: 0.07781 valid_loss: 0.08359 test_loss: 0.09176 \n",
      "验证损失减少 (0.084050 --> 0.083586). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07615 valid_loss: 0.08324 test_loss: 0.09027 \n",
      "验证损失减少 (0.083586 --> 0.083239). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07624 valid_loss: 0.08385 test_loss: 0.08956 \n",
      "[ 55/500] train_loss: 0.07485 valid_loss: 0.08278 test_loss: 0.09014 \n",
      "验证损失减少 (0.083239 --> 0.082782). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07511 valid_loss: 0.08025 test_loss: 0.08867 \n",
      "验证损失减少 (0.082782 --> 0.080253). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.07524 valid_loss: 0.08188 test_loss: 0.08918 \n",
      "[ 58/500] train_loss: 0.07634 valid_loss: 0.08166 test_loss: 0.08898 \n",
      "[ 59/500] train_loss: 0.07586 valid_loss: 0.08021 test_loss: 0.08839 \n",
      "验证损失减少 (0.080253 --> 0.080206). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07525 valid_loss: 0.08212 test_loss: 0.08902 \n",
      "[ 61/500] train_loss: 0.07487 valid_loss: 0.08138 test_loss: 0.08831 \n",
      "[ 62/500] train_loss: 0.07384 valid_loss: 0.08183 test_loss: 0.08900 \n",
      "[ 63/500] train_loss: 0.07273 valid_loss: 0.08326 test_loss: 0.08844 \n",
      "[ 64/500] train_loss: 0.07371 valid_loss: 0.08152 test_loss: 0.08804 \n",
      "[ 65/500] train_loss: 0.07211 valid_loss: 0.08398 test_loss: 0.08941 \n",
      "[ 66/500] train_loss: 0.07404 valid_loss: 0.07939 test_loss: 0.08779 \n",
      "验证损失减少 (0.080206 --> 0.079389). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.07271 valid_loss: 0.08052 test_loss: 0.08720 \n",
      "[ 68/500] train_loss: 0.07481 valid_loss: 0.08058 test_loss: 0.08729 \n",
      "[ 69/500] train_loss: 0.07172 valid_loss: 0.08147 test_loss: 0.08758 \n",
      "[ 70/500] train_loss: 0.07230 valid_loss: 0.07873 test_loss: 0.08540 \n",
      "验证损失减少 (0.079389 --> 0.078728). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.06970 valid_loss: 0.07981 test_loss: 0.08585 \n",
      "[ 72/500] train_loss: 0.07095 valid_loss: 0.08055 test_loss: 0.08673 \n",
      "[ 73/500] train_loss: 0.07117 valid_loss: 0.07957 test_loss: 0.08749 \n",
      "[ 74/500] train_loss: 0.07066 valid_loss: 0.08150 test_loss: 0.08842 \n",
      "[ 75/500] train_loss: 0.07160 valid_loss: 0.08054 test_loss: 0.08624 \n",
      "[ 76/500] train_loss: 0.07056 valid_loss: 0.08017 test_loss: 0.08649 \n",
      "[ 77/500] train_loss: 0.07053 valid_loss: 0.08071 test_loss: 0.08811 \n",
      "[ 78/500] train_loss: 0.06995 valid_loss: 0.08135 test_loss: 0.08851 \n",
      "[ 79/500] train_loss: 0.07147 valid_loss: 0.07961 test_loss: 0.08620 \n",
      "[ 80/500] train_loss: 0.07079 valid_loss: 0.07856 test_loss: 0.08579 \n",
      "验证损失减少 (0.078728 --> 0.078557). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.07051 valid_loss: 0.07884 test_loss: 0.08622 \n",
      "[ 82/500] train_loss: 0.06958 valid_loss: 0.07929 test_loss: 0.08722 \n",
      "[ 83/500] train_loss: 0.06957 valid_loss: 0.08016 test_loss: 0.08525 \n",
      "[ 84/500] train_loss: 0.06995 valid_loss: 0.07848 test_loss: 0.08531 \n",
      "验证损失减少 (0.078557 --> 0.078477). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.06943 valid_loss: 0.07907 test_loss: 0.08581 \n",
      "[ 86/500] train_loss: 0.06986 valid_loss: 0.07890 test_loss: 0.08479 \n",
      "[ 87/500] train_loss: 0.06708 valid_loss: 0.07801 test_loss: 0.08455 \n",
      "验证损失减少 (0.078477 --> 0.078008). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.06810 valid_loss: 0.08181 test_loss: 0.08520 \n",
      "[ 89/500] train_loss: 0.06814 valid_loss: 0.07927 test_loss: 0.08477 \n",
      "[ 90/500] train_loss: 0.06685 valid_loss: 0.07851 test_loss: 0.08428 \n",
      "[ 91/500] train_loss: 0.06718 valid_loss: 0.08009 test_loss: 0.08623 \n",
      "[ 92/500] train_loss: 0.06582 valid_loss: 0.07982 test_loss: 0.08504 \n",
      "[ 93/500] train_loss: 0.06789 valid_loss: 0.07857 test_loss: 0.08425 \n",
      "[ 94/500] train_loss: 0.06716 valid_loss: 0.07849 test_loss: 0.08461 \n",
      "[ 95/500] train_loss: 0.06655 valid_loss: 0.07972 test_loss: 0.08503 \n",
      "[ 96/500] train_loss: 0.06605 valid_loss: 0.07819 test_loss: 0.08393 \n",
      "[ 97/500] train_loss: 0.06566 valid_loss: 0.07773 test_loss: 0.08323 \n",
      "验证损失减少 (0.078008 --> 0.077730). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.06738 valid_loss: 0.07909 test_loss: 0.08437 \n",
      "[ 99/500] train_loss: 0.06588 valid_loss: 0.07643 test_loss: 0.08301 \n",
      "验证损失减少 (0.077730 --> 0.076432). 正在保存模型...\n",
      "[100/500] train_loss: 0.06842 valid_loss: 0.07812 test_loss: 0.08236 \n",
      "[101/500] train_loss: 0.06589 valid_loss: 0.07656 test_loss: 0.08361 \n",
      "[102/500] train_loss: 0.06577 valid_loss: 0.07903 test_loss: 0.08459 \n",
      "[103/500] train_loss: 0.06443 valid_loss: 0.07866 test_loss: 0.08344 \n",
      "[104/500] train_loss: 0.06607 valid_loss: 0.07798 test_loss: 0.08392 \n",
      "[105/500] train_loss: 0.06400 valid_loss: 0.07787 test_loss: 0.08385 \n",
      "[106/500] train_loss: 0.06335 valid_loss: 0.07625 test_loss: 0.08325 \n",
      "验证损失减少 (0.076432 --> 0.076255). 正在保存模型...\n",
      "[107/500] train_loss: 0.06692 valid_loss: 0.07833 test_loss: 0.08416 \n",
      "[108/500] train_loss: 0.06447 valid_loss: 0.08059 test_loss: 0.08432 \n",
      "[109/500] train_loss: 0.06702 valid_loss: 0.07763 test_loss: 0.08326 \n",
      "[110/500] train_loss: 0.06429 valid_loss: 0.07988 test_loss: 0.08516 \n",
      "[111/500] train_loss: 0.06382 valid_loss: 0.07887 test_loss: 0.08223 \n",
      "[112/500] train_loss: 0.06436 valid_loss: 0.07853 test_loss: 0.08335 \n",
      "[113/500] train_loss: 0.06463 valid_loss: 0.07993 test_loss: 0.08323 \n",
      "[114/500] train_loss: 0.06276 valid_loss: 0.07716 test_loss: 0.08305 \n",
      "[115/500] train_loss: 0.06470 valid_loss: 0.07774 test_loss: 0.08456 \n",
      "[116/500] train_loss: 0.06361 valid_loss: 0.07658 test_loss: 0.08329 \n",
      "[117/500] train_loss: 0.06403 valid_loss: 0.07934 test_loss: 0.08300 \n",
      "[118/500] train_loss: 0.06321 valid_loss: 0.07815 test_loss: 0.08227 \n",
      "[119/500] train_loss: 0.06354 valid_loss: 0.07630 test_loss: 0.08307 \n",
      "[120/500] train_loss: 0.06352 valid_loss: 0.07749 test_loss: 0.08433 \n",
      "[121/500] train_loss: 0.06357 valid_loss: 0.07845 test_loss: 0.08206 \n",
      "[122/500] train_loss: 0.06262 valid_loss: 0.07707 test_loss: 0.08159 \n",
      "[123/500] train_loss: 0.06278 valid_loss: 0.07865 test_loss: 0.08505 \n",
      "[124/500] train_loss: 0.06352 valid_loss: 0.07960 test_loss: 0.08273 \n",
      "[125/500] train_loss: 0.06411 valid_loss: 0.07803 test_loss: 0.08260 \n",
      "[126/500] train_loss: 0.06322 valid_loss: 0.07908 test_loss: 0.08246 \n",
      "[127/500] train_loss: 0.06111 valid_loss: 0.07730 test_loss: 0.08342 \n",
      "[128/500] train_loss: 0.06230 valid_loss: 0.07748 test_loss: 0.08207 \n",
      "[129/500] train_loss: 0.06304 valid_loss: 0.07833 test_loss: 0.08226 \n",
      "[130/500] train_loss: 0.06233 valid_loss: 0.07645 test_loss: 0.08155 \n",
      "[131/500] train_loss: 0.06179 valid_loss: 0.07644 test_loss: 0.08141 \n",
      "[132/500] train_loss: 0.06260 valid_loss: 0.07716 test_loss: 0.08148 \n",
      "[133/500] train_loss: 0.06128 valid_loss: 0.07633 test_loss: 0.08339 \n",
      "[134/500] train_loss: 0.06219 valid_loss: 0.07774 test_loss: 0.08187 \n",
      "[135/500] train_loss: 0.05971 valid_loss: 0.07717 test_loss: 0.08306 \n",
      "[136/500] train_loss: 0.06050 valid_loss: 0.07629 test_loss: 0.08178 \n",
      "[137/500] train_loss: 0.05972 valid_loss: 0.07633 test_loss: 0.08381 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138/500] train_loss: 0.06163 valid_loss: 0.07779 test_loss: 0.08219 \n",
      "[139/500] train_loss: 0.06114 valid_loss: 0.07562 test_loss: 0.08121 \n",
      "验证损失减少 (0.076255 --> 0.075622). 正在保存模型...\n",
      "[140/500] train_loss: 0.06020 valid_loss: 0.07626 test_loss: 0.08260 \n",
      "[141/500] train_loss: 0.05882 valid_loss: 0.07408 test_loss: 0.08125 \n",
      "验证损失减少 (0.075622 --> 0.074076). 正在保存模型...\n",
      "[142/500] train_loss: 0.06029 valid_loss: 0.07617 test_loss: 0.08181 \n",
      "[143/500] train_loss: 0.05947 valid_loss: 0.07441 test_loss: 0.08131 \n",
      "[144/500] train_loss: 0.05948 valid_loss: 0.07554 test_loss: 0.08167 \n",
      "[145/500] train_loss: 0.05990 valid_loss: 0.07408 test_loss: 0.08130 \n",
      "[146/500] train_loss: 0.06056 valid_loss: 0.07657 test_loss: 0.08147 \n",
      "[147/500] train_loss: 0.05935 valid_loss: 0.07727 test_loss: 0.08159 \n",
      "[148/500] train_loss: 0.06037 valid_loss: 0.07539 test_loss: 0.08217 \n",
      "[149/500] train_loss: 0.05801 valid_loss: 0.07543 test_loss: 0.08177 \n",
      "[150/500] train_loss: 0.05858 valid_loss: 0.07643 test_loss: 0.08143 \n",
      "[151/500] train_loss: 0.05951 valid_loss: 0.07637 test_loss: 0.08195 \n",
      "[152/500] train_loss: 0.05884 valid_loss: 0.07587 test_loss: 0.08102 \n",
      "[153/500] train_loss: 0.05895 valid_loss: 0.07502 test_loss: 0.08072 \n",
      "[154/500] train_loss: 0.05906 valid_loss: 0.07629 test_loss: 0.08086 \n",
      "[155/500] train_loss: 0.05857 valid_loss: 0.07547 test_loss: 0.08043 \n",
      "[156/500] train_loss: 0.05968 valid_loss: 0.07819 test_loss: 0.08181 \n",
      "[157/500] train_loss: 0.05801 valid_loss: 0.07737 test_loss: 0.08192 \n",
      "[158/500] train_loss: 0.05957 valid_loss: 0.07532 test_loss: 0.08144 \n",
      "[159/500] train_loss: 0.05921 valid_loss: 0.07569 test_loss: 0.08174 \n",
      "[160/500] train_loss: 0.05918 valid_loss: 0.07570 test_loss: 0.08082 \n",
      "[161/500] train_loss: 0.05762 valid_loss: 0.07743 test_loss: 0.08317 \n",
      "[162/500] train_loss: 0.05782 valid_loss: 0.07475 test_loss: 0.08127 \n",
      "[163/500] train_loss: 0.05816 valid_loss: 0.07625 test_loss: 0.08197 \n",
      "[164/500] train_loss: 0.05948 valid_loss: 0.07542 test_loss: 0.08232 \n",
      "[165/500] train_loss: 0.05778 valid_loss: 0.08296 test_loss: 0.08280 \n",
      "[166/500] train_loss: 0.05997 valid_loss: 0.07704 test_loss: 0.08276 \n",
      "[167/500] train_loss: 0.05796 valid_loss: 0.07786 test_loss: 0.08210 \n",
      "[168/500] train_loss: 0.05927 valid_loss: 0.07703 test_loss: 0.08156 \n",
      "[169/500] train_loss: 0.05821 valid_loss: 0.07717 test_loss: 0.08155 \n",
      "[170/500] train_loss: 0.05708 valid_loss: 0.07641 test_loss: 0.08109 \n",
      "[171/500] train_loss: 0.05800 valid_loss: 0.07563 test_loss: 0.08050 \n",
      "[172/500] train_loss: 0.05776 valid_loss: 0.07641 test_loss: 0.08102 \n",
      "[173/500] train_loss: 0.05623 valid_loss: 0.07616 test_loss: 0.08060 \n",
      "[174/500] train_loss: 0.05787 valid_loss: 0.07632 test_loss: 0.08072 \n",
      "[175/500] train_loss: 0.05603 valid_loss: 0.07550 test_loss: 0.08011 \n",
      "[176/500] train_loss: 0.05546 valid_loss: 0.07523 test_loss: 0.08016 \n",
      "[177/500] train_loss: 0.05789 valid_loss: 0.07609 test_loss: 0.08163 \n",
      "[178/500] train_loss: 0.05641 valid_loss: 0.07561 test_loss: 0.08087 \n",
      "[179/500] train_loss: 0.05560 valid_loss: 0.07716 test_loss: 0.08129 \n",
      "[180/500] train_loss: 0.05541 valid_loss: 0.07411 test_loss: 0.08034 \n",
      "[181/500] train_loss: 0.05620 valid_loss: 0.07645 test_loss: 0.08171 \n",
      "[182/500] train_loss: 0.05632 valid_loss: 0.07579 test_loss: 0.08050 \n",
      "[183/500] train_loss: 0.05681 valid_loss: 0.07631 test_loss: 0.08138 \n",
      "[184/500] train_loss: 0.05580 valid_loss: 0.07454 test_loss: 0.08166 \n",
      "[185/500] train_loss: 0.05598 valid_loss: 0.07635 test_loss: 0.08185 \n",
      "[186/500] train_loss: 0.05716 valid_loss: 0.07584 test_loss: 0.08163 \n",
      "[187/500] train_loss: 0.05750 valid_loss: 0.07494 test_loss: 0.08079 \n",
      "[188/500] train_loss: 0.05523 valid_loss: 0.07497 test_loss: 0.07966 \n",
      "[189/500] train_loss: 0.05616 valid_loss: 0.07669 test_loss: 0.08164 \n",
      "[190/500] train_loss: 0.05474 valid_loss: 0.07546 test_loss: 0.08061 \n",
      "[191/500] train_loss: 0.05550 valid_loss: 0.07579 test_loss: 0.08099 \n",
      "[192/500] train_loss: 0.05488 valid_loss: 0.07514 test_loss: 0.07992 \n",
      "[193/500] train_loss: 0.05722 valid_loss: 0.07684 test_loss: 0.07978 \n",
      "[194/500] train_loss: 0.05517 valid_loss: 0.07617 test_loss: 0.08018 \n",
      "[195/500] train_loss: 0.05539 valid_loss: 0.07447 test_loss: 0.08049 \n",
      "[196/500] train_loss: 0.05488 valid_loss: 0.07408 test_loss: 0.08008 \n",
      "[197/500] train_loss: 0.05405 valid_loss: 0.07492 test_loss: 0.08124 \n",
      "[198/500] train_loss: 0.05598 valid_loss: 0.07550 test_loss: 0.08030 \n",
      "[199/500] train_loss: 0.05399 valid_loss: 0.07622 test_loss: 0.08070 \n",
      "[200/500] train_loss: 0.05477 valid_loss: 0.07632 test_loss: 0.08040 \n",
      "[201/500] train_loss: 0.05368 valid_loss: 0.07704 test_loss: 0.08053 \n",
      "[202/500] train_loss: 0.05410 valid_loss: 0.07639 test_loss: 0.08174 \n",
      "[203/500] train_loss: 0.05502 valid_loss: 0.07661 test_loss: 0.08121 \n",
      "[204/500] train_loss: 0.05489 valid_loss: 0.07707 test_loss: 0.08018 \n",
      "[205/500] train_loss: 0.05486 valid_loss: 0.07755 test_loss: 0.08018 \n",
      "[206/500] train_loss: 0.05333 valid_loss: 0.07685 test_loss: 0.07907 \n",
      "[207/500] train_loss: 0.05459 valid_loss: 0.07863 test_loss: 0.08085 \n",
      "[208/500] train_loss: 0.05363 valid_loss: 0.07523 test_loss: 0.08019 \n",
      "[209/500] train_loss: 0.05389 valid_loss: 0.07721 test_loss: 0.07958 \n",
      "[210/500] train_loss: 0.05582 valid_loss: 0.08148 test_loss: 0.08118 \n",
      "[211/500] train_loss: 0.05515 valid_loss: 0.07632 test_loss: 0.08136 \n",
      "[212/500] train_loss: 0.05425 valid_loss: 0.07674 test_loss: 0.08011 \n",
      "[213/500] train_loss: 0.05260 valid_loss: 0.07475 test_loss: 0.08113 \n",
      "[214/500] train_loss: 0.05408 valid_loss: 0.07567 test_loss: 0.08072 \n",
      "[215/500] train_loss: 0.05228 valid_loss: 0.07725 test_loss: 0.08305 \n",
      "[216/500] train_loss: 0.05351 valid_loss: 0.07771 test_loss: 0.08161 \n",
      "[217/500] train_loss: 0.05315 valid_loss: 0.07810 test_loss: 0.08206 \n",
      "[218/500] train_loss: 0.05228 valid_loss: 0.07686 test_loss: 0.08081 \n",
      "[219/500] train_loss: 0.05394 valid_loss: 0.07624 test_loss: 0.08086 \n",
      "[220/500] train_loss: 0.05183 valid_loss: 0.07704 test_loss: 0.08176 \n",
      "[221/500] train_loss: 0.05310 valid_loss: 0.07810 test_loss: 0.08100 \n",
      "[222/500] train_loss: 0.05376 valid_loss: 0.07655 test_loss: 0.08313 \n",
      "[223/500] train_loss: 0.05401 valid_loss: 0.08031 test_loss: 0.08091 \n",
      "[224/500] train_loss: 0.05327 valid_loss: 0.07527 test_loss: 0.07967 \n",
      "[225/500] train_loss: 0.05418 valid_loss: 0.07535 test_loss: 0.07914 \n",
      "[226/500] train_loss: 0.05364 valid_loss: 0.07525 test_loss: 0.07947 \n",
      "[227/500] train_loss: 0.05257 valid_loss: 0.07532 test_loss: 0.07984 \n",
      "[228/500] train_loss: 0.05186 valid_loss: 0.07487 test_loss: 0.08033 \n",
      "[229/500] train_loss: 0.05158 valid_loss: 0.07576 test_loss: 0.07884 \n",
      "[230/500] train_loss: 0.05099 valid_loss: 0.07653 test_loss: 0.07982 \n",
      "[231/500] train_loss: 0.05247 valid_loss: 0.07469 test_loss: 0.08006 \n",
      "[232/500] train_loss: 0.05384 valid_loss: 0.07489 test_loss: 0.08073 \n",
      "[233/500] train_loss: 0.05281 valid_loss: 0.07594 test_loss: 0.08123 \n",
      "[234/500] train_loss: 0.05211 valid_loss: 0.07560 test_loss: 0.08079 \n",
      "[235/500] train_loss: 0.05245 valid_loss: 0.07703 test_loss: 0.08178 \n",
      "[236/500] train_loss: 0.05227 valid_loss: 0.07714 test_loss: 0.08077 \n",
      "[237/500] train_loss: 0.05173 valid_loss: 0.07639 test_loss: 0.07981 \n",
      "[238/500] train_loss: 0.05180 valid_loss: 0.07641 test_loss: 0.08052 \n",
      "[239/500] train_loss: 0.05339 valid_loss: 0.07594 test_loss: 0.07943 \n",
      "[240/500] train_loss: 0.05217 valid_loss: 0.07514 test_loss: 0.07937 \n",
      "[241/500] train_loss: 0.05179 valid_loss: 0.07646 test_loss: 0.08085 \n",
      "[242/500] train_loss: 0.05138 valid_loss: 0.07705 test_loss: 0.07934 \n",
      "[243/500] train_loss: 0.05134 valid_loss: 0.07745 test_loss: 0.08111 \n",
      "[244/500] train_loss: 0.05062 valid_loss: 0.07623 test_loss: 0.07903 \n",
      "[245/500] train_loss: 0.05293 valid_loss: 0.07620 test_loss: 0.07920 \n",
      "[246/500] train_loss: 0.05206 valid_loss: 0.07552 test_loss: 0.07849 \n",
      "[247/500] train_loss: 0.05174 valid_loss: 0.07644 test_loss: 0.08027 \n",
      "[248/500] train_loss: 0.05136 valid_loss: 0.07596 test_loss: 0.08105 \n",
      "[249/500] train_loss: 0.05109 valid_loss: 0.07603 test_loss: 0.08057 \n",
      "[250/500] train_loss: 0.05227 valid_loss: 0.07673 test_loss: 0.08149 \n",
      "[251/500] train_loss: 0.05039 valid_loss: 0.07538 test_loss: 0.07873 \n",
      "[252/500] train_loss: 0.05118 valid_loss: 0.07697 test_loss: 0.08086 \n",
      "[253/500] train_loss: 0.05129 valid_loss: 0.07676 test_loss: 0.08121 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[254/500] train_loss: 0.05067 valid_loss: 0.07658 test_loss: 0.08224 \n",
      "[255/500] train_loss: 0.05071 valid_loss: 0.07686 test_loss: 0.08018 \n",
      "[256/500] train_loss: 0.05089 valid_loss: 0.07719 test_loss: 0.08026 \n",
      "[257/500] train_loss: 0.05030 valid_loss: 0.07824 test_loss: 0.08029 \n",
      "[258/500] train_loss: 0.04991 valid_loss: 0.07661 test_loss: 0.07912 \n",
      "[259/500] train_loss: 0.05048 valid_loss: 0.07542 test_loss: 0.07891 \n",
      "[260/500] train_loss: 0.05068 valid_loss: 0.07679 test_loss: 0.08000 \n",
      "[261/500] train_loss: 0.05038 valid_loss: 0.07749 test_loss: 0.08029 \n",
      "[262/500] train_loss: 0.05076 valid_loss: 0.07579 test_loss: 0.07961 \n",
      "[263/500] train_loss: 0.04935 valid_loss: 0.07707 test_loss: 0.08076 \n",
      "[264/500] train_loss: 0.04963 valid_loss: 0.07571 test_loss: 0.07966 \n",
      "[265/500] train_loss: 0.05057 valid_loss: 0.07613 test_loss: 0.08109 \n",
      "[266/500] train_loss: 0.05070 valid_loss: 0.07645 test_loss: 0.08002 \n",
      "[267/500] train_loss: 0.04950 valid_loss: 0.07664 test_loss: 0.08026 \n",
      "[268/500] train_loss: 0.05027 valid_loss: 0.07740 test_loss: 0.07975 \n",
      "[269/500] train_loss: 0.05106 valid_loss: 0.07937 test_loss: 0.08131 \n",
      "[270/500] train_loss: 0.04920 valid_loss: 0.07657 test_loss: 0.08043 \n",
      "[271/500] train_loss: 0.04994 valid_loss: 0.07815 test_loss: 0.08092 \n",
      "[272/500] train_loss: 0.05041 valid_loss: 0.07843 test_loss: 0.08125 \n",
      "[273/500] train_loss: 0.05081 valid_loss: 0.07755 test_loss: 0.08004 \n",
      "[274/500] train_loss: 0.04974 valid_loss: 0.07697 test_loss: 0.07939 \n",
      "[275/500] train_loss: 0.05036 valid_loss: 0.07757 test_loss: 0.08237 \n",
      "[276/500] train_loss: 0.04913 valid_loss: 0.07687 test_loss: 0.08077 \n",
      "[277/500] train_loss: 0.04996 valid_loss: 0.07804 test_loss: 0.08034 \n",
      "[278/500] train_loss: 0.04960 valid_loss: 0.07869 test_loss: 0.08111 \n",
      "[279/500] train_loss: 0.05003 valid_loss: 0.07808 test_loss: 0.08003 \n",
      "[280/500] train_loss: 0.04745 valid_loss: 0.07849 test_loss: 0.08208 \n",
      "[281/500] train_loss: 0.04804 valid_loss: 0.07800 test_loss: 0.08036 \n",
      "[282/500] train_loss: 0.04817 valid_loss: 0.07690 test_loss: 0.07934 \n",
      "[283/500] train_loss: 0.04919 valid_loss: 0.07642 test_loss: 0.08167 \n",
      "[284/500] train_loss: 0.05031 valid_loss: 0.07727 test_loss: 0.08028 \n",
      "[285/500] train_loss: 0.04820 valid_loss: 0.07675 test_loss: 0.07966 \n",
      "[286/500] train_loss: 0.05020 valid_loss: 0.07757 test_loss: 0.08003 \n",
      "[287/500] train_loss: 0.04872 valid_loss: 0.07703 test_loss: 0.07874 \n",
      "[288/500] train_loss: 0.04801 valid_loss: 0.07823 test_loss: 0.08018 \n",
      "[289/500] train_loss: 0.04771 valid_loss: 0.07932 test_loss: 0.08303 \n",
      "[290/500] train_loss: 0.04750 valid_loss: 0.07867 test_loss: 0.08071 \n",
      "[291/500] train_loss: 0.04891 valid_loss: 0.08035 test_loss: 0.08156 \n",
      "[292/500] train_loss: 0.04837 valid_loss: 0.07700 test_loss: 0.08001 \n",
      "[293/500] train_loss: 0.04925 valid_loss: 0.07709 test_loss: 0.08133 \n",
      "[294/500] train_loss: 0.04861 valid_loss: 0.07738 test_loss: 0.08034 \n",
      "[295/500] train_loss: 0.04808 valid_loss: 0.07817 test_loss: 0.08091 \n",
      "[296/500] train_loss: 0.04878 valid_loss: 0.07578 test_loss: 0.07918 \n",
      "[297/500] train_loss: 0.05003 valid_loss: 0.07660 test_loss: 0.08009 \n",
      "[298/500] train_loss: 0.04903 valid_loss: 0.07675 test_loss: 0.07920 \n",
      "[299/500] train_loss: 0.04950 valid_loss: 0.07770 test_loss: 0.08110 \n",
      "[300/500] train_loss: 0.04906 valid_loss: 0.07788 test_loss: 0.07960 \n",
      "[301/500] train_loss: 0.04897 valid_loss: 0.07421 test_loss: 0.07869 \n",
      "[302/500] train_loss: 0.04716 valid_loss: 0.07500 test_loss: 0.07997 \n",
      "[303/500] train_loss: 0.04719 valid_loss: 0.07491 test_loss: 0.07965 \n",
      "[304/500] train_loss: 0.04755 valid_loss: 0.07534 test_loss: 0.08079 \n",
      "[305/500] train_loss: 0.04793 valid_loss: 0.07668 test_loss: 0.08075 \n",
      "[306/500] train_loss: 0.04899 valid_loss: 0.07754 test_loss: 0.08066 \n",
      "[307/500] train_loss: 0.04630 valid_loss: 0.07535 test_loss: 0.07950 \n",
      "[308/500] train_loss: 0.04788 valid_loss: 0.07634 test_loss: 0.07932 \n",
      "[309/500] train_loss: 0.04963 valid_loss: 0.07644 test_loss: 0.08127 \n",
      "[310/500] train_loss: 0.04714 valid_loss: 0.07536 test_loss: 0.07935 \n",
      "[311/500] train_loss: 0.04784 valid_loss: 0.07795 test_loss: 0.07979 \n",
      "[312/500] train_loss: 0.04699 valid_loss: 0.07649 test_loss: 0.08034 \n",
      "[313/500] train_loss: 0.04718 valid_loss: 0.07700 test_loss: 0.07991 \n",
      "[314/500] train_loss: 0.04846 valid_loss: 0.07548 test_loss: 0.08020 \n",
      "[315/500] train_loss: 0.04681 valid_loss: 0.07635 test_loss: 0.07865 \n",
      "[316/500] train_loss: 0.04638 valid_loss: 0.07723 test_loss: 0.07880 \n",
      "[317/500] train_loss: 0.04715 valid_loss: 0.07717 test_loss: 0.07938 \n",
      "[318/500] train_loss: 0.04666 valid_loss: 0.07540 test_loss: 0.08005 \n",
      "[319/500] train_loss: 0.04670 valid_loss: 0.07643 test_loss: 0.08295 \n",
      "[320/500] train_loss: 0.04751 valid_loss: 0.07719 test_loss: 0.08014 \n",
      "[321/500] train_loss: 0.04892 valid_loss: 0.07600 test_loss: 0.08014 \n",
      "[322/500] train_loss: 0.04794 valid_loss: 0.07702 test_loss: 0.08106 \n",
      "[323/500] train_loss: 0.04615 valid_loss: 0.07735 test_loss: 0.07996 \n",
      "[324/500] train_loss: 0.04741 valid_loss: 0.07635 test_loss: 0.08192 \n",
      "[325/500] train_loss: 0.04568 valid_loss: 0.07722 test_loss: 0.08124 \n",
      "[326/500] train_loss: 0.04773 valid_loss: 0.08074 test_loss: 0.07941 \n",
      "[327/500] train_loss: 0.04818 valid_loss: 0.07799 test_loss: 0.07903 \n",
      "[328/500] train_loss: 0.04756 valid_loss: 0.07764 test_loss: 0.08045 \n",
      "[329/500] train_loss: 0.04726 valid_loss: 0.07591 test_loss: 0.08160 \n",
      "[330/500] train_loss: 0.04815 valid_loss: 0.07711 test_loss: 0.08082 \n",
      "[331/500] train_loss: 0.04743 valid_loss: 0.07532 test_loss: 0.07979 \n",
      "[332/500] train_loss: 0.04780 valid_loss: 0.07635 test_loss: 0.07990 \n",
      "[333/500] train_loss: 0.04702 valid_loss: 0.07570 test_loss: 0.08121 \n",
      "[334/500] train_loss: 0.04653 valid_loss: 0.07576 test_loss: 0.08131 \n",
      "[335/500] train_loss: 0.04586 valid_loss: 0.07704 test_loss: 0.08028 \n",
      "[336/500] train_loss: 0.04558 valid_loss: 0.07620 test_loss: 0.08068 \n",
      "[337/500] train_loss: 0.04579 valid_loss: 0.07621 test_loss: 0.07930 \n",
      "[338/500] train_loss: 0.04690 valid_loss: 0.07647 test_loss: 0.08018 \n",
      "[339/500] train_loss: 0.04531 valid_loss: 0.07682 test_loss: 0.08017 \n",
      "[340/500] train_loss: 0.04686 valid_loss: 0.07592 test_loss: 0.07943 \n",
      "[341/500] train_loss: 0.04623 valid_loss: 0.07636 test_loss: 0.08048 \n",
      "[342/500] train_loss: 0.04727 valid_loss: 0.07654 test_loss: 0.08113 \n",
      "[343/500] train_loss: 0.04692 valid_loss: 0.07684 test_loss: 0.08008 \n",
      "[344/500] train_loss: 0.04525 valid_loss: 0.07638 test_loss: 0.07924 \n",
      "[345/500] train_loss: 0.04674 valid_loss: 0.07688 test_loss: 0.07898 \n",
      "[346/500] train_loss: 0.04506 valid_loss: 0.07723 test_loss: 0.08016 \n",
      "[347/500] train_loss: 0.04702 valid_loss: 0.07643 test_loss: 0.07918 \n",
      "[348/500] train_loss: 0.04621 valid_loss: 0.07568 test_loss: 0.07984 \n",
      "[349/500] train_loss: 0.04771 valid_loss: 0.07833 test_loss: 0.08005 \n",
      "[350/500] train_loss: 0.04470 valid_loss: 0.07578 test_loss: 0.07903 \n",
      "[351/500] train_loss: 0.04604 valid_loss: 0.07582 test_loss: 0.07986 \n",
      "[352/500] train_loss: 0.04589 valid_loss: 0.07545 test_loss: 0.07887 \n",
      "[353/500] train_loss: 0.04487 valid_loss: 0.07628 test_loss: 0.08095 \n",
      "[354/500] train_loss: 0.04536 valid_loss: 0.07600 test_loss: 0.08063 \n",
      "[355/500] train_loss: 0.04605 valid_loss: 0.07531 test_loss: 0.07984 \n",
      "[356/500] train_loss: 0.04488 valid_loss: 0.07617 test_loss: 0.07920 \n",
      "[357/500] train_loss: 0.04577 valid_loss: 0.07509 test_loss: 0.07987 \n",
      "[358/500] train_loss: 0.04577 valid_loss: 0.07651 test_loss: 0.07951 \n",
      "[359/500] train_loss: 0.04511 valid_loss: 0.07745 test_loss: 0.08017 \n",
      "[360/500] train_loss: 0.04621 valid_loss: 0.07723 test_loss: 0.08005 \n",
      "[361/500] train_loss: 0.04524 valid_loss: 0.07745 test_loss: 0.08157 \n",
      "[362/500] train_loss: 0.04578 valid_loss: 0.07761 test_loss: 0.08043 \n",
      "[363/500] train_loss: 0.04562 valid_loss: 0.07965 test_loss: 0.08147 \n",
      "[364/500] train_loss: 0.04501 valid_loss: 0.07797 test_loss: 0.08178 \n",
      "[365/500] train_loss: 0.04549 valid_loss: 0.07727 test_loss: 0.08151 \n",
      "[366/500] train_loss: 0.04662 valid_loss: 0.07625 test_loss: 0.07944 \n",
      "[367/500] train_loss: 0.04461 valid_loss: 0.07631 test_loss: 0.08012 \n",
      "[368/500] train_loss: 0.04578 valid_loss: 0.07600 test_loss: 0.07969 \n",
      "[369/500] train_loss: 0.04504 valid_loss: 0.07756 test_loss: 0.08097 \n",
      "[370/500] train_loss: 0.04567 valid_loss: 0.07737 test_loss: 0.07919 \n",
      "[371/500] train_loss: 0.04445 valid_loss: 0.07727 test_loss: 0.08157 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[372/500] train_loss: 0.04636 valid_loss: 0.07976 test_loss: 0.07868 \n",
      "[373/500] train_loss: 0.04602 valid_loss: 0.07669 test_loss: 0.08030 \n",
      "[374/500] train_loss: 0.04514 valid_loss: 0.07735 test_loss: 0.07896 \n",
      "[375/500] train_loss: 0.04470 valid_loss: 0.07631 test_loss: 0.07869 \n",
      "[376/500] train_loss: 0.04605 valid_loss: 0.07683 test_loss: 0.08116 \n",
      "[377/500] train_loss: 0.04464 valid_loss: 0.07679 test_loss: 0.08029 \n",
      "[378/500] train_loss: 0.04394 valid_loss: 0.07716 test_loss: 0.07959 \n",
      "[379/500] train_loss: 0.04532 valid_loss: 0.07807 test_loss: 0.08053 \n",
      "[380/500] train_loss: 0.04606 valid_loss: 0.07578 test_loss: 0.07974 \n",
      "[381/500] train_loss: 0.04435 valid_loss: 0.07680 test_loss: 0.07906 \n",
      "[382/500] train_loss: 0.04565 valid_loss: 0.07946 test_loss: 0.08255 \n",
      "[383/500] train_loss: 0.04532 valid_loss: 0.07594 test_loss: 0.08004 \n",
      "[384/500] train_loss: 0.04483 valid_loss: 0.07689 test_loss: 0.08028 \n",
      "[385/500] train_loss: 0.04418 valid_loss: 0.07705 test_loss: 0.08019 \n",
      "[386/500] train_loss: 0.04439 valid_loss: 0.07742 test_loss: 0.07940 \n",
      "[387/500] train_loss: 0.04511 valid_loss: 0.07629 test_loss: 0.07976 \n",
      "[388/500] train_loss: 0.04409 valid_loss: 0.07589 test_loss: 0.07942 \n",
      "[389/500] train_loss: 0.04477 valid_loss: 0.07603 test_loss: 0.08285 \n",
      "[390/500] train_loss: 0.04455 valid_loss: 0.07709 test_loss: 0.08134 \n",
      "[391/500] train_loss: 0.04409 valid_loss: 0.07760 test_loss: 0.08075 \n",
      "[392/500] train_loss: 0.04436 valid_loss: 0.07674 test_loss: 0.08004 \n",
      "[393/500] train_loss: 0.04459 valid_loss: 0.07624 test_loss: 0.07970 \n",
      "[394/500] train_loss: 0.04616 valid_loss: 0.07594 test_loss: 0.08044 \n",
      "[395/500] train_loss: 0.04502 valid_loss: 0.07607 test_loss: 0.07923 \n",
      "[396/500] train_loss: 0.04387 valid_loss: 0.07648 test_loss: 0.07899 \n",
      "[397/500] train_loss: 0.04446 valid_loss: 0.07625 test_loss: 0.08012 \n",
      "[398/500] train_loss: 0.04409 valid_loss: 0.07693 test_loss: 0.08109 \n",
      "[399/500] train_loss: 0.04415 valid_loss: 0.07668 test_loss: 0.07962 \n",
      "[400/500] train_loss: 0.04450 valid_loss: 0.07604 test_loss: 0.07962 \n",
      "[401/500] train_loss: 0.04245 valid_loss: 0.07865 test_loss: 0.08017 \n",
      "[402/500] train_loss: 0.04331 valid_loss: 0.07678 test_loss: 0.07971 \n",
      "[403/500] train_loss: 0.04334 valid_loss: 0.07606 test_loss: 0.08052 \n",
      "[404/500] train_loss: 0.04372 valid_loss: 0.07590 test_loss: 0.08021 \n",
      "[405/500] train_loss: 0.04398 valid_loss: 0.07653 test_loss: 0.08122 \n",
      "[406/500] train_loss: 0.04370 valid_loss: 0.07483 test_loss: 0.08177 \n",
      "[407/500] train_loss: 0.04386 valid_loss: 0.07754 test_loss: 0.08022 \n",
      "[408/500] train_loss: 0.04431 valid_loss: 0.07841 test_loss: 0.07938 \n",
      "[409/500] train_loss: 0.04469 valid_loss: 0.07691 test_loss: 0.08220 \n",
      "[410/500] train_loss: 0.04438 valid_loss: 0.07738 test_loss: 0.08171 \n",
      "[411/500] train_loss: 0.04307 valid_loss: 0.07638 test_loss: 0.08077 \n",
      "[412/500] train_loss: 0.04292 valid_loss: 0.07673 test_loss: 0.07885 \n",
      "[413/500] train_loss: 0.04350 valid_loss: 0.07623 test_loss: 0.07988 \n",
      "[414/500] train_loss: 0.04441 valid_loss: 0.07724 test_loss: 0.07838 \n",
      "[415/500] train_loss: 0.04391 valid_loss: 0.07741 test_loss: 0.07949 \n",
      "[416/500] train_loss: 0.04357 valid_loss: 0.07594 test_loss: 0.08057 \n",
      "[417/500] train_loss: 0.04402 valid_loss: 0.07749 test_loss: 0.08018 \n",
      "[418/500] train_loss: 0.04286 valid_loss: 0.07563 test_loss: 0.07984 \n",
      "[419/500] train_loss: 0.04353 valid_loss: 0.07577 test_loss: 0.07987 \n",
      "[420/500] train_loss: 0.04344 valid_loss: 0.07463 test_loss: 0.07843 \n",
      "[421/500] train_loss: 0.04373 valid_loss: 0.07635 test_loss: 0.07907 \n",
      "[422/500] train_loss: 0.04448 valid_loss: 0.07750 test_loss: 0.07866 \n",
      "[423/500] train_loss: 0.04256 valid_loss: 0.07898 test_loss: 0.08156 \n",
      "[424/500] train_loss: 0.04452 valid_loss: 0.07666 test_loss: 0.07947 \n",
      "[425/500] train_loss: 0.04373 valid_loss: 0.07578 test_loss: 0.07862 \n",
      "[426/500] train_loss: 0.04289 valid_loss: 0.07729 test_loss: 0.08021 \n",
      "[427/500] train_loss: 0.04360 valid_loss: 0.07675 test_loss: 0.08087 \n",
      "[428/500] train_loss: 0.04408 valid_loss: 0.07600 test_loss: 0.07917 \n",
      "[429/500] train_loss: 0.04374 valid_loss: 0.07802 test_loss: 0.08024 \n",
      "[430/500] train_loss: 0.04309 valid_loss: 0.07655 test_loss: 0.07914 \n",
      "[431/500] train_loss: 0.04230 valid_loss: 0.07653 test_loss: 0.07981 \n",
      "[432/500] train_loss: 0.04334 valid_loss: 0.07795 test_loss: 0.07946 \n",
      "[433/500] train_loss: 0.04337 valid_loss: 0.07874 test_loss: 0.07868 \n",
      "[434/500] train_loss: 0.04360 valid_loss: 0.07736 test_loss: 0.07928 \n",
      "[435/500] train_loss: 0.04272 valid_loss: 0.07838 test_loss: 0.08162 \n",
      "[436/500] train_loss: 0.04371 valid_loss: 0.07723 test_loss: 0.08076 \n",
      "[437/500] train_loss: 0.04366 valid_loss: 0.07748 test_loss: 0.08017 \n",
      "[438/500] train_loss: 0.04345 valid_loss: 0.07719 test_loss: 0.08045 \n",
      "[439/500] train_loss: 0.04236 valid_loss: 0.07569 test_loss: 0.08127 \n",
      "[440/500] train_loss: 0.04280 valid_loss: 0.07652 test_loss: 0.07984 \n",
      "[441/500] train_loss: 0.04071 valid_loss: 0.07758 test_loss: 0.08135 \n",
      "[442/500] train_loss: 0.04277 valid_loss: 0.07716 test_loss: 0.07992 \n",
      "[443/500] train_loss: 0.04265 valid_loss: 0.07762 test_loss: 0.08033 \n",
      "[444/500] train_loss: 0.04196 valid_loss: 0.07733 test_loss: 0.08169 \n",
      "[445/500] train_loss: 0.04312 valid_loss: 0.07633 test_loss: 0.08074 \n",
      "[446/500] train_loss: 0.04286 valid_loss: 0.07688 test_loss: 0.08199 \n",
      "[447/500] train_loss: 0.04373 valid_loss: 0.07822 test_loss: 0.07971 \n",
      "[448/500] train_loss: 0.04379 valid_loss: 0.07704 test_loss: 0.08004 \n",
      "[449/500] train_loss: 0.04223 valid_loss: 0.07822 test_loss: 0.08199 \n",
      "[450/500] train_loss: 0.04183 valid_loss: 0.07743 test_loss: 0.07964 \n",
      "[451/500] train_loss: 0.04241 valid_loss: 0.07763 test_loss: 0.08035 \n",
      "[452/500] train_loss: 0.04136 valid_loss: 0.07746 test_loss: 0.07884 \n",
      "[453/500] train_loss: 0.04198 valid_loss: 0.07786 test_loss: 0.07969 \n",
      "[454/500] train_loss: 0.04223 valid_loss: 0.07755 test_loss: 0.07974 \n",
      "[455/500] train_loss: 0.04330 valid_loss: 0.07699 test_loss: 0.08096 \n",
      "[456/500] train_loss: 0.04144 valid_loss: 0.07667 test_loss: 0.08162 \n",
      "[457/500] train_loss: 0.04129 valid_loss: 0.07737 test_loss: 0.08130 \n",
      "[458/500] train_loss: 0.04216 valid_loss: 0.07617 test_loss: 0.08145 \n",
      "[459/500] train_loss: 0.04228 valid_loss: 0.07714 test_loss: 0.08025 \n",
      "[460/500] train_loss: 0.04361 valid_loss: 0.07572 test_loss: 0.07938 \n",
      "[461/500] train_loss: 0.04253 valid_loss: 0.07574 test_loss: 0.08170 \n",
      "[462/500] train_loss: 0.04204 valid_loss: 0.07754 test_loss: 0.08149 \n",
      "[463/500] train_loss: 0.04261 valid_loss: 0.07770 test_loss: 0.08202 \n",
      "[464/500] train_loss: 0.04208 valid_loss: 0.07582 test_loss: 0.07993 \n",
      "[465/500] train_loss: 0.04173 valid_loss: 0.07654 test_loss: 0.08103 \n",
      "[466/500] train_loss: 0.04229 valid_loss: 0.07818 test_loss: 0.08176 \n",
      "[467/500] train_loss: 0.04146 valid_loss: 0.07670 test_loss: 0.08180 \n",
      "[468/500] train_loss: 0.04289 valid_loss: 0.07674 test_loss: 0.08209 \n",
      "[469/500] train_loss: 0.04174 valid_loss: 0.07673 test_loss: 0.08147 \n",
      "[470/500] train_loss: 0.04077 valid_loss: 0.07648 test_loss: 0.08235 \n",
      "[471/500] train_loss: 0.04244 valid_loss: 0.07634 test_loss: 0.08148 \n",
      "[472/500] train_loss: 0.04235 valid_loss: 0.07528 test_loss: 0.07991 \n",
      "[473/500] train_loss: 0.04224 valid_loss: 0.07630 test_loss: 0.08183 \n",
      "[474/500] train_loss: 0.04252 valid_loss: 0.07679 test_loss: 0.08230 \n",
      "[475/500] train_loss: 0.04186 valid_loss: 0.07562 test_loss: 0.08021 \n",
      "[476/500] train_loss: 0.04263 valid_loss: 0.07769 test_loss: 0.08129 \n",
      "[477/500] train_loss: 0.04196 valid_loss: 0.07627 test_loss: 0.08195 \n",
      "[478/500] train_loss: 0.04174 valid_loss: 0.07735 test_loss: 0.08336 \n",
      "[479/500] train_loss: 0.04139 valid_loss: 0.07689 test_loss: 0.08166 \n",
      "[480/500] train_loss: 0.04180 valid_loss: 0.07769 test_loss: 0.08077 \n",
      "[481/500] train_loss: 0.04124 valid_loss: 0.07810 test_loss: 0.08138 \n",
      "[482/500] train_loss: 0.04091 valid_loss: 0.07719 test_loss: 0.08073 \n",
      "[483/500] train_loss: 0.04142 valid_loss: 0.07692 test_loss: 0.08112 \n",
      "[484/500] train_loss: 0.04120 valid_loss: 0.07774 test_loss: 0.08194 \n",
      "[485/500] train_loss: 0.04089 valid_loss: 0.07697 test_loss: 0.08181 \n",
      "[486/500] train_loss: 0.04162 valid_loss: 0.07910 test_loss: 0.08307 \n",
      "[487/500] train_loss: 0.04162 valid_loss: 0.07836 test_loss: 0.08170 \n",
      "[488/500] train_loss: 0.04149 valid_loss: 0.07739 test_loss: 0.08094 \n",
      "[489/500] train_loss: 0.04185 valid_loss: 0.07962 test_loss: 0.08213 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[490/500] train_loss: 0.04020 valid_loss: 0.07525 test_loss: 0.08027 \n",
      "[491/500] train_loss: 0.04289 valid_loss: 0.07708 test_loss: 0.08191 \n",
      "[492/500] train_loss: 0.04155 valid_loss: 0.07719 test_loss: 0.08313 \n",
      "[493/500] train_loss: 0.04211 valid_loss: 0.07834 test_loss: 0.08103 \n",
      "[494/500] train_loss: 0.04161 valid_loss: 0.07742 test_loss: 0.08039 \n",
      "[495/500] train_loss: 0.04206 valid_loss: 0.07816 test_loss: 0.08054 \n",
      "[496/500] train_loss: 0.04047 valid_loss: 0.07772 test_loss: 0.08133 \n",
      "[497/500] train_loss: 0.04033 valid_loss: 0.07962 test_loss: 0.08182 \n",
      "[498/500] train_loss: 0.04030 valid_loss: 0.07949 test_loss: 0.08228 \n",
      "[499/500] train_loss: 0.04164 valid_loss: 0.07932 test_loss: 0.08346 \n",
      "[500/500] train_loss: 0.04169 valid_loss: 0.07931 test_loss: 0.08119 \n",
      "TRAINING MODEL 6\n",
      "[  1/500] train_loss: 0.35685 valid_loss: 0.24270 test_loss: 0.25079 \n",
      "验证损失减少 (inf --> 0.242704). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18454 valid_loss: 0.18843 test_loss: 0.19728 \n",
      "验证损失减少 (0.242704 --> 0.188425). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15509 valid_loss: 0.15427 test_loss: 0.16029 \n",
      "验证损失减少 (0.188425 --> 0.154271). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13849 valid_loss: 0.14182 test_loss: 0.14702 \n",
      "验证损失减少 (0.154271 --> 0.141818). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12917 valid_loss: 0.13431 test_loss: 0.14411 \n",
      "验证损失减少 (0.141818 --> 0.134313). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12436 valid_loss: 0.12651 test_loss: 0.13642 \n",
      "验证损失减少 (0.134313 --> 0.126514). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11849 valid_loss: 0.12164 test_loss: 0.13004 \n",
      "验证损失减少 (0.126514 --> 0.121639). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11459 valid_loss: 0.11794 test_loss: 0.12716 \n",
      "验证损失减少 (0.121639 --> 0.117943). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11253 valid_loss: 0.12159 test_loss: 0.13006 \n",
      "[ 10/500] train_loss: 0.10816 valid_loss: 0.11487 test_loss: 0.12261 \n",
      "验证损失减少 (0.117943 --> 0.114874). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10729 valid_loss: 0.11021 test_loss: 0.12121 \n",
      "验证损失减少 (0.114874 --> 0.110214). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10624 valid_loss: 0.11492 test_loss: 0.12235 \n",
      "[ 13/500] train_loss: 0.10406 valid_loss: 0.10603 test_loss: 0.11604 \n",
      "验证损失减少 (0.110214 --> 0.106030). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10267 valid_loss: 0.10492 test_loss: 0.11528 \n",
      "验证损失减少 (0.106030 --> 0.104920). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10137 valid_loss: 0.10441 test_loss: 0.11462 \n",
      "验证损失减少 (0.104920 --> 0.104415). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10098 valid_loss: 0.10463 test_loss: 0.11303 \n",
      "[ 17/500] train_loss: 0.09851 valid_loss: 0.10224 test_loss: 0.10959 \n",
      "验证损失减少 (0.104415 --> 0.102238). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09793 valid_loss: 0.09957 test_loss: 0.10831 \n",
      "验证损失减少 (0.102238 --> 0.099566). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09440 valid_loss: 0.09970 test_loss: 0.10742 \n",
      "[ 20/500] train_loss: 0.09568 valid_loss: 0.09693 test_loss: 0.10639 \n",
      "验证损失减少 (0.099566 --> 0.096931). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09409 valid_loss: 0.09802 test_loss: 0.10619 \n",
      "[ 22/500] train_loss: 0.09325 valid_loss: 0.09973 test_loss: 0.10671 \n",
      "[ 23/500] train_loss: 0.09311 valid_loss: 0.09561 test_loss: 0.10343 \n",
      "验证损失减少 (0.096931 --> 0.095610). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09317 valid_loss: 0.09373 test_loss: 0.10333 \n",
      "验证损失减少 (0.095610 --> 0.093726). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.09132 valid_loss: 0.09605 test_loss: 0.10428 \n",
      "[ 26/500] train_loss: 0.09115 valid_loss: 0.09463 test_loss: 0.10369 \n",
      "[ 27/500] train_loss: 0.08806 valid_loss: 0.09453 test_loss: 0.10283 \n",
      "[ 28/500] train_loss: 0.08675 valid_loss: 0.09420 test_loss: 0.10195 \n",
      "[ 29/500] train_loss: 0.08915 valid_loss: 0.09357 test_loss: 0.10280 \n",
      "验证损失减少 (0.093726 --> 0.093573). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08837 valid_loss: 0.09553 test_loss: 0.10263 \n",
      "[ 31/500] train_loss: 0.08622 valid_loss: 0.09181 test_loss: 0.10047 \n",
      "验证损失减少 (0.093573 --> 0.091812). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08589 valid_loss: 0.09087 test_loss: 0.09865 \n",
      "验证损失减少 (0.091812 --> 0.090866). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08432 valid_loss: 0.09013 test_loss: 0.09748 \n",
      "验证损失减少 (0.090866 --> 0.090131). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08511 valid_loss: 0.08890 test_loss: 0.09808 \n",
      "验证损失减少 (0.090131 --> 0.088903). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08503 valid_loss: 0.09193 test_loss: 0.10033 \n",
      "[ 36/500] train_loss: 0.08321 valid_loss: 0.08882 test_loss: 0.09621 \n",
      "验证损失减少 (0.088903 --> 0.088815). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08315 valid_loss: 0.08932 test_loss: 0.09769 \n",
      "[ 38/500] train_loss: 0.08198 valid_loss: 0.08966 test_loss: 0.09803 \n",
      "[ 39/500] train_loss: 0.08296 valid_loss: 0.08956 test_loss: 0.09451 \n",
      "[ 40/500] train_loss: 0.08440 valid_loss: 0.08615 test_loss: 0.09531 \n",
      "验证损失减少 (0.088815 --> 0.086146). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.08107 valid_loss: 0.08687 test_loss: 0.09505 \n",
      "[ 42/500] train_loss: 0.08232 valid_loss: 0.08851 test_loss: 0.09495 \n",
      "[ 43/500] train_loss: 0.07944 valid_loss: 0.08818 test_loss: 0.09431 \n",
      "[ 44/500] train_loss: 0.08098 valid_loss: 0.08615 test_loss: 0.09334 \n",
      "验证损失减少 (0.086146 --> 0.086145). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.08043 valid_loss: 0.08544 test_loss: 0.09395 \n",
      "验证损失减少 (0.086145 --> 0.085435). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07804 valid_loss: 0.08679 test_loss: 0.09266 \n",
      "[ 47/500] train_loss: 0.07907 valid_loss: 0.08681 test_loss: 0.09545 \n",
      "[ 48/500] train_loss: 0.08062 valid_loss: 0.08559 test_loss: 0.09402 \n",
      "[ 49/500] train_loss: 0.07793 valid_loss: 0.08383 test_loss: 0.09236 \n",
      "验证损失减少 (0.085435 --> 0.083829). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07943 valid_loss: 0.08550 test_loss: 0.09325 \n",
      "[ 51/500] train_loss: 0.07948 valid_loss: 0.08521 test_loss: 0.09166 \n",
      "[ 52/500] train_loss: 0.07596 valid_loss: 0.08232 test_loss: 0.09122 \n",
      "验证损失减少 (0.083829 --> 0.082315). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07735 valid_loss: 0.08311 test_loss: 0.09107 \n",
      "[ 54/500] train_loss: 0.07584 valid_loss: 0.08387 test_loss: 0.09205 \n",
      "[ 55/500] train_loss: 0.07531 valid_loss: 0.08305 test_loss: 0.09214 \n",
      "[ 56/500] train_loss: 0.07576 valid_loss: 0.08317 test_loss: 0.08998 \n",
      "[ 57/500] train_loss: 0.07618 valid_loss: 0.08279 test_loss: 0.09081 \n",
      "[ 58/500] train_loss: 0.07750 valid_loss: 0.08217 test_loss: 0.09033 \n",
      "验证损失减少 (0.082315 --> 0.082166). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07612 valid_loss: 0.08126 test_loss: 0.08820 \n",
      "验证损失减少 (0.082166 --> 0.081257). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07445 valid_loss: 0.08402 test_loss: 0.09071 \n",
      "[ 61/500] train_loss: 0.07603 valid_loss: 0.08105 test_loss: 0.09098 \n",
      "验证损失减少 (0.081257 --> 0.081054). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.07686 valid_loss: 0.08353 test_loss: 0.09054 \n",
      "[ 63/500] train_loss: 0.07357 valid_loss: 0.08120 test_loss: 0.08905 \n",
      "[ 64/500] train_loss: 0.07323 valid_loss: 0.08161 test_loss: 0.08927 \n",
      "[ 65/500] train_loss: 0.07301 valid_loss: 0.08148 test_loss: 0.08897 \n",
      "[ 66/500] train_loss: 0.07296 valid_loss: 0.08364 test_loss: 0.08925 \n",
      "[ 67/500] train_loss: 0.07230 valid_loss: 0.08093 test_loss: 0.08845 \n",
      "验证损失减少 (0.081054 --> 0.080934). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.07098 valid_loss: 0.08065 test_loss: 0.08870 \n",
      "验证损失减少 (0.080934 --> 0.080654). 正在保存模型...\n",
      "[ 69/500] train_loss: 0.07100 valid_loss: 0.08155 test_loss: 0.08797 \n",
      "[ 70/500] train_loss: 0.07384 valid_loss: 0.08393 test_loss: 0.09078 \n",
      "[ 71/500] train_loss: 0.07440 valid_loss: 0.08305 test_loss: 0.08846 \n",
      "[ 72/500] train_loss: 0.07373 valid_loss: 0.08054 test_loss: 0.08767 \n",
      "验证损失减少 (0.080654 --> 0.080542). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.07284 valid_loss: 0.08128 test_loss: 0.08802 \n",
      "[ 74/500] train_loss: 0.07455 valid_loss: 0.08021 test_loss: 0.08831 \n",
      "验证损失减少 (0.080542 --> 0.080206). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.07147 valid_loss: 0.07941 test_loss: 0.08652 \n",
      "验证损失减少 (0.080206 --> 0.079414). 正在保存模型...\n",
      "[ 76/500] train_loss: 0.07033 valid_loss: 0.08023 test_loss: 0.08674 \n",
      "[ 77/500] train_loss: 0.06960 valid_loss: 0.08030 test_loss: 0.08736 \n",
      "[ 78/500] train_loss: 0.07134 valid_loss: 0.07898 test_loss: 0.08660 \n",
      "验证损失减少 (0.079414 --> 0.078980). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.07131 valid_loss: 0.08183 test_loss: 0.08841 \n",
      "[ 80/500] train_loss: 0.07104 valid_loss: 0.07921 test_loss: 0.08623 \n",
      "[ 81/500] train_loss: 0.06865 valid_loss: 0.08027 test_loss: 0.08779 \n",
      "[ 82/500] train_loss: 0.06941 valid_loss: 0.08032 test_loss: 0.08554 \n",
      "[ 83/500] train_loss: 0.06954 valid_loss: 0.07872 test_loss: 0.08626 \n",
      "验证损失减少 (0.078980 --> 0.078719). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 84/500] train_loss: 0.07018 valid_loss: 0.07773 test_loss: 0.08588 \n",
      "验证损失减少 (0.078719 --> 0.077727). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.06999 valid_loss: 0.07793 test_loss: 0.08555 \n",
      "[ 86/500] train_loss: 0.06898 valid_loss: 0.08068 test_loss: 0.08647 \n",
      "[ 87/500] train_loss: 0.07113 valid_loss: 0.07780 test_loss: 0.08550 \n",
      "[ 88/500] train_loss: 0.06870 valid_loss: 0.07845 test_loss: 0.08597 \n",
      "[ 89/500] train_loss: 0.07033 valid_loss: 0.07841 test_loss: 0.08529 \n",
      "[ 90/500] train_loss: 0.06867 valid_loss: 0.07739 test_loss: 0.08657 \n",
      "验证损失减少 (0.077727 --> 0.077394). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.07054 valid_loss: 0.07735 test_loss: 0.08457 \n",
      "验证损失减少 (0.077394 --> 0.077346). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.06831 valid_loss: 0.07803 test_loss: 0.08487 \n",
      "[ 93/500] train_loss: 0.06793 valid_loss: 0.07777 test_loss: 0.08465 \n",
      "[ 94/500] train_loss: 0.06758 valid_loss: 0.07785 test_loss: 0.08530 \n",
      "[ 95/500] train_loss: 0.06703 valid_loss: 0.08004 test_loss: 0.08463 \n",
      "[ 96/500] train_loss: 0.06760 valid_loss: 0.07967 test_loss: 0.08516 \n",
      "[ 97/500] train_loss: 0.06664 valid_loss: 0.07913 test_loss: 0.08394 \n",
      "[ 98/500] train_loss: 0.06705 valid_loss: 0.07807 test_loss: 0.08443 \n",
      "[ 99/500] train_loss: 0.06539 valid_loss: 0.07922 test_loss: 0.08346 \n",
      "[100/500] train_loss: 0.06740 valid_loss: 0.08215 test_loss: 0.08416 \n",
      "[101/500] train_loss: 0.06579 valid_loss: 0.07901 test_loss: 0.08409 \n",
      "[102/500] train_loss: 0.06553 valid_loss: 0.07665 test_loss: 0.08511 \n",
      "验证损失减少 (0.077346 --> 0.076651). 正在保存模型...\n",
      "[103/500] train_loss: 0.06801 valid_loss: 0.07819 test_loss: 0.08411 \n",
      "[104/500] train_loss: 0.06522 valid_loss: 0.07716 test_loss: 0.08368 \n",
      "[105/500] train_loss: 0.06594 valid_loss: 0.07617 test_loss: 0.08478 \n",
      "验证损失减少 (0.076651 --> 0.076168). 正在保存模型...\n",
      "[106/500] train_loss: 0.06614 valid_loss: 0.07674 test_loss: 0.08287 \n",
      "[107/500] train_loss: 0.06498 valid_loss: 0.07686 test_loss: 0.08392 \n",
      "[108/500] train_loss: 0.06417 valid_loss: 0.07720 test_loss: 0.08443 \n",
      "[109/500] train_loss: 0.06647 valid_loss: 0.07805 test_loss: 0.08364 \n",
      "[110/500] train_loss: 0.06555 valid_loss: 0.07643 test_loss: 0.08362 \n",
      "[111/500] train_loss: 0.06406 valid_loss: 0.07611 test_loss: 0.08226 \n",
      "验证损失减少 (0.076168 --> 0.076110). 正在保存模型...\n",
      "[112/500] train_loss: 0.06502 valid_loss: 0.07712 test_loss: 0.08338 \n",
      "[113/500] train_loss: 0.06632 valid_loss: 0.07727 test_loss: 0.08364 \n",
      "[114/500] train_loss: 0.06405 valid_loss: 0.07795 test_loss: 0.08446 \n",
      "[115/500] train_loss: 0.06561 valid_loss: 0.07815 test_loss: 0.08465 \n",
      "[116/500] train_loss: 0.06452 valid_loss: 0.07620 test_loss: 0.08224 \n",
      "[117/500] train_loss: 0.06321 valid_loss: 0.07981 test_loss: 0.08663 \n",
      "[118/500] train_loss: 0.06186 valid_loss: 0.07726 test_loss: 0.08393 \n",
      "[119/500] train_loss: 0.06304 valid_loss: 0.07713 test_loss: 0.08327 \n",
      "[120/500] train_loss: 0.06288 valid_loss: 0.07711 test_loss: 0.08373 \n",
      "[121/500] train_loss: 0.06252 valid_loss: 0.08071 test_loss: 0.08302 \n",
      "[122/500] train_loss: 0.06289 valid_loss: 0.07670 test_loss: 0.08541 \n",
      "[123/500] train_loss: 0.06364 valid_loss: 0.07970 test_loss: 0.08629 \n",
      "[124/500] train_loss: 0.06305 valid_loss: 0.07692 test_loss: 0.08317 \n",
      "[125/500] train_loss: 0.06315 valid_loss: 0.07592 test_loss: 0.08269 \n",
      "验证损失减少 (0.076110 --> 0.075924). 正在保存模型...\n",
      "[126/500] train_loss: 0.06283 valid_loss: 0.07723 test_loss: 0.08321 \n",
      "[127/500] train_loss: 0.06257 valid_loss: 0.07747 test_loss: 0.08269 \n",
      "[128/500] train_loss: 0.06223 valid_loss: 0.07639 test_loss: 0.08318 \n",
      "[129/500] train_loss: 0.06183 valid_loss: 0.07827 test_loss: 0.08187 \n",
      "[130/500] train_loss: 0.06286 valid_loss: 0.07467 test_loss: 0.08326 \n",
      "验证损失减少 (0.075924 --> 0.074671). 正在保存模型...\n",
      "[131/500] train_loss: 0.06145 valid_loss: 0.07643 test_loss: 0.08138 \n",
      "[132/500] train_loss: 0.06340 valid_loss: 0.07515 test_loss: 0.08073 \n",
      "[133/500] train_loss: 0.06169 valid_loss: 0.07663 test_loss: 0.08226 \n",
      "[134/500] train_loss: 0.06010 valid_loss: 0.07612 test_loss: 0.08201 \n",
      "[135/500] train_loss: 0.06181 valid_loss: 0.07578 test_loss: 0.08212 \n",
      "[136/500] train_loss: 0.06351 valid_loss: 0.07636 test_loss: 0.08105 \n",
      "[137/500] train_loss: 0.06076 valid_loss: 0.07470 test_loss: 0.08153 \n",
      "[138/500] train_loss: 0.06108 valid_loss: 0.07775 test_loss: 0.08377 \n",
      "[139/500] train_loss: 0.06215 valid_loss: 0.07827 test_loss: 0.08271 \n",
      "[140/500] train_loss: 0.06041 valid_loss: 0.07676 test_loss: 0.08222 \n",
      "[141/500] train_loss: 0.06063 valid_loss: 0.07669 test_loss: 0.08171 \n",
      "[142/500] train_loss: 0.06052 valid_loss: 0.07521 test_loss: 0.08116 \n",
      "[143/500] train_loss: 0.06136 valid_loss: 0.07411 test_loss: 0.08131 \n",
      "验证损失减少 (0.074671 --> 0.074108). 正在保存模型...\n",
      "[144/500] train_loss: 0.06173 valid_loss: 0.07528 test_loss: 0.08061 \n",
      "[145/500] train_loss: 0.06184 valid_loss: 0.07542 test_loss: 0.08066 \n",
      "[146/500] train_loss: 0.06066 valid_loss: 0.07738 test_loss: 0.08132 \n",
      "[147/500] train_loss: 0.06047 valid_loss: 0.07514 test_loss: 0.08052 \n",
      "[148/500] train_loss: 0.06071 valid_loss: 0.07527 test_loss: 0.08316 \n",
      "[149/500] train_loss: 0.05955 valid_loss: 0.07443 test_loss: 0.08184 \n",
      "[150/500] train_loss: 0.05926 valid_loss: 0.07410 test_loss: 0.08113 \n",
      "验证损失减少 (0.074108 --> 0.074102). 正在保存模型...\n",
      "[151/500] train_loss: 0.06018 valid_loss: 0.07459 test_loss: 0.08088 \n",
      "[152/500] train_loss: 0.05966 valid_loss: 0.07645 test_loss: 0.08155 \n",
      "[153/500] train_loss: 0.05828 valid_loss: 0.07484 test_loss: 0.08057 \n",
      "[154/500] train_loss: 0.05918 valid_loss: 0.07491 test_loss: 0.08073 \n",
      "[155/500] train_loss: 0.06001 valid_loss: 0.07773 test_loss: 0.08165 \n",
      "[156/500] train_loss: 0.06041 valid_loss: 0.07489 test_loss: 0.08072 \n",
      "[157/500] train_loss: 0.05927 valid_loss: 0.07433 test_loss: 0.08128 \n",
      "[158/500] train_loss: 0.05944 valid_loss: 0.07607 test_loss: 0.08147 \n",
      "[159/500] train_loss: 0.05897 valid_loss: 0.07616 test_loss: 0.08120 \n",
      "[160/500] train_loss: 0.05940 valid_loss: 0.07506 test_loss: 0.08231 \n",
      "[161/500] train_loss: 0.05834 valid_loss: 0.07533 test_loss: 0.08284 \n",
      "[162/500] train_loss: 0.05809 valid_loss: 0.07441 test_loss: 0.08127 \n",
      "[163/500] train_loss: 0.05675 valid_loss: 0.07379 test_loss: 0.08212 \n",
      "验证损失减少 (0.074102 --> 0.073789). 正在保存模型...\n",
      "[164/500] train_loss: 0.05931 valid_loss: 0.07345 test_loss: 0.08126 \n",
      "验证损失减少 (0.073789 --> 0.073449). 正在保存模型...\n",
      "[165/500] train_loss: 0.05754 valid_loss: 0.07437 test_loss: 0.08139 \n",
      "[166/500] train_loss: 0.05709 valid_loss: 0.07620 test_loss: 0.08208 \n",
      "[167/500] train_loss: 0.05748 valid_loss: 0.07722 test_loss: 0.08197 \n",
      "[168/500] train_loss: 0.05930 valid_loss: 0.07541 test_loss: 0.08245 \n",
      "[169/500] train_loss: 0.05774 valid_loss: 0.07622 test_loss: 0.08184 \n",
      "[170/500] train_loss: 0.05700 valid_loss: 0.07523 test_loss: 0.08288 \n",
      "[171/500] train_loss: 0.05779 valid_loss: 0.07754 test_loss: 0.08255 \n",
      "[172/500] train_loss: 0.05794 valid_loss: 0.07605 test_loss: 0.08180 \n",
      "[173/500] train_loss: 0.05681 valid_loss: 0.07537 test_loss: 0.08072 \n",
      "[174/500] train_loss: 0.05784 valid_loss: 0.07553 test_loss: 0.08110 \n",
      "[175/500] train_loss: 0.05675 valid_loss: 0.07556 test_loss: 0.08159 \n",
      "[176/500] train_loss: 0.05634 valid_loss: 0.07455 test_loss: 0.08143 \n",
      "[177/500] train_loss: 0.05851 valid_loss: 0.07454 test_loss: 0.08122 \n",
      "[178/500] train_loss: 0.05627 valid_loss: 0.07349 test_loss: 0.08013 \n",
      "[179/500] train_loss: 0.05601 valid_loss: 0.07470 test_loss: 0.07998 \n",
      "[180/500] train_loss: 0.05672 valid_loss: 0.07383 test_loss: 0.08068 \n",
      "[181/500] train_loss: 0.05660 valid_loss: 0.07420 test_loss: 0.07952 \n",
      "[182/500] train_loss: 0.05726 valid_loss: 0.07472 test_loss: 0.07989 \n",
      "[183/500] train_loss: 0.05655 valid_loss: 0.07538 test_loss: 0.08089 \n",
      "[184/500] train_loss: 0.05608 valid_loss: 0.07474 test_loss: 0.08032 \n",
      "[185/500] train_loss: 0.05446 valid_loss: 0.07486 test_loss: 0.08074 \n",
      "[186/500] train_loss: 0.05649 valid_loss: 0.07387 test_loss: 0.08123 \n",
      "[187/500] train_loss: 0.05596 valid_loss: 0.07435 test_loss: 0.07985 \n",
      "[188/500] train_loss: 0.05640 valid_loss: 0.07465 test_loss: 0.08089 \n",
      "[189/500] train_loss: 0.05614 valid_loss: 0.07385 test_loss: 0.08068 \n",
      "[190/500] train_loss: 0.05430 valid_loss: 0.07108 test_loss: 0.07903 \n",
      "验证损失减少 (0.073449 --> 0.071077). 正在保存模型...\n",
      "[191/500] train_loss: 0.05651 valid_loss: 0.07352 test_loss: 0.08028 \n",
      "[192/500] train_loss: 0.05550 valid_loss: 0.07339 test_loss: 0.07960 \n",
      "[193/500] train_loss: 0.05506 valid_loss: 0.07418 test_loss: 0.07914 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[194/500] train_loss: 0.05748 valid_loss: 0.07339 test_loss: 0.07810 \n",
      "[195/500] train_loss: 0.05574 valid_loss: 0.07454 test_loss: 0.07969 \n",
      "[196/500] train_loss: 0.05563 valid_loss: 0.07424 test_loss: 0.08024 \n",
      "[197/500] train_loss: 0.05564 valid_loss: 0.07396 test_loss: 0.08193 \n",
      "[198/500] train_loss: 0.05684 valid_loss: 0.07433 test_loss: 0.07976 \n",
      "[199/500] train_loss: 0.05522 valid_loss: 0.07495 test_loss: 0.08102 \n",
      "[200/500] train_loss: 0.05436 valid_loss: 0.07816 test_loss: 0.08279 \n",
      "[201/500] train_loss: 0.05516 valid_loss: 0.07494 test_loss: 0.07946 \n",
      "[202/500] train_loss: 0.05444 valid_loss: 0.07617 test_loss: 0.08011 \n",
      "[203/500] train_loss: 0.05572 valid_loss: 0.07560 test_loss: 0.07982 \n",
      "[204/500] train_loss: 0.05377 valid_loss: 0.07468 test_loss: 0.07945 \n",
      "[205/500] train_loss: 0.05614 valid_loss: 0.07486 test_loss: 0.07927 \n",
      "[206/500] train_loss: 0.05645 valid_loss: 0.07386 test_loss: 0.07915 \n",
      "[207/500] train_loss: 0.05503 valid_loss: 0.07562 test_loss: 0.08023 \n",
      "[208/500] train_loss: 0.05505 valid_loss: 0.07529 test_loss: 0.08006 \n",
      "[209/500] train_loss: 0.05515 valid_loss: 0.07656 test_loss: 0.08129 \n",
      "[210/500] train_loss: 0.05254 valid_loss: 0.07528 test_loss: 0.07974 \n",
      "[211/500] train_loss: 0.05341 valid_loss: 0.07346 test_loss: 0.07862 \n",
      "[212/500] train_loss: 0.05392 valid_loss: 0.07492 test_loss: 0.07998 \n",
      "[213/500] train_loss: 0.05577 valid_loss: 0.07388 test_loss: 0.07918 \n",
      "[214/500] train_loss: 0.05377 valid_loss: 0.07696 test_loss: 0.08114 \n",
      "[215/500] train_loss: 0.05434 valid_loss: 0.07608 test_loss: 0.07937 \n",
      "[216/500] train_loss: 0.05388 valid_loss: 0.07390 test_loss: 0.08008 \n",
      "[217/500] train_loss: 0.05530 valid_loss: 0.07682 test_loss: 0.08033 \n",
      "[218/500] train_loss: 0.05267 valid_loss: 0.07495 test_loss: 0.08007 \n",
      "[219/500] train_loss: 0.05304 valid_loss: 0.07478 test_loss: 0.07984 \n",
      "[220/500] train_loss: 0.05252 valid_loss: 0.07596 test_loss: 0.08033 \n",
      "[221/500] train_loss: 0.05347 valid_loss: 0.07511 test_loss: 0.07939 \n",
      "[222/500] train_loss: 0.05296 valid_loss: 0.07583 test_loss: 0.08178 \n",
      "[223/500] train_loss: 0.05356 valid_loss: 0.07633 test_loss: 0.08000 \n",
      "[224/500] train_loss: 0.05327 valid_loss: 0.07621 test_loss: 0.08106 \n",
      "[225/500] train_loss: 0.05460 valid_loss: 0.07480 test_loss: 0.08055 \n",
      "[226/500] train_loss: 0.05237 valid_loss: 0.07658 test_loss: 0.08079 \n",
      "[227/500] train_loss: 0.05331 valid_loss: 0.07566 test_loss: 0.08284 \n",
      "[228/500] train_loss: 0.05336 valid_loss: 0.07487 test_loss: 0.07951 \n",
      "[229/500] train_loss: 0.05282 valid_loss: 0.07478 test_loss: 0.08017 \n",
      "[230/500] train_loss: 0.05112 valid_loss: 0.07760 test_loss: 0.08165 \n",
      "[231/500] train_loss: 0.05298 valid_loss: 0.07625 test_loss: 0.08266 \n",
      "[232/500] train_loss: 0.05217 valid_loss: 0.07742 test_loss: 0.08202 \n",
      "[233/500] train_loss: 0.05236 valid_loss: 0.07641 test_loss: 0.08231 \n",
      "[234/500] train_loss: 0.05312 valid_loss: 0.07442 test_loss: 0.08115 \n",
      "[235/500] train_loss: 0.05203 valid_loss: 0.07660 test_loss: 0.08148 \n",
      "[236/500] train_loss: 0.05299 valid_loss: 0.07399 test_loss: 0.08015 \n",
      "[237/500] train_loss: 0.05187 valid_loss: 0.07526 test_loss: 0.07998 \n",
      "[238/500] train_loss: 0.05089 valid_loss: 0.07410 test_loss: 0.08179 \n",
      "[239/500] train_loss: 0.05073 valid_loss: 0.07653 test_loss: 0.08101 \n",
      "[240/500] train_loss: 0.05109 valid_loss: 0.07636 test_loss: 0.08139 \n",
      "[241/500] train_loss: 0.05143 valid_loss: 0.07500 test_loss: 0.08024 \n",
      "[242/500] train_loss: 0.05237 valid_loss: 0.07612 test_loss: 0.08021 \n",
      "[243/500] train_loss: 0.05062 valid_loss: 0.07608 test_loss: 0.08082 \n",
      "[244/500] train_loss: 0.05133 valid_loss: 0.07581 test_loss: 0.08055 \n",
      "[245/500] train_loss: 0.05341 valid_loss: 0.07608 test_loss: 0.07915 \n",
      "[246/500] train_loss: 0.05322 valid_loss: 0.07517 test_loss: 0.07975 \n",
      "[247/500] train_loss: 0.05128 valid_loss: 0.07416 test_loss: 0.08021 \n",
      "[248/500] train_loss: 0.05187 valid_loss: 0.07737 test_loss: 0.08093 \n",
      "[249/500] train_loss: 0.05168 valid_loss: 0.07550 test_loss: 0.07988 \n",
      "[250/500] train_loss: 0.05122 valid_loss: 0.07516 test_loss: 0.08129 \n",
      "[251/500] train_loss: 0.05103 valid_loss: 0.07563 test_loss: 0.07906 \n",
      "[252/500] train_loss: 0.05042 valid_loss: 0.07657 test_loss: 0.08343 \n",
      "[253/500] train_loss: 0.05270 valid_loss: 0.07338 test_loss: 0.08077 \n",
      "[254/500] train_loss: 0.05120 valid_loss: 0.07349 test_loss: 0.08026 \n",
      "[255/500] train_loss: 0.04975 valid_loss: 0.07507 test_loss: 0.07920 \n",
      "[256/500] train_loss: 0.05019 valid_loss: 0.07462 test_loss: 0.08115 \n",
      "[257/500] train_loss: 0.05079 valid_loss: 0.07442 test_loss: 0.07989 \n",
      "[258/500] train_loss: 0.05053 valid_loss: 0.07273 test_loss: 0.08059 \n",
      "[259/500] train_loss: 0.05152 valid_loss: 0.07489 test_loss: 0.08058 \n",
      "[260/500] train_loss: 0.05079 valid_loss: 0.07487 test_loss: 0.07955 \n",
      "[261/500] train_loss: 0.05177 valid_loss: 0.07431 test_loss: 0.08028 \n",
      "[262/500] train_loss: 0.05031 valid_loss: 0.07344 test_loss: 0.07843 \n",
      "[263/500] train_loss: 0.05263 valid_loss: 0.07554 test_loss: 0.07745 \n",
      "[264/500] train_loss: 0.04906 valid_loss: 0.07416 test_loss: 0.07961 \n",
      "[265/500] train_loss: 0.05097 valid_loss: 0.07406 test_loss: 0.08037 \n",
      "[266/500] train_loss: 0.05080 valid_loss: 0.07499 test_loss: 0.08011 \n",
      "[267/500] train_loss: 0.05133 valid_loss: 0.07433 test_loss: 0.08024 \n",
      "[268/500] train_loss: 0.05120 valid_loss: 0.07450 test_loss: 0.07994 \n",
      "[269/500] train_loss: 0.04998 valid_loss: 0.07316 test_loss: 0.07824 \n",
      "[270/500] train_loss: 0.04949 valid_loss: 0.07832 test_loss: 0.07936 \n",
      "[271/500] train_loss: 0.05123 valid_loss: 0.07341 test_loss: 0.07933 \n",
      "[272/500] train_loss: 0.05063 valid_loss: 0.07423 test_loss: 0.08039 \n",
      "[273/500] train_loss: 0.05010 valid_loss: 0.07913 test_loss: 0.08146 \n",
      "[274/500] train_loss: 0.05038 valid_loss: 0.07527 test_loss: 0.07922 \n",
      "[275/500] train_loss: 0.04921 valid_loss: 0.07506 test_loss: 0.08031 \n",
      "[276/500] train_loss: 0.04964 valid_loss: 0.07351 test_loss: 0.08071 \n",
      "[277/500] train_loss: 0.04988 valid_loss: 0.07397 test_loss: 0.07977 \n",
      "[278/500] train_loss: 0.04949 valid_loss: 0.07518 test_loss: 0.08017 \n",
      "[279/500] train_loss: 0.05041 valid_loss: 0.07747 test_loss: 0.08148 \n",
      "[280/500] train_loss: 0.04867 valid_loss: 0.07555 test_loss: 0.08021 \n",
      "[281/500] train_loss: 0.04919 valid_loss: 0.07372 test_loss: 0.07905 \n",
      "[282/500] train_loss: 0.04961 valid_loss: 0.07404 test_loss: 0.08058 \n",
      "[283/500] train_loss: 0.04879 valid_loss: 0.07581 test_loss: 0.08069 \n",
      "[284/500] train_loss: 0.05030 valid_loss: 0.07896 test_loss: 0.08060 \n",
      "[285/500] train_loss: 0.05070 valid_loss: 0.07623 test_loss: 0.08023 \n",
      "[286/500] train_loss: 0.04904 valid_loss: 0.07411 test_loss: 0.07972 \n",
      "[287/500] train_loss: 0.04859 valid_loss: 0.07300 test_loss: 0.07962 \n",
      "[288/500] train_loss: 0.04880 valid_loss: 0.07400 test_loss: 0.07929 \n",
      "[289/500] train_loss: 0.05060 valid_loss: 0.07253 test_loss: 0.07983 \n",
      "[290/500] train_loss: 0.04840 valid_loss: 0.07442 test_loss: 0.08169 \n",
      "[291/500] train_loss: 0.04843 valid_loss: 0.07481 test_loss: 0.08305 \n",
      "[292/500] train_loss: 0.05026 valid_loss: 0.07768 test_loss: 0.08094 \n",
      "[293/500] train_loss: 0.04963 valid_loss: 0.07389 test_loss: 0.08110 \n",
      "[294/500] train_loss: 0.04762 valid_loss: 0.07446 test_loss: 0.08347 \n",
      "[295/500] train_loss: 0.04902 valid_loss: 0.07625 test_loss: 0.08128 \n",
      "[296/500] train_loss: 0.04800 valid_loss: 0.07559 test_loss: 0.08094 \n",
      "[297/500] train_loss: 0.04825 valid_loss: 0.07530 test_loss: 0.08088 \n",
      "[298/500] train_loss: 0.04851 valid_loss: 0.07505 test_loss: 0.08215 \n",
      "[299/500] train_loss: 0.04901 valid_loss: 0.07365 test_loss: 0.08157 \n",
      "[300/500] train_loss: 0.04803 valid_loss: 0.07541 test_loss: 0.08108 \n",
      "[301/500] train_loss: 0.04908 valid_loss: 0.07329 test_loss: 0.08007 \n",
      "[302/500] train_loss: 0.04804 valid_loss: 0.07453 test_loss: 0.08085 \n",
      "[303/500] train_loss: 0.04927 valid_loss: 0.07393 test_loss: 0.08064 \n",
      "[304/500] train_loss: 0.04838 valid_loss: 0.07415 test_loss: 0.08204 \n",
      "[305/500] train_loss: 0.04759 valid_loss: 0.07298 test_loss: 0.08107 \n",
      "[306/500] train_loss: 0.04824 valid_loss: 0.07316 test_loss: 0.08137 \n",
      "[307/500] train_loss: 0.04824 valid_loss: 0.07452 test_loss: 0.08076 \n",
      "[308/500] train_loss: 0.04839 valid_loss: 0.07276 test_loss: 0.07965 \n",
      "[309/500] train_loss: 0.04756 valid_loss: 0.07389 test_loss: 0.08031 \n",
      "[310/500] train_loss: 0.04897 valid_loss: 0.07403 test_loss: 0.08172 \n",
      "[311/500] train_loss: 0.04899 valid_loss: 0.07341 test_loss: 0.08100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312/500] train_loss: 0.04784 valid_loss: 0.07270 test_loss: 0.08040 \n",
      "[313/500] train_loss: 0.04724 valid_loss: 0.07441 test_loss: 0.08056 \n",
      "[314/500] train_loss: 0.04818 valid_loss: 0.07460 test_loss: 0.08072 \n",
      "[315/500] train_loss: 0.04746 valid_loss: 0.07302 test_loss: 0.08102 \n",
      "[316/500] train_loss: 0.04829 valid_loss: 0.07694 test_loss: 0.08053 \n",
      "[317/500] train_loss: 0.04828 valid_loss: 0.07461 test_loss: 0.08113 \n",
      "[318/500] train_loss: 0.04701 valid_loss: 0.07422 test_loss: 0.08031 \n",
      "[319/500] train_loss: 0.04827 valid_loss: 0.07343 test_loss: 0.08002 \n",
      "[320/500] train_loss: 0.04989 valid_loss: 0.07468 test_loss: 0.08078 \n",
      "[321/500] train_loss: 0.04681 valid_loss: 0.07441 test_loss: 0.08175 \n",
      "[322/500] train_loss: 0.04692 valid_loss: 0.07477 test_loss: 0.08156 \n",
      "[323/500] train_loss: 0.04890 valid_loss: 0.07539 test_loss: 0.08145 \n",
      "[324/500] train_loss: 0.04871 valid_loss: 0.07582 test_loss: 0.08043 \n",
      "[325/500] train_loss: 0.04819 valid_loss: 0.07604 test_loss: 0.07970 \n",
      "[326/500] train_loss: 0.04663 valid_loss: 0.07459 test_loss: 0.08105 \n",
      "[327/500] train_loss: 0.04802 valid_loss: 0.07359 test_loss: 0.08141 \n",
      "[328/500] train_loss: 0.04848 valid_loss: 0.07457 test_loss: 0.08061 \n",
      "[329/500] train_loss: 0.04754 valid_loss: 0.07449 test_loss: 0.08042 \n",
      "[330/500] train_loss: 0.04601 valid_loss: 0.07257 test_loss: 0.08085 \n",
      "[331/500] train_loss: 0.04767 valid_loss: 0.07461 test_loss: 0.08274 \n",
      "[332/500] train_loss: 0.04777 valid_loss: 0.07585 test_loss: 0.08191 \n",
      "[333/500] train_loss: 0.04669 valid_loss: 0.07352 test_loss: 0.08180 \n",
      "[334/500] train_loss: 0.04784 valid_loss: 0.07361 test_loss: 0.08225 \n",
      "[335/500] train_loss: 0.04709 valid_loss: 0.07447 test_loss: 0.08180 \n",
      "[336/500] train_loss: 0.04701 valid_loss: 0.07426 test_loss: 0.08100 \n",
      "[337/500] train_loss: 0.04696 valid_loss: 0.07558 test_loss: 0.08231 \n",
      "[338/500] train_loss: 0.04618 valid_loss: 0.07613 test_loss: 0.08260 \n",
      "[339/500] train_loss: 0.04650 valid_loss: 0.07507 test_loss: 0.08212 \n",
      "[340/500] train_loss: 0.04806 valid_loss: 0.07299 test_loss: 0.08064 \n",
      "[341/500] train_loss: 0.04613 valid_loss: 0.07205 test_loss: 0.08049 \n",
      "[342/500] train_loss: 0.04705 valid_loss: 0.07661 test_loss: 0.08241 \n",
      "[343/500] train_loss: 0.04621 valid_loss: 0.07257 test_loss: 0.08041 \n",
      "[344/500] train_loss: 0.04631 valid_loss: 0.07515 test_loss: 0.08112 \n",
      "[345/500] train_loss: 0.04803 valid_loss: 0.07439 test_loss: 0.08000 \n",
      "[346/500] train_loss: 0.04630 valid_loss: 0.07489 test_loss: 0.08200 \n",
      "[347/500] train_loss: 0.04675 valid_loss: 0.07430 test_loss: 0.08221 \n",
      "[348/500] train_loss: 0.04712 valid_loss: 0.07661 test_loss: 0.08139 \n",
      "[349/500] train_loss: 0.04770 valid_loss: 0.07362 test_loss: 0.08111 \n",
      "[350/500] train_loss: 0.04570 valid_loss: 0.07403 test_loss: 0.08233 \n",
      "[351/500] train_loss: 0.04539 valid_loss: 0.07408 test_loss: 0.07948 \n",
      "[352/500] train_loss: 0.04683 valid_loss: 0.07622 test_loss: 0.08084 \n",
      "[353/500] train_loss: 0.04760 valid_loss: 0.07563 test_loss: 0.08062 \n",
      "[354/500] train_loss: 0.04734 valid_loss: 0.07523 test_loss: 0.08152 \n",
      "[355/500] train_loss: 0.04690 valid_loss: 0.07595 test_loss: 0.08091 \n",
      "[356/500] train_loss: 0.04614 valid_loss: 0.07691 test_loss: 0.08016 \n",
      "[357/500] train_loss: 0.04527 valid_loss: 0.07477 test_loss: 0.08010 \n",
      "[358/500] train_loss: 0.04496 valid_loss: 0.07502 test_loss: 0.08251 \n",
      "[359/500] train_loss: 0.04561 valid_loss: 0.07577 test_loss: 0.08046 \n",
      "[360/500] train_loss: 0.04627 valid_loss: 0.07440 test_loss: 0.07993 \n",
      "[361/500] train_loss: 0.04563 valid_loss: 0.07457 test_loss: 0.08150 \n",
      "[362/500] train_loss: 0.04692 valid_loss: 0.08130 test_loss: 0.08343 \n",
      "[363/500] train_loss: 0.04761 valid_loss: 0.07527 test_loss: 0.08054 \n",
      "[364/500] train_loss: 0.04583 valid_loss: 0.07285 test_loss: 0.08118 \n",
      "[365/500] train_loss: 0.04475 valid_loss: 0.07513 test_loss: 0.08222 \n",
      "[366/500] train_loss: 0.04474 valid_loss: 0.07505 test_loss: 0.08181 \n",
      "[367/500] train_loss: 0.04414 valid_loss: 0.07420 test_loss: 0.08100 \n",
      "[368/500] train_loss: 0.04563 valid_loss: 0.07597 test_loss: 0.08239 \n",
      "[369/500] train_loss: 0.04601 valid_loss: 0.07534 test_loss: 0.08065 \n",
      "[370/500] train_loss: 0.04698 valid_loss: 0.07526 test_loss: 0.08128 \n",
      "[371/500] train_loss: 0.04539 valid_loss: 0.07529 test_loss: 0.08180 \n",
      "[372/500] train_loss: 0.04463 valid_loss: 0.07388 test_loss: 0.08157 \n",
      "[373/500] train_loss: 0.04556 valid_loss: 0.07534 test_loss: 0.08145 \n",
      "[374/500] train_loss: 0.04459 valid_loss: 0.07456 test_loss: 0.08116 \n",
      "[375/500] train_loss: 0.04479 valid_loss: 0.07580 test_loss: 0.08128 \n",
      "[376/500] train_loss: 0.04439 valid_loss: 0.07320 test_loss: 0.07956 \n",
      "[377/500] train_loss: 0.04522 valid_loss: 0.07425 test_loss: 0.07956 \n",
      "[378/500] train_loss: 0.04431 valid_loss: 0.07433 test_loss: 0.08032 \n",
      "[379/500] train_loss: 0.04480 valid_loss: 0.07351 test_loss: 0.08076 \n",
      "[380/500] train_loss: 0.04509 valid_loss: 0.07401 test_loss: 0.08020 \n",
      "[381/500] train_loss: 0.04520 valid_loss: 0.07406 test_loss: 0.08164 \n",
      "[382/500] train_loss: 0.04530 valid_loss: 0.07368 test_loss: 0.08126 \n",
      "[383/500] train_loss: 0.04459 valid_loss: 0.07483 test_loss: 0.08159 \n",
      "[384/500] train_loss: 0.04442 valid_loss: 0.07522 test_loss: 0.08099 \n",
      "[385/500] train_loss: 0.04596 valid_loss: 0.07535 test_loss: 0.08068 \n",
      "[386/500] train_loss: 0.04507 valid_loss: 0.07260 test_loss: 0.08124 \n",
      "[387/500] train_loss: 0.04429 valid_loss: 0.07397 test_loss: 0.08037 \n",
      "[388/500] train_loss: 0.04551 valid_loss: 0.07426 test_loss: 0.08170 \n",
      "[389/500] train_loss: 0.04408 valid_loss: 0.07293 test_loss: 0.08212 \n",
      "[390/500] train_loss: 0.04412 valid_loss: 0.07389 test_loss: 0.08353 \n",
      "[391/500] train_loss: 0.04393 valid_loss: 0.07645 test_loss: 0.07982 \n",
      "[392/500] train_loss: 0.04555 valid_loss: 0.07443 test_loss: 0.07977 \n",
      "[393/500] train_loss: 0.04469 valid_loss: 0.07491 test_loss: 0.08148 \n",
      "[394/500] train_loss: 0.04479 valid_loss: 0.07525 test_loss: 0.08017 \n",
      "[395/500] train_loss: 0.04442 valid_loss: 0.07570 test_loss: 0.08007 \n",
      "[396/500] train_loss: 0.04472 valid_loss: 0.07472 test_loss: 0.08001 \n",
      "[397/500] train_loss: 0.04262 valid_loss: 0.07484 test_loss: 0.07965 \n",
      "[398/500] train_loss: 0.04374 valid_loss: 0.07372 test_loss: 0.07977 \n",
      "[399/500] train_loss: 0.04480 valid_loss: 0.07379 test_loss: 0.08124 \n",
      "[400/500] train_loss: 0.04503 valid_loss: 0.07519 test_loss: 0.08107 \n",
      "[401/500] train_loss: 0.04490 valid_loss: 0.07467 test_loss: 0.08204 \n",
      "[402/500] train_loss: 0.04494 valid_loss: 0.07345 test_loss: 0.08089 \n",
      "[403/500] train_loss: 0.04414 valid_loss: 0.07505 test_loss: 0.08180 \n",
      "[404/500] train_loss: 0.04344 valid_loss: 0.07470 test_loss: 0.08115 \n",
      "[405/500] train_loss: 0.04526 valid_loss: 0.07626 test_loss: 0.08150 \n",
      "[406/500] train_loss: 0.04450 valid_loss: 0.07696 test_loss: 0.08142 \n",
      "[407/500] train_loss: 0.04482 valid_loss: 0.07597 test_loss: 0.08278 \n",
      "[408/500] train_loss: 0.04383 valid_loss: 0.07558 test_loss: 0.08040 \n",
      "[409/500] train_loss: 0.04391 valid_loss: 0.07861 test_loss: 0.08155 \n",
      "[410/500] train_loss: 0.04499 valid_loss: 0.07434 test_loss: 0.08216 \n",
      "[411/500] train_loss: 0.04369 valid_loss: 0.07395 test_loss: 0.08106 \n",
      "[412/500] train_loss: 0.04275 valid_loss: 0.07582 test_loss: 0.07988 \n",
      "[413/500] train_loss: 0.04393 valid_loss: 0.07590 test_loss: 0.08102 \n",
      "[414/500] train_loss: 0.04505 valid_loss: 0.07624 test_loss: 0.08020 \n",
      "[415/500] train_loss: 0.04388 valid_loss: 0.07520 test_loss: 0.08016 \n",
      "[416/500] train_loss: 0.04376 valid_loss: 0.07578 test_loss: 0.08061 \n",
      "[417/500] train_loss: 0.04363 valid_loss: 0.07348 test_loss: 0.08119 \n",
      "[418/500] train_loss: 0.04300 valid_loss: 0.07475 test_loss: 0.08091 \n",
      "[419/500] train_loss: 0.04472 valid_loss: 0.07655 test_loss: 0.08184 \n",
      "[420/500] train_loss: 0.04454 valid_loss: 0.07524 test_loss: 0.07992 \n",
      "[421/500] train_loss: 0.04438 valid_loss: 0.07602 test_loss: 0.08005 \n",
      "[422/500] train_loss: 0.04391 valid_loss: 0.07621 test_loss: 0.08251 \n",
      "[423/500] train_loss: 0.04438 valid_loss: 0.07392 test_loss: 0.07994 \n",
      "[424/500] train_loss: 0.04494 valid_loss: 0.07507 test_loss: 0.08055 \n",
      "[425/500] train_loss: 0.04290 valid_loss: 0.07651 test_loss: 0.08139 \n",
      "[426/500] train_loss: 0.04281 valid_loss: 0.07525 test_loss: 0.08255 \n",
      "[427/500] train_loss: 0.04250 valid_loss: 0.07426 test_loss: 0.08336 \n",
      "[428/500] train_loss: 0.04287 valid_loss: 0.07560 test_loss: 0.08311 \n",
      "[429/500] train_loss: 0.04418 valid_loss: 0.07550 test_loss: 0.08244 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[430/500] train_loss: 0.04424 valid_loss: 0.07553 test_loss: 0.08128 \n",
      "[431/500] train_loss: 0.04318 valid_loss: 0.07648 test_loss: 0.08175 \n",
      "[432/500] train_loss: 0.04296 valid_loss: 0.07604 test_loss: 0.08223 \n",
      "[433/500] train_loss: 0.04308 valid_loss: 0.07689 test_loss: 0.08314 \n",
      "[434/500] train_loss: 0.04331 valid_loss: 0.07544 test_loss: 0.08140 \n",
      "[435/500] train_loss: 0.04387 valid_loss: 0.07445 test_loss: 0.08352 \n",
      "[436/500] train_loss: 0.04357 valid_loss: 0.07562 test_loss: 0.08114 \n",
      "[437/500] train_loss: 0.04297 valid_loss: 0.07406 test_loss: 0.08114 \n",
      "[438/500] train_loss: 0.04335 valid_loss: 0.07404 test_loss: 0.08148 \n",
      "[439/500] train_loss: 0.04349 valid_loss: 0.07555 test_loss: 0.08157 \n",
      "[440/500] train_loss: 0.04359 valid_loss: 0.07455 test_loss: 0.08180 \n",
      "[441/500] train_loss: 0.04309 valid_loss: 0.07463 test_loss: 0.08177 \n",
      "[442/500] train_loss: 0.04343 valid_loss: 0.07419 test_loss: 0.08102 \n",
      "[443/500] train_loss: 0.04226 valid_loss: 0.07572 test_loss: 0.08179 \n",
      "[444/500] train_loss: 0.04280 valid_loss: 0.07554 test_loss: 0.08292 \n",
      "[445/500] train_loss: 0.04360 valid_loss: 0.07620 test_loss: 0.08156 \n",
      "[446/500] train_loss: 0.04318 valid_loss: 0.07922 test_loss: 0.08497 \n",
      "[447/500] train_loss: 0.04340 valid_loss: 0.07504 test_loss: 0.08214 \n",
      "[448/500] train_loss: 0.04318 valid_loss: 0.07524 test_loss: 0.08180 \n",
      "[449/500] train_loss: 0.04299 valid_loss: 0.07453 test_loss: 0.08168 \n",
      "[450/500] train_loss: 0.04440 valid_loss: 0.07440 test_loss: 0.08193 \n",
      "[451/500] train_loss: 0.04252 valid_loss: 0.07482 test_loss: 0.08362 \n",
      "[452/500] train_loss: 0.04318 valid_loss: 0.07408 test_loss: 0.08202 \n",
      "[453/500] train_loss: 0.04183 valid_loss: 0.07510 test_loss: 0.08040 \n",
      "[454/500] train_loss: 0.04180 valid_loss: 0.07612 test_loss: 0.08200 \n",
      "[455/500] train_loss: 0.04275 valid_loss: 0.07427 test_loss: 0.07994 \n",
      "[456/500] train_loss: 0.04207 valid_loss: 0.07350 test_loss: 0.08092 \n",
      "[457/500] train_loss: 0.04160 valid_loss: 0.07367 test_loss: 0.08054 \n",
      "[458/500] train_loss: 0.04288 valid_loss: 0.07366 test_loss: 0.08246 \n",
      "[459/500] train_loss: 0.04282 valid_loss: 0.07542 test_loss: 0.08044 \n",
      "[460/500] train_loss: 0.04234 valid_loss: 0.07522 test_loss: 0.07999 \n",
      "[461/500] train_loss: 0.04239 valid_loss: 0.07474 test_loss: 0.08023 \n",
      "[462/500] train_loss: 0.04260 valid_loss: 0.07485 test_loss: 0.08072 \n",
      "[463/500] train_loss: 0.04277 valid_loss: 0.07598 test_loss: 0.08245 \n",
      "[464/500] train_loss: 0.04265 valid_loss: 0.07497 test_loss: 0.08377 \n",
      "[465/500] train_loss: 0.04221 valid_loss: 0.07461 test_loss: 0.08247 \n",
      "[466/500] train_loss: 0.04266 valid_loss: 0.07454 test_loss: 0.08117 \n",
      "[467/500] train_loss: 0.04207 valid_loss: 0.07462 test_loss: 0.08156 \n",
      "[468/500] train_loss: 0.04185 valid_loss: 0.07388 test_loss: 0.08170 \n",
      "[469/500] train_loss: 0.04272 valid_loss: 0.07551 test_loss: 0.08195 \n",
      "[470/500] train_loss: 0.04111 valid_loss: 0.07302 test_loss: 0.08235 \n",
      "[471/500] train_loss: 0.04218 valid_loss: 0.07453 test_loss: 0.08131 \n",
      "[472/500] train_loss: 0.04298 valid_loss: 0.07451 test_loss: 0.08205 \n",
      "[473/500] train_loss: 0.04271 valid_loss: 0.07483 test_loss: 0.08237 \n",
      "[474/500] train_loss: 0.04150 valid_loss: 0.07406 test_loss: 0.08284 \n",
      "[475/500] train_loss: 0.04165 valid_loss: 0.07518 test_loss: 0.08269 \n",
      "[476/500] train_loss: 0.04166 valid_loss: 0.07550 test_loss: 0.08303 \n",
      "[477/500] train_loss: 0.04196 valid_loss: 0.07401 test_loss: 0.08401 \n",
      "[478/500] train_loss: 0.04268 valid_loss: 0.07460 test_loss: 0.08079 \n",
      "[479/500] train_loss: 0.04086 valid_loss: 0.07350 test_loss: 0.08111 \n",
      "[480/500] train_loss: 0.04224 valid_loss: 0.07326 test_loss: 0.08227 \n",
      "[481/500] train_loss: 0.04180 valid_loss: 0.07405 test_loss: 0.08074 \n",
      "[482/500] train_loss: 0.04241 valid_loss: 0.07462 test_loss: 0.08239 \n",
      "[483/500] train_loss: 0.04308 valid_loss: 0.07466 test_loss: 0.08309 \n",
      "[484/500] train_loss: 0.04119 valid_loss: 0.07512 test_loss: 0.08170 \n",
      "[485/500] train_loss: 0.04215 valid_loss: 0.07564 test_loss: 0.08180 \n",
      "[486/500] train_loss: 0.04092 valid_loss: 0.07469 test_loss: 0.08217 \n",
      "[487/500] train_loss: 0.04242 valid_loss: 0.07481 test_loss: 0.08179 \n",
      "[488/500] train_loss: 0.04164 valid_loss: 0.07507 test_loss: 0.08216 \n",
      "[489/500] train_loss: 0.04173 valid_loss: 0.07340 test_loss: 0.08212 \n",
      "[490/500] train_loss: 0.04162 valid_loss: 0.07649 test_loss: 0.08301 \n",
      "[491/500] train_loss: 0.04113 valid_loss: 0.07473 test_loss: 0.08251 \n",
      "[492/500] train_loss: 0.04205 valid_loss: 0.07341 test_loss: 0.08193 \n",
      "[493/500] train_loss: 0.04137 valid_loss: 0.07496 test_loss: 0.08376 \n",
      "[494/500] train_loss: 0.04136 valid_loss: 0.07279 test_loss: 0.08214 \n",
      "[495/500] train_loss: 0.04151 valid_loss: 0.07330 test_loss: 0.08234 \n",
      "[496/500] train_loss: 0.04173 valid_loss: 0.07405 test_loss: 0.08434 \n",
      "[497/500] train_loss: 0.04149 valid_loss: 0.07555 test_loss: 0.08424 \n",
      "[498/500] train_loss: 0.04142 valid_loss: 0.07662 test_loss: 0.08735 \n",
      "[499/500] train_loss: 0.04104 valid_loss: 0.07396 test_loss: 0.08424 \n",
      "[500/500] train_loss: 0.04076 valid_loss: 0.07489 test_loss: 0.08376 \n",
      "TRAINING MODEL 7\n",
      "[  1/500] train_loss: 0.38442 valid_loss: 0.25994 test_loss: 0.26809 \n",
      "验证损失减少 (inf --> 0.259939). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19809 valid_loss: 0.18924 test_loss: 0.19819 \n",
      "验证损失减少 (0.259939 --> 0.189238). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15912 valid_loss: 0.15405 test_loss: 0.16291 \n",
      "验证损失减少 (0.189238 --> 0.154047). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13974 valid_loss: 0.13835 test_loss: 0.14947 \n",
      "验证损失减少 (0.154047 --> 0.138347). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12557 valid_loss: 0.13025 test_loss: 0.14125 \n",
      "验证损失减少 (0.138347 --> 0.130254). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12290 valid_loss: 0.12720 test_loss: 0.13514 \n",
      "验证损失减少 (0.130254 --> 0.127203). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11963 valid_loss: 0.12393 test_loss: 0.13044 \n",
      "验证损失减少 (0.127203 --> 0.123930). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11493 valid_loss: 0.11905 test_loss: 0.13040 \n",
      "验证损失减少 (0.123930 --> 0.119049). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11160 valid_loss: 0.11528 test_loss: 0.12441 \n",
      "验证损失减少 (0.119049 --> 0.115280). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10911 valid_loss: 0.11359 test_loss: 0.12270 \n",
      "验证损失减少 (0.115280 --> 0.113595). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10566 valid_loss: 0.11030 test_loss: 0.12021 \n",
      "验证损失减少 (0.113595 --> 0.110299). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10544 valid_loss: 0.10798 test_loss: 0.11839 \n",
      "验证损失减少 (0.110299 --> 0.107981). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10320 valid_loss: 0.10556 test_loss: 0.11438 \n",
      "验证损失减少 (0.107981 --> 0.105558). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10104 valid_loss: 0.10674 test_loss: 0.11442 \n",
      "[ 15/500] train_loss: 0.10094 valid_loss: 0.10437 test_loss: 0.11363 \n",
      "验证损失减少 (0.105558 --> 0.104373). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09954 valid_loss: 0.10044 test_loss: 0.11127 \n",
      "验证损失减少 (0.104373 --> 0.100442). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09928 valid_loss: 0.09839 test_loss: 0.10860 \n",
      "验证损失减少 (0.100442 --> 0.098388). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09703 valid_loss: 0.09915 test_loss: 0.11048 \n",
      "[ 19/500] train_loss: 0.09617 valid_loss: 0.09632 test_loss: 0.10762 \n",
      "验证损失减少 (0.098388 --> 0.096322). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09272 valid_loss: 0.09661 test_loss: 0.10803 \n",
      "[ 21/500] train_loss: 0.09202 valid_loss: 0.09564 test_loss: 0.10406 \n",
      "验证损失减少 (0.096322 --> 0.095636). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09121 valid_loss: 0.09761 test_loss: 0.10848 \n",
      "[ 23/500] train_loss: 0.09040 valid_loss: 0.09224 test_loss: 0.10427 \n",
      "验证损失减少 (0.095636 --> 0.092244). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09103 valid_loss: 0.09337 test_loss: 0.10136 \n",
      "[ 25/500] train_loss: 0.08773 valid_loss: 0.09159 test_loss: 0.10069 \n",
      "验证损失减少 (0.092244 --> 0.091587). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.08735 valid_loss: 0.09150 test_loss: 0.10105 \n",
      "验证损失减少 (0.091587 --> 0.091502). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.08871 valid_loss: 0.09027 test_loss: 0.10085 \n",
      "验证损失减少 (0.091502 --> 0.090273). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08701 valid_loss: 0.08924 test_loss: 0.10041 \n",
      "验证损失减少 (0.090273 --> 0.089245). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08691 valid_loss: 0.09184 test_loss: 0.10041 \n",
      "[ 30/500] train_loss: 0.08660 valid_loss: 0.09170 test_loss: 0.10250 \n",
      "[ 31/500] train_loss: 0.08353 valid_loss: 0.08821 test_loss: 0.09761 \n",
      "验证损失减少 (0.089245 --> 0.088213). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08468 valid_loss: 0.08896 test_loss: 0.09791 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 33/500] train_loss: 0.08581 valid_loss: 0.08729 test_loss: 0.09559 \n",
      "验证损失减少 (0.088213 --> 0.087293). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08531 valid_loss: 0.08658 test_loss: 0.09575 \n",
      "验证损失减少 (0.087293 --> 0.086584). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08333 valid_loss: 0.08601 test_loss: 0.09487 \n",
      "验证损失减少 (0.086584 --> 0.086013). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08209 valid_loss: 0.08734 test_loss: 0.09533 \n",
      "[ 37/500] train_loss: 0.08346 valid_loss: 0.08600 test_loss: 0.09388 \n",
      "验证损失减少 (0.086013 --> 0.085999). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.08211 valid_loss: 0.08647 test_loss: 0.09627 \n",
      "[ 39/500] train_loss: 0.08175 valid_loss: 0.08469 test_loss: 0.09398 \n",
      "验证损失减少 (0.085999 --> 0.084686). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.07995 valid_loss: 0.08591 test_loss: 0.09337 \n",
      "[ 41/500] train_loss: 0.07994 valid_loss: 0.08606 test_loss: 0.09280 \n",
      "[ 42/500] train_loss: 0.08128 valid_loss: 0.08534 test_loss: 0.09758 \n",
      "[ 43/500] train_loss: 0.07900 valid_loss: 0.08598 test_loss: 0.09348 \n",
      "[ 44/500] train_loss: 0.08075 valid_loss: 0.08410 test_loss: 0.09381 \n",
      "验证损失减少 (0.084686 --> 0.084104). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.07697 valid_loss: 0.08298 test_loss: 0.09058 \n",
      "验证损失减少 (0.084104 --> 0.082984). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07907 valid_loss: 0.08813 test_loss: 0.09193 \n",
      "[ 47/500] train_loss: 0.07937 valid_loss: 0.08280 test_loss: 0.09046 \n",
      "验证损失减少 (0.082984 --> 0.082798). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.07690 valid_loss: 0.08194 test_loss: 0.09160 \n",
      "验证损失减少 (0.082798 --> 0.081944). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07891 valid_loss: 0.08143 test_loss: 0.09074 \n",
      "验证损失减少 (0.081944 --> 0.081429). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07526 valid_loss: 0.08227 test_loss: 0.08965 \n",
      "[ 51/500] train_loss: 0.07742 valid_loss: 0.08238 test_loss: 0.08893 \n",
      "[ 52/500] train_loss: 0.07758 valid_loss: 0.08148 test_loss: 0.09007 \n",
      "[ 53/500] train_loss: 0.07702 valid_loss: 0.08212 test_loss: 0.09002 \n",
      "[ 54/500] train_loss: 0.07597 valid_loss: 0.08059 test_loss: 0.08926 \n",
      "验证损失减少 (0.081429 --> 0.080595). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07734 valid_loss: 0.08059 test_loss: 0.08787 \n",
      "验证损失减少 (0.080595 --> 0.080594). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07571 valid_loss: 0.08098 test_loss: 0.09025 \n",
      "[ 57/500] train_loss: 0.07491 valid_loss: 0.08185 test_loss: 0.08939 \n",
      "[ 58/500] train_loss: 0.07743 valid_loss: 0.08088 test_loss: 0.08959 \n",
      "[ 59/500] train_loss: 0.07290 valid_loss: 0.08134 test_loss: 0.08750 \n",
      "[ 60/500] train_loss: 0.07456 valid_loss: 0.08074 test_loss: 0.08864 \n",
      "[ 61/500] train_loss: 0.07389 valid_loss: 0.08315 test_loss: 0.08942 \n",
      "[ 62/500] train_loss: 0.07404 valid_loss: 0.08156 test_loss: 0.08950 \n",
      "[ 63/500] train_loss: 0.07298 valid_loss: 0.08069 test_loss: 0.08842 \n",
      "[ 64/500] train_loss: 0.07396 valid_loss: 0.07946 test_loss: 0.08790 \n",
      "验证损失减少 (0.080594 --> 0.079460). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07406 valid_loss: 0.08178 test_loss: 0.08741 \n",
      "[ 66/500] train_loss: 0.07344 valid_loss: 0.07982 test_loss: 0.08759 \n",
      "[ 67/500] train_loss: 0.07452 valid_loss: 0.07810 test_loss: 0.08556 \n",
      "验证损失减少 (0.079460 --> 0.078101). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.07213 valid_loss: 0.07980 test_loss: 0.08634 \n",
      "[ 69/500] train_loss: 0.07158 valid_loss: 0.07847 test_loss: 0.08632 \n",
      "[ 70/500] train_loss: 0.07283 valid_loss: 0.07871 test_loss: 0.08641 \n",
      "[ 71/500] train_loss: 0.07234 valid_loss: 0.07820 test_loss: 0.08537 \n",
      "[ 72/500] train_loss: 0.07184 valid_loss: 0.07754 test_loss: 0.08639 \n",
      "验证损失减少 (0.078101 --> 0.077538). 正在保存模型...\n",
      "[ 73/500] train_loss: 0.06937 valid_loss: 0.07689 test_loss: 0.08823 \n",
      "验证损失减少 (0.077538 --> 0.076890). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.07159 valid_loss: 0.07796 test_loss: 0.08802 \n",
      "[ 75/500] train_loss: 0.07101 valid_loss: 0.07809 test_loss: 0.08833 \n",
      "[ 76/500] train_loss: 0.07026 valid_loss: 0.08247 test_loss: 0.08630 \n",
      "[ 77/500] train_loss: 0.07050 valid_loss: 0.07736 test_loss: 0.08688 \n",
      "[ 78/500] train_loss: 0.07152 valid_loss: 0.07568 test_loss: 0.08480 \n",
      "验证损失减少 (0.076890 --> 0.075676). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.06803 valid_loss: 0.07743 test_loss: 0.08583 \n",
      "[ 80/500] train_loss: 0.06917 valid_loss: 0.07810 test_loss: 0.08440 \n",
      "[ 81/500] train_loss: 0.06948 valid_loss: 0.07689 test_loss: 0.08622 \n",
      "[ 82/500] train_loss: 0.06930 valid_loss: 0.07635 test_loss: 0.08569 \n",
      "[ 83/500] train_loss: 0.06832 valid_loss: 0.07677 test_loss: 0.08474 \n",
      "[ 84/500] train_loss: 0.06874 valid_loss: 0.07565 test_loss: 0.08552 \n",
      "验证损失减少 (0.075676 --> 0.075655). 正在保存模型...\n",
      "[ 85/500] train_loss: 0.06951 valid_loss: 0.07773 test_loss: 0.08623 \n",
      "[ 86/500] train_loss: 0.06918 valid_loss: 0.07780 test_loss: 0.08549 \n",
      "[ 87/500] train_loss: 0.06792 valid_loss: 0.07616 test_loss: 0.08320 \n",
      "[ 88/500] train_loss: 0.06768 valid_loss: 0.07735 test_loss: 0.08466 \n",
      "[ 89/500] train_loss: 0.06942 valid_loss: 0.07704 test_loss: 0.08619 \n",
      "[ 90/500] train_loss: 0.06893 valid_loss: 0.07933 test_loss: 0.08335 \n",
      "[ 91/500] train_loss: 0.06750 valid_loss: 0.07844 test_loss: 0.08586 \n",
      "[ 92/500] train_loss: 0.06655 valid_loss: 0.07718 test_loss: 0.08359 \n",
      "[ 93/500] train_loss: 0.06750 valid_loss: 0.07832 test_loss: 0.08408 \n",
      "[ 94/500] train_loss: 0.06667 valid_loss: 0.07647 test_loss: 0.08370 \n",
      "[ 95/500] train_loss: 0.06870 valid_loss: 0.07690 test_loss: 0.08472 \n",
      "[ 96/500] train_loss: 0.06758 valid_loss: 0.07739 test_loss: 0.08555 \n",
      "[ 97/500] train_loss: 0.06573 valid_loss: 0.07608 test_loss: 0.08632 \n",
      "[ 98/500] train_loss: 0.06602 valid_loss: 0.07655 test_loss: 0.08433 \n",
      "[ 99/500] train_loss: 0.06850 valid_loss: 0.07602 test_loss: 0.08504 \n",
      "[100/500] train_loss: 0.06692 valid_loss: 0.07612 test_loss: 0.08454 \n",
      "[101/500] train_loss: 0.06603 valid_loss: 0.07897 test_loss: 0.08289 \n",
      "[102/500] train_loss: 0.06705 valid_loss: 0.07565 test_loss: 0.08499 \n",
      "验证损失减少 (0.075655 --> 0.075652). 正在保存模型...\n",
      "[103/500] train_loss: 0.06503 valid_loss: 0.07565 test_loss: 0.08439 \n",
      "验证损失减少 (0.075652 --> 0.075648). 正在保存模型...\n",
      "[104/500] train_loss: 0.06560 valid_loss: 0.07731 test_loss: 0.08456 \n",
      "[105/500] train_loss: 0.06410 valid_loss: 0.07617 test_loss: 0.08413 \n",
      "[106/500] train_loss: 0.06420 valid_loss: 0.07507 test_loss: 0.08362 \n",
      "验证损失减少 (0.075648 --> 0.075069). 正在保存模型...\n",
      "[107/500] train_loss: 0.06457 valid_loss: 0.07465 test_loss: 0.08456 \n",
      "验证损失减少 (0.075069 --> 0.074652). 正在保存模型...\n",
      "[108/500] train_loss: 0.06588 valid_loss: 0.07623 test_loss: 0.08402 \n",
      "[109/500] train_loss: 0.06408 valid_loss: 0.07592 test_loss: 0.08408 \n",
      "[110/500] train_loss: 0.06526 valid_loss: 0.07479 test_loss: 0.08144 \n",
      "[111/500] train_loss: 0.06293 valid_loss: 0.07540 test_loss: 0.08291 \n",
      "[112/500] train_loss: 0.06606 valid_loss: 0.07682 test_loss: 0.08510 \n",
      "[113/500] train_loss: 0.06244 valid_loss: 0.07801 test_loss: 0.08303 \n",
      "[114/500] train_loss: 0.06306 valid_loss: 0.07771 test_loss: 0.08353 \n",
      "[115/500] train_loss: 0.06336 valid_loss: 0.07571 test_loss: 0.08441 \n",
      "[116/500] train_loss: 0.06469 valid_loss: 0.07398 test_loss: 0.08425 \n",
      "验证损失减少 (0.074652 --> 0.073983). 正在保存模型...\n",
      "[117/500] train_loss: 0.06356 valid_loss: 0.07659 test_loss: 0.08411 \n",
      "[118/500] train_loss: 0.06159 valid_loss: 0.07413 test_loss: 0.08151 \n",
      "[119/500] train_loss: 0.06233 valid_loss: 0.07372 test_loss: 0.08200 \n",
      "验证损失减少 (0.073983 --> 0.073723). 正在保存模型...\n",
      "[120/500] train_loss: 0.06321 valid_loss: 0.07529 test_loss: 0.08229 \n",
      "[121/500] train_loss: 0.06364 valid_loss: 0.07471 test_loss: 0.08445 \n",
      "[122/500] train_loss: 0.06021 valid_loss: 0.07462 test_loss: 0.08295 \n",
      "[123/500] train_loss: 0.06093 valid_loss: 0.07517 test_loss: 0.08343 \n",
      "[124/500] train_loss: 0.06193 valid_loss: 0.07486 test_loss: 0.08140 \n",
      "[125/500] train_loss: 0.06357 valid_loss: 0.07525 test_loss: 0.08134 \n",
      "[126/500] train_loss: 0.06355 valid_loss: 0.07451 test_loss: 0.08229 \n",
      "[127/500] train_loss: 0.06045 valid_loss: 0.07611 test_loss: 0.08469 \n",
      "[128/500] train_loss: 0.06171 valid_loss: 0.07449 test_loss: 0.08277 \n",
      "[129/500] train_loss: 0.06152 valid_loss: 0.07619 test_loss: 0.08286 \n",
      "[130/500] train_loss: 0.06125 valid_loss: 0.07514 test_loss: 0.08231 \n",
      "[131/500] train_loss: 0.06137 valid_loss: 0.07603 test_loss: 0.08139 \n",
      "[132/500] train_loss: 0.06292 valid_loss: 0.07711 test_loss: 0.08338 \n",
      "[133/500] train_loss: 0.06297 valid_loss: 0.07717 test_loss: 0.08202 \n",
      "[134/500] train_loss: 0.06085 valid_loss: 0.07783 test_loss: 0.08423 \n",
      "[135/500] train_loss: 0.06210 valid_loss: 0.07586 test_loss: 0.08249 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136/500] train_loss: 0.06125 valid_loss: 0.07462 test_loss: 0.08168 \n",
      "[137/500] train_loss: 0.06042 valid_loss: 0.07667 test_loss: 0.08514 \n",
      "[138/500] train_loss: 0.06148 valid_loss: 0.07549 test_loss: 0.08205 \n",
      "[139/500] train_loss: 0.05970 valid_loss: 0.07322 test_loss: 0.08261 \n",
      "验证损失减少 (0.073723 --> 0.073215). 正在保存模型...\n",
      "[140/500] train_loss: 0.06159 valid_loss: 0.07687 test_loss: 0.08175 \n",
      "[141/500] train_loss: 0.06058 valid_loss: 0.07835 test_loss: 0.08146 \n",
      "[142/500] train_loss: 0.06089 valid_loss: 0.07598 test_loss: 0.08176 \n",
      "[143/500] train_loss: 0.06094 valid_loss: 0.07790 test_loss: 0.08099 \n",
      "[144/500] train_loss: 0.05919 valid_loss: 0.07558 test_loss: 0.08147 \n",
      "[145/500] train_loss: 0.06018 valid_loss: 0.07264 test_loss: 0.08255 \n",
      "验证损失减少 (0.073215 --> 0.072637). 正在保存模型...\n",
      "[146/500] train_loss: 0.06002 valid_loss: 0.07320 test_loss: 0.08070 \n",
      "[147/500] train_loss: 0.05879 valid_loss: 0.07237 test_loss: 0.08249 \n",
      "验证损失减少 (0.072637 --> 0.072370). 正在保存模型...\n",
      "[148/500] train_loss: 0.05877 valid_loss: 0.07425 test_loss: 0.08038 \n",
      "[149/500] train_loss: 0.06085 valid_loss: 0.07344 test_loss: 0.07946 \n",
      "[150/500] train_loss: 0.06105 valid_loss: 0.07658 test_loss: 0.08023 \n",
      "[151/500] train_loss: 0.05868 valid_loss: 0.07496 test_loss: 0.08176 \n",
      "[152/500] train_loss: 0.06001 valid_loss: 0.07283 test_loss: 0.08114 \n",
      "[153/500] train_loss: 0.05948 valid_loss: 0.07638 test_loss: 0.08103 \n",
      "[154/500] train_loss: 0.05836 valid_loss: 0.07584 test_loss: 0.08096 \n",
      "[155/500] train_loss: 0.05880 valid_loss: 0.07886 test_loss: 0.08353 \n",
      "[156/500] train_loss: 0.05956 valid_loss: 0.07602 test_loss: 0.08080 \n",
      "[157/500] train_loss: 0.05956 valid_loss: 0.07496 test_loss: 0.08210 \n",
      "[158/500] train_loss: 0.05943 valid_loss: 0.07677 test_loss: 0.08253 \n",
      "[159/500] train_loss: 0.05816 valid_loss: 0.07282 test_loss: 0.08143 \n",
      "[160/500] train_loss: 0.05844 valid_loss: 0.07170 test_loss: 0.07993 \n",
      "验证损失减少 (0.072370 --> 0.071695). 正在保存模型...\n",
      "[161/500] train_loss: 0.05817 valid_loss: 0.07333 test_loss: 0.08364 \n",
      "[162/500] train_loss: 0.05929 valid_loss: 0.07273 test_loss: 0.08185 \n",
      "[163/500] train_loss: 0.05845 valid_loss: 0.07282 test_loss: 0.08128 \n",
      "[164/500] train_loss: 0.05850 valid_loss: 0.07218 test_loss: 0.08106 \n",
      "[165/500] train_loss: 0.05787 valid_loss: 0.07264 test_loss: 0.08184 \n",
      "[166/500] train_loss: 0.05889 valid_loss: 0.07226 test_loss: 0.07975 \n",
      "[167/500] train_loss: 0.05868 valid_loss: 0.07484 test_loss: 0.08199 \n",
      "[168/500] train_loss: 0.05749 valid_loss: 0.07260 test_loss: 0.08177 \n",
      "[169/500] train_loss: 0.05644 valid_loss: 0.07250 test_loss: 0.08234 \n",
      "[170/500] train_loss: 0.05600 valid_loss: 0.07434 test_loss: 0.08000 \n",
      "[171/500] train_loss: 0.05830 valid_loss: 0.07365 test_loss: 0.08135 \n",
      "[172/500] train_loss: 0.05910 valid_loss: 0.07143 test_loss: 0.08006 \n",
      "验证损失减少 (0.071695 --> 0.071427). 正在保存模型...\n",
      "[173/500] train_loss: 0.05706 valid_loss: 0.07286 test_loss: 0.08101 \n",
      "[174/500] train_loss: 0.05686 valid_loss: 0.07226 test_loss: 0.08073 \n",
      "[175/500] train_loss: 0.05811 valid_loss: 0.07248 test_loss: 0.08077 \n",
      "[176/500] train_loss: 0.05827 valid_loss: 0.07336 test_loss: 0.08024 \n",
      "[177/500] train_loss: 0.05646 valid_loss: 0.07442 test_loss: 0.08121 \n",
      "[178/500] train_loss: 0.05809 valid_loss: 0.07271 test_loss: 0.08206 \n",
      "[179/500] train_loss: 0.05663 valid_loss: 0.07299 test_loss: 0.08017 \n",
      "[180/500] train_loss: 0.05590 valid_loss: 0.07370 test_loss: 0.07973 \n",
      "[181/500] train_loss: 0.05670 valid_loss: 0.07285 test_loss: 0.08091 \n",
      "[182/500] train_loss: 0.05703 valid_loss: 0.07168 test_loss: 0.08046 \n",
      "[183/500] train_loss: 0.05679 valid_loss: 0.07232 test_loss: 0.08150 \n",
      "[184/500] train_loss: 0.05657 valid_loss: 0.07260 test_loss: 0.08170 \n",
      "[185/500] train_loss: 0.05667 valid_loss: 0.07276 test_loss: 0.08222 \n",
      "[186/500] train_loss: 0.05678 valid_loss: 0.07246 test_loss: 0.08060 \n",
      "[187/500] train_loss: 0.05713 valid_loss: 0.07282 test_loss: 0.08168 \n",
      "[188/500] train_loss: 0.05712 valid_loss: 0.07234 test_loss: 0.08058 \n",
      "[189/500] train_loss: 0.05737 valid_loss: 0.07332 test_loss: 0.07899 \n",
      "[190/500] train_loss: 0.05663 valid_loss: 0.07245 test_loss: 0.07911 \n",
      "[191/500] train_loss: 0.05583 valid_loss: 0.07185 test_loss: 0.08168 \n",
      "[192/500] train_loss: 0.05523 valid_loss: 0.07248 test_loss: 0.07796 \n",
      "[193/500] train_loss: 0.05432 valid_loss: 0.07391 test_loss: 0.07996 \n",
      "[194/500] train_loss: 0.05351 valid_loss: 0.07465 test_loss: 0.08055 \n",
      "[195/500] train_loss: 0.05380 valid_loss: 0.07484 test_loss: 0.08111 \n",
      "[196/500] train_loss: 0.05429 valid_loss: 0.07492 test_loss: 0.07990 \n",
      "[197/500] train_loss: 0.05419 valid_loss: 0.07257 test_loss: 0.08005 \n",
      "[198/500] train_loss: 0.05548 valid_loss: 0.07447 test_loss: 0.08333 \n",
      "[199/500] train_loss: 0.05536 valid_loss: 0.07383 test_loss: 0.07944 \n",
      "[200/500] train_loss: 0.05524 valid_loss: 0.07486 test_loss: 0.08039 \n",
      "[201/500] train_loss: 0.05527 valid_loss: 0.07546 test_loss: 0.08165 \n",
      "[202/500] train_loss: 0.05423 valid_loss: 0.07452 test_loss: 0.08276 \n",
      "[203/500] train_loss: 0.05493 valid_loss: 0.07358 test_loss: 0.07929 \n",
      "[204/500] train_loss: 0.05335 valid_loss: 0.07282 test_loss: 0.08015 \n",
      "[205/500] train_loss: 0.05411 valid_loss: 0.07391 test_loss: 0.08115 \n",
      "[206/500] train_loss: 0.05433 valid_loss: 0.07351 test_loss: 0.08030 \n",
      "[207/500] train_loss: 0.05205 valid_loss: 0.07328 test_loss: 0.07934 \n",
      "[208/500] train_loss: 0.05398 valid_loss: 0.07354 test_loss: 0.08017 \n",
      "[209/500] train_loss: 0.05485 valid_loss: 0.07479 test_loss: 0.08073 \n",
      "[210/500] train_loss: 0.05600 valid_loss: 0.07428 test_loss: 0.07986 \n",
      "[211/500] train_loss: 0.05367 valid_loss: 0.07200 test_loss: 0.08176 \n",
      "[212/500] train_loss: 0.05397 valid_loss: 0.07429 test_loss: 0.08274 \n",
      "[213/500] train_loss: 0.05379 valid_loss: 0.07533 test_loss: 0.08087 \n",
      "[214/500] train_loss: 0.05380 valid_loss: 0.07400 test_loss: 0.08061 \n",
      "[215/500] train_loss: 0.05496 valid_loss: 0.07457 test_loss: 0.08066 \n",
      "[216/500] train_loss: 0.05369 valid_loss: 0.07302 test_loss: 0.08092 \n",
      "[217/500] train_loss: 0.05538 valid_loss: 0.07330 test_loss: 0.08025 \n",
      "[218/500] train_loss: 0.05273 valid_loss: 0.07430 test_loss: 0.07982 \n",
      "[219/500] train_loss: 0.05400 valid_loss: 0.07305 test_loss: 0.08025 \n",
      "[220/500] train_loss: 0.05335 valid_loss: 0.07310 test_loss: 0.08007 \n",
      "[221/500] train_loss: 0.05245 valid_loss: 0.07180 test_loss: 0.07950 \n",
      "[222/500] train_loss: 0.05328 valid_loss: 0.07429 test_loss: 0.07949 \n",
      "[223/500] train_loss: 0.05364 valid_loss: 0.07309 test_loss: 0.07980 \n",
      "[224/500] train_loss: 0.05380 valid_loss: 0.07361 test_loss: 0.08008 \n",
      "[225/500] train_loss: 0.05316 valid_loss: 0.07373 test_loss: 0.08016 \n",
      "[226/500] train_loss: 0.05211 valid_loss: 0.07502 test_loss: 0.08067 \n",
      "[227/500] train_loss: 0.05303 valid_loss: 0.07399 test_loss: 0.08375 \n",
      "[228/500] train_loss: 0.05190 valid_loss: 0.07333 test_loss: 0.08082 \n",
      "[229/500] train_loss: 0.05376 valid_loss: 0.07518 test_loss: 0.07987 \n",
      "[230/500] train_loss: 0.05298 valid_loss: 0.07369 test_loss: 0.08015 \n",
      "[231/500] train_loss: 0.05448 valid_loss: 0.07234 test_loss: 0.07920 \n",
      "[232/500] train_loss: 0.05398 valid_loss: 0.07267 test_loss: 0.07941 \n",
      "[233/500] train_loss: 0.05301 valid_loss: 0.07199 test_loss: 0.08093 \n",
      "[234/500] train_loss: 0.05409 valid_loss: 0.07261 test_loss: 0.08192 \n",
      "[235/500] train_loss: 0.05068 valid_loss: 0.07277 test_loss: 0.07925 \n",
      "[236/500] train_loss: 0.05148 valid_loss: 0.07328 test_loss: 0.08095 \n",
      "[237/500] train_loss: 0.05243 valid_loss: 0.07307 test_loss: 0.08112 \n",
      "[238/500] train_loss: 0.05218 valid_loss: 0.07434 test_loss: 0.07946 \n",
      "[239/500] train_loss: 0.05235 valid_loss: 0.07407 test_loss: 0.08017 \n",
      "[240/500] train_loss: 0.05234 valid_loss: 0.07367 test_loss: 0.07887 \n",
      "[241/500] train_loss: 0.05118 valid_loss: 0.07297 test_loss: 0.07867 \n",
      "[242/500] train_loss: 0.05273 valid_loss: 0.07452 test_loss: 0.07949 \n",
      "[243/500] train_loss: 0.05116 valid_loss: 0.07131 test_loss: 0.08078 \n",
      "验证损失减少 (0.071427 --> 0.071308). 正在保存模型...\n",
      "[244/500] train_loss: 0.05142 valid_loss: 0.07234 test_loss: 0.07932 \n",
      "[245/500] train_loss: 0.05162 valid_loss: 0.07274 test_loss: 0.07890 \n",
      "[246/500] train_loss: 0.05198 valid_loss: 0.07207 test_loss: 0.08008 \n",
      "[247/500] train_loss: 0.05154 valid_loss: 0.07135 test_loss: 0.08047 \n",
      "[248/500] train_loss: 0.05286 valid_loss: 0.07286 test_loss: 0.07984 \n",
      "[249/500] train_loss: 0.05133 valid_loss: 0.07148 test_loss: 0.07881 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250/500] train_loss: 0.05167 valid_loss: 0.07311 test_loss: 0.08130 \n",
      "[251/500] train_loss: 0.05254 valid_loss: 0.07147 test_loss: 0.08018 \n",
      "[252/500] train_loss: 0.05079 valid_loss: 0.07260 test_loss: 0.08109 \n",
      "[253/500] train_loss: 0.05088 valid_loss: 0.07283 test_loss: 0.08132 \n",
      "[254/500] train_loss: 0.05287 valid_loss: 0.07294 test_loss: 0.08228 \n",
      "[255/500] train_loss: 0.05107 valid_loss: 0.07424 test_loss: 0.08243 \n",
      "[256/500] train_loss: 0.05027 valid_loss: 0.07324 test_loss: 0.08210 \n",
      "[257/500] train_loss: 0.05074 valid_loss: 0.07249 test_loss: 0.08001 \n",
      "[258/500] train_loss: 0.05084 valid_loss: 0.07169 test_loss: 0.08018 \n",
      "[259/500] train_loss: 0.05016 valid_loss: 0.07350 test_loss: 0.08189 \n",
      "[260/500] train_loss: 0.05032 valid_loss: 0.07368 test_loss: 0.08050 \n",
      "[261/500] train_loss: 0.05041 valid_loss: 0.07376 test_loss: 0.08034 \n",
      "[262/500] train_loss: 0.05023 valid_loss: 0.07335 test_loss: 0.08009 \n",
      "[263/500] train_loss: 0.05065 valid_loss: 0.07295 test_loss: 0.07987 \n",
      "[264/500] train_loss: 0.05083 valid_loss: 0.07214 test_loss: 0.08092 \n",
      "[265/500] train_loss: 0.05114 valid_loss: 0.07182 test_loss: 0.08093 \n",
      "[266/500] train_loss: 0.05077 valid_loss: 0.07248 test_loss: 0.08100 \n",
      "[267/500] train_loss: 0.05039 valid_loss: 0.07433 test_loss: 0.08013 \n",
      "[268/500] train_loss: 0.05061 valid_loss: 0.07413 test_loss: 0.07979 \n",
      "[269/500] train_loss: 0.04941 valid_loss: 0.07311 test_loss: 0.07957 \n",
      "[270/500] train_loss: 0.05060 valid_loss: 0.07504 test_loss: 0.08425 \n",
      "[271/500] train_loss: 0.04957 valid_loss: 0.07191 test_loss: 0.08033 \n",
      "[272/500] train_loss: 0.05088 valid_loss: 0.07254 test_loss: 0.08276 \n",
      "[273/500] train_loss: 0.05091 valid_loss: 0.07259 test_loss: 0.08030 \n",
      "[274/500] train_loss: 0.05187 valid_loss: 0.07308 test_loss: 0.07994 \n",
      "[275/500] train_loss: 0.05041 valid_loss: 0.07250 test_loss: 0.08250 \n",
      "[276/500] train_loss: 0.04979 valid_loss: 0.07209 test_loss: 0.08185 \n",
      "[277/500] train_loss: 0.04803 valid_loss: 0.07143 test_loss: 0.08125 \n",
      "[278/500] train_loss: 0.04877 valid_loss: 0.07366 test_loss: 0.08276 \n",
      "[279/500] train_loss: 0.04971 valid_loss: 0.07319 test_loss: 0.08372 \n",
      "[280/500] train_loss: 0.04933 valid_loss: 0.07178 test_loss: 0.08257 \n",
      "[281/500] train_loss: 0.04982 valid_loss: 0.07243 test_loss: 0.08275 \n",
      "[282/500] train_loss: 0.04921 valid_loss: 0.07321 test_loss: 0.08195 \n",
      "[283/500] train_loss: 0.04963 valid_loss: 0.07271 test_loss: 0.08335 \n",
      "[284/500] train_loss: 0.05079 valid_loss: 0.07722 test_loss: 0.08257 \n",
      "[285/500] train_loss: 0.04971 valid_loss: 0.07320 test_loss: 0.08019 \n",
      "[286/500] train_loss: 0.05052 valid_loss: 0.07215 test_loss: 0.07986 \n",
      "[287/500] train_loss: 0.04983 valid_loss: 0.07323 test_loss: 0.08229 \n",
      "[288/500] train_loss: 0.04857 valid_loss: 0.07166 test_loss: 0.08075 \n",
      "[289/500] train_loss: 0.04919 valid_loss: 0.07219 test_loss: 0.07931 \n",
      "[290/500] train_loss: 0.04940 valid_loss: 0.07416 test_loss: 0.08060 \n",
      "[291/500] train_loss: 0.04960 valid_loss: 0.07324 test_loss: 0.07996 \n",
      "[292/500] train_loss: 0.04724 valid_loss: 0.07311 test_loss: 0.08010 \n",
      "[293/500] train_loss: 0.04849 valid_loss: 0.07248 test_loss: 0.08112 \n",
      "[294/500] train_loss: 0.04791 valid_loss: 0.07195 test_loss: 0.07939 \n",
      "[295/500] train_loss: 0.04792 valid_loss: 0.07217 test_loss: 0.07916 \n",
      "[296/500] train_loss: 0.04788 valid_loss: 0.07103 test_loss: 0.07961 \n",
      "验证损失减少 (0.071308 --> 0.071026). 正在保存模型...\n",
      "[297/500] train_loss: 0.04924 valid_loss: 0.07153 test_loss: 0.08006 \n",
      "[298/500] train_loss: 0.04670 valid_loss: 0.07232 test_loss: 0.07970 \n",
      "[299/500] train_loss: 0.04824 valid_loss: 0.07117 test_loss: 0.07952 \n",
      "[300/500] train_loss: 0.05002 valid_loss: 0.07206 test_loss: 0.07890 \n",
      "[301/500] train_loss: 0.04874 valid_loss: 0.07227 test_loss: 0.08233 \n",
      "[302/500] train_loss: 0.05010 valid_loss: 0.07272 test_loss: 0.07998 \n",
      "[303/500] train_loss: 0.04887 valid_loss: 0.07164 test_loss: 0.08130 \n",
      "[304/500] train_loss: 0.04905 valid_loss: 0.07257 test_loss: 0.08029 \n",
      "[305/500] train_loss: 0.04777 valid_loss: 0.07216 test_loss: 0.08057 \n",
      "[306/500] train_loss: 0.04727 valid_loss: 0.07232 test_loss: 0.08014 \n",
      "[307/500] train_loss: 0.04766 valid_loss: 0.07308 test_loss: 0.08045 \n",
      "[308/500] train_loss: 0.04747 valid_loss: 0.07223 test_loss: 0.08046 \n",
      "[309/500] train_loss: 0.04746 valid_loss: 0.07225 test_loss: 0.08012 \n",
      "[310/500] train_loss: 0.04733 valid_loss: 0.07319 test_loss: 0.08194 \n",
      "[311/500] train_loss: 0.04733 valid_loss: 0.07303 test_loss: 0.08182 \n",
      "[312/500] train_loss: 0.04821 valid_loss: 0.07325 test_loss: 0.08079 \n",
      "[313/500] train_loss: 0.04710 valid_loss: 0.07264 test_loss: 0.08104 \n",
      "[314/500] train_loss: 0.04679 valid_loss: 0.07242 test_loss: 0.08252 \n",
      "[315/500] train_loss: 0.04729 valid_loss: 0.07243 test_loss: 0.08350 \n",
      "[316/500] train_loss: 0.04662 valid_loss: 0.07287 test_loss: 0.08282 \n",
      "[317/500] train_loss: 0.04799 valid_loss: 0.07265 test_loss: 0.08291 \n",
      "[318/500] train_loss: 0.04693 valid_loss: 0.07309 test_loss: 0.08236 \n",
      "[319/500] train_loss: 0.04746 valid_loss: 0.07230 test_loss: 0.08221 \n",
      "[320/500] train_loss: 0.04797 valid_loss: 0.07178 test_loss: 0.08290 \n",
      "[321/500] train_loss: 0.04749 valid_loss: 0.07326 test_loss: 0.08035 \n",
      "[322/500] train_loss: 0.04695 valid_loss: 0.07318 test_loss: 0.08267 \n",
      "[323/500] train_loss: 0.04775 valid_loss: 0.07183 test_loss: 0.08132 \n",
      "[324/500] train_loss: 0.04735 valid_loss: 0.07308 test_loss: 0.08069 \n",
      "[325/500] train_loss: 0.04755 valid_loss: 0.07187 test_loss: 0.08127 \n",
      "[326/500] train_loss: 0.04714 valid_loss: 0.07460 test_loss: 0.08318 \n",
      "[327/500] train_loss: 0.04765 valid_loss: 0.07253 test_loss: 0.08097 \n",
      "[328/500] train_loss: 0.04691 valid_loss: 0.07316 test_loss: 0.08250 \n",
      "[329/500] train_loss: 0.04809 valid_loss: 0.07453 test_loss: 0.08294 \n",
      "[330/500] train_loss: 0.04745 valid_loss: 0.07111 test_loss: 0.08078 \n",
      "[331/500] train_loss: 0.04716 valid_loss: 0.07274 test_loss: 0.08244 \n",
      "[332/500] train_loss: 0.04731 valid_loss: 0.07191 test_loss: 0.08176 \n",
      "[333/500] train_loss: 0.04565 valid_loss: 0.07363 test_loss: 0.08273 \n",
      "[334/500] train_loss: 0.04636 valid_loss: 0.07433 test_loss: 0.08177 \n",
      "[335/500] train_loss: 0.04661 valid_loss: 0.07240 test_loss: 0.08139 \n",
      "[336/500] train_loss: 0.04844 valid_loss: 0.07384 test_loss: 0.08231 \n",
      "[337/500] train_loss: 0.04806 valid_loss: 0.07240 test_loss: 0.08034 \n",
      "[338/500] train_loss: 0.04796 valid_loss: 0.07538 test_loss: 0.08137 \n",
      "[339/500] train_loss: 0.04821 valid_loss: 0.07357 test_loss: 0.08176 \n",
      "[340/500] train_loss: 0.04794 valid_loss: 0.07394 test_loss: 0.08285 \n",
      "[341/500] train_loss: 0.04678 valid_loss: 0.07291 test_loss: 0.07920 \n",
      "[342/500] train_loss: 0.04636 valid_loss: 0.07392 test_loss: 0.08061 \n",
      "[343/500] train_loss: 0.04728 valid_loss: 0.07225 test_loss: 0.08055 \n",
      "[344/500] train_loss: 0.04635 valid_loss: 0.07327 test_loss: 0.07981 \n",
      "[345/500] train_loss: 0.04661 valid_loss: 0.07197 test_loss: 0.08011 \n",
      "[346/500] train_loss: 0.04549 valid_loss: 0.07345 test_loss: 0.07989 \n",
      "[347/500] train_loss: 0.04518 valid_loss: 0.07326 test_loss: 0.08158 \n",
      "[348/500] train_loss: 0.04509 valid_loss: 0.07375 test_loss: 0.08044 \n",
      "[349/500] train_loss: 0.04690 valid_loss: 0.07834 test_loss: 0.08210 \n",
      "[350/500] train_loss: 0.04564 valid_loss: 0.07355 test_loss: 0.08140 \n",
      "[351/500] train_loss: 0.04539 valid_loss: 0.07386 test_loss: 0.08291 \n",
      "[352/500] train_loss: 0.04589 valid_loss: 0.07156 test_loss: 0.08092 \n",
      "[353/500] train_loss: 0.04530 valid_loss: 0.07354 test_loss: 0.08169 \n",
      "[354/500] train_loss: 0.04689 valid_loss: 0.07380 test_loss: 0.08130 \n",
      "[355/500] train_loss: 0.04509 valid_loss: 0.07235 test_loss: 0.08131 \n",
      "[356/500] train_loss: 0.04573 valid_loss: 0.07269 test_loss: 0.08196 \n",
      "[357/500] train_loss: 0.04603 valid_loss: 0.07463 test_loss: 0.08035 \n",
      "[358/500] train_loss: 0.04675 valid_loss: 0.07514 test_loss: 0.08101 \n",
      "[359/500] train_loss: 0.04578 valid_loss: 0.07448 test_loss: 0.08260 \n",
      "[360/500] train_loss: 0.04544 valid_loss: 0.07216 test_loss: 0.08079 \n",
      "[361/500] train_loss: 0.04626 valid_loss: 0.07108 test_loss: 0.07972 \n",
      "[362/500] train_loss: 0.04535 valid_loss: 0.07352 test_loss: 0.08006 \n",
      "[363/500] train_loss: 0.04512 valid_loss: 0.07403 test_loss: 0.08126 \n",
      "[364/500] train_loss: 0.04662 valid_loss: 0.07284 test_loss: 0.08160 \n",
      "[365/500] train_loss: 0.04589 valid_loss: 0.07344 test_loss: 0.08199 \n",
      "[366/500] train_loss: 0.04488 valid_loss: 0.07255 test_loss: 0.08086 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[367/500] train_loss: 0.04594 valid_loss: 0.07317 test_loss: 0.08295 \n",
      "[368/500] train_loss: 0.04544 valid_loss: 0.07369 test_loss: 0.08128 \n",
      "[369/500] train_loss: 0.04545 valid_loss: 0.07344 test_loss: 0.08067 \n",
      "[370/500] train_loss: 0.04570 valid_loss: 0.07337 test_loss: 0.07929 \n",
      "[371/500] train_loss: 0.04638 valid_loss: 0.07287 test_loss: 0.07874 \n",
      "[372/500] train_loss: 0.04550 valid_loss: 0.07214 test_loss: 0.07890 \n",
      "[373/500] train_loss: 0.04564 valid_loss: 0.07103 test_loss: 0.07989 \n",
      "[374/500] train_loss: 0.04501 valid_loss: 0.07292 test_loss: 0.08071 \n",
      "[375/500] train_loss: 0.04570 valid_loss: 0.07185 test_loss: 0.08003 \n",
      "[376/500] train_loss: 0.04412 valid_loss: 0.07229 test_loss: 0.07990 \n",
      "[377/500] train_loss: 0.04560 valid_loss: 0.07155 test_loss: 0.08114 \n",
      "[378/500] train_loss: 0.04544 valid_loss: 0.07294 test_loss: 0.08286 \n",
      "[379/500] train_loss: 0.04573 valid_loss: 0.07367 test_loss: 0.08129 \n",
      "[380/500] train_loss: 0.04413 valid_loss: 0.07182 test_loss: 0.08081 \n",
      "[381/500] train_loss: 0.04524 valid_loss: 0.07173 test_loss: 0.08153 \n",
      "[382/500] train_loss: 0.04436 valid_loss: 0.07127 test_loss: 0.08255 \n",
      "[383/500] train_loss: 0.04590 valid_loss: 0.07147 test_loss: 0.08176 \n",
      "[384/500] train_loss: 0.04404 valid_loss: 0.07235 test_loss: 0.08130 \n",
      "[385/500] train_loss: 0.04418 valid_loss: 0.07220 test_loss: 0.07959 \n",
      "[386/500] train_loss: 0.04424 valid_loss: 0.07061 test_loss: 0.07898 \n",
      "验证损失减少 (0.071026 --> 0.070614). 正在保存模型...\n",
      "[387/500] train_loss: 0.04380 valid_loss: 0.07351 test_loss: 0.08029 \n",
      "[388/500] train_loss: 0.04505 valid_loss: 0.07218 test_loss: 0.08002 \n",
      "[389/500] train_loss: 0.04513 valid_loss: 0.07186 test_loss: 0.08078 \n",
      "[390/500] train_loss: 0.04449 valid_loss: 0.07340 test_loss: 0.08022 \n",
      "[391/500] train_loss: 0.04600 valid_loss: 0.07250 test_loss: 0.07890 \n",
      "[392/500] train_loss: 0.04477 valid_loss: 0.07210 test_loss: 0.07981 \n",
      "[393/500] train_loss: 0.04455 valid_loss: 0.07172 test_loss: 0.07934 \n",
      "[394/500] train_loss: 0.04370 valid_loss: 0.07168 test_loss: 0.07988 \n",
      "[395/500] train_loss: 0.04395 valid_loss: 0.07102 test_loss: 0.07874 \n",
      "[396/500] train_loss: 0.04304 valid_loss: 0.07426 test_loss: 0.08195 \n",
      "[397/500] train_loss: 0.04456 valid_loss: 0.07081 test_loss: 0.08022 \n",
      "[398/500] train_loss: 0.04497 valid_loss: 0.07235 test_loss: 0.08071 \n",
      "[399/500] train_loss: 0.04493 valid_loss: 0.07227 test_loss: 0.07960 \n",
      "[400/500] train_loss: 0.04539 valid_loss: 0.07409 test_loss: 0.07993 \n",
      "[401/500] train_loss: 0.04604 valid_loss: 0.07186 test_loss: 0.07972 \n",
      "[402/500] train_loss: 0.04455 valid_loss: 0.07295 test_loss: 0.08117 \n",
      "[403/500] train_loss: 0.04438 valid_loss: 0.07258 test_loss: 0.08021 \n",
      "[404/500] train_loss: 0.04428 valid_loss: 0.07452 test_loss: 0.08193 \n",
      "[405/500] train_loss: 0.04598 valid_loss: 0.07195 test_loss: 0.08098 \n",
      "[406/500] train_loss: 0.04360 valid_loss: 0.07318 test_loss: 0.08287 \n",
      "[407/500] train_loss: 0.04481 valid_loss: 0.07101 test_loss: 0.08258 \n",
      "[408/500] train_loss: 0.04511 valid_loss: 0.07420 test_loss: 0.08067 \n",
      "[409/500] train_loss: 0.04422 valid_loss: 0.07338 test_loss: 0.08065 \n",
      "[410/500] train_loss: 0.04388 valid_loss: 0.07359 test_loss: 0.08249 \n",
      "[411/500] train_loss: 0.04388 valid_loss: 0.07259 test_loss: 0.08097 \n",
      "[412/500] train_loss: 0.04383 valid_loss: 0.07141 test_loss: 0.08124 \n",
      "[413/500] train_loss: 0.04352 valid_loss: 0.07267 test_loss: 0.08193 \n",
      "[414/500] train_loss: 0.04442 valid_loss: 0.07337 test_loss: 0.08298 \n",
      "[415/500] train_loss: 0.04382 valid_loss: 0.07265 test_loss: 0.08222 \n",
      "[416/500] train_loss: 0.04408 valid_loss: 0.07267 test_loss: 0.08298 \n",
      "[417/500] train_loss: 0.04312 valid_loss: 0.07236 test_loss: 0.08131 \n",
      "[418/500] train_loss: 0.04403 valid_loss: 0.07174 test_loss: 0.08153 \n",
      "[419/500] train_loss: 0.04301 valid_loss: 0.07272 test_loss: 0.08040 \n",
      "[420/500] train_loss: 0.04367 valid_loss: 0.07314 test_loss: 0.08247 \n",
      "[421/500] train_loss: 0.04288 valid_loss: 0.07370 test_loss: 0.08149 \n",
      "[422/500] train_loss: 0.04414 valid_loss: 0.07172 test_loss: 0.08245 \n",
      "[423/500] train_loss: 0.04400 valid_loss: 0.07109 test_loss: 0.08134 \n",
      "[424/500] train_loss: 0.04291 valid_loss: 0.07327 test_loss: 0.08141 \n",
      "[425/500] train_loss: 0.04314 valid_loss: 0.07149 test_loss: 0.08101 \n",
      "[426/500] train_loss: 0.04416 valid_loss: 0.07235 test_loss: 0.08158 \n",
      "[427/500] train_loss: 0.04346 valid_loss: 0.07148 test_loss: 0.08238 \n",
      "[428/500] train_loss: 0.04417 valid_loss: 0.07275 test_loss: 0.08138 \n",
      "[429/500] train_loss: 0.04283 valid_loss: 0.07425 test_loss: 0.08159 \n",
      "[430/500] train_loss: 0.04414 valid_loss: 0.07172 test_loss: 0.08259 \n",
      "[431/500] train_loss: 0.04282 valid_loss: 0.07377 test_loss: 0.08349 \n",
      "[432/500] train_loss: 0.04374 valid_loss: 0.07363 test_loss: 0.08253 \n",
      "[433/500] train_loss: 0.04292 valid_loss: 0.07269 test_loss: 0.08302 \n",
      "[434/500] train_loss: 0.04438 valid_loss: 0.07362 test_loss: 0.08294 \n",
      "[435/500] train_loss: 0.04334 valid_loss: 0.07351 test_loss: 0.08123 \n",
      "[436/500] train_loss: 0.04402 valid_loss: 0.07850 test_loss: 0.08172 \n",
      "[437/500] train_loss: 0.04322 valid_loss: 0.07465 test_loss: 0.08133 \n",
      "[438/500] train_loss: 0.04226 valid_loss: 0.07385 test_loss: 0.08148 \n",
      "[439/500] train_loss: 0.04300 valid_loss: 0.07203 test_loss: 0.08106 \n",
      "[440/500] train_loss: 0.04320 valid_loss: 0.07336 test_loss: 0.08153 \n",
      "[441/500] train_loss: 0.04225 valid_loss: 0.07400 test_loss: 0.08149 \n",
      "[442/500] train_loss: 0.04446 valid_loss: 0.07394 test_loss: 0.08065 \n",
      "[443/500] train_loss: 0.04273 valid_loss: 0.07422 test_loss: 0.08103 \n",
      "[444/500] train_loss: 0.04234 valid_loss: 0.07329 test_loss: 0.08129 \n",
      "[445/500] train_loss: 0.04324 valid_loss: 0.07280 test_loss: 0.08160 \n",
      "[446/500] train_loss: 0.04261 valid_loss: 0.07358 test_loss: 0.08174 \n",
      "[447/500] train_loss: 0.04259 valid_loss: 0.07281 test_loss: 0.08102 \n",
      "[448/500] train_loss: 0.04242 valid_loss: 0.07300 test_loss: 0.08151 \n",
      "[449/500] train_loss: 0.04304 valid_loss: 0.07305 test_loss: 0.08125 \n",
      "[450/500] train_loss: 0.04178 valid_loss: 0.07156 test_loss: 0.08171 \n",
      "[451/500] train_loss: 0.04185 valid_loss: 0.07581 test_loss: 0.08127 \n",
      "[452/500] train_loss: 0.04196 valid_loss: 0.07167 test_loss: 0.08087 \n",
      "[453/500] train_loss: 0.04273 valid_loss: 0.07397 test_loss: 0.08318 \n",
      "[454/500] train_loss: 0.04204 valid_loss: 0.07120 test_loss: 0.08047 \n",
      "[455/500] train_loss: 0.04205 valid_loss: 0.07205 test_loss: 0.08102 \n",
      "[456/500] train_loss: 0.04242 valid_loss: 0.07257 test_loss: 0.08111 \n",
      "[457/500] train_loss: 0.04339 valid_loss: 0.07184 test_loss: 0.08038 \n",
      "[458/500] train_loss: 0.04279 valid_loss: 0.07290 test_loss: 0.08060 \n",
      "[459/500] train_loss: 0.04211 valid_loss: 0.07311 test_loss: 0.08070 \n",
      "[460/500] train_loss: 0.04312 valid_loss: 0.07273 test_loss: 0.08081 \n",
      "[461/500] train_loss: 0.04140 valid_loss: 0.07227 test_loss: 0.08093 \n",
      "[462/500] train_loss: 0.04129 valid_loss: 0.07252 test_loss: 0.08081 \n",
      "[463/500] train_loss: 0.04201 valid_loss: 0.07217 test_loss: 0.08193 \n",
      "[464/500] train_loss: 0.04348 valid_loss: 0.07289 test_loss: 0.08085 \n",
      "[465/500] train_loss: 0.04290 valid_loss: 0.07832 test_loss: 0.08083 \n",
      "[466/500] train_loss: 0.04198 valid_loss: 0.07377 test_loss: 0.07963 \n",
      "[467/500] train_loss: 0.04297 valid_loss: 0.07355 test_loss: 0.08021 \n",
      "[468/500] train_loss: 0.04123 valid_loss: 0.07409 test_loss: 0.08202 \n",
      "[469/500] train_loss: 0.04308 valid_loss: 0.07302 test_loss: 0.08069 \n",
      "[470/500] train_loss: 0.04258 valid_loss: 0.07200 test_loss: 0.08030 \n",
      "[471/500] train_loss: 0.04184 valid_loss: 0.07353 test_loss: 0.08166 \n",
      "[472/500] train_loss: 0.04276 valid_loss: 0.07358 test_loss: 0.08129 \n",
      "[473/500] train_loss: 0.04157 valid_loss: 0.07320 test_loss: 0.08214 \n",
      "[474/500] train_loss: 0.04216 valid_loss: 0.07255 test_loss: 0.07891 \n",
      "[475/500] train_loss: 0.04218 valid_loss: 0.07486 test_loss: 0.08168 \n",
      "[476/500] train_loss: 0.04174 valid_loss: 0.07251 test_loss: 0.08031 \n",
      "[477/500] train_loss: 0.04200 valid_loss: 0.07430 test_loss: 0.08090 \n",
      "[478/500] train_loss: 0.04115 valid_loss: 0.07308 test_loss: 0.08302 \n",
      "[479/500] train_loss: 0.04150 valid_loss: 0.07475 test_loss: 0.08208 \n",
      "[480/500] train_loss: 0.04160 valid_loss: 0.07340 test_loss: 0.08261 \n",
      "[481/500] train_loss: 0.04270 valid_loss: 0.07349 test_loss: 0.08127 \n",
      "[482/500] train_loss: 0.04131 valid_loss: 0.07705 test_loss: 0.08265 \n",
      "[483/500] train_loss: 0.04128 valid_loss: 0.07337 test_loss: 0.08216 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[484/500] train_loss: 0.04108 valid_loss: 0.07357 test_loss: 0.08178 \n",
      "[485/500] train_loss: 0.04064 valid_loss: 0.07283 test_loss: 0.08066 \n",
      "[486/500] train_loss: 0.04213 valid_loss: 0.07498 test_loss: 0.08108 \n",
      "[487/500] train_loss: 0.04208 valid_loss: 0.07362 test_loss: 0.08188 \n",
      "[488/500] train_loss: 0.04172 valid_loss: 0.07471 test_loss: 0.08093 \n",
      "[489/500] train_loss: 0.04086 valid_loss: 0.07500 test_loss: 0.08171 \n",
      "[490/500] train_loss: 0.04167 valid_loss: 0.07484 test_loss: 0.08066 \n",
      "[491/500] train_loss: 0.04137 valid_loss: 0.07326 test_loss: 0.08032 \n",
      "[492/500] train_loss: 0.04170 valid_loss: 0.07276 test_loss: 0.08080 \n",
      "[493/500] train_loss: 0.04166 valid_loss: 0.07290 test_loss: 0.08018 \n",
      "[494/500] train_loss: 0.04298 valid_loss: 0.07453 test_loss: 0.08278 \n",
      "[495/500] train_loss: 0.04172 valid_loss: 0.07427 test_loss: 0.08288 \n",
      "[496/500] train_loss: 0.04170 valid_loss: 0.07549 test_loss: 0.08382 \n",
      "[497/500] train_loss: 0.04058 valid_loss: 0.07389 test_loss: 0.08133 \n",
      "[498/500] train_loss: 0.04116 valid_loss: 0.07283 test_loss: 0.08258 \n",
      "[499/500] train_loss: 0.04077 valid_loss: 0.07240 test_loss: 0.08202 \n",
      "[500/500] train_loss: 0.04100 valid_loss: 0.07190 test_loss: 0.08065 \n",
      "TRAINING MODEL 8\n",
      "[  1/500] train_loss: 0.32391 valid_loss: 0.23748 test_loss: 0.24243 \n",
      "验证损失减少 (inf --> 0.237476). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18109 valid_loss: 0.17483 test_loss: 0.17790 \n",
      "验证损失减少 (0.237476 --> 0.174833). 正在保存模型...\n",
      "[  3/500] train_loss: 0.14877 valid_loss: 0.14729 test_loss: 0.15468 \n",
      "验证损失减少 (0.174833 --> 0.147291). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13227 valid_loss: 0.13384 test_loss: 0.14369 \n",
      "验证损失减少 (0.147291 --> 0.133845). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12573 valid_loss: 0.12964 test_loss: 0.13671 \n",
      "验证损失减少 (0.133845 --> 0.129645). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12266 valid_loss: 0.12309 test_loss: 0.13528 \n",
      "验证损失减少 (0.129645 --> 0.123091). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11273 valid_loss: 0.11941 test_loss: 0.12784 \n",
      "验证损失减少 (0.123091 --> 0.119414). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11364 valid_loss: 0.11337 test_loss: 0.12440 \n",
      "验证损失减少 (0.119414 --> 0.113370). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11132 valid_loss: 0.11156 test_loss: 0.12356 \n",
      "验证损失减少 (0.113370 --> 0.111564). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10744 valid_loss: 0.10734 test_loss: 0.11746 \n",
      "验证损失减少 (0.111564 --> 0.107343). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10536 valid_loss: 0.10645 test_loss: 0.11477 \n",
      "验证损失减少 (0.107343 --> 0.106448). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10409 valid_loss: 0.10430 test_loss: 0.11456 \n",
      "验证损失减少 (0.106448 --> 0.104299). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10142 valid_loss: 0.10269 test_loss: 0.11415 \n",
      "验证损失减少 (0.104299 --> 0.102686). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10030 valid_loss: 0.09977 test_loss: 0.11113 \n",
      "验证损失减少 (0.102686 --> 0.099767). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.09735 valid_loss: 0.10065 test_loss: 0.10893 \n",
      "[ 16/500] train_loss: 0.09619 valid_loss: 0.09814 test_loss: 0.11072 \n",
      "验证损失减少 (0.099767 --> 0.098140). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09439 valid_loss: 0.09658 test_loss: 0.10649 \n",
      "验证损失减少 (0.098140 --> 0.096582). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09242 valid_loss: 0.09745 test_loss: 0.10780 \n",
      "[ 19/500] train_loss: 0.09257 valid_loss: 0.09572 test_loss: 0.10566 \n",
      "验证损失减少 (0.096582 --> 0.095716). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09389 valid_loss: 0.09484 test_loss: 0.10604 \n",
      "验证损失减少 (0.095716 --> 0.094842). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09058 valid_loss: 0.09124 test_loss: 0.10197 \n",
      "验证损失减少 (0.094842 --> 0.091242). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09010 valid_loss: 0.09422 test_loss: 0.10368 \n",
      "[ 23/500] train_loss: 0.08910 valid_loss: 0.09203 test_loss: 0.10099 \n",
      "[ 24/500] train_loss: 0.08938 valid_loss: 0.08843 test_loss: 0.09906 \n",
      "验证损失减少 (0.091242 --> 0.088428). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.08821 valid_loss: 0.08970 test_loss: 0.09922 \n",
      "[ 26/500] train_loss: 0.08661 valid_loss: 0.08928 test_loss: 0.10067 \n",
      "[ 27/500] train_loss: 0.08732 valid_loss: 0.08777 test_loss: 0.09910 \n",
      "验证损失减少 (0.088428 --> 0.087773). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08663 valid_loss: 0.08832 test_loss: 0.09922 \n",
      "[ 29/500] train_loss: 0.08493 valid_loss: 0.08699 test_loss: 0.09693 \n",
      "验证损失减少 (0.087773 --> 0.086988). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08257 valid_loss: 0.08830 test_loss: 0.09718 \n",
      "[ 31/500] train_loss: 0.08403 valid_loss: 0.08800 test_loss: 0.09668 \n",
      "[ 32/500] train_loss: 0.08443 valid_loss: 0.08689 test_loss: 0.09588 \n",
      "验证损失减少 (0.086988 --> 0.086894). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08346 valid_loss: 0.08508 test_loss: 0.09448 \n",
      "验证损失减少 (0.086894 --> 0.085082). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08057 valid_loss: 0.08556 test_loss: 0.09334 \n",
      "[ 35/500] train_loss: 0.08291 valid_loss: 0.08891 test_loss: 0.09725 \n",
      "[ 36/500] train_loss: 0.08073 valid_loss: 0.08741 test_loss: 0.09479 \n",
      "[ 37/500] train_loss: 0.08201 valid_loss: 0.08744 test_loss: 0.09646 \n",
      "[ 38/500] train_loss: 0.08162 valid_loss: 0.08555 test_loss: 0.09327 \n",
      "[ 39/500] train_loss: 0.08117 valid_loss: 0.08294 test_loss: 0.09196 \n",
      "验证损失减少 (0.085082 --> 0.082940). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.07993 valid_loss: 0.08338 test_loss: 0.09215 \n",
      "[ 41/500] train_loss: 0.07803 valid_loss: 0.08443 test_loss: 0.09284 \n",
      "[ 42/500] train_loss: 0.07901 valid_loss: 0.08205 test_loss: 0.09174 \n",
      "验证损失减少 (0.082940 --> 0.082051). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.07803 valid_loss: 0.08389 test_loss: 0.09209 \n",
      "[ 44/500] train_loss: 0.07819 valid_loss: 0.08202 test_loss: 0.09087 \n",
      "验证损失减少 (0.082051 --> 0.082020). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.07878 valid_loss: 0.08158 test_loss: 0.09001 \n",
      "验证损失减少 (0.082020 --> 0.081579). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07623 valid_loss: 0.08194 test_loss: 0.09008 \n",
      "[ 47/500] train_loss: 0.07435 valid_loss: 0.08286 test_loss: 0.09169 \n",
      "[ 48/500] train_loss: 0.07623 valid_loss: 0.08157 test_loss: 0.08981 \n",
      "验证损失减少 (0.081579 --> 0.081570). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07723 valid_loss: 0.08226 test_loss: 0.09087 \n",
      "[ 50/500] train_loss: 0.07778 valid_loss: 0.08199 test_loss: 0.09089 \n",
      "[ 51/500] train_loss: 0.07413 valid_loss: 0.08085 test_loss: 0.08891 \n",
      "验证损失减少 (0.081570 --> 0.080849). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07516 valid_loss: 0.08139 test_loss: 0.08762 \n",
      "[ 53/500] train_loss: 0.07772 valid_loss: 0.08146 test_loss: 0.09092 \n",
      "[ 54/500] train_loss: 0.07581 valid_loss: 0.08156 test_loss: 0.09033 \n",
      "[ 55/500] train_loss: 0.07332 valid_loss: 0.08105 test_loss: 0.08849 \n",
      "[ 56/500] train_loss: 0.07448 valid_loss: 0.08158 test_loss: 0.08791 \n",
      "[ 57/500] train_loss: 0.07401 valid_loss: 0.08152 test_loss: 0.08910 \n",
      "[ 58/500] train_loss: 0.07551 valid_loss: 0.07969 test_loss: 0.08795 \n",
      "验证损失减少 (0.080849 --> 0.079695). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07377 valid_loss: 0.07951 test_loss: 0.08705 \n",
      "验证损失减少 (0.079695 --> 0.079515). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07286 valid_loss: 0.07935 test_loss: 0.08807 \n",
      "验证损失减少 (0.079515 --> 0.079349). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.07208 valid_loss: 0.08147 test_loss: 0.08919 \n",
      "[ 62/500] train_loss: 0.07359 valid_loss: 0.07902 test_loss: 0.08725 \n",
      "验证损失减少 (0.079349 --> 0.079023). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.07401 valid_loss: 0.07912 test_loss: 0.08841 \n",
      "[ 64/500] train_loss: 0.07245 valid_loss: 0.07857 test_loss: 0.08782 \n",
      "验证损失减少 (0.079023 --> 0.078567). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07280 valid_loss: 0.07870 test_loss: 0.08629 \n",
      "[ 66/500] train_loss: 0.07241 valid_loss: 0.07965 test_loss: 0.08943 \n",
      "[ 67/500] train_loss: 0.07075 valid_loss: 0.07966 test_loss: 0.08931 \n",
      "[ 68/500] train_loss: 0.07137 valid_loss: 0.07658 test_loss: 0.08545 \n",
      "验证损失减少 (0.078567 --> 0.076581). 正在保存模型...\n",
      "[ 69/500] train_loss: 0.07169 valid_loss: 0.07784 test_loss: 0.08721 \n",
      "[ 70/500] train_loss: 0.07047 valid_loss: 0.07880 test_loss: 0.08744 \n",
      "[ 71/500] train_loss: 0.07239 valid_loss: 0.07811 test_loss: 0.08733 \n",
      "[ 72/500] train_loss: 0.06950 valid_loss: 0.07752 test_loss: 0.08706 \n",
      "[ 73/500] train_loss: 0.07097 valid_loss: 0.07927 test_loss: 0.08861 \n",
      "[ 74/500] train_loss: 0.07058 valid_loss: 0.07522 test_loss: 0.08439 \n",
      "验证损失减少 (0.076581 --> 0.075224). 正在保存模型...\n",
      "[ 75/500] train_loss: 0.06794 valid_loss: 0.07622 test_loss: 0.08519 \n",
      "[ 76/500] train_loss: 0.06911 valid_loss: 0.07842 test_loss: 0.08822 \n",
      "[ 77/500] train_loss: 0.06904 valid_loss: 0.07854 test_loss: 0.08592 \n",
      "[ 78/500] train_loss: 0.06854 valid_loss: 0.07662 test_loss: 0.08708 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79/500] train_loss: 0.06774 valid_loss: 0.07712 test_loss: 0.08650 \n",
      "[ 80/500] train_loss: 0.06813 valid_loss: 0.07607 test_loss: 0.08452 \n",
      "[ 81/500] train_loss: 0.06800 valid_loss: 0.07796 test_loss: 0.08503 \n",
      "[ 82/500] train_loss: 0.06877 valid_loss: 0.07498 test_loss: 0.08551 \n",
      "验证损失减少 (0.075224 --> 0.074975). 正在保存模型...\n",
      "[ 83/500] train_loss: 0.06840 valid_loss: 0.07744 test_loss: 0.08531 \n",
      "[ 84/500] train_loss: 0.06769 valid_loss: 0.07717 test_loss: 0.08511 \n",
      "[ 85/500] train_loss: 0.06575 valid_loss: 0.07578 test_loss: 0.08449 \n",
      "[ 86/500] train_loss: 0.06774 valid_loss: 0.07820 test_loss: 0.08652 \n",
      "[ 87/500] train_loss: 0.06545 valid_loss: 0.07711 test_loss: 0.08442 \n",
      "[ 88/500] train_loss: 0.06636 valid_loss: 0.07761 test_loss: 0.08663 \n",
      "[ 89/500] train_loss: 0.06826 valid_loss: 0.07735 test_loss: 0.08418 \n",
      "[ 90/500] train_loss: 0.06553 valid_loss: 0.07904 test_loss: 0.08422 \n",
      "[ 91/500] train_loss: 0.06620 valid_loss: 0.07894 test_loss: 0.08501 \n",
      "[ 92/500] train_loss: 0.06727 valid_loss: 0.07661 test_loss: 0.08269 \n",
      "[ 93/500] train_loss: 0.06439 valid_loss: 0.07871 test_loss: 0.08519 \n",
      "[ 94/500] train_loss: 0.06541 valid_loss: 0.07804 test_loss: 0.08326 \n",
      "[ 95/500] train_loss: 0.06595 valid_loss: 0.07841 test_loss: 0.08376 \n",
      "[ 96/500] train_loss: 0.06570 valid_loss: 0.07666 test_loss: 0.08495 \n",
      "[ 97/500] train_loss: 0.06646 valid_loss: 0.07796 test_loss: 0.08245 \n",
      "[ 98/500] train_loss: 0.06601 valid_loss: 0.07720 test_loss: 0.08418 \n",
      "[ 99/500] train_loss: 0.06662 valid_loss: 0.07822 test_loss: 0.08371 \n",
      "[100/500] train_loss: 0.06584 valid_loss: 0.07597 test_loss: 0.08411 \n",
      "[101/500] train_loss: 0.06476 valid_loss: 0.07702 test_loss: 0.08435 \n",
      "[102/500] train_loss: 0.06673 valid_loss: 0.08156 test_loss: 0.08347 \n",
      "[103/500] train_loss: 0.06588 valid_loss: 0.07407 test_loss: 0.08322 \n",
      "验证损失减少 (0.074975 --> 0.074068). 正在保存模型...\n",
      "[104/500] train_loss: 0.06409 valid_loss: 0.07539 test_loss: 0.08348 \n",
      "[105/500] train_loss: 0.06530 valid_loss: 0.07644 test_loss: 0.08308 \n",
      "[106/500] train_loss: 0.06487 valid_loss: 0.07519 test_loss: 0.08305 \n",
      "[107/500] train_loss: 0.06493 valid_loss: 0.07407 test_loss: 0.08304 \n",
      "[108/500] train_loss: 0.06258 valid_loss: 0.07413 test_loss: 0.08285 \n",
      "[109/500] train_loss: 0.06324 valid_loss: 0.07450 test_loss: 0.08330 \n",
      "[110/500] train_loss: 0.06521 valid_loss: 0.07584 test_loss: 0.08341 \n",
      "[111/500] train_loss: 0.06257 valid_loss: 0.07481 test_loss: 0.08230 \n",
      "[112/500] train_loss: 0.06342 valid_loss: 0.07322 test_loss: 0.08337 \n",
      "验证损失减少 (0.074068 --> 0.073222). 正在保存模型...\n",
      "[113/500] train_loss: 0.06332 valid_loss: 0.07694 test_loss: 0.08348 \n",
      "[114/500] train_loss: 0.06470 valid_loss: 0.07666 test_loss: 0.08304 \n",
      "[115/500] train_loss: 0.06250 valid_loss: 0.07537 test_loss: 0.08197 \n",
      "[116/500] train_loss: 0.06293 valid_loss: 0.07388 test_loss: 0.08315 \n",
      "[117/500] train_loss: 0.06266 valid_loss: 0.07474 test_loss: 0.08214 \n",
      "[118/500] train_loss: 0.06260 valid_loss: 0.07735 test_loss: 0.08318 \n",
      "[119/500] train_loss: 0.06464 valid_loss: 0.07336 test_loss: 0.08218 \n",
      "[120/500] train_loss: 0.06205 valid_loss: 0.07559 test_loss: 0.08251 \n",
      "[121/500] train_loss: 0.06322 valid_loss: 0.07431 test_loss: 0.08273 \n",
      "[122/500] train_loss: 0.06279 valid_loss: 0.07418 test_loss: 0.08179 \n",
      "[123/500] train_loss: 0.06174 valid_loss: 0.07345 test_loss: 0.08311 \n",
      "[124/500] train_loss: 0.06157 valid_loss: 0.07461 test_loss: 0.08180 \n",
      "[125/500] train_loss: 0.06214 valid_loss: 0.07502 test_loss: 0.08370 \n",
      "[126/500] train_loss: 0.06217 valid_loss: 0.07341 test_loss: 0.08195 \n",
      "[127/500] train_loss: 0.06055 valid_loss: 0.07453 test_loss: 0.08081 \n",
      "[128/500] train_loss: 0.06098 valid_loss: 0.07400 test_loss: 0.08256 \n",
      "[129/500] train_loss: 0.06185 valid_loss: 0.07365 test_loss: 0.08282 \n",
      "[130/500] train_loss: 0.06104 valid_loss: 0.07261 test_loss: 0.08196 \n",
      "验证损失减少 (0.073222 --> 0.072609). 正在保存模型...\n",
      "[131/500] train_loss: 0.06064 valid_loss: 0.07372 test_loss: 0.08263 \n",
      "[132/500] train_loss: 0.06079 valid_loss: 0.07398 test_loss: 0.08199 \n",
      "[133/500] train_loss: 0.06157 valid_loss: 0.07196 test_loss: 0.08151 \n",
      "验证损失减少 (0.072609 --> 0.071957). 正在保存模型...\n",
      "[134/500] train_loss: 0.06011 valid_loss: 0.07247 test_loss: 0.08127 \n",
      "[135/500] train_loss: 0.05999 valid_loss: 0.07373 test_loss: 0.08314 \n",
      "[136/500] train_loss: 0.05973 valid_loss: 0.07204 test_loss: 0.08206 \n",
      "[137/500] train_loss: 0.06104 valid_loss: 0.07510 test_loss: 0.08248 \n",
      "[138/500] train_loss: 0.05848 valid_loss: 0.07371 test_loss: 0.08187 \n",
      "[139/500] train_loss: 0.05997 valid_loss: 0.07403 test_loss: 0.08177 \n",
      "[140/500] train_loss: 0.06098 valid_loss: 0.07466 test_loss: 0.08182 \n",
      "[141/500] train_loss: 0.06056 valid_loss: 0.07350 test_loss: 0.08046 \n",
      "[142/500] train_loss: 0.05888 valid_loss: 0.07419 test_loss: 0.08245 \n",
      "[143/500] train_loss: 0.05950 valid_loss: 0.07268 test_loss: 0.08121 \n",
      "[144/500] train_loss: 0.06028 valid_loss: 0.07632 test_loss: 0.07995 \n",
      "[145/500] train_loss: 0.05908 valid_loss: 0.07344 test_loss: 0.08090 \n",
      "[146/500] train_loss: 0.06116 valid_loss: 0.07395 test_loss: 0.08110 \n",
      "[147/500] train_loss: 0.05953 valid_loss: 0.07365 test_loss: 0.08137 \n",
      "[148/500] train_loss: 0.05867 valid_loss: 0.07433 test_loss: 0.08147 \n",
      "[149/500] train_loss: 0.05941 valid_loss: 0.07322 test_loss: 0.08233 \n",
      "[150/500] train_loss: 0.05993 valid_loss: 0.07331 test_loss: 0.08168 \n",
      "[151/500] train_loss: 0.05857 valid_loss: 0.07468 test_loss: 0.08139 \n",
      "[152/500] train_loss: 0.05796 valid_loss: 0.07249 test_loss: 0.08234 \n",
      "[153/500] train_loss: 0.05952 valid_loss: 0.07322 test_loss: 0.08108 \n",
      "[154/500] train_loss: 0.05809 valid_loss: 0.07463 test_loss: 0.08170 \n",
      "[155/500] train_loss: 0.05800 valid_loss: 0.07265 test_loss: 0.08149 \n",
      "[156/500] train_loss: 0.05792 valid_loss: 0.07330 test_loss: 0.08141 \n",
      "[157/500] train_loss: 0.05896 valid_loss: 0.07609 test_loss: 0.08172 \n",
      "[158/500] train_loss: 0.05834 valid_loss: 0.07505 test_loss: 0.08181 \n",
      "[159/500] train_loss: 0.05697 valid_loss: 0.07300 test_loss: 0.08065 \n",
      "[160/500] train_loss: 0.05664 valid_loss: 0.07298 test_loss: 0.08286 \n",
      "[161/500] train_loss: 0.05804 valid_loss: 0.07375 test_loss: 0.08286 \n",
      "[162/500] train_loss: 0.05948 valid_loss: 0.07391 test_loss: 0.08425 \n",
      "[163/500] train_loss: 0.05812 valid_loss: 0.07570 test_loss: 0.08153 \n",
      "[164/500] train_loss: 0.05940 valid_loss: 0.07688 test_loss: 0.08287 \n",
      "[165/500] train_loss: 0.05710 valid_loss: 0.07372 test_loss: 0.08179 \n",
      "[166/500] train_loss: 0.05777 valid_loss: 0.07333 test_loss: 0.08126 \n",
      "[167/500] train_loss: 0.05657 valid_loss: 0.07124 test_loss: 0.08179 \n",
      "验证损失减少 (0.071957 --> 0.071243). 正在保存模型...\n",
      "[168/500] train_loss: 0.05626 valid_loss: 0.07401 test_loss: 0.08136 \n",
      "[169/500] train_loss: 0.05769 valid_loss: 0.07402 test_loss: 0.08151 \n",
      "[170/500] train_loss: 0.05742 valid_loss: 0.07433 test_loss: 0.08284 \n",
      "[171/500] train_loss: 0.05786 valid_loss: 0.07190 test_loss: 0.08113 \n",
      "[172/500] train_loss: 0.05710 valid_loss: 0.07209 test_loss: 0.08194 \n",
      "[173/500] train_loss: 0.05668 valid_loss: 0.07290 test_loss: 0.08087 \n",
      "[174/500] train_loss: 0.05700 valid_loss: 0.07486 test_loss: 0.08312 \n",
      "[175/500] train_loss: 0.05645 valid_loss: 0.07554 test_loss: 0.08265 \n",
      "[176/500] train_loss: 0.05761 valid_loss: 0.07161 test_loss: 0.08076 \n",
      "[177/500] train_loss: 0.05625 valid_loss: 0.07149 test_loss: 0.08027 \n",
      "[178/500] train_loss: 0.05551 valid_loss: 0.07229 test_loss: 0.08065 \n",
      "[179/500] train_loss: 0.05641 valid_loss: 0.07251 test_loss: 0.08111 \n",
      "[180/500] train_loss: 0.05598 valid_loss: 0.07139 test_loss: 0.07983 \n",
      "[181/500] train_loss: 0.05675 valid_loss: 0.07257 test_loss: 0.08192 \n",
      "[182/500] train_loss: 0.05600 valid_loss: 0.07182 test_loss: 0.07959 \n",
      "[183/500] train_loss: 0.05431 valid_loss: 0.07319 test_loss: 0.08071 \n",
      "[184/500] train_loss: 0.05670 valid_loss: 0.07191 test_loss: 0.07995 \n",
      "[185/500] train_loss: 0.05582 valid_loss: 0.07295 test_loss: 0.08162 \n",
      "[186/500] train_loss: 0.05561 valid_loss: 0.07202 test_loss: 0.08172 \n",
      "[187/500] train_loss: 0.05549 valid_loss: 0.07467 test_loss: 0.07996 \n",
      "[188/500] train_loss: 0.05574 valid_loss: 0.07080 test_loss: 0.08048 \n",
      "验证损失减少 (0.071243 --> 0.070798). 正在保存模型...\n",
      "[189/500] train_loss: 0.05456 valid_loss: 0.07475 test_loss: 0.08103 \n",
      "[190/500] train_loss: 0.05532 valid_loss: 0.07376 test_loss: 0.08108 \n",
      "[191/500] train_loss: 0.05524 valid_loss: 0.07148 test_loss: 0.08001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192/500] train_loss: 0.05522 valid_loss: 0.07380 test_loss: 0.07948 \n",
      "[193/500] train_loss: 0.05427 valid_loss: 0.07172 test_loss: 0.08058 \n",
      "[194/500] train_loss: 0.05479 valid_loss: 0.07279 test_loss: 0.08218 \n",
      "[195/500] train_loss: 0.05415 valid_loss: 0.07361 test_loss: 0.08010 \n",
      "[196/500] train_loss: 0.05303 valid_loss: 0.07346 test_loss: 0.08009 \n",
      "[197/500] train_loss: 0.05605 valid_loss: 0.07421 test_loss: 0.08113 \n",
      "[198/500] train_loss: 0.05453 valid_loss: 0.07374 test_loss: 0.08262 \n",
      "[199/500] train_loss: 0.05435 valid_loss: 0.07298 test_loss: 0.08262 \n",
      "[200/500] train_loss: 0.05459 valid_loss: 0.07567 test_loss: 0.08194 \n",
      "[201/500] train_loss: 0.05352 valid_loss: 0.07467 test_loss: 0.08030 \n",
      "[202/500] train_loss: 0.05315 valid_loss: 0.07444 test_loss: 0.08092 \n",
      "[203/500] train_loss: 0.05340 valid_loss: 0.07205 test_loss: 0.08230 \n",
      "[204/500] train_loss: 0.05406 valid_loss: 0.07193 test_loss: 0.08013 \n",
      "[205/500] train_loss: 0.05482 valid_loss: 0.07255 test_loss: 0.08022 \n",
      "[206/500] train_loss: 0.05359 valid_loss: 0.07366 test_loss: 0.08228 \n",
      "[207/500] train_loss: 0.05294 valid_loss: 0.07415 test_loss: 0.08263 \n",
      "[208/500] train_loss: 0.05503 valid_loss: 0.07187 test_loss: 0.08028 \n",
      "[209/500] train_loss: 0.05338 valid_loss: 0.07237 test_loss: 0.08076 \n",
      "[210/500] train_loss: 0.05308 valid_loss: 0.07125 test_loss: 0.08050 \n",
      "[211/500] train_loss: 0.05293 valid_loss: 0.07425 test_loss: 0.08072 \n",
      "[212/500] train_loss: 0.05409 valid_loss: 0.07249 test_loss: 0.08064 \n",
      "[213/500] train_loss: 0.05294 valid_loss: 0.07425 test_loss: 0.08245 \n",
      "[214/500] train_loss: 0.05349 valid_loss: 0.07336 test_loss: 0.08229 \n",
      "[215/500] train_loss: 0.05265 valid_loss: 0.07360 test_loss: 0.08114 \n",
      "[216/500] train_loss: 0.05379 valid_loss: 0.07502 test_loss: 0.08141 \n",
      "[217/500] train_loss: 0.05334 valid_loss: 0.07445 test_loss: 0.08182 \n",
      "[218/500] train_loss: 0.05262 valid_loss: 0.07274 test_loss: 0.08073 \n",
      "[219/500] train_loss: 0.05310 valid_loss: 0.07382 test_loss: 0.08205 \n",
      "[220/500] train_loss: 0.05394 valid_loss: 0.07278 test_loss: 0.08166 \n",
      "[221/500] train_loss: 0.05186 valid_loss: 0.07291 test_loss: 0.08177 \n",
      "[222/500] train_loss: 0.05295 valid_loss: 0.07347 test_loss: 0.08053 \n",
      "[223/500] train_loss: 0.05233 valid_loss: 0.07551 test_loss: 0.08187 \n",
      "[224/500] train_loss: 0.05282 valid_loss: 0.07284 test_loss: 0.08150 \n",
      "[225/500] train_loss: 0.05317 valid_loss: 0.07322 test_loss: 0.08107 \n",
      "[226/500] train_loss: 0.05160 valid_loss: 0.07192 test_loss: 0.07911 \n",
      "[227/500] train_loss: 0.05283 valid_loss: 0.07323 test_loss: 0.08077 \n",
      "[228/500] train_loss: 0.05334 valid_loss: 0.07523 test_loss: 0.07946 \n",
      "[229/500] train_loss: 0.05092 valid_loss: 0.07430 test_loss: 0.08272 \n",
      "[230/500] train_loss: 0.05273 valid_loss: 0.07234 test_loss: 0.08032 \n",
      "[231/500] train_loss: 0.05106 valid_loss: 0.07282 test_loss: 0.07916 \n",
      "[232/500] train_loss: 0.05235 valid_loss: 0.07189 test_loss: 0.07964 \n",
      "[233/500] train_loss: 0.05180 valid_loss: 0.07460 test_loss: 0.08149 \n",
      "[234/500] train_loss: 0.05239 valid_loss: 0.07410 test_loss: 0.08020 \n",
      "[235/500] train_loss: 0.05236 valid_loss: 0.07381 test_loss: 0.08087 \n",
      "[236/500] train_loss: 0.05115 valid_loss: 0.07454 test_loss: 0.08208 \n",
      "[237/500] train_loss: 0.05222 valid_loss: 0.07555 test_loss: 0.07970 \n",
      "[238/500] train_loss: 0.05200 valid_loss: 0.07146 test_loss: 0.07923 \n",
      "[239/500] train_loss: 0.05164 valid_loss: 0.07116 test_loss: 0.07956 \n",
      "[240/500] train_loss: 0.05324 valid_loss: 0.07246 test_loss: 0.08003 \n",
      "[241/500] train_loss: 0.05101 valid_loss: 0.07326 test_loss: 0.08039 \n",
      "[242/500] train_loss: 0.05237 valid_loss: 0.07216 test_loss: 0.08200 \n",
      "[243/500] train_loss: 0.05131 valid_loss: 0.07053 test_loss: 0.07981 \n",
      "验证损失减少 (0.070798 --> 0.070534). 正在保存模型...\n",
      "[244/500] train_loss: 0.05093 valid_loss: 0.07159 test_loss: 0.08047 \n",
      "[245/500] train_loss: 0.05115 valid_loss: 0.07232 test_loss: 0.08202 \n",
      "[246/500] train_loss: 0.05047 valid_loss: 0.07369 test_loss: 0.08089 \n",
      "[247/500] train_loss: 0.05105 valid_loss: 0.07293 test_loss: 0.08057 \n",
      "[248/500] train_loss: 0.04916 valid_loss: 0.07201 test_loss: 0.08124 \n",
      "[249/500] train_loss: 0.05062 valid_loss: 0.07424 test_loss: 0.08423 \n",
      "[250/500] train_loss: 0.05153 valid_loss: 0.07532 test_loss: 0.08045 \n",
      "[251/500] train_loss: 0.05248 valid_loss: 0.07365 test_loss: 0.07974 \n",
      "[252/500] train_loss: 0.05100 valid_loss: 0.07304 test_loss: 0.08186 \n",
      "[253/500] train_loss: 0.05062 valid_loss: 0.07236 test_loss: 0.08176 \n",
      "[254/500] train_loss: 0.04970 valid_loss: 0.07302 test_loss: 0.08144 \n",
      "[255/500] train_loss: 0.04993 valid_loss: 0.07224 test_loss: 0.08106 \n",
      "[256/500] train_loss: 0.05048 valid_loss: 0.07234 test_loss: 0.08108 \n",
      "[257/500] train_loss: 0.05016 valid_loss: 0.07170 test_loss: 0.08078 \n",
      "[258/500] train_loss: 0.05127 valid_loss: 0.07226 test_loss: 0.07942 \n",
      "[259/500] train_loss: 0.04958 valid_loss: 0.07248 test_loss: 0.08092 \n",
      "[260/500] train_loss: 0.04999 valid_loss: 0.07257 test_loss: 0.08196 \n",
      "[261/500] train_loss: 0.05039 valid_loss: 0.07350 test_loss: 0.08120 \n",
      "[262/500] train_loss: 0.04935 valid_loss: 0.07274 test_loss: 0.08180 \n",
      "[263/500] train_loss: 0.04947 valid_loss: 0.07029 test_loss: 0.08097 \n",
      "验证损失减少 (0.070534 --> 0.070292). 正在保存模型...\n",
      "[264/500] train_loss: 0.04806 valid_loss: 0.07154 test_loss: 0.08175 \n",
      "[265/500] train_loss: 0.05124 valid_loss: 0.07249 test_loss: 0.08135 \n",
      "[266/500] train_loss: 0.05071 valid_loss: 0.07112 test_loss: 0.08057 \n",
      "[267/500] train_loss: 0.05128 valid_loss: 0.07243 test_loss: 0.08018 \n",
      "[268/500] train_loss: 0.04932 valid_loss: 0.07296 test_loss: 0.08053 \n",
      "[269/500] train_loss: 0.04966 valid_loss: 0.07175 test_loss: 0.08040 \n",
      "[270/500] train_loss: 0.04933 valid_loss: 0.07106 test_loss: 0.08018 \n",
      "[271/500] train_loss: 0.05080 valid_loss: 0.07089 test_loss: 0.08025 \n",
      "[272/500] train_loss: 0.04905 valid_loss: 0.07265 test_loss: 0.08227 \n",
      "[273/500] train_loss: 0.04915 valid_loss: 0.07287 test_loss: 0.08118 \n",
      "[274/500] train_loss: 0.04951 valid_loss: 0.07176 test_loss: 0.08118 \n",
      "[275/500] train_loss: 0.04848 valid_loss: 0.07232 test_loss: 0.08253 \n",
      "[276/500] train_loss: 0.04841 valid_loss: 0.07264 test_loss: 0.08052 \n",
      "[277/500] train_loss: 0.05004 valid_loss: 0.07283 test_loss: 0.08199 \n",
      "[278/500] train_loss: 0.04999 valid_loss: 0.07271 test_loss: 0.08167 \n",
      "[279/500] train_loss: 0.04856 valid_loss: 0.07156 test_loss: 0.08020 \n",
      "[280/500] train_loss: 0.04842 valid_loss: 0.07289 test_loss: 0.08156 \n",
      "[281/500] train_loss: 0.04969 valid_loss: 0.07355 test_loss: 0.08163 \n",
      "[282/500] train_loss: 0.04893 valid_loss: 0.07225 test_loss: 0.07950 \n",
      "[283/500] train_loss: 0.04785 valid_loss: 0.07280 test_loss: 0.08221 \n",
      "[284/500] train_loss: 0.05097 valid_loss: 0.07129 test_loss: 0.08219 \n",
      "[285/500] train_loss: 0.04872 valid_loss: 0.07215 test_loss: 0.08248 \n",
      "[286/500] train_loss: 0.04830 valid_loss: 0.07281 test_loss: 0.08194 \n",
      "[287/500] train_loss: 0.04938 valid_loss: 0.07410 test_loss: 0.08163 \n",
      "[288/500] train_loss: 0.04931 valid_loss: 0.07180 test_loss: 0.08310 \n",
      "[289/500] train_loss: 0.04979 valid_loss: 0.07197 test_loss: 0.08189 \n",
      "[290/500] train_loss: 0.04787 valid_loss: 0.07200 test_loss: 0.08228 \n",
      "[291/500] train_loss: 0.04805 valid_loss: 0.07141 test_loss: 0.08284 \n",
      "[292/500] train_loss: 0.04892 valid_loss: 0.07530 test_loss: 0.08341 \n",
      "[293/500] train_loss: 0.04869 valid_loss: 0.07150 test_loss: 0.08179 \n",
      "[294/500] train_loss: 0.04971 valid_loss: 0.07174 test_loss: 0.08250 \n",
      "[295/500] train_loss: 0.04747 valid_loss: 0.07153 test_loss: 0.08123 \n",
      "[296/500] train_loss: 0.04757 valid_loss: 0.07262 test_loss: 0.08313 \n",
      "[297/500] train_loss: 0.04770 valid_loss: 0.07082 test_loss: 0.08191 \n",
      "[298/500] train_loss: 0.04771 valid_loss: 0.07574 test_loss: 0.08290 \n",
      "[299/500] train_loss: 0.04791 valid_loss: 0.07220 test_loss: 0.08194 \n",
      "[300/500] train_loss: 0.04887 valid_loss: 0.07248 test_loss: 0.08318 \n",
      "[301/500] train_loss: 0.04579 valid_loss: 0.07211 test_loss: 0.08358 \n",
      "[302/500] train_loss: 0.04810 valid_loss: 0.07308 test_loss: 0.08400 \n",
      "[303/500] train_loss: 0.04828 valid_loss: 0.07250 test_loss: 0.08287 \n",
      "[304/500] train_loss: 0.04776 valid_loss: 0.07139 test_loss: 0.08406 \n",
      "[305/500] train_loss: 0.04625 valid_loss: 0.07183 test_loss: 0.08285 \n",
      "[306/500] train_loss: 0.04752 valid_loss: 0.07629 test_loss: 0.08371 \n",
      "[307/500] train_loss: 0.04788 valid_loss: 0.07165 test_loss: 0.08326 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308/500] train_loss: 0.04609 valid_loss: 0.07257 test_loss: 0.08184 \n",
      "[309/500] train_loss: 0.04747 valid_loss: 0.07367 test_loss: 0.08273 \n",
      "[310/500] train_loss: 0.04633 valid_loss: 0.07357 test_loss: 0.08206 \n",
      "[311/500] train_loss: 0.04741 valid_loss: 0.07507 test_loss: 0.08279 \n",
      "[312/500] train_loss: 0.04726 valid_loss: 0.07625 test_loss: 0.08214 \n",
      "[313/500] train_loss: 0.04783 valid_loss: 0.07279 test_loss: 0.08226 \n",
      "[314/500] train_loss: 0.04842 valid_loss: 0.07447 test_loss: 0.08200 \n",
      "[315/500] train_loss: 0.04733 valid_loss: 0.07429 test_loss: 0.08319 \n",
      "[316/500] train_loss: 0.04631 valid_loss: 0.07360 test_loss: 0.08139 \n",
      "[317/500] train_loss: 0.04751 valid_loss: 0.07386 test_loss: 0.08266 \n",
      "[318/500] train_loss: 0.04692 valid_loss: 0.07290 test_loss: 0.08205 \n",
      "[319/500] train_loss: 0.04769 valid_loss: 0.07361 test_loss: 0.08191 \n",
      "[320/500] train_loss: 0.04721 valid_loss: 0.07410 test_loss: 0.08268 \n",
      "[321/500] train_loss: 0.04835 valid_loss: 0.07395 test_loss: 0.08510 \n",
      "[322/500] train_loss: 0.04621 valid_loss: 0.07221 test_loss: 0.08248 \n",
      "[323/500] train_loss: 0.04689 valid_loss: 0.07407 test_loss: 0.08332 \n",
      "[324/500] train_loss: 0.04642 valid_loss: 0.07490 test_loss: 0.08450 \n",
      "[325/500] train_loss: 0.04708 valid_loss: 0.07274 test_loss: 0.08188 \n",
      "[326/500] train_loss: 0.04558 valid_loss: 0.07522 test_loss: 0.08237 \n",
      "[327/500] train_loss: 0.04599 valid_loss: 0.07256 test_loss: 0.08272 \n",
      "[328/500] train_loss: 0.04619 valid_loss: 0.07249 test_loss: 0.08228 \n",
      "[329/500] train_loss: 0.04612 valid_loss: 0.07195 test_loss: 0.08102 \n",
      "[330/500] train_loss: 0.04607 valid_loss: 0.07297 test_loss: 0.08342 \n",
      "[331/500] train_loss: 0.04577 valid_loss: 0.07205 test_loss: 0.08259 \n",
      "[332/500] train_loss: 0.04809 valid_loss: 0.07165 test_loss: 0.08203 \n",
      "[333/500] train_loss: 0.04646 valid_loss: 0.07499 test_loss: 0.08255 \n",
      "[334/500] train_loss: 0.04659 valid_loss: 0.07426 test_loss: 0.08199 \n",
      "[335/500] train_loss: 0.04615 valid_loss: 0.07324 test_loss: 0.08234 \n",
      "[336/500] train_loss: 0.04667 valid_loss: 0.07989 test_loss: 0.08316 \n",
      "[337/500] train_loss: 0.04661 valid_loss: 0.07596 test_loss: 0.08270 \n",
      "[338/500] train_loss: 0.04603 valid_loss: 0.07324 test_loss: 0.08173 \n",
      "[339/500] train_loss: 0.04536 valid_loss: 0.07280 test_loss: 0.08218 \n",
      "[340/500] train_loss: 0.04677 valid_loss: 0.07403 test_loss: 0.08229 \n",
      "[341/500] train_loss: 0.04695 valid_loss: 0.07333 test_loss: 0.08133 \n",
      "[342/500] train_loss: 0.04720 valid_loss: 0.07332 test_loss: 0.08249 \n",
      "[343/500] train_loss: 0.04674 valid_loss: 0.07330 test_loss: 0.08197 \n",
      "[344/500] train_loss: 0.04584 valid_loss: 0.07180 test_loss: 0.08238 \n",
      "[345/500] train_loss: 0.04564 valid_loss: 0.07232 test_loss: 0.08278 \n",
      "[346/500] train_loss: 0.04498 valid_loss: 0.07331 test_loss: 0.08359 \n",
      "[347/500] train_loss: 0.04597 valid_loss: 0.07399 test_loss: 0.08305 \n",
      "[348/500] train_loss: 0.04648 valid_loss: 0.07355 test_loss: 0.08355 \n",
      "[349/500] train_loss: 0.04658 valid_loss: 0.07275 test_loss: 0.08227 \n",
      "[350/500] train_loss: 0.04416 valid_loss: 0.07323 test_loss: 0.08230 \n",
      "[351/500] train_loss: 0.04531 valid_loss: 0.07242 test_loss: 0.08200 \n",
      "[352/500] train_loss: 0.04512 valid_loss: 0.07286 test_loss: 0.08219 \n",
      "[353/500] train_loss: 0.04614 valid_loss: 0.07297 test_loss: 0.08156 \n",
      "[354/500] train_loss: 0.04543 valid_loss: 0.07371 test_loss: 0.08243 \n",
      "[355/500] train_loss: 0.04451 valid_loss: 0.07490 test_loss: 0.08274 \n",
      "[356/500] train_loss: 0.04548 valid_loss: 0.07334 test_loss: 0.08061 \n",
      "[357/500] train_loss: 0.04465 valid_loss: 0.07467 test_loss: 0.08227 \n",
      "[358/500] train_loss: 0.04551 valid_loss: 0.07471 test_loss: 0.08301 \n",
      "[359/500] train_loss: 0.04508 valid_loss: 0.07515 test_loss: 0.08244 \n",
      "[360/500] train_loss: 0.04568 valid_loss: 0.07529 test_loss: 0.08198 \n",
      "[361/500] train_loss: 0.04559 valid_loss: 0.07474 test_loss: 0.08206 \n",
      "[362/500] train_loss: 0.04468 valid_loss: 0.07637 test_loss: 0.08160 \n",
      "[363/500] train_loss: 0.04532 valid_loss: 0.07498 test_loss: 0.08358 \n",
      "[364/500] train_loss: 0.04554 valid_loss: 0.07438 test_loss: 0.08172 \n",
      "[365/500] train_loss: 0.04397 valid_loss: 0.07631 test_loss: 0.08106 \n",
      "[366/500] train_loss: 0.04440 valid_loss: 0.07342 test_loss: 0.08043 \n",
      "[367/500] train_loss: 0.04462 valid_loss: 0.07473 test_loss: 0.08011 \n",
      "[368/500] train_loss: 0.04455 valid_loss: 0.07509 test_loss: 0.08191 \n",
      "[369/500] train_loss: 0.04486 valid_loss: 0.07202 test_loss: 0.08193 \n",
      "[370/500] train_loss: 0.04364 valid_loss: 0.07321 test_loss: 0.08110 \n",
      "[371/500] train_loss: 0.04449 valid_loss: 0.07402 test_loss: 0.08157 \n",
      "[372/500] train_loss: 0.04539 valid_loss: 0.07230 test_loss: 0.08145 \n",
      "[373/500] train_loss: 0.04512 valid_loss: 0.07259 test_loss: 0.08181 \n",
      "[374/500] train_loss: 0.04527 valid_loss: 0.07469 test_loss: 0.08332 \n",
      "[375/500] train_loss: 0.04410 valid_loss: 0.07421 test_loss: 0.08170 \n",
      "[376/500] train_loss: 0.04406 valid_loss: 0.07275 test_loss: 0.08234 \n",
      "[377/500] train_loss: 0.04465 valid_loss: 0.07469 test_loss: 0.08257 \n",
      "[378/500] train_loss: 0.04565 valid_loss: 0.07270 test_loss: 0.08306 \n",
      "[379/500] train_loss: 0.04378 valid_loss: 0.07344 test_loss: 0.08254 \n",
      "[380/500] train_loss: 0.04341 valid_loss: 0.07376 test_loss: 0.08310 \n",
      "[381/500] train_loss: 0.04407 valid_loss: 0.07211 test_loss: 0.08271 \n",
      "[382/500] train_loss: 0.04388 valid_loss: 0.07253 test_loss: 0.08305 \n",
      "[383/500] train_loss: 0.04542 valid_loss: 0.07467 test_loss: 0.08363 \n",
      "[384/500] train_loss: 0.04455 valid_loss: 0.07473 test_loss: 0.08334 \n",
      "[385/500] train_loss: 0.04445 valid_loss: 0.07214 test_loss: 0.08092 \n",
      "[386/500] train_loss: 0.04418 valid_loss: 0.07268 test_loss: 0.08229 \n",
      "[387/500] train_loss: 0.04402 valid_loss: 0.07403 test_loss: 0.08253 \n",
      "[388/500] train_loss: 0.04431 valid_loss: 0.07379 test_loss: 0.08229 \n",
      "[389/500] train_loss: 0.04382 valid_loss: 0.07270 test_loss: 0.08247 \n",
      "[390/500] train_loss: 0.04302 valid_loss: 0.07344 test_loss: 0.08091 \n",
      "[391/500] train_loss: 0.04279 valid_loss: 0.07451 test_loss: 0.08140 \n",
      "[392/500] train_loss: 0.04354 valid_loss: 0.07461 test_loss: 0.08427 \n",
      "[393/500] train_loss: 0.04360 valid_loss: 0.07455 test_loss: 0.08077 \n",
      "[394/500] train_loss: 0.04427 valid_loss: 0.07360 test_loss: 0.08087 \n",
      "[395/500] train_loss: 0.04463 valid_loss: 0.07451 test_loss: 0.08232 \n",
      "[396/500] train_loss: 0.04488 valid_loss: 0.07500 test_loss: 0.08066 \n",
      "[397/500] train_loss: 0.04468 valid_loss: 0.07505 test_loss: 0.08085 \n",
      "[398/500] train_loss: 0.04446 valid_loss: 0.07449 test_loss: 0.08342 \n",
      "[399/500] train_loss: 0.04436 valid_loss: 0.07527 test_loss: 0.08097 \n",
      "[400/500] train_loss: 0.04546 valid_loss: 0.07791 test_loss: 0.08245 \n",
      "[401/500] train_loss: 0.04337 valid_loss: 0.07394 test_loss: 0.08169 \n",
      "[402/500] train_loss: 0.04370 valid_loss: 0.07447 test_loss: 0.08217 \n",
      "[403/500] train_loss: 0.04356 valid_loss: 0.07449 test_loss: 0.08234 \n",
      "[404/500] train_loss: 0.04370 valid_loss: 0.07429 test_loss: 0.08230 \n",
      "[405/500] train_loss: 0.04390 valid_loss: 0.07384 test_loss: 0.08236 \n",
      "[406/500] train_loss: 0.04355 valid_loss: 0.07410 test_loss: 0.08232 \n",
      "[407/500] train_loss: 0.04348 valid_loss: 0.07354 test_loss: 0.08167 \n",
      "[408/500] train_loss: 0.04317 valid_loss: 0.07303 test_loss: 0.08274 \n",
      "[409/500] train_loss: 0.04358 valid_loss: 0.07402 test_loss: 0.08231 \n",
      "[410/500] train_loss: 0.04356 valid_loss: 0.07385 test_loss: 0.08283 \n",
      "[411/500] train_loss: 0.04283 valid_loss: 0.07288 test_loss: 0.08263 \n",
      "[412/500] train_loss: 0.04453 valid_loss: 0.07343 test_loss: 0.08191 \n",
      "[413/500] train_loss: 0.04435 valid_loss: 0.07352 test_loss: 0.08302 \n",
      "[414/500] train_loss: 0.04268 valid_loss: 0.07484 test_loss: 0.08190 \n",
      "[415/500] train_loss: 0.04405 valid_loss: 0.07461 test_loss: 0.08267 \n",
      "[416/500] train_loss: 0.04335 valid_loss: 0.07369 test_loss: 0.08274 \n",
      "[417/500] train_loss: 0.04349 valid_loss: 0.07242 test_loss: 0.08110 \n",
      "[418/500] train_loss: 0.04429 valid_loss: 0.07454 test_loss: 0.08109 \n",
      "[419/500] train_loss: 0.04238 valid_loss: 0.07396 test_loss: 0.08086 \n",
      "[420/500] train_loss: 0.04394 valid_loss: 0.07436 test_loss: 0.08174 \n",
      "[421/500] train_loss: 0.04403 valid_loss: 0.07315 test_loss: 0.08189 \n",
      "[422/500] train_loss: 0.04304 valid_loss: 0.07279 test_loss: 0.08179 \n",
      "[423/500] train_loss: 0.04415 valid_loss: 0.07118 test_loss: 0.08070 \n",
      "[424/500] train_loss: 0.04172 valid_loss: 0.07314 test_loss: 0.08088 \n",
      "[425/500] train_loss: 0.04234 valid_loss: 0.07266 test_loss: 0.08042 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426/500] train_loss: 0.04347 valid_loss: 0.07377 test_loss: 0.08125 \n",
      "[427/500] train_loss: 0.04303 valid_loss: 0.07350 test_loss: 0.08134 \n",
      "[428/500] train_loss: 0.04281 valid_loss: 0.07520 test_loss: 0.08326 \n",
      "[429/500] train_loss: 0.04353 valid_loss: 0.07742 test_loss: 0.08425 \n",
      "[430/500] train_loss: 0.04297 valid_loss: 0.07366 test_loss: 0.08244 \n",
      "[431/500] train_loss: 0.04237 valid_loss: 0.07463 test_loss: 0.08225 \n",
      "[432/500] train_loss: 0.04400 valid_loss: 0.07335 test_loss: 0.08347 \n",
      "[433/500] train_loss: 0.04287 valid_loss: 0.07529 test_loss: 0.08237 \n",
      "[434/500] train_loss: 0.04281 valid_loss: 0.07287 test_loss: 0.08307 \n",
      "[435/500] train_loss: 0.04254 valid_loss: 0.07424 test_loss: 0.08138 \n",
      "[436/500] train_loss: 0.04083 valid_loss: 0.07464 test_loss: 0.08176 \n",
      "[437/500] train_loss: 0.04267 valid_loss: 0.07373 test_loss: 0.08299 \n",
      "[438/500] train_loss: 0.04208 valid_loss: 0.07828 test_loss: 0.08205 \n",
      "[439/500] train_loss: 0.04235 valid_loss: 0.07714 test_loss: 0.08299 \n",
      "[440/500] train_loss: 0.04371 valid_loss: 0.07490 test_loss: 0.08138 \n",
      "[441/500] train_loss: 0.04236 valid_loss: 0.07470 test_loss: 0.08178 \n",
      "[442/500] train_loss: 0.04265 valid_loss: 0.07492 test_loss: 0.08132 \n",
      "[443/500] train_loss: 0.04278 valid_loss: 0.07568 test_loss: 0.08201 \n",
      "[444/500] train_loss: 0.04280 valid_loss: 0.07435 test_loss: 0.08087 \n",
      "[445/500] train_loss: 0.04244 valid_loss: 0.07493 test_loss: 0.08229 \n",
      "[446/500] train_loss: 0.04426 valid_loss: 0.07532 test_loss: 0.08522 \n",
      "[447/500] train_loss: 0.04195 valid_loss: 0.07440 test_loss: 0.08298 \n",
      "[448/500] train_loss: 0.04263 valid_loss: 0.07379 test_loss: 0.08353 \n",
      "[449/500] train_loss: 0.04209 valid_loss: 0.07339 test_loss: 0.08165 \n",
      "[450/500] train_loss: 0.04224 valid_loss: 0.07668 test_loss: 0.08280 \n",
      "[451/500] train_loss: 0.04127 valid_loss: 0.07425 test_loss: 0.08234 \n",
      "[452/500] train_loss: 0.04216 valid_loss: 0.07618 test_loss: 0.08198 \n",
      "[453/500] train_loss: 0.04224 valid_loss: 0.07468 test_loss: 0.08264 \n",
      "[454/500] train_loss: 0.04309 valid_loss: 0.07484 test_loss: 0.08279 \n",
      "[455/500] train_loss: 0.04226 valid_loss: 0.07315 test_loss: 0.08304 \n",
      "[456/500] train_loss: 0.04204 valid_loss: 0.07374 test_loss: 0.08250 \n",
      "[457/500] train_loss: 0.04226 valid_loss: 0.07364 test_loss: 0.08211 \n",
      "[458/500] train_loss: 0.04285 valid_loss: 0.07368 test_loss: 0.08206 \n",
      "[459/500] train_loss: 0.04204 valid_loss: 0.07482 test_loss: 0.08180 \n",
      "[460/500] train_loss: 0.04157 valid_loss: 0.07417 test_loss: 0.08248 \n",
      "[461/500] train_loss: 0.04313 valid_loss: 0.07447 test_loss: 0.08447 \n",
      "[462/500] train_loss: 0.04191 valid_loss: 0.07497 test_loss: 0.08278 \n",
      "[463/500] train_loss: 0.04327 valid_loss: 0.07481 test_loss: 0.08276 \n",
      "[464/500] train_loss: 0.04211 valid_loss: 0.07643 test_loss: 0.08363 \n",
      "[465/500] train_loss: 0.04147 valid_loss: 0.07602 test_loss: 0.08428 \n",
      "[466/500] train_loss: 0.04066 valid_loss: 0.07386 test_loss: 0.08187 \n",
      "[467/500] train_loss: 0.04315 valid_loss: 0.07838 test_loss: 0.08262 \n",
      "[468/500] train_loss: 0.04144 valid_loss: 0.07493 test_loss: 0.08180 \n",
      "[469/500] train_loss: 0.04270 valid_loss: 0.07521 test_loss: 0.08430 \n",
      "[470/500] train_loss: 0.04121 valid_loss: 0.07460 test_loss: 0.08251 \n",
      "[471/500] train_loss: 0.04152 valid_loss: 0.07547 test_loss: 0.08391 \n",
      "[472/500] train_loss: 0.04105 valid_loss: 0.07658 test_loss: 0.08302 \n",
      "[473/500] train_loss: 0.04104 valid_loss: 0.07571 test_loss: 0.08459 \n",
      "[474/500] train_loss: 0.04240 valid_loss: 0.07467 test_loss: 0.08302 \n",
      "[475/500] train_loss: 0.04157 valid_loss: 0.07341 test_loss: 0.08354 \n",
      "[476/500] train_loss: 0.04071 valid_loss: 0.07237 test_loss: 0.08208 \n",
      "[477/500] train_loss: 0.04082 valid_loss: 0.07577 test_loss: 0.08367 \n",
      "[478/500] train_loss: 0.04030 valid_loss: 0.07450 test_loss: 0.08278 \n",
      "[479/500] train_loss: 0.04335 valid_loss: 0.07663 test_loss: 0.08231 \n",
      "[480/500] train_loss: 0.04281 valid_loss: 0.07552 test_loss: 0.08394 \n",
      "[481/500] train_loss: 0.04077 valid_loss: 0.07414 test_loss: 0.08407 \n",
      "[482/500] train_loss: 0.04184 valid_loss: 0.07555 test_loss: 0.08245 \n",
      "[483/500] train_loss: 0.04065 valid_loss: 0.07520 test_loss: 0.08361 \n",
      "[484/500] train_loss: 0.04144 valid_loss: 0.07591 test_loss: 0.08470 \n",
      "[485/500] train_loss: 0.04161 valid_loss: 0.07692 test_loss: 0.08424 \n",
      "[486/500] train_loss: 0.04097 valid_loss: 0.07483 test_loss: 0.08083 \n",
      "[487/500] train_loss: 0.04146 valid_loss: 0.07565 test_loss: 0.08314 \n",
      "[488/500] train_loss: 0.04035 valid_loss: 0.07563 test_loss: 0.08432 \n",
      "[489/500] train_loss: 0.04192 valid_loss: 0.07407 test_loss: 0.08388 \n",
      "[490/500] train_loss: 0.03971 valid_loss: 0.07444 test_loss: 0.08375 \n",
      "[491/500] train_loss: 0.04219 valid_loss: 0.07422 test_loss: 0.08460 \n",
      "[492/500] train_loss: 0.04020 valid_loss: 0.07439 test_loss: 0.08277 \n",
      "[493/500] train_loss: 0.04134 valid_loss: 0.07335 test_loss: 0.08386 \n",
      "[494/500] train_loss: 0.04135 valid_loss: 0.07530 test_loss: 0.08356 \n",
      "[495/500] train_loss: 0.04167 valid_loss: 0.07465 test_loss: 0.08372 \n",
      "[496/500] train_loss: 0.04253 valid_loss: 0.07443 test_loss: 0.08230 \n",
      "[497/500] train_loss: 0.04144 valid_loss: 0.07523 test_loss: 0.08308 \n",
      "[498/500] train_loss: 0.04118 valid_loss: 0.07522 test_loss: 0.08319 \n",
      "[499/500] train_loss: 0.04068 valid_loss: 0.07649 test_loss: 0.08506 \n",
      "[500/500] train_loss: 0.04073 valid_loss: 0.07660 test_loss: 0.08479 \n",
      "TRAINING MODEL 9\n",
      "[  1/500] train_loss: 0.35452 valid_loss: 0.26793 test_loss: 0.27570 \n",
      "验证损失减少 (inf --> 0.267932). 正在保存模型...\n",
      "[  2/500] train_loss: 0.20725 valid_loss: 0.19029 test_loss: 0.19801 \n",
      "验证损失减少 (0.267932 --> 0.190287). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15819 valid_loss: 0.16204 test_loss: 0.16944 \n",
      "验证损失减少 (0.190287 --> 0.162040). 正在保存模型...\n",
      "[  4/500] train_loss: 0.14542 valid_loss: 0.13921 test_loss: 0.14845 \n",
      "验证损失减少 (0.162040 --> 0.139207). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13094 valid_loss: 0.13205 test_loss: 0.14539 \n",
      "验证损失减少 (0.139207 --> 0.132049). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12725 valid_loss: 0.13726 test_loss: 0.14948 \n",
      "[  7/500] train_loss: 0.11852 valid_loss: 0.12335 test_loss: 0.13370 \n",
      "验证损失减少 (0.132049 --> 0.123352). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11768 valid_loss: 0.11845 test_loss: 0.12949 \n",
      "验证损失减少 (0.123352 --> 0.118447). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11394 valid_loss: 0.11323 test_loss: 0.12637 \n",
      "验证损失减少 (0.118447 --> 0.113226). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11193 valid_loss: 0.11027 test_loss: 0.12305 \n",
      "验证损失减少 (0.113226 --> 0.110270). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.11053 valid_loss: 0.10847 test_loss: 0.12167 \n",
      "验证损失减少 (0.110270 --> 0.108466). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10639 valid_loss: 0.10687 test_loss: 0.12066 \n",
      "验证损失减少 (0.108466 --> 0.106870). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10477 valid_loss: 0.10991 test_loss: 0.12043 \n",
      "[ 14/500] train_loss: 0.10288 valid_loss: 0.10628 test_loss: 0.11751 \n",
      "验证损失减少 (0.106870 --> 0.106283). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10592 valid_loss: 0.10374 test_loss: 0.11756 \n",
      "验证损失减少 (0.106283 --> 0.103744). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10006 valid_loss: 0.10491 test_loss: 0.11842 \n",
      "[ 17/500] train_loss: 0.09718 valid_loss: 0.10129 test_loss: 0.11432 \n",
      "验证损失减少 (0.103744 --> 0.101287). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09909 valid_loss: 0.10258 test_loss: 0.11582 \n",
      "[ 19/500] train_loss: 0.09589 valid_loss: 0.09778 test_loss: 0.10975 \n",
      "验证损失减少 (0.101287 --> 0.097778). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09641 valid_loss: 0.09762 test_loss: 0.10978 \n",
      "验证损失减少 (0.097778 --> 0.097619). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09542 valid_loss: 0.09818 test_loss: 0.10778 \n",
      "[ 22/500] train_loss: 0.09304 valid_loss: 0.09469 test_loss: 0.10746 \n",
      "验证损失减少 (0.097619 --> 0.094694). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09232 valid_loss: 0.09381 test_loss: 0.10686 \n",
      "验证损失减少 (0.094694 --> 0.093810). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09300 valid_loss: 0.09219 test_loss: 0.10471 \n",
      "验证损失减少 (0.093810 --> 0.092192). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.08957 valid_loss: 0.09123 test_loss: 0.10283 \n",
      "验证损失减少 (0.092192 --> 0.091233). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.09085 valid_loss: 0.09259 test_loss: 0.10501 \n",
      "[ 27/500] train_loss: 0.08944 valid_loss: 0.09174 test_loss: 0.10267 \n",
      "[ 28/500] train_loss: 0.08857 valid_loss: 0.09138 test_loss: 0.10545 \n",
      "[ 29/500] train_loss: 0.08873 valid_loss: 0.09038 test_loss: 0.10193 \n",
      "验证损失减少 (0.091233 --> 0.090379). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08580 valid_loss: 0.09062 test_loss: 0.10209 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31/500] train_loss: 0.08562 valid_loss: 0.08932 test_loss: 0.10168 \n",
      "验证损失减少 (0.090379 --> 0.089317). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08646 valid_loss: 0.08914 test_loss: 0.09885 \n",
      "验证损失减少 (0.089317 --> 0.089135). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08625 valid_loss: 0.08841 test_loss: 0.09854 \n",
      "验证损失减少 (0.089135 --> 0.088406). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08537 valid_loss: 0.08730 test_loss: 0.09775 \n",
      "验证损失减少 (0.088406 --> 0.087301). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08569 valid_loss: 0.08721 test_loss: 0.09890 \n",
      "验证损失减少 (0.087301 --> 0.087213). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08306 valid_loss: 0.08892 test_loss: 0.09716 \n",
      "[ 37/500] train_loss: 0.08504 valid_loss: 0.08583 test_loss: 0.09670 \n",
      "验证损失减少 (0.087213 --> 0.085827). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.08248 valid_loss: 0.08743 test_loss: 0.09836 \n",
      "[ 39/500] train_loss: 0.08259 valid_loss: 0.09283 test_loss: 0.09757 \n",
      "[ 40/500] train_loss: 0.08365 valid_loss: 0.08662 test_loss: 0.09634 \n",
      "[ 41/500] train_loss: 0.08359 valid_loss: 0.08739 test_loss: 0.09662 \n",
      "[ 42/500] train_loss: 0.07999 valid_loss: 0.08357 test_loss: 0.09395 \n",
      "验证损失减少 (0.085827 --> 0.083569). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.08015 valid_loss: 0.08312 test_loss: 0.09359 \n",
      "验证损失减少 (0.083569 --> 0.083121). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.08080 valid_loss: 0.08459 test_loss: 0.09318 \n",
      "[ 45/500] train_loss: 0.07834 valid_loss: 0.08412 test_loss: 0.09465 \n",
      "[ 46/500] train_loss: 0.07822 valid_loss: 0.08362 test_loss: 0.09462 \n",
      "[ 47/500] train_loss: 0.07918 valid_loss: 0.08304 test_loss: 0.09284 \n",
      "验证损失减少 (0.083121 --> 0.083036). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.07750 valid_loss: 0.08271 test_loss: 0.09518 \n",
      "验证损失减少 (0.083036 --> 0.082713). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07750 valid_loss: 0.08301 test_loss: 0.09259 \n",
      "[ 50/500] train_loss: 0.07766 valid_loss: 0.08389 test_loss: 0.09297 \n",
      "[ 51/500] train_loss: 0.07519 valid_loss: 0.08155 test_loss: 0.09206 \n",
      "验证损失减少 (0.082713 --> 0.081549). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07688 valid_loss: 0.08162 test_loss: 0.09190 \n",
      "[ 53/500] train_loss: 0.07618 valid_loss: 0.08285 test_loss: 0.09246 \n",
      "[ 54/500] train_loss: 0.07568 valid_loss: 0.08057 test_loss: 0.08974 \n",
      "验证损失减少 (0.081549 --> 0.080565). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07620 valid_loss: 0.08081 test_loss: 0.09069 \n",
      "[ 56/500] train_loss: 0.07611 valid_loss: 0.08136 test_loss: 0.09088 \n",
      "[ 57/500] train_loss: 0.07458 valid_loss: 0.08099 test_loss: 0.08974 \n",
      "[ 58/500] train_loss: 0.07560 valid_loss: 0.08179 test_loss: 0.09040 \n",
      "[ 59/500] train_loss: 0.07539 valid_loss: 0.08095 test_loss: 0.08953 \n",
      "[ 60/500] train_loss: 0.07305 valid_loss: 0.07965 test_loss: 0.08842 \n",
      "验证损失减少 (0.080565 --> 0.079653). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.07330 valid_loss: 0.08091 test_loss: 0.08924 \n",
      "[ 62/500] train_loss: 0.07350 valid_loss: 0.08018 test_loss: 0.08833 \n",
      "[ 63/500] train_loss: 0.07322 valid_loss: 0.08049 test_loss: 0.08926 \n",
      "[ 64/500] train_loss: 0.07165 valid_loss: 0.07939 test_loss: 0.08996 \n",
      "验证损失减少 (0.079653 --> 0.079386). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07248 valid_loss: 0.07932 test_loss: 0.08814 \n",
      "验证损失减少 (0.079386 --> 0.079316). 正在保存模型...\n",
      "[ 66/500] train_loss: 0.07282 valid_loss: 0.08346 test_loss: 0.09328 \n",
      "[ 67/500] train_loss: 0.07305 valid_loss: 0.08168 test_loss: 0.08945 \n",
      "[ 68/500] train_loss: 0.07321 valid_loss: 0.08088 test_loss: 0.08815 \n",
      "[ 69/500] train_loss: 0.07154 valid_loss: 0.07773 test_loss: 0.08800 \n",
      "验证损失减少 (0.079316 --> 0.077730). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.07050 valid_loss: 0.07895 test_loss: 0.08802 \n",
      "[ 71/500] train_loss: 0.07139 valid_loss: 0.07823 test_loss: 0.08698 \n",
      "[ 72/500] train_loss: 0.07338 valid_loss: 0.07845 test_loss: 0.08815 \n",
      "[ 73/500] train_loss: 0.07158 valid_loss: 0.07801 test_loss: 0.08601 \n",
      "[ 74/500] train_loss: 0.07024 valid_loss: 0.07865 test_loss: 0.08858 \n",
      "[ 75/500] train_loss: 0.07122 valid_loss: 0.07781 test_loss: 0.08826 \n",
      "[ 76/500] train_loss: 0.07118 valid_loss: 0.07873 test_loss: 0.08684 \n",
      "[ 77/500] train_loss: 0.07033 valid_loss: 0.07826 test_loss: 0.08552 \n",
      "[ 78/500] train_loss: 0.07184 valid_loss: 0.07819 test_loss: 0.08586 \n",
      "[ 79/500] train_loss: 0.06926 valid_loss: 0.07747 test_loss: 0.08600 \n",
      "验证损失减少 (0.077730 --> 0.077467). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06964 valid_loss: 0.07998 test_loss: 0.08780 \n",
      "[ 81/500] train_loss: 0.06895 valid_loss: 0.07780 test_loss: 0.08530 \n",
      "[ 82/500] train_loss: 0.06987 valid_loss: 0.07801 test_loss: 0.08534 \n",
      "[ 83/500] train_loss: 0.06926 valid_loss: 0.07727 test_loss: 0.08468 \n",
      "验证损失减少 (0.077467 --> 0.077270). 正在保存模型...\n",
      "[ 84/500] train_loss: 0.06751 valid_loss: 0.07824 test_loss: 0.08598 \n",
      "[ 85/500] train_loss: 0.06957 valid_loss: 0.07658 test_loss: 0.08374 \n",
      "验证损失减少 (0.077270 --> 0.076583). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.06688 valid_loss: 0.07714 test_loss: 0.08401 \n",
      "[ 87/500] train_loss: 0.06609 valid_loss: 0.07812 test_loss: 0.08479 \n",
      "[ 88/500] train_loss: 0.06704 valid_loss: 0.07703 test_loss: 0.08463 \n",
      "[ 89/500] train_loss: 0.06804 valid_loss: 0.07635 test_loss: 0.08429 \n",
      "验证损失减少 (0.076583 --> 0.076352). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.06825 valid_loss: 0.07506 test_loss: 0.08350 \n",
      "验证损失减少 (0.076352 --> 0.075061). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.06747 valid_loss: 0.07820 test_loss: 0.08723 \n",
      "[ 92/500] train_loss: 0.06764 valid_loss: 0.07741 test_loss: 0.08325 \n",
      "[ 93/500] train_loss: 0.06933 valid_loss: 0.07789 test_loss: 0.08428 \n",
      "[ 94/500] train_loss: 0.06610 valid_loss: 0.07553 test_loss: 0.08437 \n",
      "[ 95/500] train_loss: 0.06657 valid_loss: 0.07652 test_loss: 0.08413 \n",
      "[ 96/500] train_loss: 0.06682 valid_loss: 0.07499 test_loss: 0.08331 \n",
      "验证损失减少 (0.075061 --> 0.074987). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.06530 valid_loss: 0.07558 test_loss: 0.08320 \n",
      "[ 98/500] train_loss: 0.06706 valid_loss: 0.07625 test_loss: 0.08344 \n",
      "[ 99/500] train_loss: 0.06518 valid_loss: 0.07692 test_loss: 0.08439 \n",
      "[100/500] train_loss: 0.06455 valid_loss: 0.07680 test_loss: 0.08242 \n",
      "[101/500] train_loss: 0.06418 valid_loss: 0.07612 test_loss: 0.08233 \n",
      "[102/500] train_loss: 0.06565 valid_loss: 0.07399 test_loss: 0.08278 \n",
      "验证损失减少 (0.074987 --> 0.073986). 正在保存模型...\n",
      "[103/500] train_loss: 0.06571 valid_loss: 0.07479 test_loss: 0.08238 \n",
      "[104/500] train_loss: 0.06646 valid_loss: 0.07561 test_loss: 0.08352 \n",
      "[105/500] train_loss: 0.06570 valid_loss: 0.07497 test_loss: 0.08257 \n",
      "[106/500] train_loss: 0.06652 valid_loss: 0.07531 test_loss: 0.08267 \n",
      "[107/500] train_loss: 0.06486 valid_loss: 0.07591 test_loss: 0.08274 \n",
      "[108/500] train_loss: 0.06420 valid_loss: 0.07507 test_loss: 0.08396 \n",
      "[109/500] train_loss: 0.06579 valid_loss: 0.07527 test_loss: 0.08303 \n",
      "[110/500] train_loss: 0.06447 valid_loss: 0.07690 test_loss: 0.08310 \n",
      "[111/500] train_loss: 0.06421 valid_loss: 0.07630 test_loss: 0.08294 \n",
      "[112/500] train_loss: 0.06404 valid_loss: 0.07590 test_loss: 0.08395 \n",
      "[113/500] train_loss: 0.06372 valid_loss: 0.07377 test_loss: 0.08300 \n",
      "验证损失减少 (0.073986 --> 0.073773). 正在保存模型...\n",
      "[114/500] train_loss: 0.06253 valid_loss: 0.07524 test_loss: 0.08309 \n",
      "[115/500] train_loss: 0.06454 valid_loss: 0.07441 test_loss: 0.08248 \n",
      "[116/500] train_loss: 0.06357 valid_loss: 0.07575 test_loss: 0.08254 \n",
      "[117/500] train_loss: 0.06443 valid_loss: 0.07571 test_loss: 0.08273 \n",
      "[118/500] train_loss: 0.06126 valid_loss: 0.07393 test_loss: 0.08214 \n",
      "[119/500] train_loss: 0.06354 valid_loss: 0.07369 test_loss: 0.08169 \n",
      "验证损失减少 (0.073773 --> 0.073685). 正在保存模型...\n",
      "[120/500] train_loss: 0.06362 valid_loss: 0.07316 test_loss: 0.08075 \n",
      "验证损失减少 (0.073685 --> 0.073161). 正在保存模型...\n",
      "[121/500] train_loss: 0.06120 valid_loss: 0.07457 test_loss: 0.08365 \n",
      "[122/500] train_loss: 0.06192 valid_loss: 0.07435 test_loss: 0.08228 \n",
      "[123/500] train_loss: 0.06008 valid_loss: 0.07429 test_loss: 0.08206 \n",
      "[124/500] train_loss: 0.06216 valid_loss: 0.07329 test_loss: 0.08189 \n",
      "[125/500] train_loss: 0.06076 valid_loss: 0.07377 test_loss: 0.08230 \n",
      "[126/500] train_loss: 0.06282 valid_loss: 0.07714 test_loss: 0.08441 \n",
      "[127/500] train_loss: 0.06116 valid_loss: 0.07471 test_loss: 0.08281 \n",
      "[128/500] train_loss: 0.06165 valid_loss: 0.07397 test_loss: 0.08501 \n",
      "[129/500] train_loss: 0.06089 valid_loss: 0.07269 test_loss: 0.08216 \n",
      "验证损失减少 (0.073161 --> 0.072688). 正在保存模型...\n",
      "[130/500] train_loss: 0.05979 valid_loss: 0.07293 test_loss: 0.08143 \n",
      "[131/500] train_loss: 0.06123 valid_loss: 0.07581 test_loss: 0.08279 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132/500] train_loss: 0.06155 valid_loss: 0.07534 test_loss: 0.08069 \n",
      "[133/500] train_loss: 0.06129 valid_loss: 0.07266 test_loss: 0.08142 \n",
      "验证损失减少 (0.072688 --> 0.072656). 正在保存模型...\n",
      "[134/500] train_loss: 0.06057 valid_loss: 0.07465 test_loss: 0.08105 \n",
      "[135/500] train_loss: 0.05878 valid_loss: 0.07555 test_loss: 0.08318 \n",
      "[136/500] train_loss: 0.06014 valid_loss: 0.07384 test_loss: 0.08314 \n",
      "[137/500] train_loss: 0.06020 valid_loss: 0.07314 test_loss: 0.08180 \n",
      "[138/500] train_loss: 0.05989 valid_loss: 0.07251 test_loss: 0.08142 \n",
      "验证损失减少 (0.072656 --> 0.072509). 正在保存模型...\n",
      "[139/500] train_loss: 0.05981 valid_loss: 0.07425 test_loss: 0.08311 \n",
      "[140/500] train_loss: 0.06040 valid_loss: 0.07299 test_loss: 0.08103 \n",
      "[141/500] train_loss: 0.06117 valid_loss: 0.07603 test_loss: 0.08290 \n",
      "[142/500] train_loss: 0.06054 valid_loss: 0.07303 test_loss: 0.08278 \n",
      "[143/500] train_loss: 0.06009 valid_loss: 0.07304 test_loss: 0.08190 \n",
      "[144/500] train_loss: 0.05829 valid_loss: 0.07338 test_loss: 0.08276 \n",
      "[145/500] train_loss: 0.05813 valid_loss: 0.07328 test_loss: 0.08128 \n",
      "[146/500] train_loss: 0.05903 valid_loss: 0.07208 test_loss: 0.08070 \n",
      "验证损失减少 (0.072509 --> 0.072077). 正在保存模型...\n",
      "[147/500] train_loss: 0.05805 valid_loss: 0.07299 test_loss: 0.08194 \n",
      "[148/500] train_loss: 0.05971 valid_loss: 0.07292 test_loss: 0.08172 \n",
      "[149/500] train_loss: 0.05715 valid_loss: 0.07365 test_loss: 0.08303 \n",
      "[150/500] train_loss: 0.05753 valid_loss: 0.07368 test_loss: 0.08193 \n",
      "[151/500] train_loss: 0.06031 valid_loss: 0.07481 test_loss: 0.08334 \n",
      "[152/500] train_loss: 0.05911 valid_loss: 0.07279 test_loss: 0.08305 \n",
      "[153/500] train_loss: 0.05959 valid_loss: 0.07278 test_loss: 0.08114 \n",
      "[154/500] train_loss: 0.05790 valid_loss: 0.07359 test_loss: 0.08059 \n",
      "[155/500] train_loss: 0.05771 valid_loss: 0.07249 test_loss: 0.08141 \n",
      "[156/500] train_loss: 0.05866 valid_loss: 0.07415 test_loss: 0.08354 \n",
      "[157/500] train_loss: 0.05700 valid_loss: 0.07746 test_loss: 0.08565 \n",
      "[158/500] train_loss: 0.05904 valid_loss: 0.07370 test_loss: 0.08160 \n",
      "[159/500] train_loss: 0.05801 valid_loss: 0.07411 test_loss: 0.08030 \n",
      "[160/500] train_loss: 0.05775 valid_loss: 0.07562 test_loss: 0.08078 \n",
      "[161/500] train_loss: 0.05814 valid_loss: 0.07537 test_loss: 0.08062 \n",
      "[162/500] train_loss: 0.05754 valid_loss: 0.07286 test_loss: 0.08127 \n",
      "[163/500] train_loss: 0.05708 valid_loss: 0.07185 test_loss: 0.08096 \n",
      "验证损失减少 (0.072077 --> 0.071850). 正在保存模型...\n",
      "[164/500] train_loss: 0.05764 valid_loss: 0.07302 test_loss: 0.08236 \n",
      "[165/500] train_loss: 0.05599 valid_loss: 0.07445 test_loss: 0.08242 \n",
      "[166/500] train_loss: 0.05739 valid_loss: 0.07189 test_loss: 0.08164 \n",
      "[167/500] train_loss: 0.05824 valid_loss: 0.07187 test_loss: 0.08322 \n",
      "[168/500] train_loss: 0.05690 valid_loss: 0.07172 test_loss: 0.08101 \n",
      "验证损失减少 (0.071850 --> 0.071720). 正在保存模型...\n",
      "[169/500] train_loss: 0.05799 valid_loss: 0.07197 test_loss: 0.08104 \n",
      "[170/500] train_loss: 0.05674 valid_loss: 0.07626 test_loss: 0.08252 \n",
      "[171/500] train_loss: 0.05689 valid_loss: 0.07243 test_loss: 0.08125 \n",
      "[172/500] train_loss: 0.05707 valid_loss: 0.07142 test_loss: 0.08239 \n",
      "验证损失减少 (0.071720 --> 0.071415). 正在保存模型...\n",
      "[173/500] train_loss: 0.05682 valid_loss: 0.07276 test_loss: 0.08180 \n",
      "[174/500] train_loss: 0.05659 valid_loss: 0.07278 test_loss: 0.08218 \n",
      "[175/500] train_loss: 0.05506 valid_loss: 0.07237 test_loss: 0.07967 \n",
      "[176/500] train_loss: 0.05763 valid_loss: 0.07241 test_loss: 0.08175 \n",
      "[177/500] train_loss: 0.05636 valid_loss: 0.07305 test_loss: 0.08200 \n",
      "[178/500] train_loss: 0.05628 valid_loss: 0.07737 test_loss: 0.08080 \n",
      "[179/500] train_loss: 0.05368 valid_loss: 0.07604 test_loss: 0.08281 \n",
      "[180/500] train_loss: 0.05592 valid_loss: 0.07904 test_loss: 0.08162 \n",
      "[181/500] train_loss: 0.05556 valid_loss: 0.07416 test_loss: 0.08049 \n",
      "[182/500] train_loss: 0.05607 valid_loss: 0.07447 test_loss: 0.08028 \n",
      "[183/500] train_loss: 0.05633 valid_loss: 0.07371 test_loss: 0.08155 \n",
      "[184/500] train_loss: 0.05537 valid_loss: 0.07317 test_loss: 0.08034 \n",
      "[185/500] train_loss: 0.05425 valid_loss: 0.07353 test_loss: 0.08223 \n",
      "[186/500] train_loss: 0.05510 valid_loss: 0.07329 test_loss: 0.08109 \n",
      "[187/500] train_loss: 0.05355 valid_loss: 0.07348 test_loss: 0.08188 \n",
      "[188/500] train_loss: 0.05389 valid_loss: 0.07251 test_loss: 0.08209 \n",
      "[189/500] train_loss: 0.05523 valid_loss: 0.07336 test_loss: 0.08068 \n",
      "[190/500] train_loss: 0.05458 valid_loss: 0.07391 test_loss: 0.08015 \n",
      "[191/500] train_loss: 0.05353 valid_loss: 0.07507 test_loss: 0.08657 \n",
      "[192/500] train_loss: 0.05498 valid_loss: 0.07453 test_loss: 0.08207 \n",
      "[193/500] train_loss: 0.05623 valid_loss: 0.07275 test_loss: 0.07988 \n",
      "[194/500] train_loss: 0.05366 valid_loss: 0.07311 test_loss: 0.08064 \n",
      "[195/500] train_loss: 0.05204 valid_loss: 0.07520 test_loss: 0.08196 \n",
      "[196/500] train_loss: 0.05494 valid_loss: 0.07563 test_loss: 0.08345 \n",
      "[197/500] train_loss: 0.05514 valid_loss: 0.07443 test_loss: 0.08280 \n",
      "[198/500] train_loss: 0.05388 valid_loss: 0.07392 test_loss: 0.08109 \n",
      "[199/500] train_loss: 0.05450 valid_loss: 0.07588 test_loss: 0.08249 \n",
      "[200/500] train_loss: 0.05573 valid_loss: 0.07281 test_loss: 0.08054 \n",
      "[201/500] train_loss: 0.05448 valid_loss: 0.07072 test_loss: 0.08116 \n",
      "验证损失减少 (0.071415 --> 0.070717). 正在保存模型...\n",
      "[202/500] train_loss: 0.05409 valid_loss: 0.07196 test_loss: 0.07954 \n",
      "[203/500] train_loss: 0.05314 valid_loss: 0.07255 test_loss: 0.08261 \n",
      "[204/500] train_loss: 0.05479 valid_loss: 0.07232 test_loss: 0.08085 \n",
      "[205/500] train_loss: 0.05199 valid_loss: 0.07146 test_loss: 0.08122 \n",
      "[206/500] train_loss: 0.05474 valid_loss: 0.07226 test_loss: 0.08133 \n",
      "[207/500] train_loss: 0.05318 valid_loss: 0.07519 test_loss: 0.08209 \n",
      "[208/500] train_loss: 0.05353 valid_loss: 0.07537 test_loss: 0.08212 \n",
      "[209/500] train_loss: 0.05447 valid_loss: 0.07127 test_loss: 0.08049 \n",
      "[210/500] train_loss: 0.05430 valid_loss: 0.07189 test_loss: 0.08095 \n",
      "[211/500] train_loss: 0.05387 valid_loss: 0.07263 test_loss: 0.08125 \n",
      "[212/500] train_loss: 0.05294 valid_loss: 0.07304 test_loss: 0.08174 \n",
      "[213/500] train_loss: 0.05264 valid_loss: 0.07139 test_loss: 0.07870 \n",
      "[214/500] train_loss: 0.05400 valid_loss: 0.07366 test_loss: 0.08046 \n",
      "[215/500] train_loss: 0.05436 valid_loss: 0.07221 test_loss: 0.07927 \n",
      "[216/500] train_loss: 0.05257 valid_loss: 0.07276 test_loss: 0.08000 \n",
      "[217/500] train_loss: 0.05315 valid_loss: 0.07209 test_loss: 0.08001 \n",
      "[218/500] train_loss: 0.05386 valid_loss: 0.07333 test_loss: 0.08024 \n",
      "[219/500] train_loss: 0.05350 valid_loss: 0.07178 test_loss: 0.08056 \n",
      "[220/500] train_loss: 0.05211 valid_loss: 0.07437 test_loss: 0.08103 \n",
      "[221/500] train_loss: 0.05298 valid_loss: 0.07422 test_loss: 0.08167 \n",
      "[222/500] train_loss: 0.05307 valid_loss: 0.07283 test_loss: 0.08142 \n",
      "[223/500] train_loss: 0.05380 valid_loss: 0.07252 test_loss: 0.07985 \n",
      "[224/500] train_loss: 0.05329 valid_loss: 0.07338 test_loss: 0.08293 \n",
      "[225/500] train_loss: 0.05187 valid_loss: 0.07140 test_loss: 0.07988 \n",
      "[226/500] train_loss: 0.05213 valid_loss: 0.07136 test_loss: 0.08070 \n",
      "[227/500] train_loss: 0.05166 valid_loss: 0.07336 test_loss: 0.08232 \n",
      "[228/500] train_loss: 0.05235 valid_loss: 0.07233 test_loss: 0.08230 \n",
      "[229/500] train_loss: 0.05109 valid_loss: 0.07334 test_loss: 0.08128 \n",
      "[230/500] train_loss: 0.05270 valid_loss: 0.07343 test_loss: 0.08124 \n",
      "[231/500] train_loss: 0.05200 valid_loss: 0.07458 test_loss: 0.08140 \n",
      "[232/500] train_loss: 0.05197 valid_loss: 0.07108 test_loss: 0.08149 \n",
      "[233/500] train_loss: 0.05130 valid_loss: 0.07256 test_loss: 0.08111 \n",
      "[234/500] train_loss: 0.05172 valid_loss: 0.07235 test_loss: 0.07996 \n",
      "[235/500] train_loss: 0.05187 valid_loss: 0.07279 test_loss: 0.07993 \n",
      "[236/500] train_loss: 0.05083 valid_loss: 0.07477 test_loss: 0.08209 \n",
      "[237/500] train_loss: 0.05110 valid_loss: 0.07253 test_loss: 0.08051 \n",
      "[238/500] train_loss: 0.05108 valid_loss: 0.07072 test_loss: 0.08021 \n",
      "[239/500] train_loss: 0.05187 valid_loss: 0.07422 test_loss: 0.08322 \n",
      "[240/500] train_loss: 0.05188 valid_loss: 0.07353 test_loss: 0.08128 \n",
      "[241/500] train_loss: 0.05054 valid_loss: 0.07246 test_loss: 0.07966 \n",
      "[242/500] train_loss: 0.04892 valid_loss: 0.07252 test_loss: 0.08024 \n",
      "[243/500] train_loss: 0.05253 valid_loss: 0.07267 test_loss: 0.08058 \n",
      "[244/500] train_loss: 0.05222 valid_loss: 0.07237 test_loss: 0.07925 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[245/500] train_loss: 0.05032 valid_loss: 0.07384 test_loss: 0.07964 \n",
      "[246/500] train_loss: 0.05061 valid_loss: 0.07252 test_loss: 0.07969 \n",
      "[247/500] train_loss: 0.05181 valid_loss: 0.07289 test_loss: 0.07967 \n",
      "[248/500] train_loss: 0.05083 valid_loss: 0.07331 test_loss: 0.08015 \n",
      "[249/500] train_loss: 0.05074 valid_loss: 0.07364 test_loss: 0.07967 \n",
      "[250/500] train_loss: 0.05057 valid_loss: 0.07402 test_loss: 0.07973 \n",
      "[251/500] train_loss: 0.05086 valid_loss: 0.07387 test_loss: 0.08042 \n",
      "[252/500] train_loss: 0.04951 valid_loss: 0.07544 test_loss: 0.08049 \n",
      "[253/500] train_loss: 0.05144 valid_loss: 0.07263 test_loss: 0.08096 \n",
      "[254/500] train_loss: 0.04999 valid_loss: 0.07328 test_loss: 0.08134 \n",
      "[255/500] train_loss: 0.04971 valid_loss: 0.07449 test_loss: 0.08175 \n",
      "[256/500] train_loss: 0.04958 valid_loss: 0.07359 test_loss: 0.08150 \n",
      "[257/500] train_loss: 0.04888 valid_loss: 0.07373 test_loss: 0.08323 \n",
      "[258/500] train_loss: 0.05110 valid_loss: 0.07390 test_loss: 0.08290 \n",
      "[259/500] train_loss: 0.04848 valid_loss: 0.07344 test_loss: 0.08245 \n",
      "[260/500] train_loss: 0.05023 valid_loss: 0.07280 test_loss: 0.08291 \n",
      "[261/500] train_loss: 0.04969 valid_loss: 0.07397 test_loss: 0.08327 \n",
      "[262/500] train_loss: 0.04925 valid_loss: 0.07328 test_loss: 0.08099 \n",
      "[263/500] train_loss: 0.05130 valid_loss: 0.07238 test_loss: 0.08090 \n",
      "[264/500] train_loss: 0.05002 valid_loss: 0.07468 test_loss: 0.08154 \n",
      "[265/500] train_loss: 0.04836 valid_loss: 0.07340 test_loss: 0.08212 \n",
      "[266/500] train_loss: 0.05054 valid_loss: 0.07256 test_loss: 0.08090 \n",
      "[267/500] train_loss: 0.04834 valid_loss: 0.07479 test_loss: 0.08222 \n",
      "[268/500] train_loss: 0.04965 valid_loss: 0.07359 test_loss: 0.08091 \n",
      "[269/500] train_loss: 0.04957 valid_loss: 0.07182 test_loss: 0.08228 \n",
      "[270/500] train_loss: 0.04895 valid_loss: 0.07235 test_loss: 0.08040 \n",
      "[271/500] train_loss: 0.04879 valid_loss: 0.07309 test_loss: 0.08091 \n",
      "[272/500] train_loss: 0.04923 valid_loss: 0.07270 test_loss: 0.08193 \n",
      "[273/500] train_loss: 0.05009 valid_loss: 0.07242 test_loss: 0.08070 \n",
      "[274/500] train_loss: 0.05006 valid_loss: 0.07294 test_loss: 0.08086 \n",
      "[275/500] train_loss: 0.04867 valid_loss: 0.07319 test_loss: 0.08070 \n",
      "[276/500] train_loss: 0.04740 valid_loss: 0.07348 test_loss: 0.08325 \n",
      "[277/500] train_loss: 0.04863 valid_loss: 0.07338 test_loss: 0.08239 \n",
      "[278/500] train_loss: 0.04910 valid_loss: 0.07275 test_loss: 0.08061 \n",
      "[279/500] train_loss: 0.04891 valid_loss: 0.07262 test_loss: 0.08104 \n",
      "[280/500] train_loss: 0.04948 valid_loss: 0.07242 test_loss: 0.08136 \n",
      "[281/500] train_loss: 0.05019 valid_loss: 0.07249 test_loss: 0.08329 \n",
      "[282/500] train_loss: 0.04852 valid_loss: 0.07203 test_loss: 0.08332 \n",
      "[283/500] train_loss: 0.04917 valid_loss: 0.07546 test_loss: 0.08366 \n",
      "[284/500] train_loss: 0.05219 valid_loss: 0.07525 test_loss: 0.08240 \n",
      "[285/500] train_loss: 0.05111 valid_loss: 0.07283 test_loss: 0.08062 \n",
      "[286/500] train_loss: 0.04990 valid_loss: 0.07277 test_loss: 0.08047 \n",
      "[287/500] train_loss: 0.04882 valid_loss: 0.07150 test_loss: 0.08006 \n",
      "[288/500] train_loss: 0.04849 valid_loss: 0.07180 test_loss: 0.08222 \n",
      "[289/500] train_loss: 0.04910 valid_loss: 0.07194 test_loss: 0.07941 \n",
      "[290/500] train_loss: 0.04908 valid_loss: 0.07309 test_loss: 0.08228 \n",
      "[291/500] train_loss: 0.04746 valid_loss: 0.07332 test_loss: 0.08118 \n",
      "[292/500] train_loss: 0.04766 valid_loss: 0.07329 test_loss: 0.08129 \n",
      "[293/500] train_loss: 0.04750 valid_loss: 0.07223 test_loss: 0.07971 \n",
      "[294/500] train_loss: 0.04854 valid_loss: 0.07316 test_loss: 0.08163 \n",
      "[295/500] train_loss: 0.04744 valid_loss: 0.07272 test_loss: 0.08137 \n",
      "[296/500] train_loss: 0.04736 valid_loss: 0.07194 test_loss: 0.08084 \n",
      "[297/500] train_loss: 0.04914 valid_loss: 0.07290 test_loss: 0.08122 \n",
      "[298/500] train_loss: 0.04840 valid_loss: 0.07230 test_loss: 0.08013 \n",
      "[299/500] train_loss: 0.04870 valid_loss: 0.07167 test_loss: 0.08089 \n",
      "[300/500] train_loss: 0.04826 valid_loss: 0.07246 test_loss: 0.08008 \n",
      "[301/500] train_loss: 0.04849 valid_loss: 0.07566 test_loss: 0.08000 \n",
      "[302/500] train_loss: 0.04828 valid_loss: 0.07278 test_loss: 0.08063 \n",
      "[303/500] train_loss: 0.04645 valid_loss: 0.07060 test_loss: 0.07972 \n",
      "验证损失减少 (0.070717 --> 0.070603). 正在保存模型...\n",
      "[304/500] train_loss: 0.04618 valid_loss: 0.07153 test_loss: 0.08027 \n",
      "[305/500] train_loss: 0.04724 valid_loss: 0.07213 test_loss: 0.08114 \n",
      "[306/500] train_loss: 0.04713 valid_loss: 0.07449 test_loss: 0.08118 \n",
      "[307/500] train_loss: 0.04702 valid_loss: 0.07451 test_loss: 0.08222 \n",
      "[308/500] train_loss: 0.04707 valid_loss: 0.07371 test_loss: 0.08052 \n",
      "[309/500] train_loss: 0.04784 valid_loss: 0.07332 test_loss: 0.08062 \n",
      "[310/500] train_loss: 0.04729 valid_loss: 0.07252 test_loss: 0.07934 \n",
      "[311/500] train_loss: 0.04835 valid_loss: 0.07391 test_loss: 0.07940 \n",
      "[312/500] train_loss: 0.04771 valid_loss: 0.07588 test_loss: 0.07984 \n",
      "[313/500] train_loss: 0.04761 valid_loss: 0.07534 test_loss: 0.08143 \n",
      "[314/500] train_loss: 0.04680 valid_loss: 0.07296 test_loss: 0.07997 \n",
      "[315/500] train_loss: 0.04792 valid_loss: 0.07316 test_loss: 0.07902 \n",
      "[316/500] train_loss: 0.04731 valid_loss: 0.07268 test_loss: 0.08049 \n",
      "[317/500] train_loss: 0.04699 valid_loss: 0.07286 test_loss: 0.08040 \n",
      "[318/500] train_loss: 0.04742 valid_loss: 0.07444 test_loss: 0.07934 \n",
      "[319/500] train_loss: 0.04577 valid_loss: 0.07472 test_loss: 0.08032 \n",
      "[320/500] train_loss: 0.04719 valid_loss: 0.07675 test_loss: 0.08038 \n",
      "[321/500] train_loss: 0.04654 valid_loss: 0.07554 test_loss: 0.08091 \n",
      "[322/500] train_loss: 0.04576 valid_loss: 0.07430 test_loss: 0.08084 \n",
      "[323/500] train_loss: 0.04663 valid_loss: 0.07688 test_loss: 0.08152 \n",
      "[324/500] train_loss: 0.04733 valid_loss: 0.07517 test_loss: 0.07981 \n",
      "[325/500] train_loss: 0.04621 valid_loss: 0.07341 test_loss: 0.08009 \n",
      "[326/500] train_loss: 0.04579 valid_loss: 0.07504 test_loss: 0.08099 \n",
      "[327/500] train_loss: 0.04604 valid_loss: 0.07516 test_loss: 0.07981 \n",
      "[328/500] train_loss: 0.04813 valid_loss: 0.07474 test_loss: 0.08027 \n",
      "[329/500] train_loss: 0.04579 valid_loss: 0.07520 test_loss: 0.08025 \n",
      "[330/500] train_loss: 0.04538 valid_loss: 0.07314 test_loss: 0.07974 \n",
      "[331/500] train_loss: 0.04680 valid_loss: 0.07365 test_loss: 0.08066 \n",
      "[332/500] train_loss: 0.04682 valid_loss: 0.07670 test_loss: 0.07947 \n",
      "[333/500] train_loss: 0.04599 valid_loss: 0.07406 test_loss: 0.08154 \n",
      "[334/500] train_loss: 0.04616 valid_loss: 0.07251 test_loss: 0.08143 \n",
      "[335/500] train_loss: 0.04535 valid_loss: 0.07208 test_loss: 0.08094 \n",
      "[336/500] train_loss: 0.04499 valid_loss: 0.07432 test_loss: 0.08093 \n",
      "[337/500] train_loss: 0.04553 valid_loss: 0.07386 test_loss: 0.08056 \n",
      "[338/500] train_loss: 0.04595 valid_loss: 0.07201 test_loss: 0.08111 \n",
      "[339/500] train_loss: 0.04732 valid_loss: 0.07398 test_loss: 0.08102 \n",
      "[340/500] train_loss: 0.04619 valid_loss: 0.07281 test_loss: 0.07982 \n",
      "[341/500] train_loss: 0.04609 valid_loss: 0.07212 test_loss: 0.08089 \n",
      "[342/500] train_loss: 0.04566 valid_loss: 0.07386 test_loss: 0.08171 \n",
      "[343/500] train_loss: 0.04668 valid_loss: 0.07706 test_loss: 0.08125 \n",
      "[344/500] train_loss: 0.04625 valid_loss: 0.07388 test_loss: 0.08094 \n",
      "[345/500] train_loss: 0.04598 valid_loss: 0.07461 test_loss: 0.08158 \n",
      "[346/500] train_loss: 0.04557 valid_loss: 0.07368 test_loss: 0.08059 \n",
      "[347/500] train_loss: 0.04458 valid_loss: 0.07564 test_loss: 0.08146 \n",
      "[348/500] train_loss: 0.04585 valid_loss: 0.07437 test_loss: 0.08134 \n",
      "[349/500] train_loss: 0.04564 valid_loss: 0.07421 test_loss: 0.08269 \n",
      "[350/500] train_loss: 0.04591 valid_loss: 0.07418 test_loss: 0.08142 \n",
      "[351/500] train_loss: 0.04412 valid_loss: 0.07345 test_loss: 0.08206 \n",
      "[352/500] train_loss: 0.04560 valid_loss: 0.07504 test_loss: 0.08194 \n",
      "[353/500] train_loss: 0.04513 valid_loss: 0.07326 test_loss: 0.08152 \n",
      "[354/500] train_loss: 0.04508 valid_loss: 0.07379 test_loss: 0.08282 \n",
      "[355/500] train_loss: 0.04533 valid_loss: 0.07378 test_loss: 0.08253 \n",
      "[356/500] train_loss: 0.04542 valid_loss: 0.07405 test_loss: 0.08138 \n",
      "[357/500] train_loss: 0.04583 valid_loss: 0.07453 test_loss: 0.08280 \n",
      "[358/500] train_loss: 0.04524 valid_loss: 0.07317 test_loss: 0.08201 \n",
      "[359/500] train_loss: 0.04610 valid_loss: 0.07291 test_loss: 0.08160 \n",
      "[360/500] train_loss: 0.04724 valid_loss: 0.07407 test_loss: 0.08319 \n",
      "[361/500] train_loss: 0.04399 valid_loss: 0.07304 test_loss: 0.08251 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362/500] train_loss: 0.04443 valid_loss: 0.07591 test_loss: 0.08253 \n",
      "[363/500] train_loss: 0.04478 valid_loss: 0.07523 test_loss: 0.08393 \n",
      "[364/500] train_loss: 0.04454 valid_loss: 0.07575 test_loss: 0.08176 \n",
      "[365/500] train_loss: 0.04466 valid_loss: 0.07353 test_loss: 0.08275 \n",
      "[366/500] train_loss: 0.04385 valid_loss: 0.07584 test_loss: 0.08335 \n",
      "[367/500] train_loss: 0.04381 valid_loss: 0.07681 test_loss: 0.08431 \n",
      "[368/500] train_loss: 0.04446 valid_loss: 0.07612 test_loss: 0.08301 \n",
      "[369/500] train_loss: 0.04564 valid_loss: 0.07347 test_loss: 0.08246 \n",
      "[370/500] train_loss: 0.04455 valid_loss: 0.07451 test_loss: 0.08209 \n",
      "[371/500] train_loss: 0.04463 valid_loss: 0.07425 test_loss: 0.08142 \n",
      "[372/500] train_loss: 0.04440 valid_loss: 0.07605 test_loss: 0.08165 \n",
      "[373/500] train_loss: 0.04553 valid_loss: 0.07399 test_loss: 0.08180 \n",
      "[374/500] train_loss: 0.04496 valid_loss: 0.07485 test_loss: 0.08260 \n",
      "[375/500] train_loss: 0.04552 valid_loss: 0.07223 test_loss: 0.08242 \n",
      "[376/500] train_loss: 0.04448 valid_loss: 0.07498 test_loss: 0.08119 \n",
      "[377/500] train_loss: 0.04423 valid_loss: 0.07293 test_loss: 0.08280 \n",
      "[378/500] train_loss: 0.04517 valid_loss: 0.07401 test_loss: 0.08167 \n",
      "[379/500] train_loss: 0.04366 valid_loss: 0.07671 test_loss: 0.08250 \n",
      "[380/500] train_loss: 0.04470 valid_loss: 0.07378 test_loss: 0.08220 \n",
      "[381/500] train_loss: 0.04475 valid_loss: 0.07528 test_loss: 0.08237 \n",
      "[382/500] train_loss: 0.04436 valid_loss: 0.07519 test_loss: 0.08258 \n",
      "[383/500] train_loss: 0.04385 valid_loss: 0.07390 test_loss: 0.08218 \n",
      "[384/500] train_loss: 0.04466 valid_loss: 0.07559 test_loss: 0.08292 \n",
      "[385/500] train_loss: 0.04383 valid_loss: 0.07354 test_loss: 0.08106 \n",
      "[386/500] train_loss: 0.04413 valid_loss: 0.07450 test_loss: 0.08248 \n",
      "[387/500] train_loss: 0.04323 valid_loss: 0.07621 test_loss: 0.08237 \n",
      "[388/500] train_loss: 0.04335 valid_loss: 0.07565 test_loss: 0.08301 \n",
      "[389/500] train_loss: 0.04602 valid_loss: 0.07417 test_loss: 0.08114 \n",
      "[390/500] train_loss: 0.04460 valid_loss: 0.07354 test_loss: 0.07968 \n",
      "[391/500] train_loss: 0.04405 valid_loss: 0.07468 test_loss: 0.08097 \n",
      "[392/500] train_loss: 0.04474 valid_loss: 0.07512 test_loss: 0.08101 \n",
      "[393/500] train_loss: 0.04354 valid_loss: 0.07527 test_loss: 0.08240 \n",
      "[394/500] train_loss: 0.04444 valid_loss: 0.07647 test_loss: 0.08239 \n",
      "[395/500] train_loss: 0.04402 valid_loss: 0.07326 test_loss: 0.08013 \n",
      "[396/500] train_loss: 0.04532 valid_loss: 0.07402 test_loss: 0.08109 \n",
      "[397/500] train_loss: 0.04431 valid_loss: 0.07535 test_loss: 0.08209 \n",
      "[398/500] train_loss: 0.04356 valid_loss: 0.07581 test_loss: 0.08158 \n",
      "[399/500] train_loss: 0.04334 valid_loss: 0.07373 test_loss: 0.08143 \n",
      "[400/500] train_loss: 0.04404 valid_loss: 0.07401 test_loss: 0.08099 \n",
      "[401/500] train_loss: 0.04375 valid_loss: 0.07325 test_loss: 0.08192 \n",
      "[402/500] train_loss: 0.04371 valid_loss: 0.07425 test_loss: 0.08273 \n",
      "[403/500] train_loss: 0.04261 valid_loss: 0.07439 test_loss: 0.08103 \n",
      "[404/500] train_loss: 0.04356 valid_loss: 0.07446 test_loss: 0.08287 \n",
      "[405/500] train_loss: 0.04275 valid_loss: 0.07381 test_loss: 0.08133 \n",
      "[406/500] train_loss: 0.04292 valid_loss: 0.07294 test_loss: 0.08288 \n",
      "[407/500] train_loss: 0.04371 valid_loss: 0.07464 test_loss: 0.08399 \n",
      "[408/500] train_loss: 0.04459 valid_loss: 0.07417 test_loss: 0.08191 \n",
      "[409/500] train_loss: 0.04332 valid_loss: 0.07405 test_loss: 0.08193 \n",
      "[410/500] train_loss: 0.04320 valid_loss: 0.07335 test_loss: 0.08108 \n",
      "[411/500] train_loss: 0.04292 valid_loss: 0.07486 test_loss: 0.08133 \n",
      "[412/500] train_loss: 0.04335 valid_loss: 0.07365 test_loss: 0.08222 \n",
      "[413/500] train_loss: 0.04368 valid_loss: 0.07413 test_loss: 0.08215 \n",
      "[414/500] train_loss: 0.04277 valid_loss: 0.07359 test_loss: 0.08146 \n",
      "[415/500] train_loss: 0.04339 valid_loss: 0.07195 test_loss: 0.08017 \n",
      "[416/500] train_loss: 0.04404 valid_loss: 0.07341 test_loss: 0.08035 \n",
      "[417/500] train_loss: 0.04289 valid_loss: 0.07355 test_loss: 0.08101 \n",
      "[418/500] train_loss: 0.04405 valid_loss: 0.07378 test_loss: 0.08129 \n",
      "[419/500] train_loss: 0.04192 valid_loss: 0.07374 test_loss: 0.08127 \n",
      "[420/500] train_loss: 0.04370 valid_loss: 0.07254 test_loss: 0.08192 \n",
      "[421/500] train_loss: 0.04310 valid_loss: 0.07454 test_loss: 0.08262 \n",
      "[422/500] train_loss: 0.04320 valid_loss: 0.07681 test_loss: 0.08224 \n",
      "[423/500] train_loss: 0.04312 valid_loss: 0.07734 test_loss: 0.08297 \n",
      "[424/500] train_loss: 0.04445 valid_loss: 0.07362 test_loss: 0.08232 \n",
      "[425/500] train_loss: 0.04213 valid_loss: 0.07485 test_loss: 0.08354 \n",
      "[426/500] train_loss: 0.04320 valid_loss: 0.07399 test_loss: 0.08412 \n",
      "[427/500] train_loss: 0.04167 valid_loss: 0.07484 test_loss: 0.08377 \n",
      "[428/500] train_loss: 0.04336 valid_loss: 0.07404 test_loss: 0.08223 \n",
      "[429/500] train_loss: 0.04335 valid_loss: 0.07406 test_loss: 0.08343 \n",
      "[430/500] train_loss: 0.04182 valid_loss: 0.07459 test_loss: 0.08248 \n",
      "[431/500] train_loss: 0.04281 valid_loss: 0.07427 test_loss: 0.08169 \n",
      "[432/500] train_loss: 0.04421 valid_loss: 0.07458 test_loss: 0.08238 \n",
      "[433/500] train_loss: 0.04428 valid_loss: 0.07446 test_loss: 0.08288 \n",
      "[434/500] train_loss: 0.04183 valid_loss: 0.07371 test_loss: 0.08316 \n",
      "[435/500] train_loss: 0.04277 valid_loss: 0.07495 test_loss: 0.08234 \n",
      "[436/500] train_loss: 0.04194 valid_loss: 0.07430 test_loss: 0.08410 \n",
      "[437/500] train_loss: 0.04196 valid_loss: 0.07605 test_loss: 0.08387 \n",
      "[438/500] train_loss: 0.04330 valid_loss: 0.07378 test_loss: 0.08317 \n",
      "[439/500] train_loss: 0.04366 valid_loss: 0.07173 test_loss: 0.08332 \n",
      "[440/500] train_loss: 0.04277 valid_loss: 0.07419 test_loss: 0.08357 \n",
      "[441/500] train_loss: 0.04325 valid_loss: 0.07389 test_loss: 0.08148 \n",
      "[442/500] train_loss: 0.04250 valid_loss: 0.07331 test_loss: 0.08154 \n",
      "[443/500] train_loss: 0.04314 valid_loss: 0.07403 test_loss: 0.08229 \n",
      "[444/500] train_loss: 0.04229 valid_loss: 0.07355 test_loss: 0.08165 \n",
      "[445/500] train_loss: 0.04259 valid_loss: 0.07380 test_loss: 0.08119 \n",
      "[446/500] train_loss: 0.04171 valid_loss: 0.07484 test_loss: 0.08210 \n",
      "[447/500] train_loss: 0.04245 valid_loss: 0.07419 test_loss: 0.08334 \n",
      "[448/500] train_loss: 0.04217 valid_loss: 0.07453 test_loss: 0.08404 \n",
      "[449/500] train_loss: 0.04226 valid_loss: 0.07325 test_loss: 0.08188 \n",
      "[450/500] train_loss: 0.04234 valid_loss: 0.07403 test_loss: 0.08075 \n",
      "[451/500] train_loss: 0.04195 valid_loss: 0.07455 test_loss: 0.08085 \n",
      "[452/500] train_loss: 0.04325 valid_loss: 0.07682 test_loss: 0.08196 \n",
      "[453/500] train_loss: 0.04268 valid_loss: 0.07533 test_loss: 0.08321 \n",
      "[454/500] train_loss: 0.04251 valid_loss: 0.07638 test_loss: 0.08295 \n",
      "[455/500] train_loss: 0.04181 valid_loss: 0.07279 test_loss: 0.08287 \n",
      "[456/500] train_loss: 0.04273 valid_loss: 0.07633 test_loss: 0.08222 \n",
      "[457/500] train_loss: 0.04143 valid_loss: 0.07391 test_loss: 0.08337 \n",
      "[458/500] train_loss: 0.04162 valid_loss: 0.07450 test_loss: 0.08311 \n",
      "[459/500] train_loss: 0.04233 valid_loss: 0.07454 test_loss: 0.08108 \n",
      "[460/500] train_loss: 0.04248 valid_loss: 0.07294 test_loss: 0.08270 \n",
      "[461/500] train_loss: 0.04214 valid_loss: 0.07544 test_loss: 0.08326 \n",
      "[462/500] train_loss: 0.04154 valid_loss: 0.07520 test_loss: 0.08260 \n",
      "[463/500] train_loss: 0.04101 valid_loss: 0.07495 test_loss: 0.08295 \n",
      "[464/500] train_loss: 0.04223 valid_loss: 0.07372 test_loss: 0.08394 \n",
      "[465/500] train_loss: 0.04122 valid_loss: 0.07617 test_loss: 0.08460 \n",
      "[466/500] train_loss: 0.04225 valid_loss: 0.07596 test_loss: 0.08331 \n",
      "[467/500] train_loss: 0.04282 valid_loss: 0.07599 test_loss: 0.08441 \n",
      "[468/500] train_loss: 0.04077 valid_loss: 0.07569 test_loss: 0.08386 \n",
      "[469/500] train_loss: 0.04140 valid_loss: 0.07514 test_loss: 0.08378 \n",
      "[470/500] train_loss: 0.04149 valid_loss: 0.07522 test_loss: 0.08507 \n",
      "[471/500] train_loss: 0.04250 valid_loss: 0.07551 test_loss: 0.08376 \n",
      "[472/500] train_loss: 0.04188 valid_loss: 0.07458 test_loss: 0.08318 \n",
      "[473/500] train_loss: 0.04219 valid_loss: 0.07570 test_loss: 0.08510 \n",
      "[474/500] train_loss: 0.04076 valid_loss: 0.07585 test_loss: 0.08449 \n",
      "[475/500] train_loss: 0.04235 valid_loss: 0.07925 test_loss: 0.08378 \n",
      "[476/500] train_loss: 0.04216 valid_loss: 0.07485 test_loss: 0.08303 \n",
      "[477/500] train_loss: 0.04190 valid_loss: 0.07490 test_loss: 0.08357 \n",
      "[478/500] train_loss: 0.04179 valid_loss: 0.07589 test_loss: 0.08401 \n",
      "[479/500] train_loss: 0.04022 valid_loss: 0.07432 test_loss: 0.08324 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[480/500] train_loss: 0.04063 valid_loss: 0.07428 test_loss: 0.08506 \n",
      "[481/500] train_loss: 0.04187 valid_loss: 0.07594 test_loss: 0.08475 \n",
      "[482/500] train_loss: 0.04128 valid_loss: 0.07685 test_loss: 0.08532 \n",
      "[483/500] train_loss: 0.04133 valid_loss: 0.07629 test_loss: 0.08519 \n",
      "[484/500] train_loss: 0.04169 valid_loss: 0.07551 test_loss: 0.08380 \n",
      "[485/500] train_loss: 0.04142 valid_loss: 0.07425 test_loss: 0.08397 \n",
      "[486/500] train_loss: 0.04202 valid_loss: 0.07404 test_loss: 0.08419 \n",
      "[487/500] train_loss: 0.04122 valid_loss: 0.07415 test_loss: 0.08491 \n",
      "[488/500] train_loss: 0.04189 valid_loss: 0.07490 test_loss: 0.08557 \n",
      "[489/500] train_loss: 0.04144 valid_loss: 0.07511 test_loss: 0.08481 \n",
      "[490/500] train_loss: 0.04091 valid_loss: 0.07708 test_loss: 0.08530 \n",
      "[491/500] train_loss: 0.04095 valid_loss: 0.07380 test_loss: 0.08402 \n",
      "[492/500] train_loss: 0.04142 valid_loss: 0.07467 test_loss: 0.08243 \n",
      "[493/500] train_loss: 0.04091 valid_loss: 0.07387 test_loss: 0.08225 \n",
      "[494/500] train_loss: 0.04065 valid_loss: 0.07404 test_loss: 0.08363 \n",
      "[495/500] train_loss: 0.04063 valid_loss: 0.07638 test_loss: 0.08229 \n",
      "[496/500] train_loss: 0.04212 valid_loss: 0.07856 test_loss: 0.08311 \n",
      "[497/500] train_loss: 0.04010 valid_loss: 0.07619 test_loss: 0.08275 \n",
      "[498/500] train_loss: 0.04141 valid_loss: 0.07545 test_loss: 0.08242 \n",
      "[499/500] train_loss: 0.04059 valid_loss: 0.07631 test_loss: 0.08316 \n",
      "[500/500] train_loss: 0.04104 valid_loss: 0.07475 test_loss: 0.08312 \n",
      "TRAINING MODEL 10\n",
      "[  1/500] train_loss: 0.35863 valid_loss: 0.27097 test_loss: 0.27606 \n",
      "验证损失减少 (inf --> 0.270971). 正在保存模型...\n",
      "[  2/500] train_loss: 0.20794 valid_loss: 0.19876 test_loss: 0.20113 \n",
      "验证损失减少 (0.270971 --> 0.198764). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15968 valid_loss: 0.16345 test_loss: 0.17334 \n",
      "验证损失减少 (0.198764 --> 0.163454). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13806 valid_loss: 0.14922 test_loss: 0.15621 \n",
      "验证损失减少 (0.163454 --> 0.149217). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13181 valid_loss: 0.13289 test_loss: 0.14272 \n",
      "验证损失减少 (0.149217 --> 0.132886). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12283 valid_loss: 0.13142 test_loss: 0.13802 \n",
      "验证损失减少 (0.132886 --> 0.131419). 正在保存模型...\n",
      "[  7/500] train_loss: 0.12060 valid_loss: 0.12571 test_loss: 0.13366 \n",
      "验证损失减少 (0.131419 --> 0.125710). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11495 valid_loss: 0.12184 test_loss: 0.12964 \n",
      "验证损失减少 (0.125710 --> 0.121840). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11286 valid_loss: 0.11778 test_loss: 0.12534 \n",
      "验证损失减少 (0.121840 --> 0.117780). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11018 valid_loss: 0.11763 test_loss: 0.12933 \n",
      "验证损失减少 (0.117780 --> 0.117628). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10992 valid_loss: 0.11046 test_loss: 0.12122 \n",
      "验证损失减少 (0.117628 --> 0.110458). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10384 valid_loss: 0.11116 test_loss: 0.11875 \n",
      "[ 13/500] train_loss: 0.10567 valid_loss: 0.10850 test_loss: 0.11818 \n",
      "验证损失减少 (0.110458 --> 0.108501). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10425 valid_loss: 0.10568 test_loss: 0.11452 \n",
      "验证损失减少 (0.108501 --> 0.105680). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10136 valid_loss: 0.10616 test_loss: 0.11425 \n",
      "[ 16/500] train_loss: 0.10077 valid_loss: 0.10277 test_loss: 0.11253 \n",
      "验证损失减少 (0.105680 --> 0.102766). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09779 valid_loss: 0.10175 test_loss: 0.11082 \n",
      "验证损失减少 (0.102766 --> 0.101745). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09766 valid_loss: 0.09981 test_loss: 0.10988 \n",
      "验证损失减少 (0.101745 --> 0.099807). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09413 valid_loss: 0.09933 test_loss: 0.10908 \n",
      "验证损失减少 (0.099807 --> 0.099331). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09526 valid_loss: 0.10079 test_loss: 0.10916 \n",
      "[ 21/500] train_loss: 0.09276 valid_loss: 0.09808 test_loss: 0.10749 \n",
      "验证损失减少 (0.099331 --> 0.098077). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09274 valid_loss: 0.09799 test_loss: 0.10557 \n",
      "验证损失减少 (0.098077 --> 0.097986). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09032 valid_loss: 0.09809 test_loss: 0.10609 \n",
      "[ 24/500] train_loss: 0.08994 valid_loss: 0.09497 test_loss: 0.10301 \n",
      "验证损失减少 (0.097986 --> 0.094970). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.08981 valid_loss: 0.09547 test_loss: 0.10745 \n",
      "[ 26/500] train_loss: 0.08993 valid_loss: 0.09384 test_loss: 0.10182 \n",
      "验证损失减少 (0.094970 --> 0.093845). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.08815 valid_loss: 0.09217 test_loss: 0.10133 \n",
      "验证损失减少 (0.093845 --> 0.092173). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08758 valid_loss: 0.09272 test_loss: 0.10302 \n",
      "[ 29/500] train_loss: 0.08688 valid_loss: 0.09169 test_loss: 0.09887 \n",
      "验证损失减少 (0.092173 --> 0.091689). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08746 valid_loss: 0.09078 test_loss: 0.10001 \n",
      "验证损失减少 (0.091689 --> 0.090782). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08594 valid_loss: 0.09318 test_loss: 0.10141 \n",
      "[ 32/500] train_loss: 0.08300 valid_loss: 0.08954 test_loss: 0.09754 \n",
      "验证损失减少 (0.090782 --> 0.089544). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08275 valid_loss: 0.08877 test_loss: 0.09881 \n",
      "验证损失减少 (0.089544 --> 0.088768). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08553 valid_loss: 0.08843 test_loss: 0.09786 \n",
      "验证损失减少 (0.088768 --> 0.088426). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08594 valid_loss: 0.08737 test_loss: 0.09718 \n",
      "验证损失减少 (0.088426 --> 0.087366). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08385 valid_loss: 0.08776 test_loss: 0.09784 \n",
      "[ 37/500] train_loss: 0.08325 valid_loss: 0.08762 test_loss: 0.09623 \n",
      "[ 38/500] train_loss: 0.08245 valid_loss: 0.08546 test_loss: 0.09433 \n",
      "验证损失减少 (0.087366 --> 0.085465). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.08168 valid_loss: 0.08527 test_loss: 0.09501 \n",
      "验证损失减少 (0.085465 --> 0.085269). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.08157 valid_loss: 0.08598 test_loss: 0.09471 \n",
      "[ 41/500] train_loss: 0.07902 valid_loss: 0.08545 test_loss: 0.09659 \n",
      "[ 42/500] train_loss: 0.07915 valid_loss: 0.08659 test_loss: 0.09584 \n",
      "[ 43/500] train_loss: 0.08055 valid_loss: 0.08386 test_loss: 0.09313 \n",
      "验证损失减少 (0.085269 --> 0.083862). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.07736 valid_loss: 0.08392 test_loss: 0.09136 \n",
      "[ 45/500] train_loss: 0.07757 valid_loss: 0.08569 test_loss: 0.09322 \n",
      "[ 46/500] train_loss: 0.07990 valid_loss: 0.08314 test_loss: 0.09186 \n",
      "验证损失减少 (0.083862 --> 0.083145). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07673 valid_loss: 0.08301 test_loss: 0.09265 \n",
      "验证损失减少 (0.083145 --> 0.083008). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.07854 valid_loss: 0.08340 test_loss: 0.09142 \n",
      "[ 49/500] train_loss: 0.07719 valid_loss: 0.08152 test_loss: 0.08988 \n",
      "验证损失减少 (0.083008 --> 0.081516). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07908 valid_loss: 0.08110 test_loss: 0.09002 \n",
      "验证损失减少 (0.081516 --> 0.081096). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.07793 valid_loss: 0.08227 test_loss: 0.09076 \n",
      "[ 52/500] train_loss: 0.07425 valid_loss: 0.08030 test_loss: 0.08843 \n",
      "验证损失减少 (0.081096 --> 0.080300). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07644 valid_loss: 0.08241 test_loss: 0.08919 \n",
      "[ 54/500] train_loss: 0.07475 valid_loss: 0.08170 test_loss: 0.08909 \n",
      "[ 55/500] train_loss: 0.07649 valid_loss: 0.08028 test_loss: 0.08792 \n",
      "验证损失减少 (0.080300 --> 0.080277). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07463 valid_loss: 0.08132 test_loss: 0.08726 \n",
      "[ 57/500] train_loss: 0.07455 valid_loss: 0.07995 test_loss: 0.08719 \n",
      "验证损失减少 (0.080277 --> 0.079955). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.07490 valid_loss: 0.08324 test_loss: 0.08827 \n",
      "[ 59/500] train_loss: 0.07523 valid_loss: 0.07953 test_loss: 0.08675 \n",
      "验证损失减少 (0.079955 --> 0.079530). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07456 valid_loss: 0.08000 test_loss: 0.08832 \n",
      "[ 61/500] train_loss: 0.07352 valid_loss: 0.07944 test_loss: 0.08837 \n",
      "验证损失减少 (0.079530 --> 0.079444). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.07515 valid_loss: 0.08008 test_loss: 0.08884 \n",
      "[ 63/500] train_loss: 0.07219 valid_loss: 0.08182 test_loss: 0.08766 \n",
      "[ 64/500] train_loss: 0.07292 valid_loss: 0.07858 test_loss: 0.08754 \n",
      "验证损失减少 (0.079444 --> 0.078575). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07182 valid_loss: 0.07893 test_loss: 0.08694 \n",
      "[ 66/500] train_loss: 0.07246 valid_loss: 0.07959 test_loss: 0.08462 \n",
      "[ 67/500] train_loss: 0.07198 valid_loss: 0.07871 test_loss: 0.08571 \n",
      "[ 68/500] train_loss: 0.07217 valid_loss: 0.07957 test_loss: 0.08840 \n",
      "[ 69/500] train_loss: 0.07187 valid_loss: 0.07916 test_loss: 0.08630 \n",
      "[ 70/500] train_loss: 0.07053 valid_loss: 0.07718 test_loss: 0.08567 \n",
      "验证损失减少 (0.078575 --> 0.077175). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.07014 valid_loss: 0.07832 test_loss: 0.08461 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 72/500] train_loss: 0.06989 valid_loss: 0.07774 test_loss: 0.08678 \n",
      "[ 73/500] train_loss: 0.07079 valid_loss: 0.07830 test_loss: 0.08587 \n",
      "[ 74/500] train_loss: 0.07094 valid_loss: 0.07809 test_loss: 0.08386 \n",
      "[ 75/500] train_loss: 0.07147 valid_loss: 0.07823 test_loss: 0.08581 \n",
      "[ 76/500] train_loss: 0.07031 valid_loss: 0.07808 test_loss: 0.08371 \n",
      "[ 77/500] train_loss: 0.06943 valid_loss: 0.07835 test_loss: 0.08290 \n",
      "[ 78/500] train_loss: 0.07143 valid_loss: 0.07942 test_loss: 0.08415 \n",
      "[ 79/500] train_loss: 0.06963 valid_loss: 0.07535 test_loss: 0.08279 \n",
      "验证损失减少 (0.077175 --> 0.075345). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06962 valid_loss: 0.07721 test_loss: 0.08387 \n",
      "[ 81/500] train_loss: 0.06961 valid_loss: 0.07748 test_loss: 0.08356 \n",
      "[ 82/500] train_loss: 0.06685 valid_loss: 0.07753 test_loss: 0.08309 \n",
      "[ 83/500] train_loss: 0.06666 valid_loss: 0.07852 test_loss: 0.08474 \n",
      "[ 84/500] train_loss: 0.06726 valid_loss: 0.07770 test_loss: 0.08281 \n",
      "[ 85/500] train_loss: 0.06848 valid_loss: 0.07634 test_loss: 0.08361 \n",
      "[ 86/500] train_loss: 0.06964 valid_loss: 0.07666 test_loss: 0.08292 \n",
      "[ 87/500] train_loss: 0.06588 valid_loss: 0.07728 test_loss: 0.08354 \n",
      "[ 88/500] train_loss: 0.06893 valid_loss: 0.07697 test_loss: 0.08377 \n",
      "[ 89/500] train_loss: 0.06746 valid_loss: 0.07775 test_loss: 0.08301 \n",
      "[ 90/500] train_loss: 0.06583 valid_loss: 0.07787 test_loss: 0.08437 \n",
      "[ 91/500] train_loss: 0.06754 valid_loss: 0.07718 test_loss: 0.08254 \n",
      "[ 92/500] train_loss: 0.06685 valid_loss: 0.07705 test_loss: 0.08351 \n",
      "[ 93/500] train_loss: 0.06795 valid_loss: 0.07771 test_loss: 0.08465 \n",
      "[ 94/500] train_loss: 0.06662 valid_loss: 0.07591 test_loss: 0.08139 \n",
      "[ 95/500] train_loss: 0.06639 valid_loss: 0.07881 test_loss: 0.08058 \n",
      "[ 96/500] train_loss: 0.06576 valid_loss: 0.07620 test_loss: 0.08225 \n",
      "[ 97/500] train_loss: 0.06587 valid_loss: 0.07621 test_loss: 0.08200 \n",
      "[ 98/500] train_loss: 0.06566 valid_loss: 0.07665 test_loss: 0.08335 \n",
      "[ 99/500] train_loss: 0.06586 valid_loss: 0.07572 test_loss: 0.08109 \n",
      "[100/500] train_loss: 0.06481 valid_loss: 0.07688 test_loss: 0.08081 \n",
      "[101/500] train_loss: 0.06677 valid_loss: 0.07757 test_loss: 0.08210 \n",
      "[102/500] train_loss: 0.06474 valid_loss: 0.07561 test_loss: 0.08098 \n",
      "[103/500] train_loss: 0.06489 valid_loss: 0.07556 test_loss: 0.08403 \n",
      "[104/500] train_loss: 0.06464 valid_loss: 0.07592 test_loss: 0.08346 \n",
      "[105/500] train_loss: 0.06373 valid_loss: 0.07673 test_loss: 0.08145 \n",
      "[106/500] train_loss: 0.06572 valid_loss: 0.07521 test_loss: 0.08057 \n",
      "验证损失减少 (0.075345 --> 0.075209). 正在保存模型...\n",
      "[107/500] train_loss: 0.06381 valid_loss: 0.07654 test_loss: 0.08175 \n",
      "[108/500] train_loss: 0.06555 valid_loss: 0.07547 test_loss: 0.08129 \n",
      "[109/500] train_loss: 0.06417 valid_loss: 0.07658 test_loss: 0.07969 \n",
      "[110/500] train_loss: 0.06350 valid_loss: 0.07617 test_loss: 0.08077 \n",
      "[111/500] train_loss: 0.06409 valid_loss: 0.07585 test_loss: 0.08026 \n",
      "[112/500] train_loss: 0.06472 valid_loss: 0.07516 test_loss: 0.08139 \n",
      "验证损失减少 (0.075209 --> 0.075163). 正在保存模型...\n",
      "[113/500] train_loss: 0.06420 valid_loss: 0.07553 test_loss: 0.08102 \n",
      "[114/500] train_loss: 0.06413 valid_loss: 0.07741 test_loss: 0.08156 \n",
      "[115/500] train_loss: 0.06349 valid_loss: 0.07697 test_loss: 0.08193 \n",
      "[116/500] train_loss: 0.06391 valid_loss: 0.07478 test_loss: 0.08180 \n",
      "验证损失减少 (0.075163 --> 0.074777). 正在保存模型...\n",
      "[117/500] train_loss: 0.06230 valid_loss: 0.07602 test_loss: 0.08002 \n",
      "[118/500] train_loss: 0.06284 valid_loss: 0.07440 test_loss: 0.08121 \n",
      "验证损失减少 (0.074777 --> 0.074405). 正在保存模型...\n",
      "[119/500] train_loss: 0.06205 valid_loss: 0.07458 test_loss: 0.08014 \n",
      "[120/500] train_loss: 0.06198 valid_loss: 0.07436 test_loss: 0.08038 \n",
      "验证损失减少 (0.074405 --> 0.074364). 正在保存模型...\n",
      "[121/500] train_loss: 0.06199 valid_loss: 0.07541 test_loss: 0.08049 \n",
      "[122/500] train_loss: 0.06243 valid_loss: 0.07395 test_loss: 0.08018 \n",
      "验证损失减少 (0.074364 --> 0.073953). 正在保存模型...\n",
      "[123/500] train_loss: 0.06364 valid_loss: 0.07689 test_loss: 0.07918 \n",
      "[124/500] train_loss: 0.06143 valid_loss: 0.07396 test_loss: 0.07969 \n",
      "[125/500] train_loss: 0.06207 valid_loss: 0.07492 test_loss: 0.07993 \n",
      "[126/500] train_loss: 0.06171 valid_loss: 0.07525 test_loss: 0.08033 \n",
      "[127/500] train_loss: 0.06188 valid_loss: 0.07491 test_loss: 0.07852 \n",
      "[128/500] train_loss: 0.06061 valid_loss: 0.07375 test_loss: 0.07878 \n",
      "验证损失减少 (0.073953 --> 0.073747). 正在保存模型...\n",
      "[129/500] train_loss: 0.06287 valid_loss: 0.07392 test_loss: 0.08043 \n",
      "[130/500] train_loss: 0.06233 valid_loss: 0.07685 test_loss: 0.08012 \n",
      "[131/500] train_loss: 0.06067 valid_loss: 0.07625 test_loss: 0.07987 \n",
      "[132/500] train_loss: 0.06261 valid_loss: 0.07527 test_loss: 0.08082 \n",
      "[133/500] train_loss: 0.06074 valid_loss: 0.07444 test_loss: 0.07986 \n",
      "[134/500] train_loss: 0.06127 valid_loss: 0.07235 test_loss: 0.07934 \n",
      "验证损失减少 (0.073747 --> 0.072348). 正在保存模型...\n",
      "[135/500] train_loss: 0.06172 valid_loss: 0.07284 test_loss: 0.07890 \n",
      "[136/500] train_loss: 0.06120 valid_loss: 0.07457 test_loss: 0.07957 \n",
      "[137/500] train_loss: 0.06054 valid_loss: 0.07358 test_loss: 0.07935 \n",
      "[138/500] train_loss: 0.06120 valid_loss: 0.07361 test_loss: 0.07852 \n",
      "[139/500] train_loss: 0.06078 valid_loss: 0.07379 test_loss: 0.07927 \n",
      "[140/500] train_loss: 0.05951 valid_loss: 0.07311 test_loss: 0.07882 \n",
      "[141/500] train_loss: 0.06002 valid_loss: 0.07314 test_loss: 0.07859 \n",
      "[142/500] train_loss: 0.05922 valid_loss: 0.07393 test_loss: 0.07932 \n",
      "[143/500] train_loss: 0.05983 valid_loss: 0.07453 test_loss: 0.07773 \n",
      "[144/500] train_loss: 0.05961 valid_loss: 0.07472 test_loss: 0.07920 \n",
      "[145/500] train_loss: 0.06025 valid_loss: 0.07410 test_loss: 0.07922 \n",
      "[146/500] train_loss: 0.05795 valid_loss: 0.07309 test_loss: 0.07848 \n",
      "[147/500] train_loss: 0.05954 valid_loss: 0.07318 test_loss: 0.07880 \n",
      "[148/500] train_loss: 0.05846 valid_loss: 0.07508 test_loss: 0.07967 \n",
      "[149/500] train_loss: 0.05923 valid_loss: 0.07506 test_loss: 0.07943 \n",
      "[150/500] train_loss: 0.05979 valid_loss: 0.07388 test_loss: 0.07772 \n",
      "[151/500] train_loss: 0.05814 valid_loss: 0.07430 test_loss: 0.07990 \n",
      "[152/500] train_loss: 0.05797 valid_loss: 0.07376 test_loss: 0.07930 \n",
      "[153/500] train_loss: 0.05767 valid_loss: 0.07469 test_loss: 0.08114 \n",
      "[154/500] train_loss: 0.05764 valid_loss: 0.07319 test_loss: 0.07934 \n",
      "[155/500] train_loss: 0.05794 valid_loss: 0.07277 test_loss: 0.07974 \n",
      "[156/500] train_loss: 0.05833 valid_loss: 0.07361 test_loss: 0.07923 \n",
      "[157/500] train_loss: 0.05848 valid_loss: 0.07519 test_loss: 0.07889 \n",
      "[158/500] train_loss: 0.05775 valid_loss: 0.07427 test_loss: 0.08011 \n",
      "[159/500] train_loss: 0.05860 valid_loss: 0.07528 test_loss: 0.07969 \n",
      "[160/500] train_loss: 0.05856 valid_loss: 0.07569 test_loss: 0.07809 \n",
      "[161/500] train_loss: 0.05703 valid_loss: 0.07350 test_loss: 0.08024 \n",
      "[162/500] train_loss: 0.05833 valid_loss: 0.07398 test_loss: 0.07743 \n",
      "[163/500] train_loss: 0.05633 valid_loss: 0.07308 test_loss: 0.08040 \n",
      "[164/500] train_loss: 0.05821 valid_loss: 0.07525 test_loss: 0.07921 \n",
      "[165/500] train_loss: 0.05619 valid_loss: 0.07400 test_loss: 0.08049 \n",
      "[166/500] train_loss: 0.05744 valid_loss: 0.07568 test_loss: 0.07907 \n",
      "[167/500] train_loss: 0.05777 valid_loss: 0.07261 test_loss: 0.07971 \n",
      "[168/500] train_loss: 0.05702 valid_loss: 0.07410 test_loss: 0.07823 \n",
      "[169/500] train_loss: 0.05652 valid_loss: 0.07350 test_loss: 0.07769 \n",
      "[170/500] train_loss: 0.05682 valid_loss: 0.07359 test_loss: 0.07779 \n",
      "[171/500] train_loss: 0.05706 valid_loss: 0.07456 test_loss: 0.07753 \n",
      "[172/500] train_loss: 0.05788 valid_loss: 0.07327 test_loss: 0.07860 \n",
      "[173/500] train_loss: 0.05632 valid_loss: 0.07283 test_loss: 0.07898 \n",
      "[174/500] train_loss: 0.05744 valid_loss: 0.07427 test_loss: 0.07801 \n",
      "[175/500] train_loss: 0.05711 valid_loss: 0.07281 test_loss: 0.07947 \n",
      "[176/500] train_loss: 0.05551 valid_loss: 0.07311 test_loss: 0.07922 \n",
      "[177/500] train_loss: 0.05501 valid_loss: 0.07124 test_loss: 0.07691 \n",
      "验证损失减少 (0.072348 --> 0.071238). 正在保存模型...\n",
      "[178/500] train_loss: 0.05603 valid_loss: 0.07399 test_loss: 0.07854 \n",
      "[179/500] train_loss: 0.05581 valid_loss: 0.07375 test_loss: 0.07752 \n",
      "[180/500] train_loss: 0.05526 valid_loss: 0.07266 test_loss: 0.07811 \n",
      "[181/500] train_loss: 0.05646 valid_loss: 0.07212 test_loss: 0.07781 \n",
      "[182/500] train_loss: 0.05572 valid_loss: 0.07270 test_loss: 0.07772 \n",
      "[183/500] train_loss: 0.05709 valid_loss: 0.07767 test_loss: 0.07767 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184/500] train_loss: 0.05567 valid_loss: 0.07284 test_loss: 0.07799 \n",
      "[185/500] train_loss: 0.05443 valid_loss: 0.07315 test_loss: 0.07757 \n",
      "[186/500] train_loss: 0.05555 valid_loss: 0.07363 test_loss: 0.07790 \n",
      "[187/500] train_loss: 0.05536 valid_loss: 0.07270 test_loss: 0.07642 \n",
      "[188/500] train_loss: 0.05549 valid_loss: 0.07448 test_loss: 0.07834 \n",
      "[189/500] train_loss: 0.05589 valid_loss: 0.07504 test_loss: 0.07810 \n",
      "[190/500] train_loss: 0.05610 valid_loss: 0.07186 test_loss: 0.07702 \n",
      "[191/500] train_loss: 0.05420 valid_loss: 0.07373 test_loss: 0.07834 \n",
      "[192/500] train_loss: 0.05466 valid_loss: 0.07444 test_loss: 0.07826 \n",
      "[193/500] train_loss: 0.05529 valid_loss: 0.07403 test_loss: 0.07968 \n",
      "[194/500] train_loss: 0.05684 valid_loss: 0.07350 test_loss: 0.07813 \n",
      "[195/500] train_loss: 0.05474 valid_loss: 0.07357 test_loss: 0.07787 \n",
      "[196/500] train_loss: 0.05362 valid_loss: 0.07246 test_loss: 0.07776 \n",
      "[197/500] train_loss: 0.05476 valid_loss: 0.07296 test_loss: 0.07887 \n",
      "[198/500] train_loss: 0.05149 valid_loss: 0.07388 test_loss: 0.07853 \n",
      "[199/500] train_loss: 0.05418 valid_loss: 0.07427 test_loss: 0.07998 \n",
      "[200/500] train_loss: 0.05331 valid_loss: 0.07281 test_loss: 0.07766 \n",
      "[201/500] train_loss: 0.05310 valid_loss: 0.07248 test_loss: 0.07683 \n",
      "[202/500] train_loss: 0.05340 valid_loss: 0.07447 test_loss: 0.07833 \n",
      "[203/500] train_loss: 0.05476 valid_loss: 0.07440 test_loss: 0.07901 \n",
      "[204/500] train_loss: 0.05437 valid_loss: 0.07351 test_loss: 0.07849 \n",
      "[205/500] train_loss: 0.05419 valid_loss: 0.07486 test_loss: 0.07868 \n",
      "[206/500] train_loss: 0.05304 valid_loss: 0.07355 test_loss: 0.07842 \n",
      "[207/500] train_loss: 0.05273 valid_loss: 0.07331 test_loss: 0.07762 \n",
      "[208/500] train_loss: 0.05374 valid_loss: 0.07547 test_loss: 0.07842 \n",
      "[209/500] train_loss: 0.05324 valid_loss: 0.07403 test_loss: 0.07884 \n",
      "[210/500] train_loss: 0.05322 valid_loss: 0.07317 test_loss: 0.07832 \n",
      "[211/500] train_loss: 0.05327 valid_loss: 0.07454 test_loss: 0.07910 \n",
      "[212/500] train_loss: 0.05416 valid_loss: 0.07273 test_loss: 0.07996 \n",
      "[213/500] train_loss: 0.05272 valid_loss: 0.07239 test_loss: 0.07831 \n",
      "[214/500] train_loss: 0.05378 valid_loss: 0.07452 test_loss: 0.07934 \n",
      "[215/500] train_loss: 0.05305 valid_loss: 0.07289 test_loss: 0.07824 \n",
      "[216/500] train_loss: 0.05141 valid_loss: 0.07319 test_loss: 0.07977 \n",
      "[217/500] train_loss: 0.05133 valid_loss: 0.07214 test_loss: 0.07744 \n",
      "[218/500] train_loss: 0.05318 valid_loss: 0.07415 test_loss: 0.08025 \n",
      "[219/500] train_loss: 0.05205 valid_loss: 0.07205 test_loss: 0.07795 \n",
      "[220/500] train_loss: 0.05300 valid_loss: 0.07384 test_loss: 0.07885 \n",
      "[221/500] train_loss: 0.05301 valid_loss: 0.07872 test_loss: 0.07823 \n",
      "[222/500] train_loss: 0.05145 valid_loss: 0.07215 test_loss: 0.07696 \n",
      "[223/500] train_loss: 0.05173 valid_loss: 0.07385 test_loss: 0.07792 \n",
      "[224/500] train_loss: 0.05194 valid_loss: 0.07487 test_loss: 0.07873 \n",
      "[225/500] train_loss: 0.05259 valid_loss: 0.07256 test_loss: 0.07897 \n",
      "[226/500] train_loss: 0.05263 valid_loss: 0.07284 test_loss: 0.07832 \n",
      "[227/500] train_loss: 0.05252 valid_loss: 0.07337 test_loss: 0.07845 \n",
      "[228/500] train_loss: 0.05265 valid_loss: 0.07334 test_loss: 0.07732 \n",
      "[229/500] train_loss: 0.05231 valid_loss: 0.07243 test_loss: 0.07925 \n",
      "[230/500] train_loss: 0.05211 valid_loss: 0.07429 test_loss: 0.07853 \n",
      "[231/500] train_loss: 0.05002 valid_loss: 0.07339 test_loss: 0.07769 \n",
      "[232/500] train_loss: 0.05102 valid_loss: 0.07268 test_loss: 0.07804 \n",
      "[233/500] train_loss: 0.05095 valid_loss: 0.07443 test_loss: 0.07722 \n",
      "[234/500] train_loss: 0.05156 valid_loss: 0.07514 test_loss: 0.07710 \n",
      "[235/500] train_loss: 0.04985 valid_loss: 0.07316 test_loss: 0.07825 \n",
      "[236/500] train_loss: 0.05131 valid_loss: 0.07458 test_loss: 0.07841 \n",
      "[237/500] train_loss: 0.05078 valid_loss: 0.07623 test_loss: 0.07789 \n",
      "[238/500] train_loss: 0.05011 valid_loss: 0.07367 test_loss: 0.07767 \n",
      "[239/500] train_loss: 0.04980 valid_loss: 0.07392 test_loss: 0.07931 \n",
      "[240/500] train_loss: 0.04948 valid_loss: 0.07332 test_loss: 0.07807 \n",
      "[241/500] train_loss: 0.05051 valid_loss: 0.07283 test_loss: 0.07788 \n",
      "[242/500] train_loss: 0.05187 valid_loss: 0.07317 test_loss: 0.07775 \n",
      "[243/500] train_loss: 0.05118 valid_loss: 0.07257 test_loss: 0.07746 \n",
      "[244/500] train_loss: 0.05131 valid_loss: 0.07241 test_loss: 0.07828 \n",
      "[245/500] train_loss: 0.05047 valid_loss: 0.07619 test_loss: 0.07811 \n",
      "[246/500] train_loss: 0.05067 valid_loss: 0.07382 test_loss: 0.07704 \n",
      "[247/500] train_loss: 0.05028 valid_loss: 0.07400 test_loss: 0.07704 \n",
      "[248/500] train_loss: 0.05184 valid_loss: 0.07975 test_loss: 0.07860 \n",
      "[249/500] train_loss: 0.05022 valid_loss: 0.07395 test_loss: 0.07784 \n",
      "[250/500] train_loss: 0.04977 valid_loss: 0.07417 test_loss: 0.07700 \n",
      "[251/500] train_loss: 0.04961 valid_loss: 0.07358 test_loss: 0.07798 \n",
      "[252/500] train_loss: 0.05095 valid_loss: 0.07356 test_loss: 0.07760 \n",
      "[253/500] train_loss: 0.05069 valid_loss: 0.07320 test_loss: 0.07685 \n",
      "[254/500] train_loss: 0.04973 valid_loss: 0.07399 test_loss: 0.07807 \n",
      "[255/500] train_loss: 0.05017 valid_loss: 0.07334 test_loss: 0.07810 \n",
      "[256/500] train_loss: 0.04967 valid_loss: 0.07346 test_loss: 0.07826 \n",
      "[257/500] train_loss: 0.05085 valid_loss: 0.07399 test_loss: 0.08053 \n",
      "[258/500] train_loss: 0.04931 valid_loss: 0.07333 test_loss: 0.07813 \n",
      "[259/500] train_loss: 0.04782 valid_loss: 0.07490 test_loss: 0.07710 \n",
      "[260/500] train_loss: 0.05025 valid_loss: 0.07470 test_loss: 0.07838 \n",
      "[261/500] train_loss: 0.04956 valid_loss: 0.07324 test_loss: 0.07710 \n",
      "[262/500] train_loss: 0.05064 valid_loss: 0.07401 test_loss: 0.07787 \n",
      "[263/500] train_loss: 0.04895 valid_loss: 0.07537 test_loss: 0.07800 \n",
      "[264/500] train_loss: 0.04982 valid_loss: 0.07374 test_loss: 0.08060 \n",
      "[265/500] train_loss: 0.04867 valid_loss: 0.07587 test_loss: 0.07930 \n",
      "[266/500] train_loss: 0.04981 valid_loss: 0.07356 test_loss: 0.07850 \n",
      "[267/500] train_loss: 0.04975 valid_loss: 0.07401 test_loss: 0.07808 \n",
      "[268/500] train_loss: 0.04961 valid_loss: 0.07485 test_loss: 0.07778 \n",
      "[269/500] train_loss: 0.04948 valid_loss: 0.07541 test_loss: 0.07887 \n",
      "[270/500] train_loss: 0.05039 valid_loss: 0.07405 test_loss: 0.07789 \n",
      "[271/500] train_loss: 0.05016 valid_loss: 0.07264 test_loss: 0.07804 \n",
      "[272/500] train_loss: 0.04963 valid_loss: 0.07438 test_loss: 0.07755 \n",
      "[273/500] train_loss: 0.04865 valid_loss: 0.07470 test_loss: 0.07887 \n",
      "[274/500] train_loss: 0.04920 valid_loss: 0.07689 test_loss: 0.07846 \n",
      "[275/500] train_loss: 0.04851 valid_loss: 0.07444 test_loss: 0.08001 \n",
      "[276/500] train_loss: 0.04922 valid_loss: 0.08054 test_loss: 0.08066 \n",
      "[277/500] train_loss: 0.04932 valid_loss: 0.07537 test_loss: 0.07785 \n",
      "[278/500] train_loss: 0.04778 valid_loss: 0.07407 test_loss: 0.07737 \n",
      "[279/500] train_loss: 0.04739 valid_loss: 0.07385 test_loss: 0.07894 \n",
      "[280/500] train_loss: 0.04859 valid_loss: 0.07558 test_loss: 0.07873 \n",
      "[281/500] train_loss: 0.04880 valid_loss: 0.07548 test_loss: 0.07766 \n",
      "[282/500] train_loss: 0.04919 valid_loss: 0.07377 test_loss: 0.07821 \n",
      "[283/500] train_loss: 0.04819 valid_loss: 0.07323 test_loss: 0.07864 \n",
      "[284/500] train_loss: 0.04869 valid_loss: 0.07415 test_loss: 0.07940 \n",
      "[285/500] train_loss: 0.04843 valid_loss: 0.07472 test_loss: 0.07766 \n",
      "[286/500] train_loss: 0.04780 valid_loss: 0.07523 test_loss: 0.07906 \n",
      "[287/500] train_loss: 0.04795 valid_loss: 0.07397 test_loss: 0.07790 \n",
      "[288/500] train_loss: 0.04824 valid_loss: 0.07618 test_loss: 0.07930 \n",
      "[289/500] train_loss: 0.04696 valid_loss: 0.07452 test_loss: 0.07871 \n",
      "[290/500] train_loss: 0.04648 valid_loss: 0.07400 test_loss: 0.07835 \n",
      "[291/500] train_loss: 0.04887 valid_loss: 0.07419 test_loss: 0.07907 \n",
      "[292/500] train_loss: 0.04929 valid_loss: 0.07416 test_loss: 0.07730 \n",
      "[293/500] train_loss: 0.04768 valid_loss: 0.07554 test_loss: 0.07871 \n",
      "[294/500] train_loss: 0.04772 valid_loss: 0.07436 test_loss: 0.07716 \n",
      "[295/500] train_loss: 0.04721 valid_loss: 0.07393 test_loss: 0.08003 \n",
      "[296/500] train_loss: 0.04771 valid_loss: 0.07547 test_loss: 0.07741 \n",
      "[297/500] train_loss: 0.04813 valid_loss: 0.07444 test_loss: 0.07842 \n",
      "[298/500] train_loss: 0.04628 valid_loss: 0.07487 test_loss: 0.07708 \n",
      "[299/500] train_loss: 0.04672 valid_loss: 0.07382 test_loss: 0.07879 \n",
      "[300/500] train_loss: 0.04736 valid_loss: 0.07671 test_loss: 0.07910 \n",
      "[301/500] train_loss: 0.04682 valid_loss: 0.07527 test_loss: 0.07956 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302/500] train_loss: 0.04765 valid_loss: 0.07585 test_loss: 0.07848 \n",
      "[303/500] train_loss: 0.04638 valid_loss: 0.07384 test_loss: 0.07882 \n",
      "[304/500] train_loss: 0.04639 valid_loss: 0.07432 test_loss: 0.07759 \n",
      "[305/500] train_loss: 0.04760 valid_loss: 0.07547 test_loss: 0.07807 \n",
      "[306/500] train_loss: 0.04693 valid_loss: 0.07445 test_loss: 0.07699 \n",
      "[307/500] train_loss: 0.04711 valid_loss: 0.07534 test_loss: 0.07745 \n",
      "[308/500] train_loss: 0.04725 valid_loss: 0.07634 test_loss: 0.08029 \n",
      "[309/500] train_loss: 0.04693 valid_loss: 0.07458 test_loss: 0.07784 \n",
      "[310/500] train_loss: 0.04686 valid_loss: 0.07463 test_loss: 0.07826 \n",
      "[311/500] train_loss: 0.04734 valid_loss: 0.07314 test_loss: 0.07820 \n",
      "[312/500] train_loss: 0.04664 valid_loss: 0.07444 test_loss: 0.07904 \n",
      "[313/500] train_loss: 0.04761 valid_loss: 0.07481 test_loss: 0.07845 \n",
      "[314/500] train_loss: 0.04653 valid_loss: 0.07533 test_loss: 0.07836 \n",
      "[315/500] train_loss: 0.04792 valid_loss: 0.07588 test_loss: 0.07943 \n",
      "[316/500] train_loss: 0.04688 valid_loss: 0.07613 test_loss: 0.07868 \n",
      "[317/500] train_loss: 0.04647 valid_loss: 0.07554 test_loss: 0.07960 \n",
      "[318/500] train_loss: 0.04638 valid_loss: 0.07449 test_loss: 0.07728 \n",
      "[319/500] train_loss: 0.04710 valid_loss: 0.07530 test_loss: 0.07811 \n",
      "[320/500] train_loss: 0.04614 valid_loss: 0.07476 test_loss: 0.07729 \n",
      "[321/500] train_loss: 0.04681 valid_loss: 0.07678 test_loss: 0.07888 \n",
      "[322/500] train_loss: 0.04676 valid_loss: 0.07505 test_loss: 0.07926 \n",
      "[323/500] train_loss: 0.04564 valid_loss: 0.07508 test_loss: 0.07799 \n",
      "[324/500] train_loss: 0.04627 valid_loss: 0.07566 test_loss: 0.07820 \n",
      "[325/500] train_loss: 0.04768 valid_loss: 0.07563 test_loss: 0.07848 \n",
      "[326/500] train_loss: 0.04724 valid_loss: 0.07223 test_loss: 0.07688 \n",
      "[327/500] train_loss: 0.04545 valid_loss: 0.07470 test_loss: 0.07784 \n",
      "[328/500] train_loss: 0.04565 valid_loss: 0.07457 test_loss: 0.07794 \n",
      "[329/500] train_loss: 0.04751 valid_loss: 0.07468 test_loss: 0.07779 \n",
      "[330/500] train_loss: 0.04538 valid_loss: 0.07548 test_loss: 0.07878 \n",
      "[331/500] train_loss: 0.04698 valid_loss: 0.07489 test_loss: 0.07853 \n",
      "[332/500] train_loss: 0.04741 valid_loss: 0.07611 test_loss: 0.08029 \n",
      "[333/500] train_loss: 0.04531 valid_loss: 0.07494 test_loss: 0.07817 \n",
      "[334/500] train_loss: 0.04633 valid_loss: 0.07451 test_loss: 0.07729 \n",
      "[335/500] train_loss: 0.04630 valid_loss: 0.07443 test_loss: 0.07886 \n",
      "[336/500] train_loss: 0.04464 valid_loss: 0.07533 test_loss: 0.07791 \n",
      "[337/500] train_loss: 0.04484 valid_loss: 0.07541 test_loss: 0.08031 \n",
      "[338/500] train_loss: 0.04612 valid_loss: 0.07416 test_loss: 0.08074 \n",
      "[339/500] train_loss: 0.04465 valid_loss: 0.07362 test_loss: 0.07916 \n",
      "[340/500] train_loss: 0.04544 valid_loss: 0.07432 test_loss: 0.07993 \n",
      "[341/500] train_loss: 0.04448 valid_loss: 0.07495 test_loss: 0.07961 \n",
      "[342/500] train_loss: 0.04478 valid_loss: 0.07553 test_loss: 0.07803 \n",
      "[343/500] train_loss: 0.04619 valid_loss: 0.07717 test_loss: 0.07955 \n",
      "[344/500] train_loss: 0.04337 valid_loss: 0.07652 test_loss: 0.08066 \n",
      "[345/500] train_loss: 0.04594 valid_loss: 0.07630 test_loss: 0.07760 \n",
      "[346/500] train_loss: 0.04574 valid_loss: 0.07453 test_loss: 0.07905 \n",
      "[347/500] train_loss: 0.04575 valid_loss: 0.07364 test_loss: 0.07953 \n",
      "[348/500] train_loss: 0.04480 valid_loss: 0.07489 test_loss: 0.07908 \n",
      "[349/500] train_loss: 0.04465 valid_loss: 0.07578 test_loss: 0.07849 \n",
      "[350/500] train_loss: 0.04533 valid_loss: 0.07332 test_loss: 0.07824 \n",
      "[351/500] train_loss: 0.04476 valid_loss: 0.07495 test_loss: 0.07651 \n",
      "[352/500] train_loss: 0.04542 valid_loss: 0.07356 test_loss: 0.07749 \n",
      "[353/500] train_loss: 0.04521 valid_loss: 0.07561 test_loss: 0.07860 \n",
      "[354/500] train_loss: 0.04531 valid_loss: 0.07347 test_loss: 0.07893 \n",
      "[355/500] train_loss: 0.04451 valid_loss: 0.07564 test_loss: 0.07903 \n",
      "[356/500] train_loss: 0.04371 valid_loss: 0.07371 test_loss: 0.07905 \n",
      "[357/500] train_loss: 0.04519 valid_loss: 0.07325 test_loss: 0.07877 \n",
      "[358/500] train_loss: 0.04437 valid_loss: 0.07688 test_loss: 0.08103 \n",
      "[359/500] train_loss: 0.04470 valid_loss: 0.07491 test_loss: 0.07928 \n",
      "[360/500] train_loss: 0.04457 valid_loss: 0.07517 test_loss: 0.07875 \n",
      "[361/500] train_loss: 0.04369 valid_loss: 0.07572 test_loss: 0.08006 \n",
      "[362/500] train_loss: 0.04481 valid_loss: 0.07340 test_loss: 0.07829 \n",
      "[363/500] train_loss: 0.04420 valid_loss: 0.07378 test_loss: 0.07804 \n",
      "[364/500] train_loss: 0.04488 valid_loss: 0.07372 test_loss: 0.08049 \n",
      "[365/500] train_loss: 0.04426 valid_loss: 0.07450 test_loss: 0.08012 \n",
      "[366/500] train_loss: 0.04296 valid_loss: 0.07373 test_loss: 0.07944 \n",
      "[367/500] train_loss: 0.04472 valid_loss: 0.07614 test_loss: 0.08013 \n",
      "[368/500] train_loss: 0.04558 valid_loss: 0.07292 test_loss: 0.07951 \n",
      "[369/500] train_loss: 0.04259 valid_loss: 0.07440 test_loss: 0.07947 \n",
      "[370/500] train_loss: 0.04389 valid_loss: 0.07387 test_loss: 0.07943 \n",
      "[371/500] train_loss: 0.04333 valid_loss: 0.07331 test_loss: 0.08056 \n",
      "[372/500] train_loss: 0.04370 valid_loss: 0.07554 test_loss: 0.07905 \n",
      "[373/500] train_loss: 0.04441 valid_loss: 0.07557 test_loss: 0.07990 \n",
      "[374/500] train_loss: 0.04487 valid_loss: 0.07362 test_loss: 0.07977 \n",
      "[375/500] train_loss: 0.04312 valid_loss: 0.07398 test_loss: 0.07883 \n",
      "[376/500] train_loss: 0.04404 valid_loss: 0.07622 test_loss: 0.07852 \n",
      "[377/500] train_loss: 0.04324 valid_loss: 0.07458 test_loss: 0.08115 \n",
      "[378/500] train_loss: 0.04407 valid_loss: 0.07351 test_loss: 0.07763 \n",
      "[379/500] train_loss: 0.04380 valid_loss: 0.07492 test_loss: 0.07963 \n",
      "[380/500] train_loss: 0.04414 valid_loss: 0.07494 test_loss: 0.08045 \n",
      "[381/500] train_loss: 0.04482 valid_loss: 0.07536 test_loss: 0.08064 \n",
      "[382/500] train_loss: 0.04269 valid_loss: 0.07437 test_loss: 0.08091 \n",
      "[383/500] train_loss: 0.04404 valid_loss: 0.07388 test_loss: 0.08006 \n",
      "[384/500] train_loss: 0.04472 valid_loss: 0.07439 test_loss: 0.08081 \n",
      "[385/500] train_loss: 0.04350 valid_loss: 0.07371 test_loss: 0.07905 \n",
      "[386/500] train_loss: 0.04359 valid_loss: 0.07637 test_loss: 0.07973 \n",
      "[387/500] train_loss: 0.04268 valid_loss: 0.07450 test_loss: 0.07905 \n",
      "[388/500] train_loss: 0.04451 valid_loss: 0.07492 test_loss: 0.07992 \n",
      "[389/500] train_loss: 0.04469 valid_loss: 0.07475 test_loss: 0.07962 \n",
      "[390/500] train_loss: 0.04430 valid_loss: 0.07255 test_loss: 0.07910 \n",
      "[391/500] train_loss: 0.04300 valid_loss: 0.07341 test_loss: 0.07896 \n",
      "[392/500] train_loss: 0.04326 valid_loss: 0.07400 test_loss: 0.08036 \n",
      "[393/500] train_loss: 0.04394 valid_loss: 0.07518 test_loss: 0.07942 \n",
      "[394/500] train_loss: 0.04229 valid_loss: 0.07444 test_loss: 0.08092 \n",
      "[395/500] train_loss: 0.04391 valid_loss: 0.07333 test_loss: 0.07980 \n",
      "[396/500] train_loss: 0.04343 valid_loss: 0.07429 test_loss: 0.07912 \n",
      "[397/500] train_loss: 0.04314 valid_loss: 0.07305 test_loss: 0.08024 \n",
      "[398/500] train_loss: 0.04347 valid_loss: 0.07247 test_loss: 0.07994 \n",
      "[399/500] train_loss: 0.04294 valid_loss: 0.07321 test_loss: 0.08085 \n",
      "[400/500] train_loss: 0.04439 valid_loss: 0.07449 test_loss: 0.08141 \n",
      "[401/500] train_loss: 0.04379 valid_loss: 0.07413 test_loss: 0.08018 \n",
      "[402/500] train_loss: 0.04388 valid_loss: 0.07519 test_loss: 0.08176 \n",
      "[403/500] train_loss: 0.04342 valid_loss: 0.07281 test_loss: 0.08117 \n",
      "[404/500] train_loss: 0.04321 valid_loss: 0.07331 test_loss: 0.07969 \n",
      "[405/500] train_loss: 0.04453 valid_loss: 0.07422 test_loss: 0.08082 \n",
      "[406/500] train_loss: 0.04270 valid_loss: 0.07416 test_loss: 0.07992 \n",
      "[407/500] train_loss: 0.04246 valid_loss: 0.07411 test_loss: 0.08050 \n",
      "[408/500] train_loss: 0.04233 valid_loss: 0.07355 test_loss: 0.07996 \n",
      "[409/500] train_loss: 0.04467 valid_loss: 0.07389 test_loss: 0.07945 \n",
      "[410/500] train_loss: 0.04334 valid_loss: 0.07696 test_loss: 0.08126 \n",
      "[411/500] train_loss: 0.04255 valid_loss: 0.07317 test_loss: 0.07874 \n",
      "[412/500] train_loss: 0.04347 valid_loss: 0.07644 test_loss: 0.08187 \n",
      "[413/500] train_loss: 0.04411 valid_loss: 0.07606 test_loss: 0.08020 \n",
      "[414/500] train_loss: 0.04289 valid_loss: 0.07504 test_loss: 0.07910 \n",
      "[415/500] train_loss: 0.04350 valid_loss: 0.07502 test_loss: 0.08187 \n",
      "[416/500] train_loss: 0.04100 valid_loss: 0.07605 test_loss: 0.08023 \n",
      "[417/500] train_loss: 0.04306 valid_loss: 0.07507 test_loss: 0.08092 \n",
      "[418/500] train_loss: 0.04309 valid_loss: 0.07599 test_loss: 0.08179 \n",
      "[419/500] train_loss: 0.04256 valid_loss: 0.07447 test_loss: 0.08009 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420/500] train_loss: 0.04203 valid_loss: 0.07386 test_loss: 0.07987 \n",
      "[421/500] train_loss: 0.04161 valid_loss: 0.07403 test_loss: 0.07968 \n",
      "[422/500] train_loss: 0.04311 valid_loss: 0.07555 test_loss: 0.08164 \n",
      "[423/500] train_loss: 0.04306 valid_loss: 0.07596 test_loss: 0.08091 \n",
      "[424/500] train_loss: 0.04072 valid_loss: 0.07400 test_loss: 0.08061 \n",
      "[425/500] train_loss: 0.04152 valid_loss: 0.07547 test_loss: 0.08018 \n",
      "[426/500] train_loss: 0.04258 valid_loss: 0.07425 test_loss: 0.07915 \n",
      "[427/500] train_loss: 0.04265 valid_loss: 0.07519 test_loss: 0.08089 \n",
      "[428/500] train_loss: 0.04234 valid_loss: 0.07492 test_loss: 0.08101 \n",
      "[429/500] train_loss: 0.04238 valid_loss: 0.07450 test_loss: 0.08003 \n",
      "[430/500] train_loss: 0.04138 valid_loss: 0.07592 test_loss: 0.08083 \n",
      "[431/500] train_loss: 0.04147 valid_loss: 0.07292 test_loss: 0.07996 \n",
      "[432/500] train_loss: 0.04257 valid_loss: 0.07482 test_loss: 0.07972 \n",
      "[433/500] train_loss: 0.04147 valid_loss: 0.07489 test_loss: 0.07995 \n",
      "[434/500] train_loss: 0.04132 valid_loss: 0.07500 test_loss: 0.08204 \n",
      "[435/500] train_loss: 0.04183 valid_loss: 0.07422 test_loss: 0.08183 \n",
      "[436/500] train_loss: 0.04249 valid_loss: 0.07558 test_loss: 0.08195 \n",
      "[437/500] train_loss: 0.04315 valid_loss: 0.07372 test_loss: 0.07930 \n",
      "[438/500] train_loss: 0.04178 valid_loss: 0.07520 test_loss: 0.08161 \n",
      "[439/500] train_loss: 0.04270 valid_loss: 0.07304 test_loss: 0.08142 \n",
      "[440/500] train_loss: 0.04105 valid_loss: 0.07441 test_loss: 0.08177 \n",
      "[441/500] train_loss: 0.04206 valid_loss: 0.07366 test_loss: 0.08055 \n",
      "[442/500] train_loss: 0.04149 valid_loss: 0.07450 test_loss: 0.08006 \n",
      "[443/500] train_loss: 0.04066 valid_loss: 0.07546 test_loss: 0.08009 \n",
      "[444/500] train_loss: 0.04231 valid_loss: 0.07589 test_loss: 0.08079 \n",
      "[445/500] train_loss: 0.04187 valid_loss: 0.07569 test_loss: 0.08112 \n",
      "[446/500] train_loss: 0.04257 valid_loss: 0.07754 test_loss: 0.08153 \n",
      "[447/500] train_loss: 0.04224 valid_loss: 0.07416 test_loss: 0.08050 \n",
      "[448/500] train_loss: 0.04184 valid_loss: 0.07672 test_loss: 0.08029 \n",
      "[449/500] train_loss: 0.04155 valid_loss: 0.07424 test_loss: 0.07955 \n",
      "[450/500] train_loss: 0.04166 valid_loss: 0.07403 test_loss: 0.07995 \n",
      "[451/500] train_loss: 0.04237 valid_loss: 0.07592 test_loss: 0.08042 \n",
      "[452/500] train_loss: 0.04031 valid_loss: 0.07331 test_loss: 0.07915 \n",
      "[453/500] train_loss: 0.04116 valid_loss: 0.07518 test_loss: 0.08249 \n",
      "[454/500] train_loss: 0.04207 valid_loss: 0.07475 test_loss: 0.08005 \n",
      "[455/500] train_loss: 0.04127 valid_loss: 0.07570 test_loss: 0.08114 \n",
      "[456/500] train_loss: 0.04089 valid_loss: 0.07620 test_loss: 0.08189 \n",
      "[457/500] train_loss: 0.04139 valid_loss: 0.07516 test_loss: 0.08200 \n",
      "[458/500] train_loss: 0.04172 valid_loss: 0.07612 test_loss: 0.08189 \n",
      "[459/500] train_loss: 0.04195 valid_loss: 0.07560 test_loss: 0.08144 \n",
      "[460/500] train_loss: 0.04058 valid_loss: 0.07427 test_loss: 0.08083 \n",
      "[461/500] train_loss: 0.04067 valid_loss: 0.07475 test_loss: 0.08103 \n",
      "[462/500] train_loss: 0.04138 valid_loss: 0.07429 test_loss: 0.07940 \n",
      "[463/500] train_loss: 0.04121 valid_loss: 0.07558 test_loss: 0.07861 \n",
      "[464/500] train_loss: 0.04259 valid_loss: 0.07692 test_loss: 0.08068 \n",
      "[465/500] train_loss: 0.04141 valid_loss: 0.07412 test_loss: 0.07958 \n",
      "[466/500] train_loss: 0.04172 valid_loss: 0.07515 test_loss: 0.08058 \n",
      "[467/500] train_loss: 0.04168 valid_loss: 0.07526 test_loss: 0.08015 \n",
      "[468/500] train_loss: 0.04119 valid_loss: 0.07518 test_loss: 0.08057 \n",
      "[469/500] train_loss: 0.04083 valid_loss: 0.07511 test_loss: 0.08135 \n",
      "[470/500] train_loss: 0.04014 valid_loss: 0.07456 test_loss: 0.07999 \n",
      "[471/500] train_loss: 0.04063 valid_loss: 0.07407 test_loss: 0.08097 \n",
      "[472/500] train_loss: 0.04042 valid_loss: 0.07323 test_loss: 0.08003 \n",
      "[473/500] train_loss: 0.04075 valid_loss: 0.07474 test_loss: 0.08158 \n",
      "[474/500] train_loss: 0.04124 valid_loss: 0.07396 test_loss: 0.08158 \n",
      "[475/500] train_loss: 0.04040 valid_loss: 0.07294 test_loss: 0.08080 \n",
      "[476/500] train_loss: 0.04053 valid_loss: 0.07408 test_loss: 0.08055 \n",
      "[477/500] train_loss: 0.04014 valid_loss: 0.07305 test_loss: 0.08033 \n",
      "[478/500] train_loss: 0.04075 valid_loss: 0.07307 test_loss: 0.08084 \n",
      "[479/500] train_loss: 0.04104 valid_loss: 0.07322 test_loss: 0.08046 \n",
      "[480/500] train_loss: 0.04028 valid_loss: 0.07548 test_loss: 0.08043 \n",
      "[481/500] train_loss: 0.04088 valid_loss: 0.07344 test_loss: 0.07987 \n",
      "[482/500] train_loss: 0.04044 valid_loss: 0.07561 test_loss: 0.08069 \n",
      "[483/500] train_loss: 0.04026 valid_loss: 0.07522 test_loss: 0.08029 \n",
      "[484/500] train_loss: 0.04117 valid_loss: 0.07580 test_loss: 0.08254 \n",
      "[485/500] train_loss: 0.04061 valid_loss: 0.07433 test_loss: 0.07902 \n",
      "[486/500] train_loss: 0.04007 valid_loss: 0.07226 test_loss: 0.07881 \n",
      "[487/500] train_loss: 0.04010 valid_loss: 0.07345 test_loss: 0.07879 \n",
      "[488/500] train_loss: 0.03943 valid_loss: 0.07424 test_loss: 0.08028 \n",
      "[489/500] train_loss: 0.04080 valid_loss: 0.07436 test_loss: 0.07994 \n",
      "[490/500] train_loss: 0.04088 valid_loss: 0.07501 test_loss: 0.08176 \n",
      "[491/500] train_loss: 0.04021 valid_loss: 0.07384 test_loss: 0.08116 \n",
      "[492/500] train_loss: 0.03970 valid_loss: 0.07331 test_loss: 0.07909 \n",
      "[493/500] train_loss: 0.04098 valid_loss: 0.07447 test_loss: 0.08083 \n",
      "[494/500] train_loss: 0.04046 valid_loss: 0.07308 test_loss: 0.07960 \n",
      "[495/500] train_loss: 0.04079 valid_loss: 0.07334 test_loss: 0.08044 \n",
      "[496/500] train_loss: 0.03934 valid_loss: 0.07297 test_loss: 0.08092 \n",
      "[497/500] train_loss: 0.03929 valid_loss: 0.07314 test_loss: 0.08005 \n",
      "[498/500] train_loss: 0.04077 valid_loss: 0.07399 test_loss: 0.08261 \n",
      "[499/500] train_loss: 0.04094 valid_loss: 0.07314 test_loss: 0.08045 \n",
      "[500/500] train_loss: 0.04070 valid_loss: 0.07499 test_loss: 0.07962 \n",
      "TRAINING MODEL 11\n",
      "[  1/500] train_loss: 0.37456 valid_loss: 0.25988 test_loss: 0.26544 \n",
      "验证损失减少 (inf --> 0.259884). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19241 valid_loss: 0.18334 test_loss: 0.19111 \n",
      "验证损失减少 (0.259884 --> 0.183338). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15512 valid_loss: 0.14973 test_loss: 0.15873 \n",
      "验证损失减少 (0.183338 --> 0.149726). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13584 valid_loss: 0.13794 test_loss: 0.14820 \n",
      "验证损失减少 (0.149726 --> 0.137942). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12900 valid_loss: 0.13427 test_loss: 0.14198 \n",
      "验证损失减少 (0.137942 --> 0.134268). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12674 valid_loss: 0.12854 test_loss: 0.13643 \n",
      "验证损失减少 (0.134268 --> 0.128540). 正在保存模型...\n",
      "[  7/500] train_loss: 0.12431 valid_loss: 0.12542 test_loss: 0.13365 \n",
      "验证损失减少 (0.128540 --> 0.125425). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11617 valid_loss: 0.12195 test_loss: 0.13099 \n",
      "验证损失减少 (0.125425 --> 0.121954). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11292 valid_loss: 0.11912 test_loss: 0.12895 \n",
      "验证损失减少 (0.121954 --> 0.119116). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11182 valid_loss: 0.12139 test_loss: 0.12876 \n",
      "[ 11/500] train_loss: 0.11036 valid_loss: 0.11255 test_loss: 0.12246 \n",
      "验证损失减少 (0.119116 --> 0.112554). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10588 valid_loss: 0.11155 test_loss: 0.12179 \n",
      "验证损失减少 (0.112554 --> 0.111549). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10739 valid_loss: 0.11059 test_loss: 0.12025 \n",
      "验证损失减少 (0.111549 --> 0.110589). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10269 valid_loss: 0.10763 test_loss: 0.11685 \n",
      "验证损失减少 (0.110589 --> 0.107633). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10161 valid_loss: 0.10840 test_loss: 0.11731 \n",
      "[ 16/500] train_loss: 0.10126 valid_loss: 0.10642 test_loss: 0.11402 \n",
      "验证损失减少 (0.107633 --> 0.106415). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09935 valid_loss: 0.10183 test_loss: 0.11219 \n",
      "验证损失减少 (0.106415 --> 0.101826). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09912 valid_loss: 0.10206 test_loss: 0.11091 \n",
      "[ 19/500] train_loss: 0.09724 valid_loss: 0.09981 test_loss: 0.10921 \n",
      "验证损失减少 (0.101826 --> 0.099809). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09780 valid_loss: 0.09915 test_loss: 0.10957 \n",
      "验证损失减少 (0.099809 --> 0.099146). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09520 valid_loss: 0.09830 test_loss: 0.10788 \n",
      "验证损失减少 (0.099146 --> 0.098303). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09488 valid_loss: 0.09795 test_loss: 0.10694 \n",
      "验证损失减少 (0.098303 --> 0.097954). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09339 valid_loss: 0.09765 test_loss: 0.10777 \n",
      "验证损失减少 (0.097954 --> 0.097648). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09070 valid_loss: 0.09634 test_loss: 0.10684 \n",
      "验证损失减少 (0.097648 --> 0.096336). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25/500] train_loss: 0.09125 valid_loss: 0.09717 test_loss: 0.10501 \n",
      "[ 26/500] train_loss: 0.09151 valid_loss: 0.09718 test_loss: 0.10374 \n",
      "[ 27/500] train_loss: 0.09232 valid_loss: 0.09479 test_loss: 0.10277 \n",
      "验证损失减少 (0.096336 --> 0.094792). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.09089 valid_loss: 0.09267 test_loss: 0.10138 \n",
      "验证损失减少 (0.094792 --> 0.092671). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08717 valid_loss: 0.09519 test_loss: 0.10522 \n",
      "[ 30/500] train_loss: 0.08853 valid_loss: 0.08946 test_loss: 0.10101 \n",
      "验证损失减少 (0.092671 --> 0.089462). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08742 valid_loss: 0.09198 test_loss: 0.10133 \n",
      "[ 32/500] train_loss: 0.08692 valid_loss: 0.09047 test_loss: 0.10041 \n",
      "[ 33/500] train_loss: 0.08624 valid_loss: 0.09078 test_loss: 0.09887 \n",
      "[ 34/500] train_loss: 0.08540 valid_loss: 0.08894 test_loss: 0.09787 \n",
      "验证损失减少 (0.089462 --> 0.088937). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08605 valid_loss: 0.08830 test_loss: 0.09739 \n",
      "验证损失减少 (0.088937 --> 0.088297). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08219 valid_loss: 0.08831 test_loss: 0.09845 \n",
      "[ 37/500] train_loss: 0.08393 valid_loss: 0.08832 test_loss: 0.09669 \n",
      "[ 38/500] train_loss: 0.08384 valid_loss: 0.08846 test_loss: 0.09651 \n",
      "[ 39/500] train_loss: 0.08217 valid_loss: 0.08737 test_loss: 0.09737 \n",
      "验证损失减少 (0.088297 --> 0.087371). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.07932 valid_loss: 0.08773 test_loss: 0.09496 \n",
      "[ 41/500] train_loss: 0.08330 valid_loss: 0.08603 test_loss: 0.09533 \n",
      "验证损失减少 (0.087371 --> 0.086032). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.08087 valid_loss: 0.08551 test_loss: 0.09459 \n",
      "验证损失减少 (0.086032 --> 0.085508). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.08185 valid_loss: 0.08489 test_loss: 0.09557 \n",
      "验证损失减少 (0.085508 --> 0.084891). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.07973 valid_loss: 0.08487 test_loss: 0.09348 \n",
      "验证损失减少 (0.084891 --> 0.084873). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.08017 valid_loss: 0.08560 test_loss: 0.09296 \n",
      "[ 46/500] train_loss: 0.08096 valid_loss: 0.08340 test_loss: 0.09152 \n",
      "验证损失减少 (0.084873 --> 0.083402). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07936 valid_loss: 0.08307 test_loss: 0.09285 \n",
      "验证损失减少 (0.083402 --> 0.083066). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.07830 valid_loss: 0.08401 test_loss: 0.09228 \n",
      "[ 49/500] train_loss: 0.07741 valid_loss: 0.08342 test_loss: 0.09375 \n",
      "[ 50/500] train_loss: 0.07789 valid_loss: 0.08377 test_loss: 0.09397 \n",
      "[ 51/500] train_loss: 0.07879 valid_loss: 0.08210 test_loss: 0.09128 \n",
      "验证损失减少 (0.083066 --> 0.082105). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07840 valid_loss: 0.08305 test_loss: 0.09182 \n",
      "[ 53/500] train_loss: 0.07792 valid_loss: 0.08290 test_loss: 0.09281 \n",
      "[ 54/500] train_loss: 0.07754 valid_loss: 0.08113 test_loss: 0.08995 \n",
      "验证损失减少 (0.082105 --> 0.081135). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07478 valid_loss: 0.08478 test_loss: 0.09131 \n",
      "[ 56/500] train_loss: 0.07445 valid_loss: 0.08055 test_loss: 0.09086 \n",
      "验证损失减少 (0.081135 --> 0.080551). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.07637 valid_loss: 0.08308 test_loss: 0.09027 \n",
      "[ 58/500] train_loss: 0.07520 valid_loss: 0.08333 test_loss: 0.09147 \n",
      "[ 59/500] train_loss: 0.07420 valid_loss: 0.08040 test_loss: 0.08962 \n",
      "验证损失减少 (0.080551 --> 0.080395). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07398 valid_loss: 0.08220 test_loss: 0.09033 \n",
      "[ 61/500] train_loss: 0.07492 valid_loss: 0.08200 test_loss: 0.09149 \n",
      "[ 62/500] train_loss: 0.07523 valid_loss: 0.08223 test_loss: 0.08998 \n",
      "[ 63/500] train_loss: 0.07455 valid_loss: 0.08331 test_loss: 0.08949 \n",
      "[ 64/500] train_loss: 0.07360 valid_loss: 0.08159 test_loss: 0.09028 \n",
      "[ 65/500] train_loss: 0.07311 valid_loss: 0.08077 test_loss: 0.08874 \n",
      "[ 66/500] train_loss: 0.07142 valid_loss: 0.08047 test_loss: 0.08860 \n",
      "[ 67/500] train_loss: 0.07191 valid_loss: 0.07973 test_loss: 0.09023 \n",
      "验证损失减少 (0.080395 --> 0.079727). 正在保存模型...\n",
      "[ 68/500] train_loss: 0.07234 valid_loss: 0.08098 test_loss: 0.08775 \n",
      "[ 69/500] train_loss: 0.07268 valid_loss: 0.08125 test_loss: 0.08926 \n",
      "[ 70/500] train_loss: 0.07278 valid_loss: 0.08188 test_loss: 0.08887 \n",
      "[ 71/500] train_loss: 0.07126 valid_loss: 0.08019 test_loss: 0.08723 \n",
      "[ 72/500] train_loss: 0.07168 valid_loss: 0.08261 test_loss: 0.08960 \n",
      "[ 73/500] train_loss: 0.07109 valid_loss: 0.08112 test_loss: 0.08817 \n",
      "[ 74/500] train_loss: 0.07059 valid_loss: 0.08012 test_loss: 0.08819 \n",
      "[ 75/500] train_loss: 0.07233 valid_loss: 0.08185 test_loss: 0.08852 \n",
      "[ 76/500] train_loss: 0.07061 valid_loss: 0.08008 test_loss: 0.08913 \n",
      "[ 77/500] train_loss: 0.07088 valid_loss: 0.08148 test_loss: 0.08733 \n",
      "[ 78/500] train_loss: 0.07142 valid_loss: 0.08027 test_loss: 0.08605 \n",
      "[ 79/500] train_loss: 0.07145 valid_loss: 0.07926 test_loss: 0.08659 \n",
      "验证损失减少 (0.079727 --> 0.079257). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06960 valid_loss: 0.07919 test_loss: 0.08617 \n",
      "验证损失减少 (0.079257 --> 0.079190). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.06792 valid_loss: 0.07704 test_loss: 0.08620 \n",
      "验证损失减少 (0.079190 --> 0.077044). 正在保存模型...\n",
      "[ 82/500] train_loss: 0.06913 valid_loss: 0.07975 test_loss: 0.08560 \n",
      "[ 83/500] train_loss: 0.07009 valid_loss: 0.07789 test_loss: 0.08479 \n",
      "[ 84/500] train_loss: 0.06911 valid_loss: 0.07958 test_loss: 0.08667 \n",
      "[ 85/500] train_loss: 0.06910 valid_loss: 0.07750 test_loss: 0.08576 \n",
      "[ 86/500] train_loss: 0.06997 valid_loss: 0.07806 test_loss: 0.08570 \n",
      "[ 87/500] train_loss: 0.06775 valid_loss: 0.07843 test_loss: 0.08528 \n",
      "[ 88/500] train_loss: 0.06742 valid_loss: 0.07883 test_loss: 0.08718 \n",
      "[ 89/500] train_loss: 0.06893 valid_loss: 0.07750 test_loss: 0.08430 \n",
      "[ 90/500] train_loss: 0.06805 valid_loss: 0.07889 test_loss: 0.08530 \n",
      "[ 91/500] train_loss: 0.06836 valid_loss: 0.07732 test_loss: 0.08441 \n",
      "[ 92/500] train_loss: 0.06793 valid_loss: 0.07781 test_loss: 0.08599 \n",
      "[ 93/500] train_loss: 0.06970 valid_loss: 0.07784 test_loss: 0.08577 \n",
      "[ 94/500] train_loss: 0.06710 valid_loss: 0.07828 test_loss: 0.08630 \n",
      "[ 95/500] train_loss: 0.06728 valid_loss: 0.07598 test_loss: 0.08498 \n",
      "验证损失减少 (0.077044 --> 0.075984). 正在保存模型...\n",
      "[ 96/500] train_loss: 0.06445 valid_loss: 0.07752 test_loss: 0.08357 \n",
      "[ 97/500] train_loss: 0.06714 valid_loss: 0.07671 test_loss: 0.08617 \n",
      "[ 98/500] train_loss: 0.06594 valid_loss: 0.07690 test_loss: 0.08560 \n",
      "[ 99/500] train_loss: 0.06791 valid_loss: 0.07798 test_loss: 0.08383 \n",
      "[100/500] train_loss: 0.06575 valid_loss: 0.07659 test_loss: 0.08340 \n",
      "[101/500] train_loss: 0.06816 valid_loss: 0.07611 test_loss: 0.08386 \n",
      "[102/500] train_loss: 0.06507 valid_loss: 0.07773 test_loss: 0.08487 \n",
      "[103/500] train_loss: 0.06529 valid_loss: 0.07549 test_loss: 0.08315 \n",
      "验证损失减少 (0.075984 --> 0.075487). 正在保存模型...\n",
      "[104/500] train_loss: 0.06572 valid_loss: 0.07593 test_loss: 0.08435 \n",
      "[105/500] train_loss: 0.06513 valid_loss: 0.07739 test_loss: 0.08446 \n",
      "[106/500] train_loss: 0.06381 valid_loss: 0.07637 test_loss: 0.08356 \n",
      "[107/500] train_loss: 0.06661 valid_loss: 0.07734 test_loss: 0.08392 \n",
      "[108/500] train_loss: 0.06575 valid_loss: 0.07535 test_loss: 0.08375 \n",
      "验证损失减少 (0.075487 --> 0.075354). 正在保存模型...\n",
      "[109/500] train_loss: 0.06364 valid_loss: 0.07605 test_loss: 0.08440 \n",
      "[110/500] train_loss: 0.06547 valid_loss: 0.07663 test_loss: 0.08451 \n",
      "[111/500] train_loss: 0.06453 valid_loss: 0.07654 test_loss: 0.08309 \n",
      "[112/500] train_loss: 0.06519 valid_loss: 0.07511 test_loss: 0.08250 \n",
      "验证损失减少 (0.075354 --> 0.075113). 正在保存模型...\n",
      "[113/500] train_loss: 0.06306 valid_loss: 0.07479 test_loss: 0.08346 \n",
      "验证损失减少 (0.075113 --> 0.074795). 正在保存模型...\n",
      "[114/500] train_loss: 0.06525 valid_loss: 0.07576 test_loss: 0.08400 \n",
      "[115/500] train_loss: 0.06435 valid_loss: 0.07770 test_loss: 0.08278 \n",
      "[116/500] train_loss: 0.06507 valid_loss: 0.07512 test_loss: 0.08187 \n",
      "[117/500] train_loss: 0.06303 valid_loss: 0.07505 test_loss: 0.08259 \n",
      "[118/500] train_loss: 0.06343 valid_loss: 0.07560 test_loss: 0.08324 \n",
      "[119/500] train_loss: 0.06321 valid_loss: 0.07566 test_loss: 0.08257 \n",
      "[120/500] train_loss: 0.06458 valid_loss: 0.07582 test_loss: 0.08277 \n",
      "[121/500] train_loss: 0.06450 valid_loss: 0.07572 test_loss: 0.08253 \n",
      "[122/500] train_loss: 0.06321 valid_loss: 0.07546 test_loss: 0.08174 \n",
      "[123/500] train_loss: 0.06295 valid_loss: 0.07608 test_loss: 0.08140 \n",
      "[124/500] train_loss: 0.06436 valid_loss: 0.07530 test_loss: 0.08183 \n",
      "[125/500] train_loss: 0.06166 valid_loss: 0.07519 test_loss: 0.08151 \n",
      "[126/500] train_loss: 0.06239 valid_loss: 0.07565 test_loss: 0.08215 \n",
      "[127/500] train_loss: 0.06148 valid_loss: 0.07741 test_loss: 0.08295 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128/500] train_loss: 0.06139 valid_loss: 0.07552 test_loss: 0.08283 \n",
      "[129/500] train_loss: 0.06175 valid_loss: 0.07678 test_loss: 0.08337 \n",
      "[130/500] train_loss: 0.06325 valid_loss: 0.07499 test_loss: 0.08190 \n",
      "[131/500] train_loss: 0.06173 valid_loss: 0.07573 test_loss: 0.08296 \n",
      "[132/500] train_loss: 0.06083 valid_loss: 0.07744 test_loss: 0.08529 \n",
      "[133/500] train_loss: 0.06054 valid_loss: 0.07514 test_loss: 0.08144 \n",
      "[134/500] train_loss: 0.06067 valid_loss: 0.07666 test_loss: 0.08184 \n",
      "[135/500] train_loss: 0.06105 valid_loss: 0.07644 test_loss: 0.08147 \n",
      "[136/500] train_loss: 0.06137 valid_loss: 0.07532 test_loss: 0.08136 \n",
      "[137/500] train_loss: 0.06179 valid_loss: 0.07577 test_loss: 0.08170 \n",
      "[138/500] train_loss: 0.06118 valid_loss: 0.07379 test_loss: 0.08188 \n",
      "验证损失减少 (0.074795 --> 0.073794). 正在保存模型...\n",
      "[139/500] train_loss: 0.05980 valid_loss: 0.07493 test_loss: 0.08299 \n",
      "[140/500] train_loss: 0.06138 valid_loss: 0.07671 test_loss: 0.08276 \n",
      "[141/500] train_loss: 0.06238 valid_loss: 0.08083 test_loss: 0.08229 \n",
      "[142/500] train_loss: 0.06181 valid_loss: 0.07596 test_loss: 0.08216 \n",
      "[143/500] train_loss: 0.06122 valid_loss: 0.07660 test_loss: 0.08126 \n",
      "[144/500] train_loss: 0.05866 valid_loss: 0.07503 test_loss: 0.08299 \n",
      "[145/500] train_loss: 0.06120 valid_loss: 0.07561 test_loss: 0.08029 \n",
      "[146/500] train_loss: 0.06140 valid_loss: 0.07586 test_loss: 0.08212 \n",
      "[147/500] train_loss: 0.06086 valid_loss: 0.07577 test_loss: 0.08179 \n",
      "[148/500] train_loss: 0.05977 valid_loss: 0.07548 test_loss: 0.08012 \n",
      "[149/500] train_loss: 0.05929 valid_loss: 0.07545 test_loss: 0.08146 \n",
      "[150/500] train_loss: 0.06004 valid_loss: 0.07435 test_loss: 0.08083 \n",
      "[151/500] train_loss: 0.05975 valid_loss: 0.07543 test_loss: 0.08106 \n",
      "[152/500] train_loss: 0.05961 valid_loss: 0.07345 test_loss: 0.08033 \n",
      "验证损失减少 (0.073794 --> 0.073449). 正在保存模型...\n",
      "[153/500] train_loss: 0.05878 valid_loss: 0.07370 test_loss: 0.08099 \n",
      "[154/500] train_loss: 0.06182 valid_loss: 0.07247 test_loss: 0.08221 \n",
      "验证损失减少 (0.073449 --> 0.072472). 正在保存模型...\n",
      "[155/500] train_loss: 0.05962 valid_loss: 0.07340 test_loss: 0.08249 \n",
      "[156/500] train_loss: 0.06002 valid_loss: 0.07369 test_loss: 0.08031 \n",
      "[157/500] train_loss: 0.05918 valid_loss: 0.07397 test_loss: 0.08464 \n",
      "[158/500] train_loss: 0.05864 valid_loss: 0.07258 test_loss: 0.08100 \n",
      "[159/500] train_loss: 0.05915 valid_loss: 0.07286 test_loss: 0.08202 \n",
      "[160/500] train_loss: 0.05926 valid_loss: 0.07386 test_loss: 0.08166 \n",
      "[161/500] train_loss: 0.06012 valid_loss: 0.07307 test_loss: 0.08194 \n",
      "[162/500] train_loss: 0.05774 valid_loss: 0.07384 test_loss: 0.08295 \n",
      "[163/500] train_loss: 0.05794 valid_loss: 0.07328 test_loss: 0.08132 \n",
      "[164/500] train_loss: 0.05726 valid_loss: 0.07403 test_loss: 0.08283 \n",
      "[165/500] train_loss: 0.05797 valid_loss: 0.07481 test_loss: 0.08218 \n",
      "[166/500] train_loss: 0.05779 valid_loss: 0.07270 test_loss: 0.08264 \n",
      "[167/500] train_loss: 0.05694 valid_loss: 0.07294 test_loss: 0.08297 \n",
      "[168/500] train_loss: 0.05849 valid_loss: 0.07378 test_loss: 0.08102 \n",
      "[169/500] train_loss: 0.05795 valid_loss: 0.07487 test_loss: 0.08369 \n",
      "[170/500] train_loss: 0.05834 valid_loss: 0.07405 test_loss: 0.08249 \n",
      "[171/500] train_loss: 0.05837 valid_loss: 0.07554 test_loss: 0.08305 \n",
      "[172/500] train_loss: 0.05879 valid_loss: 0.07156 test_loss: 0.08102 \n",
      "验证损失减少 (0.072472 --> 0.071556). 正在保存模型...\n",
      "[173/500] train_loss: 0.05897 valid_loss: 0.07373 test_loss: 0.08167 \n",
      "[174/500] train_loss: 0.05591 valid_loss: 0.07526 test_loss: 0.08181 \n",
      "[175/500] train_loss: 0.05749 valid_loss: 0.07378 test_loss: 0.08123 \n",
      "[176/500] train_loss: 0.05815 valid_loss: 0.07292 test_loss: 0.08186 \n",
      "[177/500] train_loss: 0.05760 valid_loss: 0.07510 test_loss: 0.08254 \n",
      "[178/500] train_loss: 0.05660 valid_loss: 0.07326 test_loss: 0.08155 \n",
      "[179/500] train_loss: 0.05790 valid_loss: 0.07351 test_loss: 0.08123 \n",
      "[180/500] train_loss: 0.05631 valid_loss: 0.07366 test_loss: 0.08258 \n",
      "[181/500] train_loss: 0.05524 valid_loss: 0.07330 test_loss: 0.08251 \n",
      "[182/500] train_loss: 0.05688 valid_loss: 0.07364 test_loss: 0.08267 \n",
      "[183/500] train_loss: 0.05456 valid_loss: 0.07208 test_loss: 0.08082 \n",
      "[184/500] train_loss: 0.05655 valid_loss: 0.07464 test_loss: 0.08218 \n",
      "[185/500] train_loss: 0.05662 valid_loss: 0.07304 test_loss: 0.08209 \n",
      "[186/500] train_loss: 0.05602 valid_loss: 0.07476 test_loss: 0.08167 \n",
      "[187/500] train_loss: 0.05667 valid_loss: 0.07210 test_loss: 0.08000 \n",
      "[188/500] train_loss: 0.05662 valid_loss: 0.07562 test_loss: 0.08295 \n",
      "[189/500] train_loss: 0.05454 valid_loss: 0.07360 test_loss: 0.08142 \n",
      "[190/500] train_loss: 0.05573 valid_loss: 0.07634 test_loss: 0.08068 \n",
      "[191/500] train_loss: 0.05498 valid_loss: 0.07349 test_loss: 0.08195 \n",
      "[192/500] train_loss: 0.05626 valid_loss: 0.07585 test_loss: 0.08297 \n",
      "[193/500] train_loss: 0.05556 valid_loss: 0.07296 test_loss: 0.08131 \n",
      "[194/500] train_loss: 0.05518 valid_loss: 0.07139 test_loss: 0.08040 \n",
      "验证损失减少 (0.071556 --> 0.071391). 正在保存模型...\n",
      "[195/500] train_loss: 0.05572 valid_loss: 0.07224 test_loss: 0.08061 \n",
      "[196/500] train_loss: 0.05599 valid_loss: 0.07216 test_loss: 0.08034 \n",
      "[197/500] train_loss: 0.05439 valid_loss: 0.07321 test_loss: 0.08062 \n",
      "[198/500] train_loss: 0.05522 valid_loss: 0.07232 test_loss: 0.08161 \n",
      "[199/500] train_loss: 0.05462 valid_loss: 0.07200 test_loss: 0.08037 \n",
      "[200/500] train_loss: 0.05509 valid_loss: 0.07169 test_loss: 0.08051 \n",
      "[201/500] train_loss: 0.05448 valid_loss: 0.07259 test_loss: 0.08184 \n",
      "[202/500] train_loss: 0.05483 valid_loss: 0.07117 test_loss: 0.08030 \n",
      "验证损失减少 (0.071391 --> 0.071174). 正在保存模型...\n",
      "[203/500] train_loss: 0.05623 valid_loss: 0.07298 test_loss: 0.08091 \n",
      "[204/500] train_loss: 0.05462 valid_loss: 0.07479 test_loss: 0.08258 \n",
      "[205/500] train_loss: 0.05416 valid_loss: 0.07268 test_loss: 0.08132 \n",
      "[206/500] train_loss: 0.05311 valid_loss: 0.07158 test_loss: 0.07852 \n",
      "[207/500] train_loss: 0.05290 valid_loss: 0.07232 test_loss: 0.08180 \n",
      "[208/500] train_loss: 0.05551 valid_loss: 0.07389 test_loss: 0.08210 \n",
      "[209/500] train_loss: 0.05405 valid_loss: 0.07555 test_loss: 0.08262 \n",
      "[210/500] train_loss: 0.05392 valid_loss: 0.07251 test_loss: 0.08295 \n",
      "[211/500] train_loss: 0.05375 valid_loss: 0.07158 test_loss: 0.08115 \n",
      "[212/500] train_loss: 0.05331 valid_loss: 0.07362 test_loss: 0.08232 \n",
      "[213/500] train_loss: 0.05481 valid_loss: 0.07109 test_loss: 0.08042 \n",
      "验证损失减少 (0.071174 --> 0.071087). 正在保存模型...\n",
      "[214/500] train_loss: 0.05435 valid_loss: 0.07228 test_loss: 0.08192 \n",
      "[215/500] train_loss: 0.05344 valid_loss: 0.07297 test_loss: 0.08059 \n",
      "[216/500] train_loss: 0.05398 valid_loss: 0.07306 test_loss: 0.08150 \n",
      "[217/500] train_loss: 0.05322 valid_loss: 0.07266 test_loss: 0.08106 \n",
      "[218/500] train_loss: 0.05328 valid_loss: 0.07374 test_loss: 0.08036 \n",
      "[219/500] train_loss: 0.05506 valid_loss: 0.07249 test_loss: 0.08091 \n",
      "[220/500] train_loss: 0.05299 valid_loss: 0.07338 test_loss: 0.08016 \n",
      "[221/500] train_loss: 0.05335 valid_loss: 0.07367 test_loss: 0.08023 \n",
      "[222/500] train_loss: 0.05476 valid_loss: 0.07416 test_loss: 0.08116 \n",
      "[223/500] train_loss: 0.05404 valid_loss: 0.07366 test_loss: 0.08229 \n",
      "[224/500] train_loss: 0.05294 valid_loss: 0.07386 test_loss: 0.08297 \n",
      "[225/500] train_loss: 0.05331 valid_loss: 0.07193 test_loss: 0.08174 \n",
      "[226/500] train_loss: 0.05374 valid_loss: 0.07133 test_loss: 0.08108 \n",
      "[227/500] train_loss: 0.05376 valid_loss: 0.07290 test_loss: 0.08238 \n",
      "[228/500] train_loss: 0.05271 valid_loss: 0.07278 test_loss: 0.08120 \n",
      "[229/500] train_loss: 0.05341 valid_loss: 0.07274 test_loss: 0.08106 \n",
      "[230/500] train_loss: 0.05234 valid_loss: 0.07347 test_loss: 0.08013 \n",
      "[231/500] train_loss: 0.05381 valid_loss: 0.07428 test_loss: 0.08066 \n",
      "[232/500] train_loss: 0.05309 valid_loss: 0.07124 test_loss: 0.08109 \n",
      "[233/500] train_loss: 0.05202 valid_loss: 0.07376 test_loss: 0.07982 \n",
      "[234/500] train_loss: 0.05146 valid_loss: 0.07289 test_loss: 0.08066 \n",
      "[235/500] train_loss: 0.05175 valid_loss: 0.07417 test_loss: 0.08033 \n",
      "[236/500] train_loss: 0.05204 valid_loss: 0.07497 test_loss: 0.08127 \n",
      "[237/500] train_loss: 0.05191 valid_loss: 0.07292 test_loss: 0.08162 \n",
      "[238/500] train_loss: 0.05054 valid_loss: 0.06996 test_loss: 0.08232 \n",
      "验证损失减少 (0.071087 --> 0.069960). 正在保存模型...\n",
      "[239/500] train_loss: 0.05297 valid_loss: 0.07533 test_loss: 0.08181 \n",
      "[240/500] train_loss: 0.05301 valid_loss: 0.07267 test_loss: 0.08125 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[241/500] train_loss: 0.05226 valid_loss: 0.07210 test_loss: 0.08076 \n",
      "[242/500] train_loss: 0.05256 valid_loss: 0.07133 test_loss: 0.08035 \n",
      "[243/500] train_loss: 0.05289 valid_loss: 0.07162 test_loss: 0.07969 \n",
      "[244/500] train_loss: 0.05114 valid_loss: 0.07171 test_loss: 0.08012 \n",
      "[245/500] train_loss: 0.05105 valid_loss: 0.07282 test_loss: 0.08242 \n",
      "[246/500] train_loss: 0.05084 valid_loss: 0.07103 test_loss: 0.08079 \n",
      "[247/500] train_loss: 0.05278 valid_loss: 0.07308 test_loss: 0.08050 \n",
      "[248/500] train_loss: 0.05019 valid_loss: 0.07280 test_loss: 0.07975 \n",
      "[249/500] train_loss: 0.05130 valid_loss: 0.07131 test_loss: 0.08250 \n",
      "[250/500] train_loss: 0.05163 valid_loss: 0.07378 test_loss: 0.08316 \n",
      "[251/500] train_loss: 0.05088 valid_loss: 0.07206 test_loss: 0.08186 \n",
      "[252/500] train_loss: 0.05184 valid_loss: 0.07435 test_loss: 0.08101 \n",
      "[253/500] train_loss: 0.05056 valid_loss: 0.07309 test_loss: 0.08100 \n",
      "[254/500] train_loss: 0.05075 valid_loss: 0.07246 test_loss: 0.08054 \n",
      "[255/500] train_loss: 0.05100 valid_loss: 0.07141 test_loss: 0.08031 \n",
      "[256/500] train_loss: 0.05120 valid_loss: 0.07397 test_loss: 0.08194 \n",
      "[257/500] train_loss: 0.05133 valid_loss: 0.07334 test_loss: 0.08015 \n",
      "[258/500] train_loss: 0.05117 valid_loss: 0.07437 test_loss: 0.07944 \n",
      "[259/500] train_loss: 0.05186 valid_loss: 0.07165 test_loss: 0.08090 \n",
      "[260/500] train_loss: 0.05170 valid_loss: 0.07130 test_loss: 0.07968 \n",
      "[261/500] train_loss: 0.05058 valid_loss: 0.07075 test_loss: 0.08085 \n",
      "[262/500] train_loss: 0.05113 valid_loss: 0.07273 test_loss: 0.08091 \n",
      "[263/500] train_loss: 0.05122 valid_loss: 0.07215 test_loss: 0.08032 \n",
      "[264/500] train_loss: 0.05015 valid_loss: 0.07282 test_loss: 0.07948 \n",
      "[265/500] train_loss: 0.05072 valid_loss: 0.07136 test_loss: 0.07995 \n",
      "[266/500] train_loss: 0.05052 valid_loss: 0.07007 test_loss: 0.07946 \n",
      "[267/500] train_loss: 0.05144 valid_loss: 0.07151 test_loss: 0.08017 \n",
      "[268/500] train_loss: 0.04999 valid_loss: 0.07048 test_loss: 0.07815 \n",
      "[269/500] train_loss: 0.05071 valid_loss: 0.07243 test_loss: 0.08043 \n",
      "[270/500] train_loss: 0.04964 valid_loss: 0.07445 test_loss: 0.08153 \n",
      "[271/500] train_loss: 0.05011 valid_loss: 0.07131 test_loss: 0.08046 \n",
      "[272/500] train_loss: 0.04807 valid_loss: 0.07208 test_loss: 0.07967 \n",
      "[273/500] train_loss: 0.04989 valid_loss: 0.07395 test_loss: 0.08159 \n",
      "[274/500] train_loss: 0.05014 valid_loss: 0.07453 test_loss: 0.08759 \n",
      "[275/500] train_loss: 0.05120 valid_loss: 0.07197 test_loss: 0.08027 \n",
      "[276/500] train_loss: 0.05066 valid_loss: 0.07097 test_loss: 0.07832 \n",
      "[277/500] train_loss: 0.05175 valid_loss: 0.07149 test_loss: 0.07917 \n",
      "[278/500] train_loss: 0.05040 valid_loss: 0.07127 test_loss: 0.08016 \n",
      "[279/500] train_loss: 0.04935 valid_loss: 0.07651 test_loss: 0.07950 \n",
      "[280/500] train_loss: 0.04915 valid_loss: 0.07154 test_loss: 0.08077 \n",
      "[281/500] train_loss: 0.04878 valid_loss: 0.07226 test_loss: 0.07971 \n",
      "[282/500] train_loss: 0.05012 valid_loss: 0.07391 test_loss: 0.08018 \n",
      "[283/500] train_loss: 0.04941 valid_loss: 0.07241 test_loss: 0.08110 \n",
      "[284/500] train_loss: 0.04901 valid_loss: 0.07276 test_loss: 0.08083 \n",
      "[285/500] train_loss: 0.04900 valid_loss: 0.07156 test_loss: 0.07930 \n",
      "[286/500] train_loss: 0.04902 valid_loss: 0.07166 test_loss: 0.08025 \n",
      "[287/500] train_loss: 0.04866 valid_loss: 0.07097 test_loss: 0.07922 \n",
      "[288/500] train_loss: 0.04963 valid_loss: 0.07134 test_loss: 0.08111 \n",
      "[289/500] train_loss: 0.04875 valid_loss: 0.07033 test_loss: 0.08045 \n",
      "[290/500] train_loss: 0.04986 valid_loss: 0.07127 test_loss: 0.08051 \n",
      "[291/500] train_loss: 0.04927 valid_loss: 0.07132 test_loss: 0.07995 \n",
      "[292/500] train_loss: 0.04821 valid_loss: 0.07170 test_loss: 0.08102 \n",
      "[293/500] train_loss: 0.04985 valid_loss: 0.07292 test_loss: 0.08151 \n",
      "[294/500] train_loss: 0.04950 valid_loss: 0.07436 test_loss: 0.08015 \n",
      "[295/500] train_loss: 0.04924 valid_loss: 0.07265 test_loss: 0.08080 \n",
      "[296/500] train_loss: 0.04954 valid_loss: 0.07435 test_loss: 0.08204 \n",
      "[297/500] train_loss: 0.04857 valid_loss: 0.07289 test_loss: 0.07998 \n",
      "[298/500] train_loss: 0.04850 valid_loss: 0.07436 test_loss: 0.08021 \n",
      "[299/500] train_loss: 0.04904 valid_loss: 0.07276 test_loss: 0.07881 \n",
      "[300/500] train_loss: 0.04882 valid_loss: 0.07670 test_loss: 0.08209 \n",
      "[301/500] train_loss: 0.04898 valid_loss: 0.07441 test_loss: 0.07962 \n",
      "[302/500] train_loss: 0.04828 valid_loss: 0.07324 test_loss: 0.08117 \n",
      "[303/500] train_loss: 0.04861 valid_loss: 0.07326 test_loss: 0.08016 \n",
      "[304/500] train_loss: 0.04766 valid_loss: 0.07418 test_loss: 0.08053 \n",
      "[305/500] train_loss: 0.04837 valid_loss: 0.07326 test_loss: 0.08101 \n",
      "[306/500] train_loss: 0.04833 valid_loss: 0.07508 test_loss: 0.08102 \n",
      "[307/500] train_loss: 0.04819 valid_loss: 0.07542 test_loss: 0.08165 \n",
      "[308/500] train_loss: 0.04759 valid_loss: 0.07506 test_loss: 0.08158 \n",
      "[309/500] train_loss: 0.04880 valid_loss: 0.07825 test_loss: 0.08027 \n",
      "[310/500] train_loss: 0.04706 valid_loss: 0.07635 test_loss: 0.08134 \n",
      "[311/500] train_loss: 0.04866 valid_loss: 0.07379 test_loss: 0.08028 \n",
      "[312/500] train_loss: 0.04816 valid_loss: 0.07440 test_loss: 0.08295 \n",
      "[313/500] train_loss: 0.04750 valid_loss: 0.07277 test_loss: 0.08020 \n",
      "[314/500] train_loss: 0.04749 valid_loss: 0.07215 test_loss: 0.08008 \n",
      "[315/500] train_loss: 0.04725 valid_loss: 0.07265 test_loss: 0.08010 \n",
      "[316/500] train_loss: 0.04853 valid_loss: 0.07242 test_loss: 0.07972 \n",
      "[317/500] train_loss: 0.04767 valid_loss: 0.07300 test_loss: 0.07945 \n",
      "[318/500] train_loss: 0.04778 valid_loss: 0.07337 test_loss: 0.08075 \n",
      "[319/500] train_loss: 0.04859 valid_loss: 0.07456 test_loss: 0.08005 \n",
      "[320/500] train_loss: 0.04810 valid_loss: 0.07191 test_loss: 0.08010 \n",
      "[321/500] train_loss: 0.04845 valid_loss: 0.07312 test_loss: 0.08055 \n",
      "[322/500] train_loss: 0.04905 valid_loss: 0.07189 test_loss: 0.07933 \n",
      "[323/500] train_loss: 0.04807 valid_loss: 0.07381 test_loss: 0.08108 \n",
      "[324/500] train_loss: 0.04748 valid_loss: 0.07235 test_loss: 0.07889 \n",
      "[325/500] train_loss: 0.04777 valid_loss: 0.07235 test_loss: 0.08103 \n",
      "[326/500] train_loss: 0.04731 valid_loss: 0.07245 test_loss: 0.08089 \n",
      "[327/500] train_loss: 0.04648 valid_loss: 0.07237 test_loss: 0.08196 \n",
      "[328/500] train_loss: 0.04675 valid_loss: 0.07186 test_loss: 0.08040 \n",
      "[329/500] train_loss: 0.04782 valid_loss: 0.07100 test_loss: 0.08039 \n",
      "[330/500] train_loss: 0.04647 valid_loss: 0.07370 test_loss: 0.08089 \n",
      "[331/500] train_loss: 0.04857 valid_loss: 0.07168 test_loss: 0.07945 \n",
      "[332/500] train_loss: 0.04679 valid_loss: 0.07183 test_loss: 0.07861 \n",
      "[333/500] train_loss: 0.04681 valid_loss: 0.07161 test_loss: 0.08081 \n",
      "[334/500] train_loss: 0.04673 valid_loss: 0.07368 test_loss: 0.08001 \n",
      "[335/500] train_loss: 0.04719 valid_loss: 0.07293 test_loss: 0.08033 \n",
      "[336/500] train_loss: 0.04631 valid_loss: 0.07096 test_loss: 0.07948 \n",
      "[337/500] train_loss: 0.04621 valid_loss: 0.06936 test_loss: 0.07788 \n",
      "验证损失减少 (0.069960 --> 0.069359). 正在保存模型...\n",
      "[338/500] train_loss: 0.04597 valid_loss: 0.07207 test_loss: 0.07971 \n",
      "[339/500] train_loss: 0.04652 valid_loss: 0.07222 test_loss: 0.07994 \n",
      "[340/500] train_loss: 0.04638 valid_loss: 0.07217 test_loss: 0.08034 \n",
      "[341/500] train_loss: 0.04715 valid_loss: 0.07187 test_loss: 0.07959 \n",
      "[342/500] train_loss: 0.04630 valid_loss: 0.06928 test_loss: 0.08054 \n",
      "验证损失减少 (0.069359 --> 0.069275). 正在保存模型...\n",
      "[343/500] train_loss: 0.04482 valid_loss: 0.07312 test_loss: 0.07983 \n",
      "[344/500] train_loss: 0.04620 valid_loss: 0.07175 test_loss: 0.07962 \n",
      "[345/500] train_loss: 0.04571 valid_loss: 0.07161 test_loss: 0.07959 \n",
      "[346/500] train_loss: 0.04480 valid_loss: 0.07295 test_loss: 0.07969 \n",
      "[347/500] train_loss: 0.04685 valid_loss: 0.07264 test_loss: 0.08037 \n",
      "[348/500] train_loss: 0.04697 valid_loss: 0.07286 test_loss: 0.08003 \n",
      "[349/500] train_loss: 0.04732 valid_loss: 0.07174 test_loss: 0.07939 \n",
      "[350/500] train_loss: 0.04678 valid_loss: 0.07309 test_loss: 0.07966 \n",
      "[351/500] train_loss: 0.04626 valid_loss: 0.07158 test_loss: 0.07877 \n",
      "[352/500] train_loss: 0.04414 valid_loss: 0.07223 test_loss: 0.08018 \n",
      "[353/500] train_loss: 0.04656 valid_loss: 0.07075 test_loss: 0.07792 \n",
      "[354/500] train_loss: 0.04596 valid_loss: 0.07236 test_loss: 0.07987 \n",
      "[355/500] train_loss: 0.04670 valid_loss: 0.07358 test_loss: 0.08154 \n",
      "[356/500] train_loss: 0.04606 valid_loss: 0.07246 test_loss: 0.08013 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[357/500] train_loss: 0.04502 valid_loss: 0.07287 test_loss: 0.07981 \n",
      "[358/500] train_loss: 0.04608 valid_loss: 0.07518 test_loss: 0.08116 \n",
      "[359/500] train_loss: 0.04537 valid_loss: 0.07529 test_loss: 0.08164 \n",
      "[360/500] train_loss: 0.04502 valid_loss: 0.07338 test_loss: 0.07875 \n",
      "[361/500] train_loss: 0.04538 valid_loss: 0.07289 test_loss: 0.08057 \n",
      "[362/500] train_loss: 0.04469 valid_loss: 0.07298 test_loss: 0.07920 \n",
      "[363/500] train_loss: 0.04629 valid_loss: 0.07293 test_loss: 0.07938 \n",
      "[364/500] train_loss: 0.04718 valid_loss: 0.07373 test_loss: 0.08037 \n",
      "[365/500] train_loss: 0.04612 valid_loss: 0.07306 test_loss: 0.08024 \n",
      "[366/500] train_loss: 0.04617 valid_loss: 0.07374 test_loss: 0.07976 \n",
      "[367/500] train_loss: 0.04629 valid_loss: 0.07527 test_loss: 0.08198 \n",
      "[368/500] train_loss: 0.04411 valid_loss: 0.07294 test_loss: 0.08058 \n",
      "[369/500] train_loss: 0.04569 valid_loss: 0.07508 test_loss: 0.08063 \n",
      "[370/500] train_loss: 0.04467 valid_loss: 0.07458 test_loss: 0.08025 \n",
      "[371/500] train_loss: 0.04590 valid_loss: 0.07510 test_loss: 0.08135 \n",
      "[372/500] train_loss: 0.04439 valid_loss: 0.07237 test_loss: 0.08008 \n",
      "[373/500] train_loss: 0.04502 valid_loss: 0.07439 test_loss: 0.08062 \n",
      "[374/500] train_loss: 0.04505 valid_loss: 0.07347 test_loss: 0.07926 \n",
      "[375/500] train_loss: 0.04567 valid_loss: 0.07614 test_loss: 0.08151 \n",
      "[376/500] train_loss: 0.04531 valid_loss: 0.07376 test_loss: 0.08076 \n",
      "[377/500] train_loss: 0.04617 valid_loss: 0.07444 test_loss: 0.08194 \n",
      "[378/500] train_loss: 0.04417 valid_loss: 0.07276 test_loss: 0.08013 \n",
      "[379/500] train_loss: 0.04379 valid_loss: 0.07457 test_loss: 0.08034 \n",
      "[380/500] train_loss: 0.04632 valid_loss: 0.07723 test_loss: 0.08147 \n",
      "[381/500] train_loss: 0.04470 valid_loss: 0.07437 test_loss: 0.08007 \n",
      "[382/500] train_loss: 0.04398 valid_loss: 0.07397 test_loss: 0.08160 \n",
      "[383/500] train_loss: 0.04655 valid_loss: 0.07513 test_loss: 0.08058 \n",
      "[384/500] train_loss: 0.04471 valid_loss: 0.07544 test_loss: 0.07971 \n",
      "[385/500] train_loss: 0.04476 valid_loss: 0.07398 test_loss: 0.07942 \n",
      "[386/500] train_loss: 0.04451 valid_loss: 0.07373 test_loss: 0.08020 \n",
      "[387/500] train_loss: 0.04529 valid_loss: 0.07388 test_loss: 0.07923 \n",
      "[388/500] train_loss: 0.04485 valid_loss: 0.07360 test_loss: 0.07911 \n",
      "[389/500] train_loss: 0.04486 valid_loss: 0.07413 test_loss: 0.07933 \n",
      "[390/500] train_loss: 0.04556 valid_loss: 0.07409 test_loss: 0.08001 \n",
      "[391/500] train_loss: 0.04352 valid_loss: 0.07178 test_loss: 0.07863 \n",
      "[392/500] train_loss: 0.04543 valid_loss: 0.07343 test_loss: 0.08084 \n",
      "[393/500] train_loss: 0.04575 valid_loss: 0.07288 test_loss: 0.07863 \n",
      "[394/500] train_loss: 0.04410 valid_loss: 0.07248 test_loss: 0.07892 \n",
      "[395/500] train_loss: 0.04431 valid_loss: 0.07530 test_loss: 0.07972 \n",
      "[396/500] train_loss: 0.04453 valid_loss: 0.07398 test_loss: 0.08168 \n",
      "[397/500] train_loss: 0.04419 valid_loss: 0.07438 test_loss: 0.08089 \n",
      "[398/500] train_loss: 0.04579 valid_loss: 0.07379 test_loss: 0.08129 \n",
      "[399/500] train_loss: 0.04381 valid_loss: 0.07434 test_loss: 0.08044 \n",
      "[400/500] train_loss: 0.04323 valid_loss: 0.07351 test_loss: 0.08044 \n",
      "[401/500] train_loss: 0.04354 valid_loss: 0.07406 test_loss: 0.08060 \n",
      "[402/500] train_loss: 0.04467 valid_loss: 0.07397 test_loss: 0.08100 \n",
      "[403/500] train_loss: 0.04427 valid_loss: 0.07488 test_loss: 0.08229 \n",
      "[404/500] train_loss: 0.04438 valid_loss: 0.07355 test_loss: 0.07989 \n",
      "[405/500] train_loss: 0.04507 valid_loss: 0.07427 test_loss: 0.08088 \n",
      "[406/500] train_loss: 0.04497 valid_loss: 0.07321 test_loss: 0.07955 \n",
      "[407/500] train_loss: 0.04419 valid_loss: 0.07446 test_loss: 0.07937 \n",
      "[408/500] train_loss: 0.04475 valid_loss: 0.07500 test_loss: 0.08006 \n",
      "[409/500] train_loss: 0.04470 valid_loss: 0.07246 test_loss: 0.08148 \n",
      "[410/500] train_loss: 0.04428 valid_loss: 0.07549 test_loss: 0.08185 \n",
      "[411/500] train_loss: 0.04325 valid_loss: 0.07333 test_loss: 0.08080 \n",
      "[412/500] train_loss: 0.04396 valid_loss: 0.07705 test_loss: 0.08079 \n",
      "[413/500] train_loss: 0.04395 valid_loss: 0.07371 test_loss: 0.08122 \n",
      "[414/500] train_loss: 0.04373 valid_loss: 0.07373 test_loss: 0.08078 \n",
      "[415/500] train_loss: 0.04442 valid_loss: 0.07216 test_loss: 0.07994 \n",
      "[416/500] train_loss: 0.04434 valid_loss: 0.07359 test_loss: 0.08218 \n",
      "[417/500] train_loss: 0.04553 valid_loss: 0.07419 test_loss: 0.08160 \n",
      "[418/500] train_loss: 0.04270 valid_loss: 0.07369 test_loss: 0.08045 \n",
      "[419/500] train_loss: 0.04360 valid_loss: 0.07389 test_loss: 0.08253 \n",
      "[420/500] train_loss: 0.04505 valid_loss: 0.07356 test_loss: 0.08122 \n",
      "[421/500] train_loss: 0.04309 valid_loss: 0.07179 test_loss: 0.07954 \n",
      "[422/500] train_loss: 0.04387 valid_loss: 0.07345 test_loss: 0.07990 \n",
      "[423/500] train_loss: 0.04263 valid_loss: 0.07779 test_loss: 0.08227 \n",
      "[424/500] train_loss: 0.04335 valid_loss: 0.07269 test_loss: 0.08020 \n",
      "[425/500] train_loss: 0.04505 valid_loss: 0.07388 test_loss: 0.07994 \n",
      "[426/500] train_loss: 0.04316 valid_loss: 0.07473 test_loss: 0.08149 \n",
      "[427/500] train_loss: 0.04308 valid_loss: 0.07207 test_loss: 0.08017 \n",
      "[428/500] train_loss: 0.04463 valid_loss: 0.07266 test_loss: 0.08011 \n",
      "[429/500] train_loss: 0.04236 valid_loss: 0.07370 test_loss: 0.08293 \n",
      "[430/500] train_loss: 0.04475 valid_loss: 0.07408 test_loss: 0.08150 \n",
      "[431/500] train_loss: 0.04413 valid_loss: 0.07614 test_loss: 0.08216 \n",
      "[432/500] train_loss: 0.04338 valid_loss: 0.07307 test_loss: 0.08133 \n",
      "[433/500] train_loss: 0.04283 valid_loss: 0.07356 test_loss: 0.08198 \n",
      "[434/500] train_loss: 0.04336 valid_loss: 0.07289 test_loss: 0.08139 \n",
      "[435/500] train_loss: 0.04340 valid_loss: 0.07305 test_loss: 0.08231 \n",
      "[436/500] train_loss: 0.04218 valid_loss: 0.07518 test_loss: 0.08158 \n",
      "[437/500] train_loss: 0.04213 valid_loss: 0.07424 test_loss: 0.08147 \n",
      "[438/500] train_loss: 0.04260 valid_loss: 0.07329 test_loss: 0.08051 \n",
      "[439/500] train_loss: 0.04335 valid_loss: 0.07647 test_loss: 0.08000 \n",
      "[440/500] train_loss: 0.04182 valid_loss: 0.07321 test_loss: 0.08028 \n",
      "[441/500] train_loss: 0.04273 valid_loss: 0.07322 test_loss: 0.08047 \n",
      "[442/500] train_loss: 0.04230 valid_loss: 0.07485 test_loss: 0.08223 \n",
      "[443/500] train_loss: 0.04309 valid_loss: 0.07572 test_loss: 0.08108 \n",
      "[444/500] train_loss: 0.04235 valid_loss: 0.07579 test_loss: 0.07995 \n",
      "[445/500] train_loss: 0.04305 valid_loss: 0.07574 test_loss: 0.08202 \n",
      "[446/500] train_loss: 0.04309 valid_loss: 0.07617 test_loss: 0.08049 \n",
      "[447/500] train_loss: 0.04329 valid_loss: 0.07336 test_loss: 0.08186 \n",
      "[448/500] train_loss: 0.04209 valid_loss: 0.07555 test_loss: 0.08046 \n",
      "[449/500] train_loss: 0.04224 valid_loss: 0.07540 test_loss: 0.08093 \n",
      "[450/500] train_loss: 0.04375 valid_loss: 0.07378 test_loss: 0.08090 \n",
      "[451/500] train_loss: 0.04392 valid_loss: 0.07195 test_loss: 0.08028 \n",
      "[452/500] train_loss: 0.04175 valid_loss: 0.07362 test_loss: 0.08081 \n",
      "[453/500] train_loss: 0.04153 valid_loss: 0.07192 test_loss: 0.08189 \n",
      "[454/500] train_loss: 0.04249 valid_loss: 0.07528 test_loss: 0.08120 \n",
      "[455/500] train_loss: 0.04382 valid_loss: 0.07385 test_loss: 0.07994 \n",
      "[456/500] train_loss: 0.04286 valid_loss: 0.07394 test_loss: 0.07956 \n",
      "[457/500] train_loss: 0.04289 valid_loss: 0.07411 test_loss: 0.08139 \n",
      "[458/500] train_loss: 0.04277 valid_loss: 0.07416 test_loss: 0.08086 \n",
      "[459/500] train_loss: 0.04325 valid_loss: 0.07283 test_loss: 0.08252 \n",
      "[460/500] train_loss: 0.04149 valid_loss: 0.07354 test_loss: 0.08353 \n",
      "[461/500] train_loss: 0.04333 valid_loss: 0.07830 test_loss: 0.08214 \n",
      "[462/500] train_loss: 0.04350 valid_loss: 0.07801 test_loss: 0.08163 \n",
      "[463/500] train_loss: 0.04173 valid_loss: 0.07335 test_loss: 0.07997 \n",
      "[464/500] train_loss: 0.04234 valid_loss: 0.07268 test_loss: 0.07950 \n",
      "[465/500] train_loss: 0.04199 valid_loss: 0.07256 test_loss: 0.08106 \n",
      "[466/500] train_loss: 0.04155 valid_loss: 0.07430 test_loss: 0.08033 \n",
      "[467/500] train_loss: 0.04284 valid_loss: 0.07380 test_loss: 0.08092 \n",
      "[468/500] train_loss: 0.04280 valid_loss: 0.07633 test_loss: 0.08090 \n",
      "[469/500] train_loss: 0.04312 valid_loss: 0.07757 test_loss: 0.07948 \n",
      "[470/500] train_loss: 0.04167 valid_loss: 0.07284 test_loss: 0.08049 \n",
      "[471/500] train_loss: 0.04286 valid_loss: 0.07315 test_loss: 0.07902 \n",
      "[472/500] train_loss: 0.04087 valid_loss: 0.07454 test_loss: 0.08057 \n",
      "[473/500] train_loss: 0.04204 valid_loss: 0.07617 test_loss: 0.08088 \n",
      "[474/500] train_loss: 0.04260 valid_loss: 0.07519 test_loss: 0.08078 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475/500] train_loss: 0.04267 valid_loss: 0.07370 test_loss: 0.08099 \n",
      "[476/500] train_loss: 0.04176 valid_loss: 0.07455 test_loss: 0.08121 \n",
      "[477/500] train_loss: 0.04227 valid_loss: 0.07426 test_loss: 0.08151 \n",
      "[478/500] train_loss: 0.04216 valid_loss: 0.07420 test_loss: 0.08038 \n",
      "[479/500] train_loss: 0.04218 valid_loss: 0.07287 test_loss: 0.08146 \n",
      "[480/500] train_loss: 0.04240 valid_loss: 0.07303 test_loss: 0.08079 \n",
      "[481/500] train_loss: 0.04231 valid_loss: 0.07338 test_loss: 0.07972 \n",
      "[482/500] train_loss: 0.04125 valid_loss: 0.07514 test_loss: 0.08229 \n",
      "[483/500] train_loss: 0.04234 valid_loss: 0.07325 test_loss: 0.08032 \n",
      "[484/500] train_loss: 0.04290 valid_loss: 0.07152 test_loss: 0.08014 \n",
      "[485/500] train_loss: 0.04073 valid_loss: 0.07629 test_loss: 0.08156 \n",
      "[486/500] train_loss: 0.04160 valid_loss: 0.07425 test_loss: 0.08224 \n",
      "[487/500] train_loss: 0.04186 valid_loss: 0.07423 test_loss: 0.08138 \n",
      "[488/500] train_loss: 0.04101 valid_loss: 0.07308 test_loss: 0.08266 \n",
      "[489/500] train_loss: 0.04288 valid_loss: 0.07946 test_loss: 0.08283 \n",
      "[490/500] train_loss: 0.04160 valid_loss: 0.07496 test_loss: 0.08226 \n",
      "[491/500] train_loss: 0.04207 valid_loss: 0.07443 test_loss: 0.08262 \n",
      "[492/500] train_loss: 0.04183 valid_loss: 0.07370 test_loss: 0.08079 \n",
      "[493/500] train_loss: 0.04113 valid_loss: 0.07248 test_loss: 0.08274 \n",
      "[494/500] train_loss: 0.04135 valid_loss: 0.07483 test_loss: 0.08098 \n",
      "[495/500] train_loss: 0.04111 valid_loss: 0.07321 test_loss: 0.08093 \n",
      "[496/500] train_loss: 0.04088 valid_loss: 0.07401 test_loss: 0.08259 \n",
      "[497/500] train_loss: 0.04176 valid_loss: 0.07279 test_loss: 0.08041 \n",
      "[498/500] train_loss: 0.04045 valid_loss: 0.07290 test_loss: 0.08319 \n",
      "[499/500] train_loss: 0.04025 valid_loss: 0.07405 test_loss: 0.08215 \n",
      "[500/500] train_loss: 0.04176 valid_loss: 0.07309 test_loss: 0.08362 \n",
      "TRAINING MODEL 12\n",
      "[  1/500] train_loss: 0.35397 valid_loss: 0.25236 test_loss: 0.25761 \n",
      "验证损失减少 (inf --> 0.252365). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19096 valid_loss: 0.18014 test_loss: 0.18705 \n",
      "验证损失减少 (0.252365 --> 0.180138). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15304 valid_loss: 0.14874 test_loss: 0.15827 \n",
      "验证损失减少 (0.180138 --> 0.148745). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13553 valid_loss: 0.13730 test_loss: 0.14979 \n",
      "验证损失减少 (0.148745 --> 0.137295). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12738 valid_loss: 0.12869 test_loss: 0.13774 \n",
      "验证损失减少 (0.137295 --> 0.128686). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12246 valid_loss: 0.12302 test_loss: 0.13315 \n",
      "验证损失减少 (0.128686 --> 0.123023). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11608 valid_loss: 0.12030 test_loss: 0.13051 \n",
      "验证损失减少 (0.123023 --> 0.120304). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11374 valid_loss: 0.11472 test_loss: 0.12519 \n",
      "验证损失减少 (0.120304 --> 0.114720). 正在保存模型...\n",
      "[  9/500] train_loss: 0.10909 valid_loss: 0.11310 test_loss: 0.12592 \n",
      "验证损失减少 (0.114720 --> 0.113097). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10859 valid_loss: 0.11292 test_loss: 0.12450 \n",
      "验证损失减少 (0.113097 --> 0.112916). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10652 valid_loss: 0.10878 test_loss: 0.12013 \n",
      "验证损失减少 (0.112916 --> 0.108780). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10544 valid_loss: 0.11100 test_loss: 0.11903 \n",
      "[ 13/500] train_loss: 0.10174 valid_loss: 0.10463 test_loss: 0.11537 \n",
      "验证损失减少 (0.108780 --> 0.104627). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10411 valid_loss: 0.10400 test_loss: 0.11404 \n",
      "验证损失减少 (0.104627 --> 0.104001). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.09764 valid_loss: 0.10197 test_loss: 0.11374 \n",
      "验证损失减少 (0.104001 --> 0.101975). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09753 valid_loss: 0.10103 test_loss: 0.11245 \n",
      "验证损失减少 (0.101975 --> 0.101035). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09944 valid_loss: 0.10104 test_loss: 0.11085 \n",
      "[ 18/500] train_loss: 0.09610 valid_loss: 0.09881 test_loss: 0.10989 \n",
      "验证损失减少 (0.101035 --> 0.098813). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09485 valid_loss: 0.09642 test_loss: 0.10805 \n",
      "验证损失减少 (0.098813 --> 0.096418). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09360 valid_loss: 0.09548 test_loss: 0.10765 \n",
      "验证损失减少 (0.096418 --> 0.095478). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09275 valid_loss: 0.09388 test_loss: 0.10572 \n",
      "验证损失减少 (0.095478 --> 0.093878). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09235 valid_loss: 0.09377 test_loss: 0.10600 \n",
      "验证损失减少 (0.093878 --> 0.093766). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09044 valid_loss: 0.09249 test_loss: 0.10620 \n",
      "验证损失减少 (0.093766 --> 0.092495). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.08868 valid_loss: 0.09349 test_loss: 0.10375 \n",
      "[ 25/500] train_loss: 0.08993 valid_loss: 0.09203 test_loss: 0.10272 \n",
      "验证损失减少 (0.092495 --> 0.092029). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.08773 valid_loss: 0.09304 test_loss: 0.10568 \n",
      "[ 27/500] train_loss: 0.08946 valid_loss: 0.08986 test_loss: 0.10192 \n",
      "验证损失减少 (0.092029 --> 0.089863). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08669 valid_loss: 0.09057 test_loss: 0.10146 \n",
      "[ 29/500] train_loss: 0.08853 valid_loss: 0.09039 test_loss: 0.10083 \n",
      "[ 30/500] train_loss: 0.08593 valid_loss: 0.08958 test_loss: 0.09982 \n",
      "验证损失减少 (0.089863 --> 0.089583). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08715 valid_loss: 0.08888 test_loss: 0.09776 \n",
      "验证损失减少 (0.089583 --> 0.088884). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08534 valid_loss: 0.08806 test_loss: 0.09781 \n",
      "验证损失减少 (0.088884 --> 0.088063). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08473 valid_loss: 0.08796 test_loss: 0.09762 \n",
      "验证损失减少 (0.088063 --> 0.087964). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08313 valid_loss: 0.08985 test_loss: 0.10013 \n",
      "[ 35/500] train_loss: 0.08190 valid_loss: 0.08540 test_loss: 0.09786 \n",
      "验证损失减少 (0.087964 --> 0.085397). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08413 valid_loss: 0.08661 test_loss: 0.09793 \n",
      "[ 37/500] train_loss: 0.07925 valid_loss: 0.08409 test_loss: 0.09631 \n",
      "验证损失减少 (0.085397 --> 0.084086). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.07963 valid_loss: 0.08579 test_loss: 0.09534 \n",
      "[ 39/500] train_loss: 0.08158 valid_loss: 0.08406 test_loss: 0.09633 \n",
      "验证损失减少 (0.084086 --> 0.084061). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.08081 valid_loss: 0.08725 test_loss: 0.09811 \n",
      "[ 41/500] train_loss: 0.08152 valid_loss: 0.08400 test_loss: 0.09441 \n",
      "验证损失减少 (0.084061 --> 0.083997). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.07903 valid_loss: 0.08176 test_loss: 0.09399 \n",
      "验证损失减少 (0.083997 --> 0.081756). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.07935 valid_loss: 0.08273 test_loss: 0.09302 \n",
      "[ 44/500] train_loss: 0.08034 valid_loss: 0.08185 test_loss: 0.09195 \n",
      "[ 45/500] train_loss: 0.07801 valid_loss: 0.08243 test_loss: 0.09162 \n",
      "[ 46/500] train_loss: 0.07966 valid_loss: 0.08374 test_loss: 0.09231 \n",
      "[ 47/500] train_loss: 0.07875 valid_loss: 0.08498 test_loss: 0.09732 \n",
      "[ 48/500] train_loss: 0.07580 valid_loss: 0.08185 test_loss: 0.09430 \n",
      "[ 49/500] train_loss: 0.07787 valid_loss: 0.08139 test_loss: 0.09204 \n",
      "验证损失减少 (0.081756 --> 0.081385). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07634 valid_loss: 0.08178 test_loss: 0.09295 \n",
      "[ 51/500] train_loss: 0.07508 valid_loss: 0.08185 test_loss: 0.09103 \n",
      "[ 52/500] train_loss: 0.07583 valid_loss: 0.08182 test_loss: 0.09023 \n",
      "[ 53/500] train_loss: 0.07454 valid_loss: 0.08045 test_loss: 0.08941 \n",
      "验证损失减少 (0.081385 --> 0.080450). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07531 valid_loss: 0.08030 test_loss: 0.09048 \n",
      "验证损失减少 (0.080450 --> 0.080302). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07638 valid_loss: 0.08076 test_loss: 0.09144 \n",
      "[ 56/500] train_loss: 0.07529 valid_loss: 0.07944 test_loss: 0.08823 \n",
      "验证损失减少 (0.080302 --> 0.079440). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.07284 valid_loss: 0.08155 test_loss: 0.09117 \n",
      "[ 58/500] train_loss: 0.07878 valid_loss: 0.07852 test_loss: 0.08839 \n",
      "验证损失减少 (0.079440 --> 0.078518). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07398 valid_loss: 0.08274 test_loss: 0.09088 \n",
      "[ 60/500] train_loss: 0.07310 valid_loss: 0.07967 test_loss: 0.08884 \n",
      "[ 61/500] train_loss: 0.07175 valid_loss: 0.08131 test_loss: 0.08899 \n",
      "[ 62/500] train_loss: 0.07318 valid_loss: 0.08026 test_loss: 0.08926 \n",
      "[ 63/500] train_loss: 0.07131 valid_loss: 0.08152 test_loss: 0.08863 \n",
      "[ 64/500] train_loss: 0.07271 valid_loss: 0.07812 test_loss: 0.08775 \n",
      "验证损失减少 (0.078518 --> 0.078118). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07310 valid_loss: 0.08004 test_loss: 0.08838 \n",
      "[ 66/500] train_loss: 0.07103 valid_loss: 0.07859 test_loss: 0.08882 \n",
      "[ 67/500] train_loss: 0.07335 valid_loss: 0.08025 test_loss: 0.08899 \n",
      "[ 68/500] train_loss: 0.07198 valid_loss: 0.07844 test_loss: 0.08711 \n",
      "[ 69/500] train_loss: 0.07106 valid_loss: 0.07745 test_loss: 0.08867 \n",
      "验证损失减少 (0.078118 --> 0.077446). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 70/500] train_loss: 0.07199 valid_loss: 0.07663 test_loss: 0.08601 \n",
      "验证损失减少 (0.077446 --> 0.076625). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.07035 valid_loss: 0.07735 test_loss: 0.08612 \n",
      "[ 72/500] train_loss: 0.06881 valid_loss: 0.07858 test_loss: 0.08733 \n",
      "[ 73/500] train_loss: 0.06989 valid_loss: 0.07692 test_loss: 0.08625 \n",
      "[ 74/500] train_loss: 0.06891 valid_loss: 0.07796 test_loss: 0.08672 \n",
      "[ 75/500] train_loss: 0.06964 valid_loss: 0.07768 test_loss: 0.08855 \n",
      "[ 76/500] train_loss: 0.06910 valid_loss: 0.07732 test_loss: 0.08528 \n",
      "[ 77/500] train_loss: 0.07049 valid_loss: 0.07750 test_loss: 0.08565 \n",
      "[ 78/500] train_loss: 0.07018 valid_loss: 0.07685 test_loss: 0.08761 \n",
      "[ 79/500] train_loss: 0.07087 valid_loss: 0.07559 test_loss: 0.08452 \n",
      "验证损失减少 (0.076625 --> 0.075594). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06912 valid_loss: 0.07514 test_loss: 0.08439 \n",
      "验证损失减少 (0.075594 --> 0.075138). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.06905 valid_loss: 0.07748 test_loss: 0.08532 \n",
      "[ 82/500] train_loss: 0.06787 valid_loss: 0.07681 test_loss: 0.08474 \n",
      "[ 83/500] train_loss: 0.06902 valid_loss: 0.07800 test_loss: 0.08355 \n",
      "[ 84/500] train_loss: 0.06800 valid_loss: 0.07564 test_loss: 0.08411 \n",
      "[ 85/500] train_loss: 0.06741 valid_loss: 0.07573 test_loss: 0.08504 \n",
      "[ 86/500] train_loss: 0.06719 valid_loss: 0.07476 test_loss: 0.08313 \n",
      "验证损失减少 (0.075138 --> 0.074763). 正在保存模型...\n",
      "[ 87/500] train_loss: 0.06828 valid_loss: 0.07565 test_loss: 0.08398 \n",
      "[ 88/500] train_loss: 0.06877 valid_loss: 0.07512 test_loss: 0.08509 \n",
      "[ 89/500] train_loss: 0.06674 valid_loss: 0.07659 test_loss: 0.08579 \n",
      "[ 90/500] train_loss: 0.06603 valid_loss: 0.07529 test_loss: 0.08443 \n",
      "[ 91/500] train_loss: 0.06754 valid_loss: 0.07524 test_loss: 0.08474 \n",
      "[ 92/500] train_loss: 0.06654 valid_loss: 0.07715 test_loss: 0.08480 \n",
      "[ 93/500] train_loss: 0.06723 valid_loss: 0.07556 test_loss: 0.08335 \n",
      "[ 94/500] train_loss: 0.06642 valid_loss: 0.07389 test_loss: 0.08399 \n",
      "验证损失减少 (0.074763 --> 0.073890). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.06590 valid_loss: 0.07560 test_loss: 0.08452 \n",
      "[ 96/500] train_loss: 0.06676 valid_loss: 0.07457 test_loss: 0.08496 \n",
      "[ 97/500] train_loss: 0.06686 valid_loss: 0.07419 test_loss: 0.08385 \n",
      "[ 98/500] train_loss: 0.06527 valid_loss: 0.07566 test_loss: 0.08444 \n",
      "[ 99/500] train_loss: 0.06703 valid_loss: 0.07560 test_loss: 0.08398 \n",
      "[100/500] train_loss: 0.06389 valid_loss: 0.07683 test_loss: 0.08599 \n",
      "[101/500] train_loss: 0.06413 valid_loss: 0.07687 test_loss: 0.08372 \n",
      "[102/500] train_loss: 0.06385 valid_loss: 0.07574 test_loss: 0.08373 \n",
      "[103/500] train_loss: 0.06615 valid_loss: 0.07487 test_loss: 0.08369 \n",
      "[104/500] train_loss: 0.06495 valid_loss: 0.07543 test_loss: 0.08382 \n",
      "[105/500] train_loss: 0.06550 valid_loss: 0.07481 test_loss: 0.08329 \n",
      "[106/500] train_loss: 0.06452 valid_loss: 0.07461 test_loss: 0.08334 \n",
      "[107/500] train_loss: 0.06602 valid_loss: 0.07524 test_loss: 0.08371 \n",
      "[108/500] train_loss: 0.06393 valid_loss: 0.07583 test_loss: 0.08379 \n",
      "[109/500] train_loss: 0.06467 valid_loss: 0.07553 test_loss: 0.08324 \n",
      "[110/500] train_loss: 0.06375 valid_loss: 0.07486 test_loss: 0.08297 \n",
      "[111/500] train_loss: 0.06479 valid_loss: 0.07570 test_loss: 0.08213 \n",
      "[112/500] train_loss: 0.06268 valid_loss: 0.07471 test_loss: 0.08112 \n",
      "[113/500] train_loss: 0.06430 valid_loss: 0.07551 test_loss: 0.08448 \n",
      "[114/500] train_loss: 0.06279 valid_loss: 0.07582 test_loss: 0.08301 \n",
      "[115/500] train_loss: 0.06332 valid_loss: 0.07517 test_loss: 0.08294 \n",
      "[116/500] train_loss: 0.06388 valid_loss: 0.07607 test_loss: 0.08340 \n",
      "[117/500] train_loss: 0.06183 valid_loss: 0.07492 test_loss: 0.08342 \n",
      "[118/500] train_loss: 0.06406 valid_loss: 0.07426 test_loss: 0.08283 \n",
      "[119/500] train_loss: 0.06253 valid_loss: 0.07404 test_loss: 0.08292 \n",
      "[120/500] train_loss: 0.06155 valid_loss: 0.07442 test_loss: 0.08176 \n",
      "[121/500] train_loss: 0.06138 valid_loss: 0.07491 test_loss: 0.08217 \n",
      "[122/500] train_loss: 0.06309 valid_loss: 0.07658 test_loss: 0.08219 \n",
      "[123/500] train_loss: 0.06312 valid_loss: 0.07444 test_loss: 0.08171 \n",
      "[124/500] train_loss: 0.06218 valid_loss: 0.07336 test_loss: 0.08257 \n",
      "验证损失减少 (0.073890 --> 0.073364). 正在保存模型...\n",
      "[125/500] train_loss: 0.06191 valid_loss: 0.07288 test_loss: 0.08155 \n",
      "验证损失减少 (0.073364 --> 0.072881). 正在保存模型...\n",
      "[126/500] train_loss: 0.06104 valid_loss: 0.07386 test_loss: 0.08071 \n",
      "[127/500] train_loss: 0.06072 valid_loss: 0.07618 test_loss: 0.08310 \n",
      "[128/500] train_loss: 0.06081 valid_loss: 0.07386 test_loss: 0.08196 \n",
      "[129/500] train_loss: 0.06086 valid_loss: 0.07411 test_loss: 0.08347 \n",
      "[130/500] train_loss: 0.06072 valid_loss: 0.07417 test_loss: 0.08352 \n",
      "[131/500] train_loss: 0.06155 valid_loss: 0.07380 test_loss: 0.08160 \n",
      "[132/500] train_loss: 0.06169 valid_loss: 0.07348 test_loss: 0.08129 \n",
      "[133/500] train_loss: 0.06087 valid_loss: 0.07339 test_loss: 0.08227 \n",
      "[134/500] train_loss: 0.06019 valid_loss: 0.07570 test_loss: 0.08279 \n",
      "[135/500] train_loss: 0.06147 valid_loss: 0.07449 test_loss: 0.08107 \n",
      "[136/500] train_loss: 0.06019 valid_loss: 0.07462 test_loss: 0.08180 \n",
      "[137/500] train_loss: 0.06121 valid_loss: 0.07442 test_loss: 0.08236 \n",
      "[138/500] train_loss: 0.05864 valid_loss: 0.07521 test_loss: 0.08091 \n",
      "[139/500] train_loss: 0.06145 valid_loss: 0.07481 test_loss: 0.08157 \n",
      "[140/500] train_loss: 0.05952 valid_loss: 0.07360 test_loss: 0.08124 \n",
      "[141/500] train_loss: 0.05956 valid_loss: 0.07590 test_loss: 0.08216 \n",
      "[142/500] train_loss: 0.05992 valid_loss: 0.07326 test_loss: 0.08155 \n",
      "[143/500] train_loss: 0.06048 valid_loss: 0.07458 test_loss: 0.08372 \n",
      "[144/500] train_loss: 0.05940 valid_loss: 0.07267 test_loss: 0.08090 \n",
      "验证损失减少 (0.072881 --> 0.072671). 正在保存模型...\n",
      "[145/500] train_loss: 0.05757 valid_loss: 0.07460 test_loss: 0.07958 \n",
      "[146/500] train_loss: 0.05990 valid_loss: 0.07362 test_loss: 0.08123 \n",
      "[147/500] train_loss: 0.05752 valid_loss: 0.07328 test_loss: 0.08097 \n",
      "[148/500] train_loss: 0.06022 valid_loss: 0.07308 test_loss: 0.08091 \n",
      "[149/500] train_loss: 0.05830 valid_loss: 0.07331 test_loss: 0.08228 \n",
      "[150/500] train_loss: 0.05982 valid_loss: 0.07457 test_loss: 0.08231 \n",
      "[151/500] train_loss: 0.05909 valid_loss: 0.07378 test_loss: 0.08094 \n",
      "[152/500] train_loss: 0.05931 valid_loss: 0.07402 test_loss: 0.08155 \n",
      "[153/500] train_loss: 0.05946 valid_loss: 0.07450 test_loss: 0.08213 \n",
      "[154/500] train_loss: 0.05903 valid_loss: 0.07344 test_loss: 0.08119 \n",
      "[155/500] train_loss: 0.05938 valid_loss: 0.07382 test_loss: 0.08044 \n",
      "[156/500] train_loss: 0.05768 valid_loss: 0.07387 test_loss: 0.08094 \n",
      "[157/500] train_loss: 0.05963 valid_loss: 0.07371 test_loss: 0.08084 \n",
      "[158/500] train_loss: 0.05773 valid_loss: 0.07596 test_loss: 0.08011 \n",
      "[159/500] train_loss: 0.05628 valid_loss: 0.07311 test_loss: 0.08172 \n",
      "[160/500] train_loss: 0.05926 valid_loss: 0.07407 test_loss: 0.08345 \n",
      "[161/500] train_loss: 0.05821 valid_loss: 0.07181 test_loss: 0.08134 \n",
      "验证损失减少 (0.072671 --> 0.071813). 正在保存模型...\n",
      "[162/500] train_loss: 0.05606 valid_loss: 0.07398 test_loss: 0.08066 \n",
      "[163/500] train_loss: 0.05935 valid_loss: 0.07440 test_loss: 0.08131 \n",
      "[164/500] train_loss: 0.05702 valid_loss: 0.07220 test_loss: 0.08164 \n",
      "[165/500] train_loss: 0.05749 valid_loss: 0.07357 test_loss: 0.07965 \n",
      "[166/500] train_loss: 0.05836 valid_loss: 0.07398 test_loss: 0.08087 \n",
      "[167/500] train_loss: 0.05748 valid_loss: 0.07312 test_loss: 0.07961 \n",
      "[168/500] train_loss: 0.05653 valid_loss: 0.07242 test_loss: 0.08020 \n",
      "[169/500] train_loss: 0.05585 valid_loss: 0.07112 test_loss: 0.08049 \n",
      "验证损失减少 (0.071813 --> 0.071120). 正在保存模型...\n",
      "[170/500] train_loss: 0.05730 valid_loss: 0.07189 test_loss: 0.08080 \n",
      "[171/500] train_loss: 0.05667 valid_loss: 0.07299 test_loss: 0.08112 \n",
      "[172/500] train_loss: 0.05642 valid_loss: 0.07130 test_loss: 0.07977 \n",
      "[173/500] train_loss: 0.05566 valid_loss: 0.07563 test_loss: 0.08429 \n",
      "[174/500] train_loss: 0.05599 valid_loss: 0.07415 test_loss: 0.07955 \n",
      "[175/500] train_loss: 0.05740 valid_loss: 0.07428 test_loss: 0.08171 \n",
      "[176/500] train_loss: 0.05713 valid_loss: 0.07285 test_loss: 0.08118 \n",
      "[177/500] train_loss: 0.05713 valid_loss: 0.07271 test_loss: 0.07946 \n",
      "[178/500] train_loss: 0.05622 valid_loss: 0.07404 test_loss: 0.08035 \n",
      "[179/500] train_loss: 0.05543 valid_loss: 0.07324 test_loss: 0.07962 \n",
      "[180/500] train_loss: 0.05550 valid_loss: 0.07251 test_loss: 0.08037 \n",
      "[181/500] train_loss: 0.05579 valid_loss: 0.07319 test_loss: 0.08010 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[182/500] train_loss: 0.05528 valid_loss: 0.07358 test_loss: 0.08095 \n",
      "[183/500] train_loss: 0.05663 valid_loss: 0.07227 test_loss: 0.08003 \n",
      "[184/500] train_loss: 0.05529 valid_loss: 0.07440 test_loss: 0.08076 \n",
      "[185/500] train_loss: 0.05540 valid_loss: 0.07381 test_loss: 0.08044 \n",
      "[186/500] train_loss: 0.05634 valid_loss: 0.07179 test_loss: 0.07918 \n",
      "[187/500] train_loss: 0.05564 valid_loss: 0.07397 test_loss: 0.08054 \n",
      "[188/500] train_loss: 0.05574 valid_loss: 0.07226 test_loss: 0.07950 \n",
      "[189/500] train_loss: 0.05453 valid_loss: 0.07458 test_loss: 0.08012 \n",
      "[190/500] train_loss: 0.05517 valid_loss: 0.07509 test_loss: 0.08161 \n",
      "[191/500] train_loss: 0.05525 valid_loss: 0.07320 test_loss: 0.08000 \n",
      "[192/500] train_loss: 0.05450 valid_loss: 0.07317 test_loss: 0.08063 \n",
      "[193/500] train_loss: 0.05481 valid_loss: 0.07391 test_loss: 0.08100 \n",
      "[194/500] train_loss: 0.05450 valid_loss: 0.07339 test_loss: 0.08014 \n",
      "[195/500] train_loss: 0.05473 valid_loss: 0.07286 test_loss: 0.08008 \n",
      "[196/500] train_loss: 0.05497 valid_loss: 0.07434 test_loss: 0.08032 \n",
      "[197/500] train_loss: 0.05438 valid_loss: 0.07392 test_loss: 0.08035 \n",
      "[198/500] train_loss: 0.05523 valid_loss: 0.07362 test_loss: 0.07918 \n",
      "[199/500] train_loss: 0.05686 valid_loss: 0.07615 test_loss: 0.08158 \n",
      "[200/500] train_loss: 0.05557 valid_loss: 0.07366 test_loss: 0.08114 \n",
      "[201/500] train_loss: 0.05330 valid_loss: 0.07308 test_loss: 0.08096 \n",
      "[202/500] train_loss: 0.05474 valid_loss: 0.07342 test_loss: 0.07842 \n",
      "[203/500] train_loss: 0.05225 valid_loss: 0.07324 test_loss: 0.08188 \n",
      "[204/500] train_loss: 0.05457 valid_loss: 0.07247 test_loss: 0.07965 \n",
      "[205/500] train_loss: 0.05378 valid_loss: 0.07273 test_loss: 0.08005 \n",
      "[206/500] train_loss: 0.05307 valid_loss: 0.07304 test_loss: 0.07915 \n",
      "[207/500] train_loss: 0.05385 valid_loss: 0.07294 test_loss: 0.08033 \n",
      "[208/500] train_loss: 0.05332 valid_loss: 0.07548 test_loss: 0.08088 \n",
      "[209/500] train_loss: 0.05426 valid_loss: 0.07443 test_loss: 0.07904 \n",
      "[210/500] train_loss: 0.05507 valid_loss: 0.07363 test_loss: 0.07947 \n",
      "[211/500] train_loss: 0.05172 valid_loss: 0.07320 test_loss: 0.07967 \n",
      "[212/500] train_loss: 0.05324 valid_loss: 0.07458 test_loss: 0.08048 \n",
      "[213/500] train_loss: 0.05355 valid_loss: 0.07569 test_loss: 0.08008 \n",
      "[214/500] train_loss: 0.05386 valid_loss: 0.07440 test_loss: 0.07933 \n",
      "[215/500] train_loss: 0.05413 valid_loss: 0.07425 test_loss: 0.07979 \n",
      "[216/500] train_loss: 0.05529 valid_loss: 0.07264 test_loss: 0.07907 \n",
      "[217/500] train_loss: 0.05229 valid_loss: 0.07703 test_loss: 0.08195 \n",
      "[218/500] train_loss: 0.05265 valid_loss: 0.07269 test_loss: 0.07890 \n",
      "[219/500] train_loss: 0.05214 valid_loss: 0.07298 test_loss: 0.07872 \n",
      "[220/500] train_loss: 0.05291 valid_loss: 0.07263 test_loss: 0.08038 \n",
      "[221/500] train_loss: 0.05339 valid_loss: 0.07354 test_loss: 0.07897 \n",
      "[222/500] train_loss: 0.05144 valid_loss: 0.07358 test_loss: 0.07964 \n",
      "[223/500] train_loss: 0.05316 valid_loss: 0.07414 test_loss: 0.08094 \n",
      "[224/500] train_loss: 0.05348 valid_loss: 0.07329 test_loss: 0.07949 \n",
      "[225/500] train_loss: 0.05323 valid_loss: 0.07397 test_loss: 0.08037 \n",
      "[226/500] train_loss: 0.05174 valid_loss: 0.07463 test_loss: 0.08007 \n",
      "[227/500] train_loss: 0.05179 valid_loss: 0.07542 test_loss: 0.07909 \n",
      "[228/500] train_loss: 0.05329 valid_loss: 0.07348 test_loss: 0.07831 \n",
      "[229/500] train_loss: 0.05233 valid_loss: 0.07516 test_loss: 0.07907 \n",
      "[230/500] train_loss: 0.05056 valid_loss: 0.07374 test_loss: 0.07985 \n",
      "[231/500] train_loss: 0.05242 valid_loss: 0.07348 test_loss: 0.07937 \n",
      "[232/500] train_loss: 0.05067 valid_loss: 0.07444 test_loss: 0.08035 \n",
      "[233/500] train_loss: 0.05256 valid_loss: 0.07371 test_loss: 0.07827 \n",
      "[234/500] train_loss: 0.05174 valid_loss: 0.07335 test_loss: 0.07886 \n",
      "[235/500] train_loss: 0.05120 valid_loss: 0.07460 test_loss: 0.08037 \n",
      "[236/500] train_loss: 0.05184 valid_loss: 0.07533 test_loss: 0.08006 \n",
      "[237/500] train_loss: 0.05107 valid_loss: 0.07296 test_loss: 0.07944 \n",
      "[238/500] train_loss: 0.05096 valid_loss: 0.07358 test_loss: 0.07963 \n",
      "[239/500] train_loss: 0.05269 valid_loss: 0.07353 test_loss: 0.07773 \n",
      "[240/500] train_loss: 0.05107 valid_loss: 0.07350 test_loss: 0.07960 \n",
      "[241/500] train_loss: 0.05269 valid_loss: 0.07301 test_loss: 0.07801 \n",
      "[242/500] train_loss: 0.05258 valid_loss: 0.07415 test_loss: 0.07914 \n",
      "[243/500] train_loss: 0.05136 valid_loss: 0.07328 test_loss: 0.07809 \n",
      "[244/500] train_loss: 0.05169 valid_loss: 0.07476 test_loss: 0.07955 \n",
      "[245/500] train_loss: 0.05079 valid_loss: 0.07408 test_loss: 0.07843 \n",
      "[246/500] train_loss: 0.05061 valid_loss: 0.07287 test_loss: 0.07783 \n",
      "[247/500] train_loss: 0.05153 valid_loss: 0.07363 test_loss: 0.07867 \n",
      "[248/500] train_loss: 0.05181 valid_loss: 0.07397 test_loss: 0.07840 \n",
      "[249/500] train_loss: 0.05101 valid_loss: 0.07354 test_loss: 0.07765 \n",
      "[250/500] train_loss: 0.05085 valid_loss: 0.07274 test_loss: 0.07791 \n",
      "[251/500] train_loss: 0.05119 valid_loss: 0.07273 test_loss: 0.07769 \n",
      "[252/500] train_loss: 0.05144 valid_loss: 0.07486 test_loss: 0.07880 \n",
      "[253/500] train_loss: 0.05100 valid_loss: 0.07403 test_loss: 0.07817 \n",
      "[254/500] train_loss: 0.05056 valid_loss: 0.07372 test_loss: 0.07905 \n",
      "[255/500] train_loss: 0.05131 valid_loss: 0.07390 test_loss: 0.07871 \n",
      "[256/500] train_loss: 0.05040 valid_loss: 0.07253 test_loss: 0.07853 \n",
      "[257/500] train_loss: 0.05135 valid_loss: 0.07386 test_loss: 0.07960 \n",
      "[258/500] train_loss: 0.05043 valid_loss: 0.07404 test_loss: 0.07808 \n",
      "[259/500] train_loss: 0.04923 valid_loss: 0.07436 test_loss: 0.08012 \n",
      "[260/500] train_loss: 0.04977 valid_loss: 0.07511 test_loss: 0.07798 \n",
      "[261/500] train_loss: 0.04845 valid_loss: 0.07458 test_loss: 0.07711 \n",
      "[262/500] train_loss: 0.05039 valid_loss: 0.07379 test_loss: 0.07921 \n",
      "[263/500] train_loss: 0.05024 valid_loss: 0.07369 test_loss: 0.07848 \n",
      "[264/500] train_loss: 0.04962 valid_loss: 0.07507 test_loss: 0.08017 \n",
      "[265/500] train_loss: 0.04827 valid_loss: 0.07476 test_loss: 0.07915 \n",
      "[266/500] train_loss: 0.05020 valid_loss: 0.07476 test_loss: 0.07964 \n",
      "[267/500] train_loss: 0.05218 valid_loss: 0.07365 test_loss: 0.07961 \n",
      "[268/500] train_loss: 0.04887 valid_loss: 0.07432 test_loss: 0.07984 \n",
      "[269/500] train_loss: 0.04922 valid_loss: 0.07421 test_loss: 0.07883 \n",
      "[270/500] train_loss: 0.04927 valid_loss: 0.07371 test_loss: 0.07970 \n",
      "[271/500] train_loss: 0.04951 valid_loss: 0.07509 test_loss: 0.07976 \n",
      "[272/500] train_loss: 0.04949 valid_loss: 0.07344 test_loss: 0.07872 \n",
      "[273/500] train_loss: 0.04973 valid_loss: 0.07170 test_loss: 0.07866 \n",
      "[274/500] train_loss: 0.04745 valid_loss: 0.07443 test_loss: 0.07821 \n",
      "[275/500] train_loss: 0.04856 valid_loss: 0.07424 test_loss: 0.07910 \n",
      "[276/500] train_loss: 0.05021 valid_loss: 0.07402 test_loss: 0.07882 \n",
      "[277/500] train_loss: 0.04849 valid_loss: 0.07290 test_loss: 0.08041 \n",
      "[278/500] train_loss: 0.04816 valid_loss: 0.07432 test_loss: 0.07908 \n",
      "[279/500] train_loss: 0.04936 valid_loss: 0.07333 test_loss: 0.07889 \n",
      "[280/500] train_loss: 0.04990 valid_loss: 0.07338 test_loss: 0.07794 \n",
      "[281/500] train_loss: 0.04945 valid_loss: 0.07290 test_loss: 0.07781 \n",
      "[282/500] train_loss: 0.04973 valid_loss: 0.07493 test_loss: 0.07779 \n",
      "[283/500] train_loss: 0.04959 valid_loss: 0.07441 test_loss: 0.07826 \n",
      "[284/500] train_loss: 0.04916 valid_loss: 0.07711 test_loss: 0.07944 \n",
      "[285/500] train_loss: 0.04926 valid_loss: 0.07451 test_loss: 0.07894 \n",
      "[286/500] train_loss: 0.04959 valid_loss: 0.07428 test_loss: 0.07895 \n",
      "[287/500] train_loss: 0.04839 valid_loss: 0.07423 test_loss: 0.07871 \n",
      "[288/500] train_loss: 0.04875 valid_loss: 0.07370 test_loss: 0.07897 \n",
      "[289/500] train_loss: 0.04907 valid_loss: 0.07629 test_loss: 0.07952 \n",
      "[290/500] train_loss: 0.04906 valid_loss: 0.07398 test_loss: 0.07900 \n",
      "[291/500] train_loss: 0.04954 valid_loss: 0.07443 test_loss: 0.08105 \n",
      "[292/500] train_loss: 0.04704 valid_loss: 0.07512 test_loss: 0.07957 \n",
      "[293/500] train_loss: 0.04878 valid_loss: 0.07676 test_loss: 0.08022 \n",
      "[294/500] train_loss: 0.04780 valid_loss: 0.07561 test_loss: 0.08007 \n",
      "[295/500] train_loss: 0.04854 valid_loss: 0.07510 test_loss: 0.08108 \n",
      "[296/500] train_loss: 0.04955 valid_loss: 0.07479 test_loss: 0.07949 \n",
      "[297/500] train_loss: 0.04730 valid_loss: 0.07516 test_loss: 0.07972 \n",
      "[298/500] train_loss: 0.04768 valid_loss: 0.07396 test_loss: 0.07968 \n",
      "[299/500] train_loss: 0.04771 valid_loss: 0.07416 test_loss: 0.07916 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300/500] train_loss: 0.04876 valid_loss: 0.07433 test_loss: 0.07880 \n",
      "[301/500] train_loss: 0.04810 valid_loss: 0.07440 test_loss: 0.07921 \n",
      "[302/500] train_loss: 0.04869 valid_loss: 0.07377 test_loss: 0.07899 \n",
      "[303/500] train_loss: 0.04571 valid_loss: 0.07268 test_loss: 0.07837 \n",
      "[304/500] train_loss: 0.04681 valid_loss: 0.07373 test_loss: 0.07792 \n",
      "[305/500] train_loss: 0.04737 valid_loss: 0.07399 test_loss: 0.07818 \n",
      "[306/500] train_loss: 0.04788 valid_loss: 0.07739 test_loss: 0.07810 \n",
      "[307/500] train_loss: 0.04821 valid_loss: 0.07526 test_loss: 0.07829 \n",
      "[308/500] train_loss: 0.04729 valid_loss: 0.07425 test_loss: 0.07885 \n",
      "[309/500] train_loss: 0.04825 valid_loss: 0.07445 test_loss: 0.07846 \n",
      "[310/500] train_loss: 0.04801 valid_loss: 0.07511 test_loss: 0.07917 \n",
      "[311/500] train_loss: 0.04850 valid_loss: 0.07458 test_loss: 0.07947 \n",
      "[312/500] train_loss: 0.04825 valid_loss: 0.07303 test_loss: 0.07734 \n",
      "[313/500] train_loss: 0.04778 valid_loss: 0.07403 test_loss: 0.07806 \n",
      "[314/500] train_loss: 0.04722 valid_loss: 0.07527 test_loss: 0.07984 \n",
      "[315/500] train_loss: 0.04912 valid_loss: 0.07455 test_loss: 0.08105 \n",
      "[316/500] train_loss: 0.04793 valid_loss: 0.07391 test_loss: 0.08003 \n",
      "[317/500] train_loss: 0.04635 valid_loss: 0.07528 test_loss: 0.07979 \n",
      "[318/500] train_loss: 0.04665 valid_loss: 0.07345 test_loss: 0.08004 \n",
      "[319/500] train_loss: 0.04632 valid_loss: 0.07515 test_loss: 0.08102 \n",
      "[320/500] train_loss: 0.04657 valid_loss: 0.07618 test_loss: 0.07898 \n",
      "[321/500] train_loss: 0.04693 valid_loss: 0.07546 test_loss: 0.07910 \n",
      "[322/500] train_loss: 0.04618 valid_loss: 0.07439 test_loss: 0.07911 \n",
      "[323/500] train_loss: 0.04651 valid_loss: 0.07408 test_loss: 0.07986 \n",
      "[324/500] train_loss: 0.04649 valid_loss: 0.07381 test_loss: 0.07813 \n",
      "[325/500] train_loss: 0.04740 valid_loss: 0.07413 test_loss: 0.07940 \n",
      "[326/500] train_loss: 0.04653 valid_loss: 0.07333 test_loss: 0.07948 \n",
      "[327/500] train_loss: 0.04682 valid_loss: 0.07480 test_loss: 0.07839 \n",
      "[328/500] train_loss: 0.04614 valid_loss: 0.07540 test_loss: 0.08049 \n",
      "[329/500] train_loss: 0.04742 valid_loss: 0.07481 test_loss: 0.08009 \n",
      "[330/500] train_loss: 0.04517 valid_loss: 0.07485 test_loss: 0.07930 \n",
      "[331/500] train_loss: 0.04582 valid_loss: 0.07404 test_loss: 0.08091 \n",
      "[332/500] train_loss: 0.04672 valid_loss: 0.07485 test_loss: 0.07998 \n",
      "[333/500] train_loss: 0.04589 valid_loss: 0.07555 test_loss: 0.08030 \n",
      "[334/500] train_loss: 0.04664 valid_loss: 0.07464 test_loss: 0.07984 \n",
      "[335/500] train_loss: 0.04666 valid_loss: 0.07470 test_loss: 0.07952 \n",
      "[336/500] train_loss: 0.04706 valid_loss: 0.07389 test_loss: 0.07851 \n",
      "[337/500] train_loss: 0.04710 valid_loss: 0.07419 test_loss: 0.08066 \n",
      "[338/500] train_loss: 0.04609 valid_loss: 0.07416 test_loss: 0.08003 \n",
      "[339/500] train_loss: 0.04620 valid_loss: 0.07388 test_loss: 0.07992 \n",
      "[340/500] train_loss: 0.04604 valid_loss: 0.07349 test_loss: 0.08114 \n",
      "[341/500] train_loss: 0.04589 valid_loss: 0.07383 test_loss: 0.07908 \n",
      "[342/500] train_loss: 0.04566 valid_loss: 0.07450 test_loss: 0.08029 \n",
      "[343/500] train_loss: 0.04682 valid_loss: 0.07508 test_loss: 0.07934 \n",
      "[344/500] train_loss: 0.04634 valid_loss: 0.07534 test_loss: 0.08069 \n",
      "[345/500] train_loss: 0.04608 valid_loss: 0.07459 test_loss: 0.08021 \n",
      "[346/500] train_loss: 0.04631 valid_loss: 0.07435 test_loss: 0.07925 \n",
      "[347/500] train_loss: 0.04630 valid_loss: 0.07435 test_loss: 0.07894 \n",
      "[348/500] train_loss: 0.04479 valid_loss: 0.07316 test_loss: 0.07841 \n",
      "[349/500] train_loss: 0.04566 valid_loss: 0.07399 test_loss: 0.08054 \n",
      "[350/500] train_loss: 0.04543 valid_loss: 0.07458 test_loss: 0.08109 \n",
      "[351/500] train_loss: 0.04591 valid_loss: 0.07419 test_loss: 0.07871 \n",
      "[352/500] train_loss: 0.04518 valid_loss: 0.07524 test_loss: 0.08022 \n",
      "[353/500] train_loss: 0.04529 valid_loss: 0.07506 test_loss: 0.07958 \n",
      "[354/500] train_loss: 0.04660 valid_loss: 0.07429 test_loss: 0.08081 \n",
      "[355/500] train_loss: 0.04494 valid_loss: 0.07449 test_loss: 0.08010 \n",
      "[356/500] train_loss: 0.04613 valid_loss: 0.07422 test_loss: 0.07990 \n",
      "[357/500] train_loss: 0.04599 valid_loss: 0.07385 test_loss: 0.08031 \n",
      "[358/500] train_loss: 0.04641 valid_loss: 0.07372 test_loss: 0.08016 \n",
      "[359/500] train_loss: 0.04608 valid_loss: 0.07488 test_loss: 0.08024 \n",
      "[360/500] train_loss: 0.04616 valid_loss: 0.07433 test_loss: 0.07970 \n",
      "[361/500] train_loss: 0.04608 valid_loss: 0.07278 test_loss: 0.07917 \n",
      "[362/500] train_loss: 0.04565 valid_loss: 0.07430 test_loss: 0.07929 \n",
      "[363/500] train_loss: 0.04496 valid_loss: 0.07445 test_loss: 0.07961 \n",
      "[364/500] train_loss: 0.04486 valid_loss: 0.07451 test_loss: 0.07955 \n",
      "[365/500] train_loss: 0.04422 valid_loss: 0.07401 test_loss: 0.07924 \n",
      "[366/500] train_loss: 0.04477 valid_loss: 0.07313 test_loss: 0.08011 \n",
      "[367/500] train_loss: 0.04599 valid_loss: 0.07341 test_loss: 0.08000 \n",
      "[368/500] train_loss: 0.04466 valid_loss: 0.07545 test_loss: 0.08099 \n",
      "[369/500] train_loss: 0.04449 valid_loss: 0.07348 test_loss: 0.07919 \n",
      "[370/500] train_loss: 0.04440 valid_loss: 0.07383 test_loss: 0.08037 \n",
      "[371/500] train_loss: 0.04505 valid_loss: 0.07501 test_loss: 0.08159 \n",
      "[372/500] train_loss: 0.04549 valid_loss: 0.07491 test_loss: 0.07980 \n",
      "[373/500] train_loss: 0.04433 valid_loss: 0.07460 test_loss: 0.08059 \n",
      "[374/500] train_loss: 0.04423 valid_loss: 0.07501 test_loss: 0.08041 \n",
      "[375/500] train_loss: 0.04441 valid_loss: 0.07338 test_loss: 0.08159 \n",
      "[376/500] train_loss: 0.04428 valid_loss: 0.07376 test_loss: 0.08039 \n",
      "[377/500] train_loss: 0.04454 valid_loss: 0.07502 test_loss: 0.08011 \n",
      "[378/500] train_loss: 0.04539 valid_loss: 0.07496 test_loss: 0.07964 \n",
      "[379/500] train_loss: 0.04422 valid_loss: 0.07326 test_loss: 0.07984 \n",
      "[380/500] train_loss: 0.04386 valid_loss: 0.07432 test_loss: 0.07919 \n",
      "[381/500] train_loss: 0.04538 valid_loss: 0.07492 test_loss: 0.07834 \n",
      "[382/500] train_loss: 0.04493 valid_loss: 0.07492 test_loss: 0.07856 \n",
      "[383/500] train_loss: 0.04375 valid_loss: 0.07518 test_loss: 0.08013 \n",
      "[384/500] train_loss: 0.04485 valid_loss: 0.07565 test_loss: 0.08087 \n",
      "[385/500] train_loss: 0.04452 valid_loss: 0.07401 test_loss: 0.07945 \n",
      "[386/500] train_loss: 0.04444 valid_loss: 0.07427 test_loss: 0.08091 \n",
      "[387/500] train_loss: 0.04287 valid_loss: 0.07287 test_loss: 0.07899 \n",
      "[388/500] train_loss: 0.04478 valid_loss: 0.07249 test_loss: 0.08049 \n",
      "[389/500] train_loss: 0.04424 valid_loss: 0.07390 test_loss: 0.07865 \n",
      "[390/500] train_loss: 0.04498 valid_loss: 0.07552 test_loss: 0.07974 \n",
      "[391/500] train_loss: 0.04495 valid_loss: 0.07405 test_loss: 0.07973 \n",
      "[392/500] train_loss: 0.04318 valid_loss: 0.07642 test_loss: 0.08091 \n",
      "[393/500] train_loss: 0.04459 valid_loss: 0.07636 test_loss: 0.08030 \n",
      "[394/500] train_loss: 0.04520 valid_loss: 0.07486 test_loss: 0.07867 \n",
      "[395/500] train_loss: 0.04491 valid_loss: 0.07240 test_loss: 0.07883 \n",
      "[396/500] train_loss: 0.04340 valid_loss: 0.07471 test_loss: 0.08090 \n",
      "[397/500] train_loss: 0.04398 valid_loss: 0.07238 test_loss: 0.08018 \n",
      "[398/500] train_loss: 0.04344 valid_loss: 0.07750 test_loss: 0.08117 \n",
      "[399/500] train_loss: 0.04515 valid_loss: 0.07352 test_loss: 0.08227 \n",
      "[400/500] train_loss: 0.04265 valid_loss: 0.07347 test_loss: 0.08060 \n",
      "[401/500] train_loss: 0.04354 valid_loss: 0.07286 test_loss: 0.07978 \n",
      "[402/500] train_loss: 0.04321 valid_loss: 0.07429 test_loss: 0.07983 \n",
      "[403/500] train_loss: 0.04374 valid_loss: 0.07491 test_loss: 0.08030 \n",
      "[404/500] train_loss: 0.04823 valid_loss: 0.07473 test_loss: 0.08123 \n",
      "[405/500] train_loss: 0.04529 valid_loss: 0.07461 test_loss: 0.08062 \n",
      "[406/500] train_loss: 0.04266 valid_loss: 0.07411 test_loss: 0.08065 \n",
      "[407/500] train_loss: 0.04366 valid_loss: 0.07304 test_loss: 0.07874 \n",
      "[408/500] train_loss: 0.04318 valid_loss: 0.07168 test_loss: 0.08155 \n",
      "[409/500] train_loss: 0.04267 valid_loss: 0.07204 test_loss: 0.07799 \n",
      "[410/500] train_loss: 0.04274 valid_loss: 0.07410 test_loss: 0.07980 \n",
      "[411/500] train_loss: 0.04455 valid_loss: 0.07366 test_loss: 0.08005 \n",
      "[412/500] train_loss: 0.04381 valid_loss: 0.07461 test_loss: 0.07942 \n",
      "[413/500] train_loss: 0.04384 valid_loss: 0.07292 test_loss: 0.07896 \n",
      "[414/500] train_loss: 0.04327 valid_loss: 0.07341 test_loss: 0.08085 \n",
      "[415/500] train_loss: 0.04319 valid_loss: 0.07839 test_loss: 0.08309 \n",
      "[416/500] train_loss: 0.04393 valid_loss: 0.07417 test_loss: 0.07975 \n",
      "[417/500] train_loss: 0.04310 valid_loss: 0.07468 test_loss: 0.08220 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[418/500] train_loss: 0.04224 valid_loss: 0.07478 test_loss: 0.07995 \n",
      "[419/500] train_loss: 0.04346 valid_loss: 0.07383 test_loss: 0.08100 \n",
      "[420/500] train_loss: 0.04353 valid_loss: 0.07268 test_loss: 0.08019 \n",
      "[421/500] train_loss: 0.04407 valid_loss: 0.07315 test_loss: 0.08101 \n",
      "[422/500] train_loss: 0.04213 valid_loss: 0.07466 test_loss: 0.08110 \n",
      "[423/500] train_loss: 0.04229 valid_loss: 0.07418 test_loss: 0.07969 \n",
      "[424/500] train_loss: 0.04363 valid_loss: 0.07415 test_loss: 0.07941 \n",
      "[425/500] train_loss: 0.04352 valid_loss: 0.07404 test_loss: 0.08014 \n",
      "[426/500] train_loss: 0.04243 valid_loss: 0.07673 test_loss: 0.08135 \n",
      "[427/500] train_loss: 0.04277 valid_loss: 0.07448 test_loss: 0.08088 \n",
      "[428/500] train_loss: 0.04435 valid_loss: 0.07392 test_loss: 0.08050 \n",
      "[429/500] train_loss: 0.04300 valid_loss: 0.07485 test_loss: 0.08048 \n",
      "[430/500] train_loss: 0.04244 valid_loss: 0.07657 test_loss: 0.08090 \n",
      "[431/500] train_loss: 0.04292 valid_loss: 0.07551 test_loss: 0.08098 \n",
      "[432/500] train_loss: 0.04290 valid_loss: 0.07410 test_loss: 0.08147 \n",
      "[433/500] train_loss: 0.04240 valid_loss: 0.07445 test_loss: 0.08194 \n",
      "[434/500] train_loss: 0.04260 valid_loss: 0.07464 test_loss: 0.08147 \n",
      "[435/500] train_loss: 0.04234 valid_loss: 0.07550 test_loss: 0.08259 \n",
      "[436/500] train_loss: 0.04237 valid_loss: 0.07566 test_loss: 0.08192 \n",
      "[437/500] train_loss: 0.04157 valid_loss: 0.07505 test_loss: 0.08270 \n",
      "[438/500] train_loss: 0.04311 valid_loss: 0.07531 test_loss: 0.08209 \n",
      "[439/500] train_loss: 0.04293 valid_loss: 0.07422 test_loss: 0.08048 \n",
      "[440/500] train_loss: 0.04274 valid_loss: 0.07509 test_loss: 0.08265 \n",
      "[441/500] train_loss: 0.04402 valid_loss: 0.07484 test_loss: 0.08179 \n",
      "[442/500] train_loss: 0.04146 valid_loss: 0.07601 test_loss: 0.08072 \n",
      "[443/500] train_loss: 0.04290 valid_loss: 0.07474 test_loss: 0.08177 \n",
      "[444/500] train_loss: 0.04278 valid_loss: 0.07340 test_loss: 0.07993 \n",
      "[445/500] train_loss: 0.04272 valid_loss: 0.07551 test_loss: 0.08006 \n",
      "[446/500] train_loss: 0.04179 valid_loss: 0.07667 test_loss: 0.08187 \n",
      "[447/500] train_loss: 0.04201 valid_loss: 0.07446 test_loss: 0.08130 \n",
      "[448/500] train_loss: 0.04245 valid_loss: 0.07459 test_loss: 0.08048 \n",
      "[449/500] train_loss: 0.04192 valid_loss: 0.07563 test_loss: 0.08071 \n",
      "[450/500] train_loss: 0.04316 valid_loss: 0.07461 test_loss: 0.08305 \n",
      "[451/500] train_loss: 0.04159 valid_loss: 0.07520 test_loss: 0.07927 \n",
      "[452/500] train_loss: 0.04338 valid_loss: 0.07435 test_loss: 0.08099 \n",
      "[453/500] train_loss: 0.04347 valid_loss: 0.07365 test_loss: 0.08123 \n",
      "[454/500] train_loss: 0.04181 valid_loss: 0.07444 test_loss: 0.08194 \n",
      "[455/500] train_loss: 0.04249 valid_loss: 0.07276 test_loss: 0.08253 \n",
      "[456/500] train_loss: 0.04134 valid_loss: 0.07368 test_loss: 0.08283 \n",
      "[457/500] train_loss: 0.04269 valid_loss: 0.07404 test_loss: 0.08231 \n",
      "[458/500] train_loss: 0.04245 valid_loss: 0.07352 test_loss: 0.08067 \n",
      "[459/500] train_loss: 0.04329 valid_loss: 0.07459 test_loss: 0.08197 \n",
      "[460/500] train_loss: 0.04152 valid_loss: 0.07413 test_loss: 0.08178 \n",
      "[461/500] train_loss: 0.04267 valid_loss: 0.07307 test_loss: 0.08099 \n",
      "[462/500] train_loss: 0.04123 valid_loss: 0.07640 test_loss: 0.08233 \n",
      "[463/500] train_loss: 0.04120 valid_loss: 0.07512 test_loss: 0.08033 \n",
      "[464/500] train_loss: 0.04148 valid_loss: 0.07346 test_loss: 0.08028 \n",
      "[465/500] train_loss: 0.04136 valid_loss: 0.07492 test_loss: 0.08173 \n",
      "[466/500] train_loss: 0.04189 valid_loss: 0.07441 test_loss: 0.08057 \n",
      "[467/500] train_loss: 0.04152 valid_loss: 0.07322 test_loss: 0.08187 \n",
      "[468/500] train_loss: 0.04070 valid_loss: 0.07425 test_loss: 0.08200 \n",
      "[469/500] train_loss: 0.04201 valid_loss: 0.07339 test_loss: 0.08194 \n",
      "[470/500] train_loss: 0.04088 valid_loss: 0.07489 test_loss: 0.08196 \n",
      "[471/500] train_loss: 0.04312 valid_loss: 0.07493 test_loss: 0.08123 \n",
      "[472/500] train_loss: 0.04275 valid_loss: 0.07309 test_loss: 0.08263 \n",
      "[473/500] train_loss: 0.04198 valid_loss: 0.07563 test_loss: 0.08124 \n",
      "[474/500] train_loss: 0.04326 valid_loss: 0.07746 test_loss: 0.08048 \n",
      "[475/500] train_loss: 0.04313 valid_loss: 0.07618 test_loss: 0.08264 \n",
      "[476/500] train_loss: 0.04041 valid_loss: 0.07618 test_loss: 0.08178 \n",
      "[477/500] train_loss: 0.04068 valid_loss: 0.07567 test_loss: 0.08393 \n",
      "[478/500] train_loss: 0.04167 valid_loss: 0.07463 test_loss: 0.08263 \n",
      "[479/500] train_loss: 0.04081 valid_loss: 0.07447 test_loss: 0.08173 \n",
      "[480/500] train_loss: 0.04170 valid_loss: 0.07554 test_loss: 0.08190 \n",
      "[481/500] train_loss: 0.04155 valid_loss: 0.07405 test_loss: 0.08039 \n",
      "[482/500] train_loss: 0.04260 valid_loss: 0.08051 test_loss: 0.08387 \n",
      "[483/500] train_loss: 0.04092 valid_loss: 0.07376 test_loss: 0.08019 \n",
      "[484/500] train_loss: 0.04155 valid_loss: 0.07545 test_loss: 0.08160 \n",
      "[485/500] train_loss: 0.04105 valid_loss: 0.07499 test_loss: 0.08220 \n",
      "[486/500] train_loss: 0.04253 valid_loss: 0.07532 test_loss: 0.08116 \n",
      "[487/500] train_loss: 0.04231 valid_loss: 0.07403 test_loss: 0.08168 \n",
      "[488/500] train_loss: 0.04126 valid_loss: 0.07399 test_loss: 0.07994 \n",
      "[489/500] train_loss: 0.04079 valid_loss: 0.07468 test_loss: 0.08077 \n",
      "[490/500] train_loss: 0.04068 valid_loss: 0.07444 test_loss: 0.08058 \n",
      "[491/500] train_loss: 0.04032 valid_loss: 0.07337 test_loss: 0.08038 \n",
      "[492/500] train_loss: 0.04099 valid_loss: 0.07270 test_loss: 0.07993 \n",
      "[493/500] train_loss: 0.04058 valid_loss: 0.07443 test_loss: 0.08163 \n",
      "[494/500] train_loss: 0.04174 valid_loss: 0.07408 test_loss: 0.08071 \n",
      "[495/500] train_loss: 0.04185 valid_loss: 0.07295 test_loss: 0.07989 \n",
      "[496/500] train_loss: 0.04094 valid_loss: 0.07358 test_loss: 0.08014 \n",
      "[497/500] train_loss: 0.04223 valid_loss: 0.07485 test_loss: 0.08046 \n",
      "[498/500] train_loss: 0.04158 valid_loss: 0.07433 test_loss: 0.08077 \n",
      "[499/500] train_loss: 0.04056 valid_loss: 0.07388 test_loss: 0.08072 \n",
      "[500/500] train_loss: 0.04060 valid_loss: 0.07491 test_loss: 0.08065 \n",
      "TRAINING MODEL 13\n",
      "[  1/500] train_loss: 0.34375 valid_loss: 0.24280 test_loss: 0.24806 \n",
      "验证损失减少 (inf --> 0.242804). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18926 valid_loss: 0.18233 test_loss: 0.18896 \n",
      "验证损失减少 (0.242804 --> 0.182327). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15382 valid_loss: 0.15663 test_loss: 0.16126 \n",
      "验证损失减少 (0.182327 --> 0.156628). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13989 valid_loss: 0.14346 test_loss: 0.14951 \n",
      "验证损失减少 (0.156628 --> 0.143463). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13200 valid_loss: 0.13769 test_loss: 0.14408 \n",
      "验证损失减少 (0.143463 --> 0.137687). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12423 valid_loss: 0.12935 test_loss: 0.13566 \n",
      "验证损失减少 (0.137687 --> 0.129354). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11999 valid_loss: 0.12135 test_loss: 0.13144 \n",
      "验证损失减少 (0.129354 --> 0.121355). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11740 valid_loss: 0.11779 test_loss: 0.12966 \n",
      "验证损失减少 (0.121355 --> 0.117790). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11624 valid_loss: 0.11624 test_loss: 0.12568 \n",
      "验证损失减少 (0.117790 --> 0.116236). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11086 valid_loss: 0.11225 test_loss: 0.12249 \n",
      "验证损失减少 (0.116236 --> 0.112247). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10845 valid_loss: 0.11123 test_loss: 0.12197 \n",
      "验证损失减少 (0.112247 --> 0.111232). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10589 valid_loss: 0.10833 test_loss: 0.11997 \n",
      "验证损失减少 (0.111232 --> 0.108332). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10417 valid_loss: 0.10557 test_loss: 0.11724 \n",
      "验证损失减少 (0.108332 --> 0.105571). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10354 valid_loss: 0.11253 test_loss: 0.12459 \n",
      "[ 15/500] train_loss: 0.10396 valid_loss: 0.10168 test_loss: 0.11649 \n",
      "验证损失减少 (0.105571 --> 0.101683). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10139 valid_loss: 0.10483 test_loss: 0.11374 \n",
      "[ 17/500] train_loss: 0.09941 valid_loss: 0.10302 test_loss: 0.11521 \n",
      "[ 18/500] train_loss: 0.09775 valid_loss: 0.09985 test_loss: 0.11042 \n",
      "验证损失减少 (0.101683 --> 0.099849). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09620 valid_loss: 0.10018 test_loss: 0.11194 \n",
      "[ 20/500] train_loss: 0.09743 valid_loss: 0.09846 test_loss: 0.10794 \n",
      "验证损失减少 (0.099849 --> 0.098457). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09441 valid_loss: 0.09834 test_loss: 0.10786 \n",
      "验证损失减少 (0.098457 --> 0.098342). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09166 valid_loss: 0.09652 test_loss: 0.10862 \n",
      "验证损失减少 (0.098342 --> 0.096523). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09139 valid_loss: 0.09427 test_loss: 0.10621 \n",
      "验证损失减少 (0.096523 --> 0.094274). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 24/500] train_loss: 0.09197 valid_loss: 0.09589 test_loss: 0.10648 \n",
      "[ 25/500] train_loss: 0.09136 valid_loss: 0.09345 test_loss: 0.10388 \n",
      "验证损失减少 (0.094274 --> 0.093452). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.09164 valid_loss: 0.09245 test_loss: 0.10244 \n",
      "验证损失减少 (0.093452 --> 0.092449). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.09038 valid_loss: 0.09356 test_loss: 0.10305 \n",
      "[ 28/500] train_loss: 0.08752 valid_loss: 0.09145 test_loss: 0.10053 \n",
      "验证损失减少 (0.092449 --> 0.091446). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08735 valid_loss: 0.08927 test_loss: 0.10104 \n",
      "验证损失减少 (0.091446 --> 0.089269). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08677 valid_loss: 0.08848 test_loss: 0.10046 \n",
      "验证损失减少 (0.089269 --> 0.088480). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08617 valid_loss: 0.09191 test_loss: 0.09988 \n",
      "[ 32/500] train_loss: 0.08665 valid_loss: 0.08850 test_loss: 0.09848 \n",
      "[ 33/500] train_loss: 0.08524 valid_loss: 0.08914 test_loss: 0.10010 \n",
      "[ 34/500] train_loss: 0.08435 valid_loss: 0.08792 test_loss: 0.09896 \n",
      "验证损失减少 (0.088480 --> 0.087915). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08490 valid_loss: 0.08562 test_loss: 0.09754 \n",
      "验证损失减少 (0.087915 --> 0.085617). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08674 valid_loss: 0.08653 test_loss: 0.09632 \n",
      "[ 37/500] train_loss: 0.08337 valid_loss: 0.08767 test_loss: 0.09705 \n",
      "[ 38/500] train_loss: 0.08387 valid_loss: 0.08750 test_loss: 0.09820 \n",
      "[ 39/500] train_loss: 0.08207 valid_loss: 0.08746 test_loss: 0.09601 \n",
      "[ 40/500] train_loss: 0.08275 valid_loss: 0.08482 test_loss: 0.09433 \n",
      "验证损失减少 (0.085617 --> 0.084823). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.08190 valid_loss: 0.08608 test_loss: 0.09667 \n",
      "[ 42/500] train_loss: 0.08039 valid_loss: 0.08474 test_loss: 0.09514 \n",
      "验证损失减少 (0.084823 --> 0.084736). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.08010 valid_loss: 0.08364 test_loss: 0.09535 \n",
      "验证损失减少 (0.084736 --> 0.083639). 正在保存模型...\n",
      "[ 44/500] train_loss: 0.08289 valid_loss: 0.08450 test_loss: 0.09388 \n",
      "[ 45/500] train_loss: 0.08103 valid_loss: 0.08497 test_loss: 0.09407 \n",
      "[ 46/500] train_loss: 0.08005 valid_loss: 0.08282 test_loss: 0.09277 \n",
      "验证损失减少 (0.083639 --> 0.082818). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07887 valid_loss: 0.08396 test_loss: 0.09293 \n",
      "[ 48/500] train_loss: 0.08077 valid_loss: 0.08257 test_loss: 0.09096 \n",
      "验证损失减少 (0.082818 --> 0.082568). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07992 valid_loss: 0.08357 test_loss: 0.09133 \n",
      "[ 50/500] train_loss: 0.07766 valid_loss: 0.08182 test_loss: 0.09210 \n",
      "验证损失减少 (0.082568 --> 0.081818). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.07905 valid_loss: 0.08549 test_loss: 0.09368 \n",
      "[ 52/500] train_loss: 0.07737 valid_loss: 0.08189 test_loss: 0.09072 \n",
      "[ 53/500] train_loss: 0.07657 valid_loss: 0.08222 test_loss: 0.09177 \n",
      "[ 54/500] train_loss: 0.07701 valid_loss: 0.08449 test_loss: 0.09285 \n",
      "[ 55/500] train_loss: 0.07590 valid_loss: 0.08433 test_loss: 0.09068 \n",
      "[ 56/500] train_loss: 0.07523 valid_loss: 0.08227 test_loss: 0.09075 \n",
      "[ 57/500] train_loss: 0.07709 valid_loss: 0.08421 test_loss: 0.08899 \n",
      "[ 58/500] train_loss: 0.07672 valid_loss: 0.07992 test_loss: 0.08947 \n",
      "验证损失减少 (0.081818 --> 0.079920). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07477 valid_loss: 0.08157 test_loss: 0.08900 \n",
      "[ 60/500] train_loss: 0.07397 valid_loss: 0.08158 test_loss: 0.08783 \n",
      "[ 61/500] train_loss: 0.07438 valid_loss: 0.08038 test_loss: 0.08939 \n",
      "[ 62/500] train_loss: 0.07436 valid_loss: 0.08036 test_loss: 0.08703 \n",
      "[ 63/500] train_loss: 0.07352 valid_loss: 0.08055 test_loss: 0.09011 \n",
      "[ 64/500] train_loss: 0.07346 valid_loss: 0.07958 test_loss: 0.08807 \n",
      "验证损失减少 (0.079920 --> 0.079585). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07325 valid_loss: 0.08028 test_loss: 0.08838 \n",
      "[ 66/500] train_loss: 0.07229 valid_loss: 0.08030 test_loss: 0.09066 \n",
      "[ 67/500] train_loss: 0.07246 valid_loss: 0.07976 test_loss: 0.08693 \n",
      "[ 68/500] train_loss: 0.07166 valid_loss: 0.08022 test_loss: 0.08781 \n",
      "[ 69/500] train_loss: 0.07112 valid_loss: 0.08092 test_loss: 0.08836 \n",
      "[ 70/500] train_loss: 0.07161 valid_loss: 0.07884 test_loss: 0.08721 \n",
      "验证损失减少 (0.079585 --> 0.078836). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.07294 valid_loss: 0.07936 test_loss: 0.08637 \n",
      "[ 72/500] train_loss: 0.07307 valid_loss: 0.07961 test_loss: 0.08585 \n",
      "[ 73/500] train_loss: 0.07237 valid_loss: 0.08596 test_loss: 0.09017 \n",
      "[ 74/500] train_loss: 0.07336 valid_loss: 0.07973 test_loss: 0.08606 \n",
      "[ 75/500] train_loss: 0.07202 valid_loss: 0.08004 test_loss: 0.08559 \n",
      "[ 76/500] train_loss: 0.07097 valid_loss: 0.07819 test_loss: 0.08580 \n",
      "验证损失减少 (0.078836 --> 0.078195). 正在保存模型...\n",
      "[ 77/500] train_loss: 0.07030 valid_loss: 0.07925 test_loss: 0.08821 \n",
      "[ 78/500] train_loss: 0.07170 valid_loss: 0.07902 test_loss: 0.08449 \n",
      "[ 79/500] train_loss: 0.07089 valid_loss: 0.07889 test_loss: 0.08564 \n",
      "[ 80/500] train_loss: 0.07134 valid_loss: 0.07904 test_loss: 0.08694 \n",
      "[ 81/500] train_loss: 0.06921 valid_loss: 0.07940 test_loss: 0.08698 \n",
      "[ 82/500] train_loss: 0.06830 valid_loss: 0.07851 test_loss: 0.08686 \n",
      "[ 83/500] train_loss: 0.06917 valid_loss: 0.07945 test_loss: 0.08545 \n",
      "[ 84/500] train_loss: 0.06900 valid_loss: 0.07953 test_loss: 0.08657 \n",
      "[ 85/500] train_loss: 0.06860 valid_loss: 0.07676 test_loss: 0.08635 \n",
      "验证损失减少 (0.078195 --> 0.076756). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.06807 valid_loss: 0.07787 test_loss: 0.08542 \n",
      "[ 87/500] train_loss: 0.06848 valid_loss: 0.07818 test_loss: 0.08538 \n",
      "[ 88/500] train_loss: 0.06821 valid_loss: 0.08045 test_loss: 0.08522 \n",
      "[ 89/500] train_loss: 0.06727 valid_loss: 0.07692 test_loss: 0.08514 \n",
      "[ 90/500] train_loss: 0.06740 valid_loss: 0.07694 test_loss: 0.08396 \n",
      "[ 91/500] train_loss: 0.06625 valid_loss: 0.07760 test_loss: 0.08371 \n",
      "[ 92/500] train_loss: 0.06861 valid_loss: 0.07733 test_loss: 0.08417 \n",
      "[ 93/500] train_loss: 0.06676 valid_loss: 0.07996 test_loss: 0.08545 \n",
      "[ 94/500] train_loss: 0.06839 valid_loss: 0.07860 test_loss: 0.08374 \n",
      "[ 95/500] train_loss: 0.06806 valid_loss: 0.07820 test_loss: 0.08345 \n",
      "[ 96/500] train_loss: 0.06531 valid_loss: 0.07721 test_loss: 0.08355 \n",
      "[ 97/500] train_loss: 0.06736 valid_loss: 0.07747 test_loss: 0.08357 \n",
      "[ 98/500] train_loss: 0.06827 valid_loss: 0.07961 test_loss: 0.08514 \n",
      "[ 99/500] train_loss: 0.06562 valid_loss: 0.07865 test_loss: 0.08559 \n",
      "[100/500] train_loss: 0.06671 valid_loss: 0.07826 test_loss: 0.08374 \n",
      "[101/500] train_loss: 0.06847 valid_loss: 0.07635 test_loss: 0.08372 \n",
      "验证损失减少 (0.076756 --> 0.076351). 正在保存模型...\n",
      "[102/500] train_loss: 0.06568 valid_loss: 0.07816 test_loss: 0.08502 \n",
      "[103/500] train_loss: 0.06672 valid_loss: 0.07514 test_loss: 0.08436 \n",
      "验证损失减少 (0.076351 --> 0.075137). 正在保存模型...\n",
      "[104/500] train_loss: 0.06605 valid_loss: 0.07572 test_loss: 0.08260 \n",
      "[105/500] train_loss: 0.06484 valid_loss: 0.07722 test_loss: 0.08440 \n",
      "[106/500] train_loss: 0.06428 valid_loss: 0.07763 test_loss: 0.08290 \n",
      "[107/500] train_loss: 0.06590 valid_loss: 0.07527 test_loss: 0.08258 \n",
      "[108/500] train_loss: 0.06540 valid_loss: 0.07612 test_loss: 0.08477 \n",
      "[109/500] train_loss: 0.06516 valid_loss: 0.07667 test_loss: 0.08391 \n",
      "[110/500] train_loss: 0.06521 valid_loss: 0.07801 test_loss: 0.08288 \n",
      "[111/500] train_loss: 0.06440 valid_loss: 0.07723 test_loss: 0.08233 \n",
      "[112/500] train_loss: 0.06410 valid_loss: 0.07688 test_loss: 0.08239 \n",
      "[113/500] train_loss: 0.06579 valid_loss: 0.07677 test_loss: 0.08165 \n",
      "[114/500] train_loss: 0.06626 valid_loss: 0.07512 test_loss: 0.08218 \n",
      "验证损失减少 (0.075137 --> 0.075119). 正在保存模型...\n",
      "[115/500] train_loss: 0.06369 valid_loss: 0.07482 test_loss: 0.08208 \n",
      "验证损失减少 (0.075119 --> 0.074823). 正在保存模型...\n",
      "[116/500] train_loss: 0.06351 valid_loss: 0.07385 test_loss: 0.08123 \n",
      "验证损失减少 (0.074823 --> 0.073845). 正在保存模型...\n",
      "[117/500] train_loss: 0.06344 valid_loss: 0.07733 test_loss: 0.08321 \n",
      "[118/500] train_loss: 0.06318 valid_loss: 0.07612 test_loss: 0.08252 \n",
      "[119/500] train_loss: 0.06192 valid_loss: 0.07532 test_loss: 0.08201 \n",
      "[120/500] train_loss: 0.06325 valid_loss: 0.08164 test_loss: 0.08413 \n",
      "[121/500] train_loss: 0.06334 valid_loss: 0.07623 test_loss: 0.08379 \n",
      "[122/500] train_loss: 0.06260 valid_loss: 0.07735 test_loss: 0.08325 \n",
      "[123/500] train_loss: 0.06050 valid_loss: 0.07588 test_loss: 0.08235 \n",
      "[124/500] train_loss: 0.06222 valid_loss: 0.07567 test_loss: 0.08243 \n",
      "[125/500] train_loss: 0.06348 valid_loss: 0.07459 test_loss: 0.08160 \n",
      "[126/500] train_loss: 0.06241 valid_loss: 0.07560 test_loss: 0.08304 \n",
      "[127/500] train_loss: 0.06500 valid_loss: 0.07703 test_loss: 0.08240 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128/500] train_loss: 0.06288 valid_loss: 0.07553 test_loss: 0.08169 \n",
      "[129/500] train_loss: 0.06229 valid_loss: 0.07520 test_loss: 0.08162 \n",
      "[130/500] train_loss: 0.06070 valid_loss: 0.07489 test_loss: 0.08155 \n",
      "[131/500] train_loss: 0.05979 valid_loss: 0.07496 test_loss: 0.08086 \n",
      "[132/500] train_loss: 0.06096 valid_loss: 0.07573 test_loss: 0.08231 \n",
      "[133/500] train_loss: 0.06034 valid_loss: 0.07477 test_loss: 0.08270 \n",
      "[134/500] train_loss: 0.06206 valid_loss: 0.07359 test_loss: 0.08173 \n",
      "验证损失减少 (0.073845 --> 0.073585). 正在保存模型...\n",
      "[135/500] train_loss: 0.06066 valid_loss: 0.07524 test_loss: 0.08169 \n",
      "[136/500] train_loss: 0.06050 valid_loss: 0.07493 test_loss: 0.08159 \n",
      "[137/500] train_loss: 0.06006 valid_loss: 0.07524 test_loss: 0.08235 \n",
      "[138/500] train_loss: 0.06001 valid_loss: 0.07482 test_loss: 0.08172 \n",
      "[139/500] train_loss: 0.06029 valid_loss: 0.07515 test_loss: 0.08154 \n",
      "[140/500] train_loss: 0.06112 valid_loss: 0.07563 test_loss: 0.08100 \n",
      "[141/500] train_loss: 0.06043 valid_loss: 0.07514 test_loss: 0.08126 \n",
      "[142/500] train_loss: 0.06032 valid_loss: 0.07557 test_loss: 0.08091 \n",
      "[143/500] train_loss: 0.05974 valid_loss: 0.07488 test_loss: 0.08124 \n",
      "[144/500] train_loss: 0.05996 valid_loss: 0.07514 test_loss: 0.08082 \n",
      "[145/500] train_loss: 0.05930 valid_loss: 0.07320 test_loss: 0.08044 \n",
      "验证损失减少 (0.073585 --> 0.073199). 正在保存模型...\n",
      "[146/500] train_loss: 0.05809 valid_loss: 0.07460 test_loss: 0.08244 \n",
      "[147/500] train_loss: 0.05965 valid_loss: 0.07539 test_loss: 0.08223 \n",
      "[148/500] train_loss: 0.05790 valid_loss: 0.07475 test_loss: 0.08132 \n",
      "[149/500] train_loss: 0.06047 valid_loss: 0.07349 test_loss: 0.08005 \n",
      "[150/500] train_loss: 0.05924 valid_loss: 0.07483 test_loss: 0.08281 \n",
      "[151/500] train_loss: 0.05883 valid_loss: 0.07418 test_loss: 0.08194 \n",
      "[152/500] train_loss: 0.05781 valid_loss: 0.07324 test_loss: 0.08115 \n",
      "[153/500] train_loss: 0.05754 valid_loss: 0.07306 test_loss: 0.08047 \n",
      "验证损失减少 (0.073199 --> 0.073056). 正在保存模型...\n",
      "[154/500] train_loss: 0.05951 valid_loss: 0.07500 test_loss: 0.08177 \n",
      "[155/500] train_loss: 0.05795 valid_loss: 0.07339 test_loss: 0.08050 \n",
      "[156/500] train_loss: 0.05829 valid_loss: 0.07426 test_loss: 0.08209 \n",
      "[157/500] train_loss: 0.05834 valid_loss: 0.07503 test_loss: 0.08253 \n",
      "[158/500] train_loss: 0.05749 valid_loss: 0.07345 test_loss: 0.08192 \n",
      "[159/500] train_loss: 0.05800 valid_loss: 0.07489 test_loss: 0.08195 \n",
      "[160/500] train_loss: 0.05973 valid_loss: 0.07367 test_loss: 0.08098 \n",
      "[161/500] train_loss: 0.05733 valid_loss: 0.07540 test_loss: 0.08189 \n",
      "[162/500] train_loss: 0.05781 valid_loss: 0.07460 test_loss: 0.08176 \n",
      "[163/500] train_loss: 0.05684 valid_loss: 0.07556 test_loss: 0.08235 \n",
      "[164/500] train_loss: 0.05723 valid_loss: 0.07488 test_loss: 0.08077 \n",
      "[165/500] train_loss: 0.05898 valid_loss: 0.07334 test_loss: 0.08070 \n",
      "[166/500] train_loss: 0.05657 valid_loss: 0.07442 test_loss: 0.08071 \n",
      "[167/500] train_loss: 0.05722 valid_loss: 0.07395 test_loss: 0.07999 \n",
      "[168/500] train_loss: 0.05780 valid_loss: 0.07447 test_loss: 0.08143 \n",
      "[169/500] train_loss: 0.05815 valid_loss: 0.07416 test_loss: 0.08199 \n",
      "[170/500] train_loss: 0.05696 valid_loss: 0.07536 test_loss: 0.08219 \n",
      "[171/500] train_loss: 0.05746 valid_loss: 0.07436 test_loss: 0.08094 \n",
      "[172/500] train_loss: 0.05668 valid_loss: 0.07430 test_loss: 0.08218 \n",
      "[173/500] train_loss: 0.05705 valid_loss: 0.07396 test_loss: 0.08127 \n",
      "[174/500] train_loss: 0.05671 valid_loss: 0.07407 test_loss: 0.08071 \n",
      "[175/500] train_loss: 0.05661 valid_loss: 0.07566 test_loss: 0.08122 \n",
      "[176/500] train_loss: 0.05666 valid_loss: 0.07445 test_loss: 0.08065 \n",
      "[177/500] train_loss: 0.05681 valid_loss: 0.07890 test_loss: 0.08116 \n",
      "[178/500] train_loss: 0.05621 valid_loss: 0.07482 test_loss: 0.08140 \n",
      "[179/500] train_loss: 0.05786 valid_loss: 0.07377 test_loss: 0.08098 \n",
      "[180/500] train_loss: 0.05720 valid_loss: 0.07438 test_loss: 0.08109 \n",
      "[181/500] train_loss: 0.05544 valid_loss: 0.07358 test_loss: 0.08110 \n",
      "[182/500] train_loss: 0.05486 valid_loss: 0.07443 test_loss: 0.08147 \n",
      "[183/500] train_loss: 0.05692 valid_loss: 0.07513 test_loss: 0.08048 \n",
      "[184/500] train_loss: 0.05525 valid_loss: 0.07392 test_loss: 0.08082 \n",
      "[185/500] train_loss: 0.05625 valid_loss: 0.07331 test_loss: 0.08116 \n",
      "[186/500] train_loss: 0.05588 valid_loss: 0.07328 test_loss: 0.07972 \n",
      "[187/500] train_loss: 0.05752 valid_loss: 0.07437 test_loss: 0.08182 \n",
      "[188/500] train_loss: 0.05623 valid_loss: 0.07501 test_loss: 0.08040 \n",
      "[189/500] train_loss: 0.05713 valid_loss: 0.07459 test_loss: 0.07970 \n",
      "[190/500] train_loss: 0.05585 valid_loss: 0.07503 test_loss: 0.07999 \n",
      "[191/500] train_loss: 0.05546 valid_loss: 0.07520 test_loss: 0.08484 \n",
      "[192/500] train_loss: 0.05603 valid_loss: 0.07552 test_loss: 0.08066 \n",
      "[193/500] train_loss: 0.05694 valid_loss: 0.07365 test_loss: 0.08011 \n",
      "[194/500] train_loss: 0.05465 valid_loss: 0.07513 test_loss: 0.07994 \n",
      "[195/500] train_loss: 0.05545 valid_loss: 0.07445 test_loss: 0.08077 \n",
      "[196/500] train_loss: 0.05511 valid_loss: 0.07590 test_loss: 0.08091 \n",
      "[197/500] train_loss: 0.05557 valid_loss: 0.07683 test_loss: 0.08185 \n",
      "[198/500] train_loss: 0.05409 valid_loss: 0.07511 test_loss: 0.08045 \n",
      "[199/500] train_loss: 0.05469 valid_loss: 0.07778 test_loss: 0.08111 \n",
      "[200/500] train_loss: 0.05395 valid_loss: 0.07548 test_loss: 0.08082 \n",
      "[201/500] train_loss: 0.05632 valid_loss: 0.07465 test_loss: 0.08078 \n",
      "[202/500] train_loss: 0.05351 valid_loss: 0.07422 test_loss: 0.08101 \n",
      "[203/500] train_loss: 0.05431 valid_loss: 0.07338 test_loss: 0.08112 \n",
      "[204/500] train_loss: 0.05492 valid_loss: 0.07547 test_loss: 0.08046 \n",
      "[205/500] train_loss: 0.05480 valid_loss: 0.07861 test_loss: 0.08131 \n",
      "[206/500] train_loss: 0.05492 valid_loss: 0.07430 test_loss: 0.07978 \n",
      "[207/500] train_loss: 0.05522 valid_loss: 0.07411 test_loss: 0.08079 \n",
      "[208/500] train_loss: 0.05426 valid_loss: 0.07473 test_loss: 0.07947 \n",
      "[209/500] train_loss: 0.05397 valid_loss: 0.07504 test_loss: 0.08002 \n",
      "[210/500] train_loss: 0.05494 valid_loss: 0.07547 test_loss: 0.08128 \n",
      "[211/500] train_loss: 0.05367 valid_loss: 0.07458 test_loss: 0.08139 \n",
      "[212/500] train_loss: 0.05422 valid_loss: 0.07460 test_loss: 0.08034 \n",
      "[213/500] train_loss: 0.05385 valid_loss: 0.07503 test_loss: 0.08087 \n",
      "[214/500] train_loss: 0.05249 valid_loss: 0.07570 test_loss: 0.08070 \n",
      "[215/500] train_loss: 0.05395 valid_loss: 0.07452 test_loss: 0.08038 \n",
      "[216/500] train_loss: 0.05440 valid_loss: 0.07488 test_loss: 0.08028 \n",
      "[217/500] train_loss: 0.05308 valid_loss: 0.07488 test_loss: 0.08115 \n",
      "[218/500] train_loss: 0.05431 valid_loss: 0.07379 test_loss: 0.08114 \n",
      "[219/500] train_loss: 0.05276 valid_loss: 0.07415 test_loss: 0.08004 \n",
      "[220/500] train_loss: 0.05300 valid_loss: 0.07614 test_loss: 0.08113 \n",
      "[221/500] train_loss: 0.05247 valid_loss: 0.07529 test_loss: 0.07973 \n",
      "[222/500] train_loss: 0.05302 valid_loss: 0.07517 test_loss: 0.07987 \n",
      "[223/500] train_loss: 0.05081 valid_loss: 0.07487 test_loss: 0.08032 \n",
      "[224/500] train_loss: 0.05339 valid_loss: 0.07547 test_loss: 0.07957 \n",
      "[225/500] train_loss: 0.05288 valid_loss: 0.07765 test_loss: 0.07993 \n",
      "[226/500] train_loss: 0.05172 valid_loss: 0.07411 test_loss: 0.08078 \n",
      "[227/500] train_loss: 0.05125 valid_loss: 0.07448 test_loss: 0.08097 \n",
      "[228/500] train_loss: 0.05274 valid_loss: 0.07362 test_loss: 0.07944 \n",
      "[229/500] train_loss: 0.05232 valid_loss: 0.07559 test_loss: 0.07958 \n",
      "[230/500] train_loss: 0.05132 valid_loss: 0.07416 test_loss: 0.08035 \n",
      "[231/500] train_loss: 0.05373 valid_loss: 0.07492 test_loss: 0.08084 \n",
      "[232/500] train_loss: 0.05250 valid_loss: 0.07623 test_loss: 0.08114 \n",
      "[233/500] train_loss: 0.05300 valid_loss: 0.07434 test_loss: 0.08085 \n",
      "[234/500] train_loss: 0.05227 valid_loss: 0.07366 test_loss: 0.07938 \n",
      "[235/500] train_loss: 0.05193 valid_loss: 0.07368 test_loss: 0.08003 \n",
      "[236/500] train_loss: 0.05238 valid_loss: 0.07344 test_loss: 0.08162 \n",
      "[237/500] train_loss: 0.05311 valid_loss: 0.07403 test_loss: 0.08116 \n",
      "[238/500] train_loss: 0.05216 valid_loss: 0.07373 test_loss: 0.07860 \n",
      "[239/500] train_loss: 0.05342 valid_loss: 0.07544 test_loss: 0.07929 \n",
      "[240/500] train_loss: 0.05112 valid_loss: 0.07479 test_loss: 0.08049 \n",
      "[241/500] train_loss: 0.05236 valid_loss: 0.07536 test_loss: 0.08059 \n",
      "[242/500] train_loss: 0.05387 valid_loss: 0.07638 test_loss: 0.08149 \n",
      "[243/500] train_loss: 0.05139 valid_loss: 0.07473 test_loss: 0.07998 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[244/500] train_loss: 0.05167 valid_loss: 0.07406 test_loss: 0.08020 \n",
      "[245/500] train_loss: 0.04986 valid_loss: 0.07293 test_loss: 0.08234 \n",
      "验证损失减少 (0.073056 --> 0.072935). 正在保存模型...\n",
      "[246/500] train_loss: 0.05153 valid_loss: 0.07685 test_loss: 0.08203 \n",
      "[247/500] train_loss: 0.05205 valid_loss: 0.07323 test_loss: 0.08024 \n",
      "[248/500] train_loss: 0.04992 valid_loss: 0.07380 test_loss: 0.07967 \n",
      "[249/500] train_loss: 0.05026 valid_loss: 0.07487 test_loss: 0.08063 \n",
      "[250/500] train_loss: 0.05125 valid_loss: 0.07602 test_loss: 0.08025 \n",
      "[251/500] train_loss: 0.05170 valid_loss: 0.07465 test_loss: 0.08161 \n",
      "[252/500] train_loss: 0.05118 valid_loss: 0.07423 test_loss: 0.07953 \n",
      "[253/500] train_loss: 0.05052 valid_loss: 0.07349 test_loss: 0.07983 \n",
      "[254/500] train_loss: 0.05102 valid_loss: 0.07249 test_loss: 0.08032 \n",
      "验证损失减少 (0.072935 --> 0.072490). 正在保存模型...\n",
      "[255/500] train_loss: 0.05072 valid_loss: 0.07455 test_loss: 0.08164 \n",
      "[256/500] train_loss: 0.04974 valid_loss: 0.07516 test_loss: 0.08103 \n",
      "[257/500] train_loss: 0.05113 valid_loss: 0.07535 test_loss: 0.08020 \n",
      "[258/500] train_loss: 0.05101 valid_loss: 0.07480 test_loss: 0.08043 \n",
      "[259/500] train_loss: 0.05047 valid_loss: 0.07487 test_loss: 0.08050 \n",
      "[260/500] train_loss: 0.04944 valid_loss: 0.07641 test_loss: 0.08125 \n",
      "[261/500] train_loss: 0.04999 valid_loss: 0.07572 test_loss: 0.08241 \n",
      "[262/500] train_loss: 0.04938 valid_loss: 0.07495 test_loss: 0.08239 \n",
      "[263/500] train_loss: 0.04992 valid_loss: 0.07507 test_loss: 0.08133 \n",
      "[264/500] train_loss: 0.04892 valid_loss: 0.07565 test_loss: 0.07985 \n",
      "[265/500] train_loss: 0.05200 valid_loss: 0.07469 test_loss: 0.08017 \n",
      "[266/500] train_loss: 0.04961 valid_loss: 0.07636 test_loss: 0.08232 \n",
      "[267/500] train_loss: 0.05112 valid_loss: 0.07382 test_loss: 0.07928 \n",
      "[268/500] train_loss: 0.05028 valid_loss: 0.07523 test_loss: 0.08081 \n",
      "[269/500] train_loss: 0.04995 valid_loss: 0.07576 test_loss: 0.07949 \n",
      "[270/500] train_loss: 0.04925 valid_loss: 0.07467 test_loss: 0.08008 \n",
      "[271/500] train_loss: 0.05083 valid_loss: 0.07331 test_loss: 0.07909 \n",
      "[272/500] train_loss: 0.04903 valid_loss: 0.07507 test_loss: 0.08236 \n",
      "[273/500] train_loss: 0.04930 valid_loss: 0.07349 test_loss: 0.08047 \n",
      "[274/500] train_loss: 0.05055 valid_loss: 0.07422 test_loss: 0.08058 \n",
      "[275/500] train_loss: 0.05014 valid_loss: 0.07460 test_loss: 0.08126 \n",
      "[276/500] train_loss: 0.04963 valid_loss: 0.07383 test_loss: 0.08133 \n",
      "[277/500] train_loss: 0.04876 valid_loss: 0.07323 test_loss: 0.08104 \n",
      "[278/500] train_loss: 0.04985 valid_loss: 0.07286 test_loss: 0.08059 \n",
      "[279/500] train_loss: 0.04908 valid_loss: 0.07442 test_loss: 0.08078 \n",
      "[280/500] train_loss: 0.05101 valid_loss: 0.07721 test_loss: 0.08237 \n",
      "[281/500] train_loss: 0.04990 valid_loss: 0.07413 test_loss: 0.08230 \n",
      "[282/500] train_loss: 0.04889 valid_loss: 0.07713 test_loss: 0.08243 \n",
      "[283/500] train_loss: 0.04812 valid_loss: 0.07632 test_loss: 0.08271 \n",
      "[284/500] train_loss: 0.04899 valid_loss: 0.07530 test_loss: 0.08193 \n",
      "[285/500] train_loss: 0.04820 valid_loss: 0.07525 test_loss: 0.08121 \n",
      "[286/500] train_loss: 0.04863 valid_loss: 0.07473 test_loss: 0.08099 \n",
      "[287/500] train_loss: 0.04880 valid_loss: 0.07600 test_loss: 0.08353 \n",
      "[288/500] train_loss: 0.04663 valid_loss: 0.07462 test_loss: 0.07963 \n",
      "[289/500] train_loss: 0.04867 valid_loss: 0.07340 test_loss: 0.08108 \n",
      "[290/500] train_loss: 0.04841 valid_loss: 0.07404 test_loss: 0.08004 \n",
      "[291/500] train_loss: 0.04886 valid_loss: 0.07379 test_loss: 0.08139 \n",
      "[292/500] train_loss: 0.04950 valid_loss: 0.07720 test_loss: 0.08161 \n",
      "[293/500] train_loss: 0.04820 valid_loss: 0.07632 test_loss: 0.08272 \n",
      "[294/500] train_loss: 0.04693 valid_loss: 0.07416 test_loss: 0.08027 \n",
      "[295/500] train_loss: 0.05037 valid_loss: 0.07910 test_loss: 0.08159 \n",
      "[296/500] train_loss: 0.04729 valid_loss: 0.07541 test_loss: 0.08057 \n",
      "[297/500] train_loss: 0.04851 valid_loss: 0.07554 test_loss: 0.08154 \n",
      "[298/500] train_loss: 0.04794 valid_loss: 0.07428 test_loss: 0.08101 \n",
      "[299/500] train_loss: 0.04714 valid_loss: 0.07499 test_loss: 0.08101 \n",
      "[300/500] train_loss: 0.04935 valid_loss: 0.07417 test_loss: 0.08186 \n",
      "[301/500] train_loss: 0.04856 valid_loss: 0.07259 test_loss: 0.07964 \n",
      "[302/500] train_loss: 0.04833 valid_loss: 0.07352 test_loss: 0.08018 \n",
      "[303/500] train_loss: 0.04841 valid_loss: 0.07333 test_loss: 0.08077 \n",
      "[304/500] train_loss: 0.04911 valid_loss: 0.07399 test_loss: 0.07986 \n",
      "[305/500] train_loss: 0.04771 valid_loss: 0.07480 test_loss: 0.08224 \n",
      "[306/500] train_loss: 0.04755 valid_loss: 0.07503 test_loss: 0.08147 \n",
      "[307/500] train_loss: 0.04788 valid_loss: 0.07461 test_loss: 0.08029 \n",
      "[308/500] train_loss: 0.04851 valid_loss: 0.07584 test_loss: 0.08035 \n",
      "[309/500] train_loss: 0.04914 valid_loss: 0.07547 test_loss: 0.08214 \n",
      "[310/500] train_loss: 0.04767 valid_loss: 0.07472 test_loss: 0.08320 \n",
      "[311/500] train_loss: 0.04726 valid_loss: 0.07450 test_loss: 0.08017 \n",
      "[312/500] train_loss: 0.04784 valid_loss: 0.07375 test_loss: 0.07885 \n",
      "[313/500] train_loss: 0.04739 valid_loss: 0.07605 test_loss: 0.08366 \n",
      "[314/500] train_loss: 0.04783 valid_loss: 0.07279 test_loss: 0.08105 \n",
      "[315/500] train_loss: 0.04736 valid_loss: 0.07430 test_loss: 0.07978 \n",
      "[316/500] train_loss: 0.04712 valid_loss: 0.07289 test_loss: 0.08176 \n",
      "[317/500] train_loss: 0.04704 valid_loss: 0.07336 test_loss: 0.08185 \n",
      "[318/500] train_loss: 0.04666 valid_loss: 0.07316 test_loss: 0.08127 \n",
      "[319/500] train_loss: 0.04739 valid_loss: 0.07368 test_loss: 0.08149 \n",
      "[320/500] train_loss: 0.04736 valid_loss: 0.07491 test_loss: 0.08299 \n",
      "[321/500] train_loss: 0.04730 valid_loss: 0.07527 test_loss: 0.08137 \n",
      "[322/500] train_loss: 0.04714 valid_loss: 0.07372 test_loss: 0.08080 \n",
      "[323/500] train_loss: 0.04714 valid_loss: 0.07356 test_loss: 0.08192 \n",
      "[324/500] train_loss: 0.04645 valid_loss: 0.07551 test_loss: 0.08165 \n",
      "[325/500] train_loss: 0.04742 valid_loss: 0.07356 test_loss: 0.08294 \n",
      "[326/500] train_loss: 0.04765 valid_loss: 0.07883 test_loss: 0.08198 \n",
      "[327/500] train_loss: 0.04721 valid_loss: 0.07387 test_loss: 0.08061 \n",
      "[328/500] train_loss: 0.04591 valid_loss: 0.07524 test_loss: 0.08179 \n",
      "[329/500] train_loss: 0.04815 valid_loss: 0.07408 test_loss: 0.08142 \n",
      "[330/500] train_loss: 0.04787 valid_loss: 0.07479 test_loss: 0.08093 \n",
      "[331/500] train_loss: 0.04732 valid_loss: 0.07374 test_loss: 0.08177 \n",
      "[332/500] train_loss: 0.04742 valid_loss: 0.07446 test_loss: 0.08230 \n",
      "[333/500] train_loss: 0.04662 valid_loss: 0.07358 test_loss: 0.08224 \n",
      "[334/500] train_loss: 0.04646 valid_loss: 0.07423 test_loss: 0.08324 \n",
      "[335/500] train_loss: 0.04736 valid_loss: 0.07280 test_loss: 0.08149 \n",
      "[336/500] train_loss: 0.04653 valid_loss: 0.07585 test_loss: 0.08230 \n",
      "[337/500] train_loss: 0.04609 valid_loss: 0.07532 test_loss: 0.08264 \n",
      "[338/500] train_loss: 0.04735 valid_loss: 0.07714 test_loss: 0.08222 \n",
      "[339/500] train_loss: 0.04595 valid_loss: 0.07274 test_loss: 0.08188 \n",
      "[340/500] train_loss: 0.04707 valid_loss: 0.07337 test_loss: 0.08153 \n",
      "[341/500] train_loss: 0.04536 valid_loss: 0.07288 test_loss: 0.08197 \n",
      "[342/500] train_loss: 0.04636 valid_loss: 0.07342 test_loss: 0.08091 \n",
      "[343/500] train_loss: 0.04625 valid_loss: 0.07351 test_loss: 0.08194 \n",
      "[344/500] train_loss: 0.04630 valid_loss: 0.07214 test_loss: 0.08025 \n",
      "验证损失减少 (0.072490 --> 0.072139). 正在保存模型...\n",
      "[345/500] train_loss: 0.04690 valid_loss: 0.07460 test_loss: 0.08088 \n",
      "[346/500] train_loss: 0.04655 valid_loss: 0.07373 test_loss: 0.08297 \n",
      "[347/500] train_loss: 0.04638 valid_loss: 0.07632 test_loss: 0.08169 \n",
      "[348/500] train_loss: 0.04664 valid_loss: 0.07482 test_loss: 0.08195 \n",
      "[349/500] train_loss: 0.04610 valid_loss: 0.07483 test_loss: 0.08139 \n",
      "[350/500] train_loss: 0.04572 valid_loss: 0.07523 test_loss: 0.07989 \n",
      "[351/500] train_loss: 0.04692 valid_loss: 0.07410 test_loss: 0.08027 \n",
      "[352/500] train_loss: 0.04557 valid_loss: 0.07421 test_loss: 0.08198 \n",
      "[353/500] train_loss: 0.04546 valid_loss: 0.07468 test_loss: 0.08031 \n",
      "[354/500] train_loss: 0.04455 valid_loss: 0.07559 test_loss: 0.08356 \n",
      "[355/500] train_loss: 0.04635 valid_loss: 0.07414 test_loss: 0.08102 \n",
      "[356/500] train_loss: 0.04638 valid_loss: 0.07404 test_loss: 0.08156 \n",
      "[357/500] train_loss: 0.04576 valid_loss: 0.07377 test_loss: 0.08182 \n",
      "[358/500] train_loss: 0.04571 valid_loss: 0.07648 test_loss: 0.08292 \n",
      "[359/500] train_loss: 0.04503 valid_loss: 0.07518 test_loss: 0.08272 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[360/500] train_loss: 0.04587 valid_loss: 0.07523 test_loss: 0.08079 \n",
      "[361/500] train_loss: 0.04625 valid_loss: 0.07352 test_loss: 0.08155 \n",
      "[362/500] train_loss: 0.04441 valid_loss: 0.07564 test_loss: 0.08343 \n",
      "[363/500] train_loss: 0.04575 valid_loss: 0.07325 test_loss: 0.08159 \n",
      "[364/500] train_loss: 0.04480 valid_loss: 0.07431 test_loss: 0.08160 \n",
      "[365/500] train_loss: 0.04503 valid_loss: 0.07388 test_loss: 0.08152 \n",
      "[366/500] train_loss: 0.04479 valid_loss: 0.07323 test_loss: 0.08133 \n",
      "[367/500] train_loss: 0.04451 valid_loss: 0.07402 test_loss: 0.08065 \n",
      "[368/500] train_loss: 0.04473 valid_loss: 0.07293 test_loss: 0.08135 \n",
      "[369/500] train_loss: 0.04503 valid_loss: 0.07696 test_loss: 0.08163 \n",
      "[370/500] train_loss: 0.04572 valid_loss: 0.07507 test_loss: 0.08090 \n",
      "[371/500] train_loss: 0.04475 valid_loss: 0.07538 test_loss: 0.08254 \n",
      "[372/500] train_loss: 0.04473 valid_loss: 0.07448 test_loss: 0.08226 \n",
      "[373/500] train_loss: 0.04574 valid_loss: 0.07463 test_loss: 0.08231 \n",
      "[374/500] train_loss: 0.04404 valid_loss: 0.07379 test_loss: 0.08171 \n",
      "[375/500] train_loss: 0.04509 valid_loss: 0.07560 test_loss: 0.08275 \n",
      "[376/500] train_loss: 0.04486 valid_loss: 0.07561 test_loss: 0.08374 \n",
      "[377/500] train_loss: 0.04541 valid_loss: 0.07585 test_loss: 0.08493 \n",
      "[378/500] train_loss: 0.04631 valid_loss: 0.07391 test_loss: 0.08210 \n",
      "[379/500] train_loss: 0.04636 valid_loss: 0.07766 test_loss: 0.08319 \n",
      "[380/500] train_loss: 0.04455 valid_loss: 0.07308 test_loss: 0.08102 \n",
      "[381/500] train_loss: 0.04443 valid_loss: 0.07510 test_loss: 0.08270 \n",
      "[382/500] train_loss: 0.04516 valid_loss: 0.07367 test_loss: 0.08191 \n",
      "[383/500] train_loss: 0.04379 valid_loss: 0.07468 test_loss: 0.08322 \n",
      "[384/500] train_loss: 0.04347 valid_loss: 0.07416 test_loss: 0.08255 \n",
      "[385/500] train_loss: 0.04417 valid_loss: 0.07381 test_loss: 0.08082 \n",
      "[386/500] train_loss: 0.04455 valid_loss: 0.07562 test_loss: 0.08341 \n",
      "[387/500] train_loss: 0.04595 valid_loss: 0.07318 test_loss: 0.08119 \n",
      "[388/500] train_loss: 0.04550 valid_loss: 0.07325 test_loss: 0.08229 \n",
      "[389/500] train_loss: 0.04474 valid_loss: 0.07467 test_loss: 0.08189 \n",
      "[390/500] train_loss: 0.04538 valid_loss: 0.07361 test_loss: 0.08332 \n",
      "[391/500] train_loss: 0.04485 valid_loss: 0.07489 test_loss: 0.08318 \n",
      "[392/500] train_loss: 0.04529 valid_loss: 0.07499 test_loss: 0.08181 \n",
      "[393/500] train_loss: 0.04376 valid_loss: 0.07493 test_loss: 0.08284 \n",
      "[394/500] train_loss: 0.04437 valid_loss: 0.07496 test_loss: 0.08128 \n",
      "[395/500] train_loss: 0.04475 valid_loss: 0.07462 test_loss: 0.08357 \n",
      "[396/500] train_loss: 0.04457 valid_loss: 0.07462 test_loss: 0.08195 \n",
      "[397/500] train_loss: 0.04427 valid_loss: 0.07238 test_loss: 0.08103 \n",
      "[398/500] train_loss: 0.04544 valid_loss: 0.07460 test_loss: 0.08234 \n",
      "[399/500] train_loss: 0.04394 valid_loss: 0.07209 test_loss: 0.08162 \n",
      "验证损失减少 (0.072139 --> 0.072085). 正在保存模型...\n",
      "[400/500] train_loss: 0.04323 valid_loss: 0.07351 test_loss: 0.08220 \n",
      "[401/500] train_loss: 0.04392 valid_loss: 0.07471 test_loss: 0.08249 \n",
      "[402/500] train_loss: 0.04553 valid_loss: 0.07565 test_loss: 0.08131 \n",
      "[403/500] train_loss: 0.04445 valid_loss: 0.07461 test_loss: 0.08213 \n",
      "[404/500] train_loss: 0.04289 valid_loss: 0.07518 test_loss: 0.08337 \n",
      "[405/500] train_loss: 0.04395 valid_loss: 0.07560 test_loss: 0.08154 \n",
      "[406/500] train_loss: 0.04345 valid_loss: 0.07636 test_loss: 0.08194 \n",
      "[407/500] train_loss: 0.04405 valid_loss: 0.07640 test_loss: 0.08196 \n",
      "[408/500] train_loss: 0.04497 valid_loss: 0.07449 test_loss: 0.08196 \n",
      "[409/500] train_loss: 0.04325 valid_loss: 0.07632 test_loss: 0.08234 \n",
      "[410/500] train_loss: 0.04372 valid_loss: 0.07561 test_loss: 0.08428 \n",
      "[411/500] train_loss: 0.04402 valid_loss: 0.07336 test_loss: 0.08366 \n",
      "[412/500] train_loss: 0.04389 valid_loss: 0.07447 test_loss: 0.08472 \n",
      "[413/500] train_loss: 0.04363 valid_loss: 0.07277 test_loss: 0.08293 \n",
      "[414/500] train_loss: 0.04406 valid_loss: 0.07608 test_loss: 0.08500 \n",
      "[415/500] train_loss: 0.04262 valid_loss: 0.07348 test_loss: 0.08305 \n",
      "[416/500] train_loss: 0.04317 valid_loss: 0.07327 test_loss: 0.08253 \n",
      "[417/500] train_loss: 0.04314 valid_loss: 0.07343 test_loss: 0.08156 \n",
      "[418/500] train_loss: 0.04272 valid_loss: 0.07459 test_loss: 0.08402 \n",
      "[419/500] train_loss: 0.04461 valid_loss: 0.07357 test_loss: 0.08397 \n",
      "[420/500] train_loss: 0.04317 valid_loss: 0.07394 test_loss: 0.08277 \n",
      "[421/500] train_loss: 0.04315 valid_loss: 0.07421 test_loss: 0.08544 \n",
      "[422/500] train_loss: 0.04400 valid_loss: 0.07492 test_loss: 0.08196 \n",
      "[423/500] train_loss: 0.04390 valid_loss: 0.07419 test_loss: 0.08335 \n",
      "[424/500] train_loss: 0.04285 valid_loss: 0.07476 test_loss: 0.08392 \n",
      "[425/500] train_loss: 0.04365 valid_loss: 0.07548 test_loss: 0.08291 \n",
      "[426/500] train_loss: 0.04247 valid_loss: 0.07314 test_loss: 0.08197 \n",
      "[427/500] train_loss: 0.04409 valid_loss: 0.07430 test_loss: 0.08371 \n",
      "[428/500] train_loss: 0.04299 valid_loss: 0.07421 test_loss: 0.08393 \n",
      "[429/500] train_loss: 0.04341 valid_loss: 0.07435 test_loss: 0.08411 \n",
      "[430/500] train_loss: 0.04332 valid_loss: 0.07307 test_loss: 0.08227 \n",
      "[431/500] train_loss: 0.04264 valid_loss: 0.07268 test_loss: 0.08243 \n",
      "[432/500] train_loss: 0.04351 valid_loss: 0.07398 test_loss: 0.08097 \n",
      "[433/500] train_loss: 0.04441 valid_loss: 0.07308 test_loss: 0.08255 \n",
      "[434/500] train_loss: 0.04271 valid_loss: 0.07407 test_loss: 0.08293 \n",
      "[435/500] train_loss: 0.04281 valid_loss: 0.07319 test_loss: 0.08399 \n",
      "[436/500] train_loss: 0.04377 valid_loss: 0.07471 test_loss: 0.08172 \n",
      "[437/500] train_loss: 0.04311 valid_loss: 0.07261 test_loss: 0.08147 \n",
      "[438/500] train_loss: 0.04194 valid_loss: 0.07510 test_loss: 0.08200 \n",
      "[439/500] train_loss: 0.04390 valid_loss: 0.07452 test_loss: 0.08394 \n",
      "[440/500] train_loss: 0.04202 valid_loss: 0.07468 test_loss: 0.08261 \n",
      "[441/500] train_loss: 0.04280 valid_loss: 0.07385 test_loss: 0.08263 \n",
      "[442/500] train_loss: 0.04313 valid_loss: 0.07403 test_loss: 0.08271 \n",
      "[443/500] train_loss: 0.04324 valid_loss: 0.07407 test_loss: 0.08285 \n",
      "[444/500] train_loss: 0.04374 valid_loss: 0.07511 test_loss: 0.08271 \n",
      "[445/500] train_loss: 0.04233 valid_loss: 0.07583 test_loss: 0.08239 \n",
      "[446/500] train_loss: 0.04201 valid_loss: 0.07435 test_loss: 0.08276 \n",
      "[447/500] train_loss: 0.04187 valid_loss: 0.07434 test_loss: 0.08479 \n",
      "[448/500] train_loss: 0.04321 valid_loss: 0.07384 test_loss: 0.08288 \n",
      "[449/500] train_loss: 0.04245 valid_loss: 0.07379 test_loss: 0.08112 \n",
      "[450/500] train_loss: 0.04223 valid_loss: 0.07288 test_loss: 0.08318 \n",
      "[451/500] train_loss: 0.04207 valid_loss: 0.07293 test_loss: 0.08255 \n",
      "[452/500] train_loss: 0.04251 valid_loss: 0.07444 test_loss: 0.08167 \n",
      "[453/500] train_loss: 0.04174 valid_loss: 0.07351 test_loss: 0.08214 \n",
      "[454/500] train_loss: 0.04165 valid_loss: 0.07434 test_loss: 0.08483 \n",
      "[455/500] train_loss: 0.04198 valid_loss: 0.07536 test_loss: 0.08392 \n",
      "[456/500] train_loss: 0.04147 valid_loss: 0.07415 test_loss: 0.08416 \n",
      "[457/500] train_loss: 0.04156 valid_loss: 0.07594 test_loss: 0.08155 \n",
      "[458/500] train_loss: 0.04196 valid_loss: 0.07466 test_loss: 0.08442 \n",
      "[459/500] train_loss: 0.04198 valid_loss: 0.07393 test_loss: 0.08341 \n",
      "[460/500] train_loss: 0.04236 valid_loss: 0.07339 test_loss: 0.08487 \n",
      "[461/500] train_loss: 0.04124 valid_loss: 0.07324 test_loss: 0.08294 \n",
      "[462/500] train_loss: 0.04365 valid_loss: 0.07614 test_loss: 0.08320 \n",
      "[463/500] train_loss: 0.04195 valid_loss: 0.07337 test_loss: 0.08429 \n",
      "[464/500] train_loss: 0.04291 valid_loss: 0.07399 test_loss: 0.08304 \n",
      "[465/500] train_loss: 0.04330 valid_loss: 0.07352 test_loss: 0.08254 \n",
      "[466/500] train_loss: 0.04256 valid_loss: 0.07412 test_loss: 0.08289 \n",
      "[467/500] train_loss: 0.04316 valid_loss: 0.07388 test_loss: 0.08283 \n",
      "[468/500] train_loss: 0.04179 valid_loss: 0.07386 test_loss: 0.08301 \n",
      "[469/500] train_loss: 0.04226 valid_loss: 0.07405 test_loss: 0.08173 \n",
      "[470/500] train_loss: 0.04041 valid_loss: 0.07419 test_loss: 0.08260 \n",
      "[471/500] train_loss: 0.04037 valid_loss: 0.07430 test_loss: 0.08165 \n",
      "[472/500] train_loss: 0.04270 valid_loss: 0.07592 test_loss: 0.08218 \n",
      "[473/500] train_loss: 0.04206 valid_loss: 0.07402 test_loss: 0.08284 \n",
      "[474/500] train_loss: 0.04153 valid_loss: 0.07564 test_loss: 0.08436 \n",
      "[475/500] train_loss: 0.04188 valid_loss: 0.07457 test_loss: 0.08207 \n",
      "[476/500] train_loss: 0.04143 valid_loss: 0.07573 test_loss: 0.08469 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[477/500] train_loss: 0.04215 valid_loss: 0.07357 test_loss: 0.08286 \n",
      "[478/500] train_loss: 0.04233 valid_loss: 0.07474 test_loss: 0.08287 \n",
      "[479/500] train_loss: 0.04228 valid_loss: 0.07300 test_loss: 0.08408 \n",
      "[480/500] train_loss: 0.04116 valid_loss: 0.07366 test_loss: 0.08125 \n",
      "[481/500] train_loss: 0.04195 valid_loss: 0.07361 test_loss: 0.08432 \n",
      "[482/500] train_loss: 0.04156 valid_loss: 0.07337 test_loss: 0.08274 \n",
      "[483/500] train_loss: 0.04215 valid_loss: 0.07431 test_loss: 0.08369 \n",
      "[484/500] train_loss: 0.04204 valid_loss: 0.07394 test_loss: 0.08351 \n",
      "[485/500] train_loss: 0.04166 valid_loss: 0.07387 test_loss: 0.08249 \n",
      "[486/500] train_loss: 0.04060 valid_loss: 0.07489 test_loss: 0.08260 \n",
      "[487/500] train_loss: 0.04296 valid_loss: 0.07401 test_loss: 0.08457 \n",
      "[488/500] train_loss: 0.04160 valid_loss: 0.07420 test_loss: 0.08341 \n",
      "[489/500] train_loss: 0.04252 valid_loss: 0.07382 test_loss: 0.08339 \n",
      "[490/500] train_loss: 0.04179 valid_loss: 0.07406 test_loss: 0.08511 \n",
      "[491/500] train_loss: 0.04047 valid_loss: 0.07328 test_loss: 0.08500 \n",
      "[492/500] train_loss: 0.04175 valid_loss: 0.07376 test_loss: 0.08305 \n",
      "[493/500] train_loss: 0.04122 valid_loss: 0.07424 test_loss: 0.08511 \n",
      "[494/500] train_loss: 0.04150 valid_loss: 0.07290 test_loss: 0.08331 \n",
      "[495/500] train_loss: 0.04121 valid_loss: 0.07392 test_loss: 0.08482 \n",
      "[496/500] train_loss: 0.04057 valid_loss: 0.07257 test_loss: 0.08382 \n",
      "[497/500] train_loss: 0.04119 valid_loss: 0.07414 test_loss: 0.08429 \n",
      "[498/500] train_loss: 0.04044 valid_loss: 0.07376 test_loss: 0.08456 \n",
      "[499/500] train_loss: 0.04136 valid_loss: 0.07376 test_loss: 0.08366 \n",
      "[500/500] train_loss: 0.04202 valid_loss: 0.07330 test_loss: 0.08248 \n",
      "TRAINING MODEL 14\n",
      "[  1/500] train_loss: 0.38047 valid_loss: 0.27094 test_loss: 0.27774 \n",
      "验证损失减少 (inf --> 0.270940). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19780 valid_loss: 0.18945 test_loss: 0.19830 \n",
      "验证损失减少 (0.270940 --> 0.189449). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15357 valid_loss: 0.15333 test_loss: 0.15999 \n",
      "验证损失减少 (0.189449 --> 0.153334). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13407 valid_loss: 0.14165 test_loss: 0.14819 \n",
      "验证损失减少 (0.153334 --> 0.141650). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12689 valid_loss: 0.13515 test_loss: 0.14309 \n",
      "验证损失减少 (0.141650 --> 0.135150). 正在保存模型...\n",
      "[  6/500] train_loss: 0.11921 valid_loss: 0.12855 test_loss: 0.13612 \n",
      "验证损失减少 (0.135150 --> 0.128547). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11611 valid_loss: 0.12083 test_loss: 0.12747 \n",
      "验证损失减少 (0.128547 --> 0.120831). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11679 valid_loss: 0.11719 test_loss: 0.12578 \n",
      "验证损失减少 (0.120831 --> 0.117195). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11125 valid_loss: 0.11946 test_loss: 0.12621 \n",
      "[ 10/500] train_loss: 0.10705 valid_loss: 0.11888 test_loss: 0.13002 \n",
      "[ 11/500] train_loss: 0.10625 valid_loss: 0.11059 test_loss: 0.11560 \n",
      "验证损失减少 (0.117195 --> 0.110588). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10203 valid_loss: 0.10839 test_loss: 0.11468 \n",
      "验证损失减少 (0.110588 --> 0.108395). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10275 valid_loss: 0.10410 test_loss: 0.11385 \n",
      "验证损失减少 (0.108395 --> 0.104103). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.09768 valid_loss: 0.10440 test_loss: 0.11202 \n",
      "[ 15/500] train_loss: 0.09660 valid_loss: 0.10243 test_loss: 0.10971 \n",
      "验证损失减少 (0.104103 --> 0.102433). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09527 valid_loss: 0.10792 test_loss: 0.11147 \n",
      "[ 17/500] train_loss: 0.09503 valid_loss: 0.10233 test_loss: 0.11021 \n",
      "验证损失减少 (0.102433 --> 0.102332). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09515 valid_loss: 0.09827 test_loss: 0.10567 \n",
      "验证损失减少 (0.102332 --> 0.098266). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09149 valid_loss: 0.09773 test_loss: 0.10557 \n",
      "验证损失减少 (0.098266 --> 0.097728). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09105 valid_loss: 0.09789 test_loss: 0.10469 \n",
      "[ 21/500] train_loss: 0.09102 valid_loss: 0.09658 test_loss: 0.10288 \n",
      "验证损失减少 (0.097728 --> 0.096576). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.08915 valid_loss: 0.09657 test_loss: 0.10382 \n",
      "验证损失减少 (0.096576 --> 0.096571). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09105 valid_loss: 0.09423 test_loss: 0.10223 \n",
      "验证损失减少 (0.096571 --> 0.094234). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09133 valid_loss: 0.09545 test_loss: 0.10175 \n",
      "[ 25/500] train_loss: 0.08816 valid_loss: 0.09180 test_loss: 0.09907 \n",
      "验证损失减少 (0.094234 --> 0.091801). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.08954 valid_loss: 0.09169 test_loss: 0.10049 \n",
      "验证损失减少 (0.091801 --> 0.091686). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.08573 valid_loss: 0.09120 test_loss: 0.09944 \n",
      "验证损失减少 (0.091686 --> 0.091196). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08312 valid_loss: 0.09150 test_loss: 0.10010 \n",
      "[ 29/500] train_loss: 0.08494 valid_loss: 0.09037 test_loss: 0.09690 \n",
      "验证损失减少 (0.091196 --> 0.090373). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08316 valid_loss: 0.09000 test_loss: 0.09859 \n",
      "验证损失减少 (0.090373 --> 0.089999). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08386 valid_loss: 0.08937 test_loss: 0.09594 \n",
      "验证损失减少 (0.089999 --> 0.089368). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08403 valid_loss: 0.08770 test_loss: 0.09366 \n",
      "验证损失减少 (0.089368 --> 0.087697). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08262 valid_loss: 0.09075 test_loss: 0.09671 \n",
      "[ 34/500] train_loss: 0.08422 valid_loss: 0.08676 test_loss: 0.09310 \n",
      "验证损失减少 (0.087697 --> 0.086757). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08224 valid_loss: 0.08647 test_loss: 0.09576 \n",
      "验证损失减少 (0.086757 --> 0.086469). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.07938 valid_loss: 0.08564 test_loss: 0.09269 \n",
      "验证损失减少 (0.086469 --> 0.085642). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08123 valid_loss: 0.09052 test_loss: 0.09353 \n",
      "[ 38/500] train_loss: 0.08080 valid_loss: 0.08649 test_loss: 0.09364 \n",
      "[ 39/500] train_loss: 0.08167 valid_loss: 0.08599 test_loss: 0.09474 \n",
      "[ 40/500] train_loss: 0.07936 valid_loss: 0.08705 test_loss: 0.09243 \n",
      "[ 41/500] train_loss: 0.08055 valid_loss: 0.08511 test_loss: 0.09162 \n",
      "验证损失减少 (0.085642 --> 0.085110). 正在保存模型...\n",
      "[ 42/500] train_loss: 0.07829 valid_loss: 0.08477 test_loss: 0.09173 \n",
      "验证损失减少 (0.085110 --> 0.084771). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.07795 valid_loss: 0.08590 test_loss: 0.09210 \n",
      "[ 44/500] train_loss: 0.07845 valid_loss: 0.08404 test_loss: 0.09207 \n",
      "验证损失减少 (0.084771 --> 0.084036). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.07755 valid_loss: 0.08399 test_loss: 0.09339 \n",
      "验证损失减少 (0.084036 --> 0.083995). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07684 valid_loss: 0.08343 test_loss: 0.09065 \n",
      "验证损失减少 (0.083995 --> 0.083434). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07445 valid_loss: 0.08456 test_loss: 0.09046 \n",
      "[ 48/500] train_loss: 0.07559 valid_loss: 0.08577 test_loss: 0.09576 \n",
      "[ 49/500] train_loss: 0.07674 valid_loss: 0.08427 test_loss: 0.09028 \n",
      "[ 50/500] train_loss: 0.07671 valid_loss: 0.08256 test_loss: 0.09134 \n",
      "验证损失减少 (0.083434 --> 0.082560). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.07405 valid_loss: 0.08373 test_loss: 0.09046 \n",
      "[ 52/500] train_loss: 0.07298 valid_loss: 0.08430 test_loss: 0.09035 \n",
      "[ 53/500] train_loss: 0.07351 valid_loss: 0.08221 test_loss: 0.09028 \n",
      "验证损失减少 (0.082560 --> 0.082207). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07408 valid_loss: 0.08115 test_loss: 0.09098 \n",
      "验证损失减少 (0.082207 --> 0.081151). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07287 valid_loss: 0.08143 test_loss: 0.08962 \n",
      "[ 56/500] train_loss: 0.07401 valid_loss: 0.08071 test_loss: 0.09093 \n",
      "验证损失减少 (0.081151 --> 0.080710). 正在保存模型...\n",
      "[ 57/500] train_loss: 0.07393 valid_loss: 0.08035 test_loss: 0.08797 \n",
      "验证损失减少 (0.080710 --> 0.080345). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.07159 valid_loss: 0.08030 test_loss: 0.08705 \n",
      "验证损失减少 (0.080345 --> 0.080301). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07009 valid_loss: 0.07999 test_loss: 0.08680 \n",
      "验证损失减少 (0.080301 --> 0.079989). 正在保存模型...\n",
      "[ 60/500] train_loss: 0.07229 valid_loss: 0.07970 test_loss: 0.08645 \n",
      "验证损失减少 (0.079989 --> 0.079705). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.07423 valid_loss: 0.08025 test_loss: 0.08968 \n",
      "[ 62/500] train_loss: 0.07036 valid_loss: 0.07975 test_loss: 0.08748 \n",
      "[ 63/500] train_loss: 0.06911 valid_loss: 0.07867 test_loss: 0.08705 \n",
      "验证损失减少 (0.079705 --> 0.078674). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.06821 valid_loss: 0.07850 test_loss: 0.08683 \n",
      "验证损失减少 (0.078674 --> 0.078496). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07237 valid_loss: 0.07904 test_loss: 0.08677 \n",
      "[ 66/500] train_loss: 0.07026 valid_loss: 0.07947 test_loss: 0.08716 \n",
      "[ 67/500] train_loss: 0.07029 valid_loss: 0.07949 test_loss: 0.08837 \n",
      "[ 68/500] train_loss: 0.06938 valid_loss: 0.08128 test_loss: 0.08746 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 69/500] train_loss: 0.07060 valid_loss: 0.07876 test_loss: 0.08645 \n",
      "[ 70/500] train_loss: 0.07010 valid_loss: 0.07968 test_loss: 0.08599 \n",
      "[ 71/500] train_loss: 0.07048 valid_loss: 0.07811 test_loss: 0.08627 \n",
      "验证损失减少 (0.078496 --> 0.078106). 正在保存模型...\n",
      "[ 72/500] train_loss: 0.06930 valid_loss: 0.07935 test_loss: 0.08606 \n",
      "[ 73/500] train_loss: 0.06759 valid_loss: 0.07668 test_loss: 0.08608 \n",
      "验证损失减少 (0.078106 --> 0.076685). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.06843 valid_loss: 0.07843 test_loss: 0.08614 \n",
      "[ 75/500] train_loss: 0.06900 valid_loss: 0.07947 test_loss: 0.08620 \n",
      "[ 76/500] train_loss: 0.06562 valid_loss: 0.08058 test_loss: 0.08645 \n",
      "[ 77/500] train_loss: 0.06680 valid_loss: 0.08198 test_loss: 0.08536 \n",
      "[ 78/500] train_loss: 0.06859 valid_loss: 0.07781 test_loss: 0.08477 \n",
      "[ 79/500] train_loss: 0.06758 valid_loss: 0.07832 test_loss: 0.08572 \n",
      "[ 80/500] train_loss: 0.06902 valid_loss: 0.07754 test_loss: 0.08379 \n",
      "[ 81/500] train_loss: 0.07003 valid_loss: 0.07716 test_loss: 0.08456 \n",
      "[ 82/500] train_loss: 0.06767 valid_loss: 0.07751 test_loss: 0.08481 \n",
      "[ 83/500] train_loss: 0.06718 valid_loss: 0.07699 test_loss: 0.08614 \n",
      "[ 84/500] train_loss: 0.06756 valid_loss: 0.07736 test_loss: 0.08561 \n",
      "[ 85/500] train_loss: 0.06629 valid_loss: 0.07736 test_loss: 0.08427 \n",
      "[ 86/500] train_loss: 0.06597 valid_loss: 0.07846 test_loss: 0.08441 \n",
      "[ 87/500] train_loss: 0.06652 valid_loss: 0.07805 test_loss: 0.08554 \n",
      "[ 88/500] train_loss: 0.06638 valid_loss: 0.07847 test_loss: 0.08531 \n",
      "[ 89/500] train_loss: 0.06544 valid_loss: 0.07556 test_loss: 0.08407 \n",
      "验证损失减少 (0.076685 --> 0.075560). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.06561 valid_loss: 0.07540 test_loss: 0.08359 \n",
      "验证损失减少 (0.075560 --> 0.075399). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.06507 valid_loss: 0.07673 test_loss: 0.08589 \n",
      "[ 92/500] train_loss: 0.06591 valid_loss: 0.07682 test_loss: 0.08320 \n",
      "[ 93/500] train_loss: 0.06532 valid_loss: 0.07969 test_loss: 0.08287 \n",
      "[ 94/500] train_loss: 0.06427 valid_loss: 0.07608 test_loss: 0.08298 \n",
      "[ 95/500] train_loss: 0.06359 valid_loss: 0.07783 test_loss: 0.08348 \n",
      "[ 96/500] train_loss: 0.06567 valid_loss: 0.07643 test_loss: 0.08240 \n",
      "[ 97/500] train_loss: 0.06425 valid_loss: 0.07637 test_loss: 0.08160 \n",
      "[ 98/500] train_loss: 0.06374 valid_loss: 0.07712 test_loss: 0.08305 \n",
      "[ 99/500] train_loss: 0.06445 valid_loss: 0.07757 test_loss: 0.08435 \n",
      "[100/500] train_loss: 0.06417 valid_loss: 0.07450 test_loss: 0.08196 \n",
      "验证损失减少 (0.075399 --> 0.074501). 正在保存模型...\n",
      "[101/500] train_loss: 0.06448 valid_loss: 0.07787 test_loss: 0.08205 \n",
      "[102/500] train_loss: 0.06294 valid_loss: 0.07642 test_loss: 0.08220 \n",
      "[103/500] train_loss: 0.06200 valid_loss: 0.07406 test_loss: 0.08177 \n",
      "验证损失减少 (0.074501 --> 0.074060). 正在保存模型...\n",
      "[104/500] train_loss: 0.06423 valid_loss: 0.07759 test_loss: 0.08392 \n",
      "[105/500] train_loss: 0.06196 valid_loss: 0.07528 test_loss: 0.08275 \n",
      "[106/500] train_loss: 0.06345 valid_loss: 0.07623 test_loss: 0.08138 \n",
      "[107/500] train_loss: 0.06329 valid_loss: 0.07564 test_loss: 0.08131 \n",
      "[108/500] train_loss: 0.06335 valid_loss: 0.07731 test_loss: 0.08393 \n",
      "[109/500] train_loss: 0.06293 valid_loss: 0.07547 test_loss: 0.08416 \n",
      "[110/500] train_loss: 0.06331 valid_loss: 0.07554 test_loss: 0.08228 \n",
      "[111/500] train_loss: 0.06152 valid_loss: 0.07318 test_loss: 0.08148 \n",
      "验证损失减少 (0.074060 --> 0.073179). 正在保存模型...\n",
      "[112/500] train_loss: 0.06232 valid_loss: 0.07556 test_loss: 0.08251 \n",
      "[113/500] train_loss: 0.06148 valid_loss: 0.07596 test_loss: 0.08368 \n",
      "[114/500] train_loss: 0.06257 valid_loss: 0.07546 test_loss: 0.08084 \n",
      "[115/500] train_loss: 0.06180 valid_loss: 0.07647 test_loss: 0.08223 \n",
      "[116/500] train_loss: 0.06141 valid_loss: 0.07617 test_loss: 0.08035 \n",
      "[117/500] train_loss: 0.06209 valid_loss: 0.07450 test_loss: 0.08065 \n",
      "[118/500] train_loss: 0.06004 valid_loss: 0.07430 test_loss: 0.08083 \n",
      "[119/500] train_loss: 0.06106 valid_loss: 0.07567 test_loss: 0.08120 \n",
      "[120/500] train_loss: 0.06225 valid_loss: 0.07469 test_loss: 0.08233 \n",
      "[121/500] train_loss: 0.06056 valid_loss: 0.07638 test_loss: 0.08214 \n",
      "[122/500] train_loss: 0.05939 valid_loss: 0.07628 test_loss: 0.07995 \n",
      "[123/500] train_loss: 0.06011 valid_loss: 0.07466 test_loss: 0.08019 \n",
      "[124/500] train_loss: 0.05970 valid_loss: 0.07532 test_loss: 0.08063 \n",
      "[125/500] train_loss: 0.06162 valid_loss: 0.07463 test_loss: 0.08027 \n",
      "[126/500] train_loss: 0.06005 valid_loss: 0.07303 test_loss: 0.08035 \n",
      "验证损失减少 (0.073179 --> 0.073033). 正在保存模型...\n",
      "[127/500] train_loss: 0.05951 valid_loss: 0.07484 test_loss: 0.08294 \n",
      "[128/500] train_loss: 0.05957 valid_loss: 0.07341 test_loss: 0.08100 \n",
      "[129/500] train_loss: 0.05888 valid_loss: 0.07363 test_loss: 0.08340 \n",
      "[130/500] train_loss: 0.05862 valid_loss: 0.07492 test_loss: 0.08234 \n",
      "[131/500] train_loss: 0.05987 valid_loss: 0.07562 test_loss: 0.07995 \n",
      "[132/500] train_loss: 0.05791 valid_loss: 0.07414 test_loss: 0.08090 \n",
      "[133/500] train_loss: 0.05953 valid_loss: 0.07397 test_loss: 0.08166 \n",
      "[134/500] train_loss: 0.05912 valid_loss: 0.07384 test_loss: 0.08128 \n",
      "[135/500] train_loss: 0.05787 valid_loss: 0.07486 test_loss: 0.08419 \n",
      "[136/500] train_loss: 0.05839 valid_loss: 0.07521 test_loss: 0.08075 \n",
      "[137/500] train_loss: 0.05973 valid_loss: 0.07592 test_loss: 0.08221 \n",
      "[138/500] train_loss: 0.05738 valid_loss: 0.07688 test_loss: 0.08114 \n",
      "[139/500] train_loss: 0.05858 valid_loss: 0.07661 test_loss: 0.08012 \n",
      "[140/500] train_loss: 0.05916 valid_loss: 0.07521 test_loss: 0.08019 \n",
      "[141/500] train_loss: 0.05567 valid_loss: 0.07619 test_loss: 0.08226 \n",
      "[142/500] train_loss: 0.05827 valid_loss: 0.07561 test_loss: 0.08021 \n",
      "[143/500] train_loss: 0.05733 valid_loss: 0.07444 test_loss: 0.08000 \n",
      "[144/500] train_loss: 0.05697 valid_loss: 0.07617 test_loss: 0.08072 \n",
      "[145/500] train_loss: 0.05803 valid_loss: 0.07535 test_loss: 0.08217 \n",
      "[146/500] train_loss: 0.05618 valid_loss: 0.07394 test_loss: 0.07941 \n",
      "[147/500] train_loss: 0.05797 valid_loss: 0.07566 test_loss: 0.08089 \n",
      "[148/500] train_loss: 0.05793 valid_loss: 0.07399 test_loss: 0.07902 \n",
      "[149/500] train_loss: 0.05789 valid_loss: 0.07539 test_loss: 0.08114 \n",
      "[150/500] train_loss: 0.05757 valid_loss: 0.07473 test_loss: 0.08077 \n",
      "[151/500] train_loss: 0.05682 valid_loss: 0.07489 test_loss: 0.08069 \n",
      "[152/500] train_loss: 0.05696 valid_loss: 0.07467 test_loss: 0.08018 \n",
      "[153/500] train_loss: 0.05645 valid_loss: 0.07435 test_loss: 0.08096 \n",
      "[154/500] train_loss: 0.05701 valid_loss: 0.07374 test_loss: 0.08009 \n",
      "[155/500] train_loss: 0.05739 valid_loss: 0.07369 test_loss: 0.07969 \n",
      "[156/500] train_loss: 0.05560 valid_loss: 0.07424 test_loss: 0.07968 \n",
      "[157/500] train_loss: 0.05640 valid_loss: 0.07354 test_loss: 0.08092 \n",
      "[158/500] train_loss: 0.05781 valid_loss: 0.07280 test_loss: 0.07956 \n",
      "验证损失减少 (0.073033 --> 0.072805). 正在保存模型...\n",
      "[159/500] train_loss: 0.05726 valid_loss: 0.07220 test_loss: 0.07953 \n",
      "验证损失减少 (0.072805 --> 0.072195). 正在保存模型...\n",
      "[160/500] train_loss: 0.05703 valid_loss: 0.07221 test_loss: 0.07912 \n",
      "[161/500] train_loss: 0.05528 valid_loss: 0.07243 test_loss: 0.07949 \n",
      "[162/500] train_loss: 0.05549 valid_loss: 0.07371 test_loss: 0.07982 \n",
      "[163/500] train_loss: 0.05677 valid_loss: 0.07352 test_loss: 0.08131 \n",
      "[164/500] train_loss: 0.05605 valid_loss: 0.07317 test_loss: 0.07990 \n",
      "[165/500] train_loss: 0.05461 valid_loss: 0.07290 test_loss: 0.08057 \n",
      "[166/500] train_loss: 0.05553 valid_loss: 0.07151 test_loss: 0.08083 \n",
      "验证损失减少 (0.072195 --> 0.071514). 正在保存模型...\n",
      "[167/500] train_loss: 0.05472 valid_loss: 0.07154 test_loss: 0.08095 \n",
      "[168/500] train_loss: 0.05332 valid_loss: 0.07319 test_loss: 0.08022 \n",
      "[169/500] train_loss: 0.05472 valid_loss: 0.07235 test_loss: 0.08017 \n",
      "[170/500] train_loss: 0.05506 valid_loss: 0.07275 test_loss: 0.08155 \n",
      "[171/500] train_loss: 0.05435 valid_loss: 0.07201 test_loss: 0.07917 \n",
      "[172/500] train_loss: 0.05448 valid_loss: 0.07115 test_loss: 0.07878 \n",
      "验证损失减少 (0.071514 --> 0.071149). 正在保存模型...\n",
      "[173/500] train_loss: 0.05624 valid_loss: 0.07345 test_loss: 0.08089 \n",
      "[174/500] train_loss: 0.05711 valid_loss: 0.07224 test_loss: 0.07941 \n",
      "[175/500] train_loss: 0.05491 valid_loss: 0.07271 test_loss: 0.07909 \n",
      "[176/500] train_loss: 0.05490 valid_loss: 0.07275 test_loss: 0.07936 \n",
      "[177/500] train_loss: 0.05595 valid_loss: 0.07034 test_loss: 0.07859 \n",
      "验证损失减少 (0.071149 --> 0.070340). 正在保存模型...\n",
      "[178/500] train_loss: 0.05571 valid_loss: 0.07146 test_loss: 0.07806 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179/500] train_loss: 0.05535 valid_loss: 0.07133 test_loss: 0.07848 \n",
      "[180/500] train_loss: 0.05479 valid_loss: 0.07787 test_loss: 0.08020 \n",
      "[181/500] train_loss: 0.05552 valid_loss: 0.07259 test_loss: 0.07946 \n",
      "[182/500] train_loss: 0.05540 valid_loss: 0.07270 test_loss: 0.07997 \n",
      "[183/500] train_loss: 0.05455 valid_loss: 0.07112 test_loss: 0.07853 \n",
      "[184/500] train_loss: 0.05403 valid_loss: 0.07190 test_loss: 0.07806 \n",
      "[185/500] train_loss: 0.05394 valid_loss: 0.07331 test_loss: 0.07978 \n",
      "[186/500] train_loss: 0.05442 valid_loss: 0.07528 test_loss: 0.07872 \n",
      "[187/500] train_loss: 0.05321 valid_loss: 0.07515 test_loss: 0.07877 \n",
      "[188/500] train_loss: 0.05202 valid_loss: 0.07249 test_loss: 0.07878 \n",
      "[189/500] train_loss: 0.05414 valid_loss: 0.07258 test_loss: 0.08134 \n",
      "[190/500] train_loss: 0.05525 valid_loss: 0.07224 test_loss: 0.07889 \n",
      "[191/500] train_loss: 0.05229 valid_loss: 0.07157 test_loss: 0.07945 \n",
      "[192/500] train_loss: 0.05350 valid_loss: 0.07175 test_loss: 0.07773 \n",
      "[193/500] train_loss: 0.05293 valid_loss: 0.07234 test_loss: 0.07952 \n",
      "[194/500] train_loss: 0.05338 valid_loss: 0.07358 test_loss: 0.07905 \n",
      "[195/500] train_loss: 0.05283 valid_loss: 0.07164 test_loss: 0.07927 \n",
      "[196/500] train_loss: 0.05324 valid_loss: 0.07186 test_loss: 0.07804 \n",
      "[197/500] train_loss: 0.05188 valid_loss: 0.07229 test_loss: 0.07741 \n",
      "[198/500] train_loss: 0.05358 valid_loss: 0.07291 test_loss: 0.07904 \n",
      "[199/500] train_loss: 0.05165 valid_loss: 0.07261 test_loss: 0.07835 \n",
      "[200/500] train_loss: 0.05231 valid_loss: 0.07264 test_loss: 0.07776 \n",
      "[201/500] train_loss: 0.05163 valid_loss: 0.07238 test_loss: 0.07930 \n",
      "[202/500] train_loss: 0.05393 valid_loss: 0.07318 test_loss: 0.08042 \n",
      "[203/500] train_loss: 0.05212 valid_loss: 0.07144 test_loss: 0.07977 \n",
      "[204/500] train_loss: 0.05294 valid_loss: 0.07288 test_loss: 0.07985 \n",
      "[205/500] train_loss: 0.05206 valid_loss: 0.07223 test_loss: 0.07998 \n",
      "[206/500] train_loss: 0.05220 valid_loss: 0.07168 test_loss: 0.07787 \n",
      "[207/500] train_loss: 0.05342 valid_loss: 0.07252 test_loss: 0.07747 \n",
      "[208/500] train_loss: 0.05147 valid_loss: 0.07282 test_loss: 0.07895 \n",
      "[209/500] train_loss: 0.05086 valid_loss: 0.07227 test_loss: 0.07812 \n",
      "[210/500] train_loss: 0.05158 valid_loss: 0.07234 test_loss: 0.07949 \n",
      "[211/500] train_loss: 0.05161 valid_loss: 0.07145 test_loss: 0.07930 \n",
      "[212/500] train_loss: 0.05093 valid_loss: 0.07237 test_loss: 0.07880 \n",
      "[213/500] train_loss: 0.05088 valid_loss: 0.07212 test_loss: 0.07793 \n",
      "[214/500] train_loss: 0.05341 valid_loss: 0.07155 test_loss: 0.07851 \n",
      "[215/500] train_loss: 0.05297 valid_loss: 0.07326 test_loss: 0.07936 \n",
      "[216/500] train_loss: 0.05075 valid_loss: 0.07258 test_loss: 0.07927 \n",
      "[217/500] train_loss: 0.05354 valid_loss: 0.07198 test_loss: 0.07951 \n",
      "[218/500] train_loss: 0.05131 valid_loss: 0.07355 test_loss: 0.07991 \n",
      "[219/500] train_loss: 0.05155 valid_loss: 0.07254 test_loss: 0.07920 \n",
      "[220/500] train_loss: 0.05065 valid_loss: 0.07341 test_loss: 0.08321 \n",
      "[221/500] train_loss: 0.05161 valid_loss: 0.07334 test_loss: 0.07996 \n",
      "[222/500] train_loss: 0.05031 valid_loss: 0.07374 test_loss: 0.08144 \n",
      "[223/500] train_loss: 0.05068 valid_loss: 0.07256 test_loss: 0.08073 \n",
      "[224/500] train_loss: 0.04992 valid_loss: 0.07196 test_loss: 0.07949 \n",
      "[225/500] train_loss: 0.04978 valid_loss: 0.07355 test_loss: 0.08075 \n",
      "[226/500] train_loss: 0.05111 valid_loss: 0.07230 test_loss: 0.08004 \n",
      "[227/500] train_loss: 0.05005 valid_loss: 0.07405 test_loss: 0.08028 \n",
      "[228/500] train_loss: 0.05027 valid_loss: 0.07231 test_loss: 0.07933 \n",
      "[229/500] train_loss: 0.05033 valid_loss: 0.07329 test_loss: 0.07842 \n",
      "[230/500] train_loss: 0.05184 valid_loss: 0.07122 test_loss: 0.07818 \n",
      "[231/500] train_loss: 0.05022 valid_loss: 0.07320 test_loss: 0.07933 \n",
      "[232/500] train_loss: 0.05100 valid_loss: 0.07190 test_loss: 0.07889 \n",
      "[233/500] train_loss: 0.04840 valid_loss: 0.07434 test_loss: 0.07945 \n",
      "[234/500] train_loss: 0.04852 valid_loss: 0.07577 test_loss: 0.07896 \n",
      "[235/500] train_loss: 0.04977 valid_loss: 0.07390 test_loss: 0.07909 \n",
      "[236/500] train_loss: 0.04879 valid_loss: 0.07766 test_loss: 0.08079 \n",
      "[237/500] train_loss: 0.05012 valid_loss: 0.07196 test_loss: 0.08214 \n",
      "[238/500] train_loss: 0.04854 valid_loss: 0.07291 test_loss: 0.07853 \n",
      "[239/500] train_loss: 0.05004 valid_loss: 0.07362 test_loss: 0.08126 \n",
      "[240/500] train_loss: 0.04957 valid_loss: 0.07206 test_loss: 0.07958 \n",
      "[241/500] train_loss: 0.04906 valid_loss: 0.07397 test_loss: 0.08032 \n",
      "[242/500] train_loss: 0.05006 valid_loss: 0.07223 test_loss: 0.07987 \n",
      "[243/500] train_loss: 0.05030 valid_loss: 0.07621 test_loss: 0.08179 \n",
      "[244/500] train_loss: 0.04997 valid_loss: 0.07224 test_loss: 0.07975 \n",
      "[245/500] train_loss: 0.04942 valid_loss: 0.07188 test_loss: 0.07948 \n",
      "[246/500] train_loss: 0.05004 valid_loss: 0.07215 test_loss: 0.07915 \n",
      "[247/500] train_loss: 0.04950 valid_loss: 0.07191 test_loss: 0.07912 \n",
      "[248/500] train_loss: 0.04874 valid_loss: 0.07263 test_loss: 0.08072 \n",
      "[249/500] train_loss: 0.04872 valid_loss: 0.07137 test_loss: 0.07898 \n",
      "[250/500] train_loss: 0.04802 valid_loss: 0.07169 test_loss: 0.07937 \n",
      "[251/500] train_loss: 0.04939 valid_loss: 0.07296 test_loss: 0.08082 \n",
      "[252/500] train_loss: 0.04733 valid_loss: 0.07061 test_loss: 0.07949 \n",
      "[253/500] train_loss: 0.04935 valid_loss: 0.07149 test_loss: 0.07911 \n",
      "[254/500] train_loss: 0.04958 valid_loss: 0.07287 test_loss: 0.07938 \n",
      "[255/500] train_loss: 0.04779 valid_loss: 0.07282 test_loss: 0.08099 \n",
      "[256/500] train_loss: 0.04814 valid_loss: 0.07248 test_loss: 0.07975 \n",
      "[257/500] train_loss: 0.04913 valid_loss: 0.07116 test_loss: 0.08038 \n",
      "[258/500] train_loss: 0.04865 valid_loss: 0.07102 test_loss: 0.07890 \n",
      "[259/500] train_loss: 0.04826 valid_loss: 0.07245 test_loss: 0.08006 \n",
      "[260/500] train_loss: 0.04781 valid_loss: 0.07215 test_loss: 0.07865 \n",
      "[261/500] train_loss: 0.04662 valid_loss: 0.07177 test_loss: 0.07922 \n",
      "[262/500] train_loss: 0.04972 valid_loss: 0.07564 test_loss: 0.07845 \n",
      "[263/500] train_loss: 0.05021 valid_loss: 0.07213 test_loss: 0.07813 \n",
      "[264/500] train_loss: 0.04923 valid_loss: 0.07185 test_loss: 0.07756 \n",
      "[265/500] train_loss: 0.04840 valid_loss: 0.07144 test_loss: 0.07911 \n",
      "[266/500] train_loss: 0.04808 valid_loss: 0.07295 test_loss: 0.07920 \n",
      "[267/500] train_loss: 0.04650 valid_loss: 0.07193 test_loss: 0.07833 \n",
      "[268/500] train_loss: 0.04858 valid_loss: 0.07456 test_loss: 0.07905 \n",
      "[269/500] train_loss: 0.04840 valid_loss: 0.07301 test_loss: 0.07901 \n",
      "[270/500] train_loss: 0.04625 valid_loss: 0.07328 test_loss: 0.08064 \n",
      "[271/500] train_loss: 0.04728 valid_loss: 0.07402 test_loss: 0.07956 \n",
      "[272/500] train_loss: 0.04728 valid_loss: 0.07267 test_loss: 0.07912 \n",
      "[273/500] train_loss: 0.04690 valid_loss: 0.07241 test_loss: 0.08039 \n",
      "[274/500] train_loss: 0.04782 valid_loss: 0.07399 test_loss: 0.07885 \n",
      "[275/500] train_loss: 0.04753 valid_loss: 0.07271 test_loss: 0.07937 \n",
      "[276/500] train_loss: 0.04923 valid_loss: 0.07195 test_loss: 0.07919 \n",
      "[277/500] train_loss: 0.04845 valid_loss: 0.07720 test_loss: 0.07900 \n",
      "[278/500] train_loss: 0.04772 valid_loss: 0.07238 test_loss: 0.07957 \n",
      "[279/500] train_loss: 0.04721 valid_loss: 0.07156 test_loss: 0.07808 \n",
      "[280/500] train_loss: 0.04771 valid_loss: 0.07268 test_loss: 0.07856 \n",
      "[281/500] train_loss: 0.04876 valid_loss: 0.07195 test_loss: 0.07895 \n",
      "[282/500] train_loss: 0.04597 valid_loss: 0.07235 test_loss: 0.07963 \n",
      "[283/500] train_loss: 0.04748 valid_loss: 0.07387 test_loss: 0.08005 \n",
      "[284/500] train_loss: 0.04700 valid_loss: 0.07372 test_loss: 0.08000 \n",
      "[285/500] train_loss: 0.04701 valid_loss: 0.07274 test_loss: 0.08108 \n",
      "[286/500] train_loss: 0.04669 valid_loss: 0.07402 test_loss: 0.08086 \n",
      "[287/500] train_loss: 0.04714 valid_loss: 0.07133 test_loss: 0.07849 \n",
      "[288/500] train_loss: 0.04689 valid_loss: 0.07156 test_loss: 0.07800 \n",
      "[289/500] train_loss: 0.04666 valid_loss: 0.07081 test_loss: 0.08038 \n",
      "[290/500] train_loss: 0.04885 valid_loss: 0.07155 test_loss: 0.07957 \n",
      "[291/500] train_loss: 0.04748 valid_loss: 0.07307 test_loss: 0.08033 \n",
      "[292/500] train_loss: 0.04635 valid_loss: 0.07308 test_loss: 0.08108 \n",
      "[293/500] train_loss: 0.04633 valid_loss: 0.07348 test_loss: 0.08058 \n",
      "[294/500] train_loss: 0.04636 valid_loss: 0.07235 test_loss: 0.07968 \n",
      "[295/500] train_loss: 0.04713 valid_loss: 0.07230 test_loss: 0.07949 \n",
      "[296/500] train_loss: 0.04627 valid_loss: 0.07239 test_loss: 0.08083 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[297/500] train_loss: 0.04622 valid_loss: 0.07288 test_loss: 0.08020 \n",
      "[298/500] train_loss: 0.04516 valid_loss: 0.07285 test_loss: 0.08118 \n",
      "[299/500] train_loss: 0.04740 valid_loss: 0.07424 test_loss: 0.08016 \n",
      "[300/500] train_loss: 0.04645 valid_loss: 0.07379 test_loss: 0.08019 \n",
      "[301/500] train_loss: 0.04699 valid_loss: 0.07210 test_loss: 0.07885 \n",
      "[302/500] train_loss: 0.04644 valid_loss: 0.07216 test_loss: 0.08035 \n",
      "[303/500] train_loss: 0.04515 valid_loss: 0.07211 test_loss: 0.07943 \n",
      "[304/500] train_loss: 0.04565 valid_loss: 0.07257 test_loss: 0.08050 \n",
      "[305/500] train_loss: 0.04533 valid_loss: 0.07206 test_loss: 0.08039 \n",
      "[306/500] train_loss: 0.04684 valid_loss: 0.07176 test_loss: 0.07938 \n",
      "[307/500] train_loss: 0.04596 valid_loss: 0.07200 test_loss: 0.08032 \n",
      "[308/500] train_loss: 0.04691 valid_loss: 0.07222 test_loss: 0.07938 \n",
      "[309/500] train_loss: 0.04585 valid_loss: 0.07293 test_loss: 0.08137 \n",
      "[310/500] train_loss: 0.04573 valid_loss: 0.07171 test_loss: 0.07946 \n",
      "[311/500] train_loss: 0.04572 valid_loss: 0.07260 test_loss: 0.07988 \n",
      "[312/500] train_loss: 0.04671 valid_loss: 0.07197 test_loss: 0.07924 \n",
      "[313/500] train_loss: 0.04617 valid_loss: 0.07165 test_loss: 0.07948 \n",
      "[314/500] train_loss: 0.04458 valid_loss: 0.07094 test_loss: 0.07971 \n",
      "[315/500] train_loss: 0.04684 valid_loss: 0.07231 test_loss: 0.07934 \n",
      "[316/500] train_loss: 0.04484 valid_loss: 0.07161 test_loss: 0.08034 \n",
      "[317/500] train_loss: 0.04613 valid_loss: 0.07378 test_loss: 0.08151 \n",
      "[318/500] train_loss: 0.04546 valid_loss: 0.07129 test_loss: 0.08035 \n",
      "[319/500] train_loss: 0.04588 valid_loss: 0.07105 test_loss: 0.08008 \n",
      "[320/500] train_loss: 0.04598 valid_loss: 0.07585 test_loss: 0.07919 \n",
      "[321/500] train_loss: 0.04543 valid_loss: 0.07674 test_loss: 0.07930 \n",
      "[322/500] train_loss: 0.04634 valid_loss: 0.07146 test_loss: 0.07975 \n",
      "[323/500] train_loss: 0.04440 valid_loss: 0.07259 test_loss: 0.08062 \n",
      "[324/500] train_loss: 0.04492 valid_loss: 0.07260 test_loss: 0.07921 \n",
      "[325/500] train_loss: 0.04548 valid_loss: 0.07565 test_loss: 0.08159 \n",
      "[326/500] train_loss: 0.04681 valid_loss: 0.07600 test_loss: 0.07879 \n",
      "[327/500] train_loss: 0.04490 valid_loss: 0.07319 test_loss: 0.07973 \n",
      "[328/500] train_loss: 0.04668 valid_loss: 0.07299 test_loss: 0.07907 \n",
      "[329/500] train_loss: 0.04554 valid_loss: 0.07448 test_loss: 0.07850 \n",
      "[330/500] train_loss: 0.04391 valid_loss: 0.08097 test_loss: 0.08076 \n",
      "[331/500] train_loss: 0.04526 valid_loss: 0.07655 test_loss: 0.07934 \n",
      "[332/500] train_loss: 0.04546 valid_loss: 0.07140 test_loss: 0.07860 \n",
      "[333/500] train_loss: 0.04546 valid_loss: 0.07468 test_loss: 0.07944 \n",
      "[334/500] train_loss: 0.04453 valid_loss: 0.07313 test_loss: 0.07947 \n",
      "[335/500] train_loss: 0.04468 valid_loss: 0.07475 test_loss: 0.07902 \n",
      "[336/500] train_loss: 0.04429 valid_loss: 0.07337 test_loss: 0.07916 \n",
      "[337/500] train_loss: 0.04453 valid_loss: 0.07221 test_loss: 0.07968 \n",
      "[338/500] train_loss: 0.04512 valid_loss: 0.07327 test_loss: 0.08220 \n",
      "[339/500] train_loss: 0.04408 valid_loss: 0.07422 test_loss: 0.07973 \n",
      "[340/500] train_loss: 0.04425 valid_loss: 0.07236 test_loss: 0.07965 \n",
      "[341/500] train_loss: 0.04371 valid_loss: 0.07417 test_loss: 0.08097 \n",
      "[342/500] train_loss: 0.04319 valid_loss: 0.07311 test_loss: 0.07942 \n",
      "[343/500] train_loss: 0.04505 valid_loss: 0.07207 test_loss: 0.07944 \n",
      "[344/500] train_loss: 0.04427 valid_loss: 0.07473 test_loss: 0.07966 \n",
      "[345/500] train_loss: 0.04457 valid_loss: 0.07212 test_loss: 0.07965 \n",
      "[346/500] train_loss: 0.04549 valid_loss: 0.07455 test_loss: 0.08104 \n",
      "[347/500] train_loss: 0.04442 valid_loss: 0.07312 test_loss: 0.07844 \n",
      "[348/500] train_loss: 0.04363 valid_loss: 0.07254 test_loss: 0.07978 \n",
      "[349/500] train_loss: 0.04512 valid_loss: 0.07215 test_loss: 0.07885 \n",
      "[350/500] train_loss: 0.04448 valid_loss: 0.07389 test_loss: 0.07844 \n",
      "[351/500] train_loss: 0.04543 valid_loss: 0.07304 test_loss: 0.08055 \n",
      "[352/500] train_loss: 0.04242 valid_loss: 0.07339 test_loss: 0.07976 \n",
      "[353/500] train_loss: 0.04336 valid_loss: 0.07292 test_loss: 0.07999 \n",
      "[354/500] train_loss: 0.04356 valid_loss: 0.07259 test_loss: 0.07973 \n",
      "[355/500] train_loss: 0.04329 valid_loss: 0.07217 test_loss: 0.08005 \n",
      "[356/500] train_loss: 0.04465 valid_loss: 0.07153 test_loss: 0.07903 \n",
      "[357/500] train_loss: 0.04438 valid_loss: 0.07446 test_loss: 0.07966 \n",
      "[358/500] train_loss: 0.04406 valid_loss: 0.07312 test_loss: 0.07917 \n",
      "[359/500] train_loss: 0.04358 valid_loss: 0.07301 test_loss: 0.08051 \n",
      "[360/500] train_loss: 0.04411 valid_loss: 0.07282 test_loss: 0.08149 \n",
      "[361/500] train_loss: 0.04379 valid_loss: 0.07664 test_loss: 0.07998 \n",
      "[362/500] train_loss: 0.04338 valid_loss: 0.07244 test_loss: 0.08114 \n",
      "[363/500] train_loss: 0.04443 valid_loss: 0.07108 test_loss: 0.08117 \n",
      "[364/500] train_loss: 0.04403 valid_loss: 0.07333 test_loss: 0.07925 \n",
      "[365/500] train_loss: 0.04384 valid_loss: 0.07229 test_loss: 0.07963 \n",
      "[366/500] train_loss: 0.04351 valid_loss: 0.07196 test_loss: 0.08007 \n",
      "[367/500] train_loss: 0.04177 valid_loss: 0.07557 test_loss: 0.08171 \n",
      "[368/500] train_loss: 0.04314 valid_loss: 0.07161 test_loss: 0.07966 \n",
      "[369/500] train_loss: 0.04317 valid_loss: 0.07124 test_loss: 0.08090 \n",
      "[370/500] train_loss: 0.04360 valid_loss: 0.07278 test_loss: 0.08140 \n",
      "[371/500] train_loss: 0.04375 valid_loss: 0.07261 test_loss: 0.08156 \n",
      "[372/500] train_loss: 0.04260 valid_loss: 0.07233 test_loss: 0.08054 \n",
      "[373/500] train_loss: 0.04342 valid_loss: 0.07343 test_loss: 0.07981 \n",
      "[374/500] train_loss: 0.04338 valid_loss: 0.07271 test_loss: 0.08021 \n",
      "[375/500] train_loss: 0.04262 valid_loss: 0.07311 test_loss: 0.08142 \n",
      "[376/500] train_loss: 0.04331 valid_loss: 0.07236 test_loss: 0.08007 \n",
      "[377/500] train_loss: 0.04357 valid_loss: 0.07366 test_loss: 0.07997 \n",
      "[378/500] train_loss: 0.04280 valid_loss: 0.07410 test_loss: 0.08259 \n",
      "[379/500] train_loss: 0.04445 valid_loss: 0.07495 test_loss: 0.08143 \n",
      "[380/500] train_loss: 0.04381 valid_loss: 0.07472 test_loss: 0.08044 \n",
      "[381/500] train_loss: 0.04414 valid_loss: 0.07318 test_loss: 0.08197 \n",
      "[382/500] train_loss: 0.04315 valid_loss: 0.07184 test_loss: 0.08180 \n",
      "[383/500] train_loss: 0.04174 valid_loss: 0.07465 test_loss: 0.08093 \n",
      "[384/500] train_loss: 0.04328 valid_loss: 0.07316 test_loss: 0.08078 \n",
      "[385/500] train_loss: 0.04301 valid_loss: 0.07491 test_loss: 0.08213 \n",
      "[386/500] train_loss: 0.04225 valid_loss: 0.07384 test_loss: 0.08130 \n",
      "[387/500] train_loss: 0.04351 valid_loss: 0.07410 test_loss: 0.08176 \n",
      "[388/500] train_loss: 0.04325 valid_loss: 0.07363 test_loss: 0.07939 \n",
      "[389/500] train_loss: 0.04289 valid_loss: 0.07427 test_loss: 0.07987 \n",
      "[390/500] train_loss: 0.04221 valid_loss: 0.07407 test_loss: 0.07968 \n",
      "[391/500] train_loss: 0.04367 valid_loss: 0.07235 test_loss: 0.08075 \n",
      "[392/500] train_loss: 0.04273 valid_loss: 0.07307 test_loss: 0.08008 \n",
      "[393/500] train_loss: 0.04351 valid_loss: 0.07268 test_loss: 0.07997 \n",
      "[394/500] train_loss: 0.04353 valid_loss: 0.07230 test_loss: 0.08017 \n",
      "[395/500] train_loss: 0.04271 valid_loss: 0.07656 test_loss: 0.07940 \n",
      "[396/500] train_loss: 0.04305 valid_loss: 0.07297 test_loss: 0.08029 \n",
      "[397/500] train_loss: 0.04298 valid_loss: 0.07180 test_loss: 0.07941 \n",
      "[398/500] train_loss: 0.04156 valid_loss: 0.07305 test_loss: 0.08153 \n",
      "[399/500] train_loss: 0.04253 valid_loss: 0.07194 test_loss: 0.08069 \n",
      "[400/500] train_loss: 0.04128 valid_loss: 0.07301 test_loss: 0.08171 \n",
      "[401/500] train_loss: 0.04312 valid_loss: 0.07392 test_loss: 0.08294 \n",
      "[402/500] train_loss: 0.04105 valid_loss: 0.07340 test_loss: 0.08103 \n",
      "[403/500] train_loss: 0.04248 valid_loss: 0.07442 test_loss: 0.08138 \n",
      "[404/500] train_loss: 0.04190 valid_loss: 0.07319 test_loss: 0.08120 \n",
      "[405/500] train_loss: 0.04253 valid_loss: 0.07371 test_loss: 0.08148 \n",
      "[406/500] train_loss: 0.04234 valid_loss: 0.07344 test_loss: 0.07989 \n",
      "[407/500] train_loss: 0.04262 valid_loss: 0.07359 test_loss: 0.08260 \n",
      "[408/500] train_loss: 0.04106 valid_loss: 0.07373 test_loss: 0.08100 \n",
      "[409/500] train_loss: 0.04217 valid_loss: 0.07285 test_loss: 0.08090 \n",
      "[410/500] train_loss: 0.04265 valid_loss: 0.07701 test_loss: 0.08014 \n",
      "[411/500] train_loss: 0.04189 valid_loss: 0.07301 test_loss: 0.08144 \n",
      "[412/500] train_loss: 0.04300 valid_loss: 0.07301 test_loss: 0.08087 \n",
      "[413/500] train_loss: 0.04294 valid_loss: 0.07236 test_loss: 0.08091 \n",
      "[414/500] train_loss: 0.04236 valid_loss: 0.07411 test_loss: 0.08009 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[415/500] train_loss: 0.04140 valid_loss: 0.07422 test_loss: 0.08056 \n",
      "[416/500] train_loss: 0.04196 valid_loss: 0.07386 test_loss: 0.08081 \n",
      "[417/500] train_loss: 0.04240 valid_loss: 0.07354 test_loss: 0.08070 \n",
      "[418/500] train_loss: 0.04159 valid_loss: 0.07371 test_loss: 0.08122 \n",
      "[419/500] train_loss: 0.04327 valid_loss: 0.07682 test_loss: 0.08157 \n",
      "[420/500] train_loss: 0.04188 valid_loss: 0.07337 test_loss: 0.08194 \n",
      "[421/500] train_loss: 0.04159 valid_loss: 0.07370 test_loss: 0.08119 \n",
      "[422/500] train_loss: 0.04122 valid_loss: 0.07340 test_loss: 0.08031 \n",
      "[423/500] train_loss: 0.04102 valid_loss: 0.07335 test_loss: 0.08198 \n",
      "[424/500] train_loss: 0.04184 valid_loss: 0.07422 test_loss: 0.08118 \n",
      "[425/500] train_loss: 0.04191 valid_loss: 0.07402 test_loss: 0.08203 \n",
      "[426/500] train_loss: 0.04270 valid_loss: 0.07444 test_loss: 0.08289 \n",
      "[427/500] train_loss: 0.04108 valid_loss: 0.07338 test_loss: 0.08294 \n",
      "[428/500] train_loss: 0.04224 valid_loss: 0.07267 test_loss: 0.08114 \n",
      "[429/500] train_loss: 0.04308 valid_loss: 0.07213 test_loss: 0.08029 \n",
      "[430/500] train_loss: 0.04112 valid_loss: 0.07197 test_loss: 0.08006 \n",
      "[431/500] train_loss: 0.04239 valid_loss: 0.07426 test_loss: 0.08280 \n",
      "[432/500] train_loss: 0.04114 valid_loss: 0.07429 test_loss: 0.08256 \n",
      "[433/500] train_loss: 0.04160 valid_loss: 0.07458 test_loss: 0.08286 \n",
      "[434/500] train_loss: 0.04179 valid_loss: 0.07352 test_loss: 0.08199 \n",
      "[435/500] train_loss: 0.04169 valid_loss: 0.07427 test_loss: 0.08177 \n",
      "[436/500] train_loss: 0.04366 valid_loss: 0.07307 test_loss: 0.08126 \n",
      "[437/500] train_loss: 0.04257 valid_loss: 0.07318 test_loss: 0.08223 \n",
      "[438/500] train_loss: 0.04136 valid_loss: 0.07409 test_loss: 0.08163 \n",
      "[439/500] train_loss: 0.04116 valid_loss: 0.07474 test_loss: 0.08182 \n",
      "[440/500] train_loss: 0.04193 valid_loss: 0.07401 test_loss: 0.08215 \n",
      "[441/500] train_loss: 0.04074 valid_loss: 0.07724 test_loss: 0.08559 \n",
      "[442/500] train_loss: 0.04083 valid_loss: 0.07701 test_loss: 0.08458 \n",
      "[443/500] train_loss: 0.04152 valid_loss: 0.07500 test_loss: 0.08213 \n",
      "[444/500] train_loss: 0.04066 valid_loss: 0.07587 test_loss: 0.08351 \n",
      "[445/500] train_loss: 0.04135 valid_loss: 0.07331 test_loss: 0.08075 \n",
      "[446/500] train_loss: 0.04220 valid_loss: 0.07367 test_loss: 0.08132 \n",
      "[447/500] train_loss: 0.04039 valid_loss: 0.07360 test_loss: 0.08230 \n",
      "[448/500] train_loss: 0.04122 valid_loss: 0.07356 test_loss: 0.08085 \n",
      "[449/500] train_loss: 0.04085 valid_loss: 0.07414 test_loss: 0.08207 \n",
      "[450/500] train_loss: 0.04117 valid_loss: 0.07454 test_loss: 0.08241 \n",
      "[451/500] train_loss: 0.04135 valid_loss: 0.07736 test_loss: 0.08204 \n",
      "[452/500] train_loss: 0.04001 valid_loss: 0.07586 test_loss: 0.08354 \n",
      "[453/500] train_loss: 0.04016 valid_loss: 0.07870 test_loss: 0.08555 \n",
      "[454/500] train_loss: 0.04089 valid_loss: 0.07566 test_loss: 0.08333 \n",
      "[455/500] train_loss: 0.04024 valid_loss: 0.07709 test_loss: 0.08338 \n",
      "[456/500] train_loss: 0.04168 valid_loss: 0.07351 test_loss: 0.08152 \n",
      "[457/500] train_loss: 0.03987 valid_loss: 0.07489 test_loss: 0.08225 \n",
      "[458/500] train_loss: 0.04053 valid_loss: 0.07423 test_loss: 0.08218 \n",
      "[459/500] train_loss: 0.04004 valid_loss: 0.07583 test_loss: 0.08216 \n",
      "[460/500] train_loss: 0.04087 valid_loss: 0.07510 test_loss: 0.08131 \n",
      "[461/500] train_loss: 0.04133 valid_loss: 0.07410 test_loss: 0.08237 \n",
      "[462/500] train_loss: 0.04159 valid_loss: 0.07410 test_loss: 0.08140 \n",
      "[463/500] train_loss: 0.04054 valid_loss: 0.07402 test_loss: 0.08183 \n",
      "[464/500] train_loss: 0.04173 valid_loss: 0.07571 test_loss: 0.08327 \n",
      "[465/500] train_loss: 0.04000 valid_loss: 0.07407 test_loss: 0.08251 \n",
      "[466/500] train_loss: 0.04078 valid_loss: 0.07595 test_loss: 0.08243 \n",
      "[467/500] train_loss: 0.04039 valid_loss: 0.07380 test_loss: 0.08210 \n",
      "[468/500] train_loss: 0.04036 valid_loss: 0.07515 test_loss: 0.08316 \n",
      "[469/500] train_loss: 0.04121 valid_loss: 0.07543 test_loss: 0.08210 \n",
      "[470/500] train_loss: 0.04149 valid_loss: 0.07287 test_loss: 0.08029 \n",
      "[471/500] train_loss: 0.03992 valid_loss: 0.07430 test_loss: 0.08428 \n",
      "[472/500] train_loss: 0.04084 valid_loss: 0.07512 test_loss: 0.08189 \n",
      "[473/500] train_loss: 0.04041 valid_loss: 0.07375 test_loss: 0.08201 \n",
      "[474/500] train_loss: 0.04101 valid_loss: 0.07477 test_loss: 0.08191 \n",
      "[475/500] train_loss: 0.04096 valid_loss: 0.07456 test_loss: 0.08212 \n",
      "[476/500] train_loss: 0.04013 valid_loss: 0.07418 test_loss: 0.08246 \n",
      "[477/500] train_loss: 0.04032 valid_loss: 0.07311 test_loss: 0.08208 \n",
      "[478/500] train_loss: 0.04114 valid_loss: 0.07380 test_loss: 0.08171 \n",
      "[479/500] train_loss: 0.04078 valid_loss: 0.07303 test_loss: 0.08341 \n",
      "[480/500] train_loss: 0.04084 valid_loss: 0.07316 test_loss: 0.08204 \n",
      "[481/500] train_loss: 0.04039 valid_loss: 0.07438 test_loss: 0.08294 \n",
      "[482/500] train_loss: 0.04044 valid_loss: 0.07897 test_loss: 0.08284 \n",
      "[483/500] train_loss: 0.04119 valid_loss: 0.07300 test_loss: 0.08047 \n",
      "[484/500] train_loss: 0.04026 valid_loss: 0.07330 test_loss: 0.08180 \n",
      "[485/500] train_loss: 0.04003 valid_loss: 0.07444 test_loss: 0.08132 \n",
      "[486/500] train_loss: 0.03985 valid_loss: 0.07460 test_loss: 0.08079 \n",
      "[487/500] train_loss: 0.04046 valid_loss: 0.07349 test_loss: 0.08148 \n",
      "[488/500] train_loss: 0.03952 valid_loss: 0.07574 test_loss: 0.08240 \n",
      "[489/500] train_loss: 0.04015 valid_loss: 0.07417 test_loss: 0.08229 \n",
      "[490/500] train_loss: 0.03972 valid_loss: 0.07419 test_loss: 0.08142 \n",
      "[491/500] train_loss: 0.04173 valid_loss: 0.07313 test_loss: 0.08197 \n",
      "[492/500] train_loss: 0.03894 valid_loss: 0.07246 test_loss: 0.08198 \n",
      "[493/500] train_loss: 0.04068 valid_loss: 0.07379 test_loss: 0.08219 \n",
      "[494/500] train_loss: 0.03903 valid_loss: 0.08557 test_loss: 0.08215 \n",
      "[495/500] train_loss: 0.03993 valid_loss: 0.07436 test_loss: 0.08144 \n",
      "[496/500] train_loss: 0.03947 valid_loss: 0.07519 test_loss: 0.08182 \n",
      "[497/500] train_loss: 0.03927 valid_loss: 0.07488 test_loss: 0.08048 \n",
      "[498/500] train_loss: 0.04021 valid_loss: 0.07486 test_loss: 0.08032 \n",
      "[499/500] train_loss: 0.04046 valid_loss: 0.07413 test_loss: 0.08217 \n",
      "[500/500] train_loss: 0.03948 valid_loss: 0.07499 test_loss: 0.08299 \n",
      "TRAINING MODEL 15\n",
      "[  1/500] train_loss: 0.35346 valid_loss: 0.24166 test_loss: 0.24529 \n",
      "验证损失减少 (inf --> 0.241659). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18341 valid_loss: 0.17419 test_loss: 0.17984 \n",
      "验证损失减少 (0.241659 --> 0.174191). 正在保存模型...\n",
      "[  3/500] train_loss: 0.14571 valid_loss: 0.14902 test_loss: 0.15715 \n",
      "验证损失减少 (0.174191 --> 0.149025). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13680 valid_loss: 0.13499 test_loss: 0.14328 \n",
      "验证损失减少 (0.149025 --> 0.134993). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12471 valid_loss: 0.13060 test_loss: 0.13536 \n",
      "验证损失减少 (0.134993 --> 0.130599). 正在保存模型...\n",
      "[  6/500] train_loss: 0.11985 valid_loss: 0.12473 test_loss: 0.13156 \n",
      "验证损失减少 (0.130599 --> 0.124731). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11663 valid_loss: 0.12314 test_loss: 0.12814 \n",
      "验证损失减少 (0.124731 --> 0.123138). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11262 valid_loss: 0.11857 test_loss: 0.12544 \n",
      "验证损失减少 (0.123138 --> 0.118566). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11101 valid_loss: 0.11420 test_loss: 0.12249 \n",
      "验证损失减少 (0.118566 --> 0.114196). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10878 valid_loss: 0.11295 test_loss: 0.12295 \n",
      "验证损失减少 (0.114196 --> 0.112948). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10475 valid_loss: 0.10781 test_loss: 0.11811 \n",
      "验证损失减少 (0.112948 --> 0.107813). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10290 valid_loss: 0.10686 test_loss: 0.11588 \n",
      "验证损失减少 (0.107813 --> 0.106857). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10156 valid_loss: 0.10363 test_loss: 0.11395 \n",
      "验证损失减少 (0.106857 --> 0.103635). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.09873 valid_loss: 0.10305 test_loss: 0.11141 \n",
      "验证损失减少 (0.103635 --> 0.103052). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.09906 valid_loss: 0.10255 test_loss: 0.11210 \n",
      "验证损失减少 (0.103052 --> 0.102549). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.09864 valid_loss: 0.10200 test_loss: 0.11242 \n",
      "验证损失减少 (0.102549 --> 0.102000). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09531 valid_loss: 0.09754 test_loss: 0.10774 \n",
      "验证损失减少 (0.102000 --> 0.097542). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09512 valid_loss: 0.09890 test_loss: 0.10752 \n",
      "[ 19/500] train_loss: 0.09209 valid_loss: 0.09792 test_loss: 0.10997 \n",
      "[ 20/500] train_loss: 0.09300 valid_loss: 0.09485 test_loss: 0.10508 \n",
      "验证损失减少 (0.097542 --> 0.094849). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09121 valid_loss: 0.09703 test_loss: 0.10420 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 22/500] train_loss: 0.09154 valid_loss: 0.09498 test_loss: 0.10431 \n",
      "[ 23/500] train_loss: 0.08793 valid_loss: 0.09248 test_loss: 0.10294 \n",
      "验证损失减少 (0.094849 --> 0.092475). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.08931 valid_loss: 0.09348 test_loss: 0.10184 \n",
      "[ 25/500] train_loss: 0.08946 valid_loss: 0.09417 test_loss: 0.10031 \n",
      "[ 26/500] train_loss: 0.08932 valid_loss: 0.09436 test_loss: 0.10558 \n",
      "[ 27/500] train_loss: 0.08704 valid_loss: 0.09164 test_loss: 0.10150 \n",
      "验证损失减少 (0.092475 --> 0.091643). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08752 valid_loss: 0.09171 test_loss: 0.09767 \n",
      "[ 29/500] train_loss: 0.08648 valid_loss: 0.09005 test_loss: 0.10019 \n",
      "验证损失减少 (0.091643 --> 0.090049). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08428 valid_loss: 0.09583 test_loss: 0.09927 \n",
      "[ 31/500] train_loss: 0.08652 valid_loss: 0.09077 test_loss: 0.10114 \n",
      "[ 32/500] train_loss: 0.08403 valid_loss: 0.08914 test_loss: 0.09710 \n",
      "验证损失减少 (0.090049 --> 0.089141). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08544 valid_loss: 0.08944 test_loss: 0.09832 \n",
      "[ 34/500] train_loss: 0.08363 valid_loss: 0.08550 test_loss: 0.09373 \n",
      "验证损失减少 (0.089141 --> 0.085503). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08223 valid_loss: 0.08707 test_loss: 0.09516 \n",
      "[ 36/500] train_loss: 0.08277 valid_loss: 0.08795 test_loss: 0.09488 \n",
      "[ 37/500] train_loss: 0.08256 valid_loss: 0.08688 test_loss: 0.09427 \n",
      "[ 38/500] train_loss: 0.08197 valid_loss: 0.08508 test_loss: 0.09351 \n",
      "验证损失减少 (0.085503 --> 0.085079). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.07968 valid_loss: 0.08444 test_loss: 0.09302 \n",
      "验证损失减少 (0.085079 --> 0.084440). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.07910 valid_loss: 0.08256 test_loss: 0.09186 \n",
      "验证损失减少 (0.084440 --> 0.082562). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.07959 valid_loss: 0.08460 test_loss: 0.09306 \n",
      "[ 42/500] train_loss: 0.07995 valid_loss: 0.08429 test_loss: 0.09351 \n",
      "[ 43/500] train_loss: 0.08050 valid_loss: 0.08434 test_loss: 0.09349 \n",
      "[ 44/500] train_loss: 0.07895 valid_loss: 0.08284 test_loss: 0.09267 \n",
      "[ 45/500] train_loss: 0.07838 valid_loss: 0.08496 test_loss: 0.09354 \n",
      "[ 46/500] train_loss: 0.07860 valid_loss: 0.08317 test_loss: 0.09232 \n",
      "[ 47/500] train_loss: 0.07926 valid_loss: 0.08443 test_loss: 0.09005 \n",
      "[ 48/500] train_loss: 0.07769 valid_loss: 0.08222 test_loss: 0.09087 \n",
      "验证损失减少 (0.082562 --> 0.082215). 正在保存模型...\n",
      "[ 49/500] train_loss: 0.07672 valid_loss: 0.08207 test_loss: 0.09025 \n",
      "验证损失减少 (0.082215 --> 0.082068). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07818 valid_loss: 0.08301 test_loss: 0.09173 \n",
      "[ 51/500] train_loss: 0.07723 valid_loss: 0.08157 test_loss: 0.09206 \n",
      "验证损失减少 (0.082068 --> 0.081567). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07660 valid_loss: 0.08180 test_loss: 0.08944 \n",
      "[ 53/500] train_loss: 0.07402 valid_loss: 0.08136 test_loss: 0.08982 \n",
      "验证损失减少 (0.081567 --> 0.081356). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07496 valid_loss: 0.07986 test_loss: 0.08796 \n",
      "验证损失减少 (0.081356 --> 0.079864). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07529 valid_loss: 0.08018 test_loss: 0.08912 \n",
      "[ 56/500] train_loss: 0.07530 valid_loss: 0.08162 test_loss: 0.09065 \n",
      "[ 57/500] train_loss: 0.07487 valid_loss: 0.07977 test_loss: 0.08868 \n",
      "验证损失减少 (0.079864 --> 0.079767). 正在保存模型...\n",
      "[ 58/500] train_loss: 0.07398 valid_loss: 0.07973 test_loss: 0.08806 \n",
      "验证损失减少 (0.079767 --> 0.079725). 正在保存模型...\n",
      "[ 59/500] train_loss: 0.07391 valid_loss: 0.08099 test_loss: 0.08972 \n",
      "[ 60/500] train_loss: 0.07326 valid_loss: 0.07996 test_loss: 0.08685 \n",
      "[ 61/500] train_loss: 0.07457 valid_loss: 0.08171 test_loss: 0.08735 \n",
      "[ 62/500] train_loss: 0.07224 valid_loss: 0.08024 test_loss: 0.08693 \n",
      "[ 63/500] train_loss: 0.07351 valid_loss: 0.07974 test_loss: 0.08764 \n",
      "[ 64/500] train_loss: 0.07165 valid_loss: 0.08041 test_loss: 0.08729 \n",
      "[ 65/500] train_loss: 0.07103 valid_loss: 0.08149 test_loss: 0.08749 \n",
      "[ 66/500] train_loss: 0.07360 valid_loss: 0.08397 test_loss: 0.08785 \n",
      "[ 67/500] train_loss: 0.07260 valid_loss: 0.07979 test_loss: 0.08712 \n",
      "[ 68/500] train_loss: 0.07153 valid_loss: 0.08012 test_loss: 0.08548 \n",
      "[ 69/500] train_loss: 0.07012 valid_loss: 0.07779 test_loss: 0.08440 \n",
      "验证损失减少 (0.079725 --> 0.077791). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.07124 valid_loss: 0.07898 test_loss: 0.08442 \n",
      "[ 71/500] train_loss: 0.06958 valid_loss: 0.08229 test_loss: 0.08979 \n",
      "[ 72/500] train_loss: 0.07199 valid_loss: 0.07841 test_loss: 0.08494 \n",
      "[ 73/500] train_loss: 0.07098 valid_loss: 0.08013 test_loss: 0.08568 \n",
      "[ 74/500] train_loss: 0.07291 valid_loss: 0.07834 test_loss: 0.08403 \n",
      "[ 75/500] train_loss: 0.06804 valid_loss: 0.07801 test_loss: 0.08508 \n",
      "[ 76/500] train_loss: 0.07113 valid_loss: 0.07838 test_loss: 0.08573 \n",
      "[ 77/500] train_loss: 0.06985 valid_loss: 0.07907 test_loss: 0.08408 \n",
      "[ 78/500] train_loss: 0.06964 valid_loss: 0.07736 test_loss: 0.08403 \n",
      "验证损失减少 (0.077791 --> 0.077362). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.07024 valid_loss: 0.08012 test_loss: 0.08486 \n",
      "[ 80/500] train_loss: 0.07076 valid_loss: 0.08011 test_loss: 0.08726 \n",
      "[ 81/500] train_loss: 0.06956 valid_loss: 0.08436 test_loss: 0.08628 \n",
      "[ 82/500] train_loss: 0.06965 valid_loss: 0.08189 test_loss: 0.08381 \n",
      "[ 83/500] train_loss: 0.06757 valid_loss: 0.07786 test_loss: 0.08376 \n",
      "[ 84/500] train_loss: 0.06803 valid_loss: 0.08031 test_loss: 0.08367 \n",
      "[ 85/500] train_loss: 0.06960 valid_loss: 0.07804 test_loss: 0.08489 \n",
      "[ 86/500] train_loss: 0.06663 valid_loss: 0.07771 test_loss: 0.08569 \n",
      "[ 87/500] train_loss: 0.06671 valid_loss: 0.07805 test_loss: 0.08558 \n",
      "[ 88/500] train_loss: 0.06742 valid_loss: 0.08043 test_loss: 0.08390 \n",
      "[ 89/500] train_loss: 0.06745 valid_loss: 0.07943 test_loss: 0.08520 \n",
      "[ 90/500] train_loss: 0.06588 valid_loss: 0.07662 test_loss: 0.08381 \n",
      "验证损失减少 (0.077362 --> 0.076617). 正在保存模型...\n",
      "[ 91/500] train_loss: 0.06688 valid_loss: 0.07575 test_loss: 0.08342 \n",
      "验证损失减少 (0.076617 --> 0.075746). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.06752 valid_loss: 0.07662 test_loss: 0.08321 \n",
      "[ 93/500] train_loss: 0.06524 valid_loss: 0.07810 test_loss: 0.08526 \n",
      "[ 94/500] train_loss: 0.06711 valid_loss: 0.07563 test_loss: 0.08417 \n",
      "验证损失减少 (0.075746 --> 0.075632). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.06710 valid_loss: 0.07668 test_loss: 0.08397 \n",
      "[ 96/500] train_loss: 0.06366 valid_loss: 0.07491 test_loss: 0.08391 \n",
      "验证损失减少 (0.075632 --> 0.074911). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.06623 valid_loss: 0.07728 test_loss: 0.08318 \n",
      "[ 98/500] train_loss: 0.06583 valid_loss: 0.07679 test_loss: 0.08321 \n",
      "[ 99/500] train_loss: 0.06663 valid_loss: 0.07616 test_loss: 0.08453 \n",
      "[100/500] train_loss: 0.06416 valid_loss: 0.07497 test_loss: 0.08305 \n",
      "[101/500] train_loss: 0.06452 valid_loss: 0.07558 test_loss: 0.08235 \n",
      "[102/500] train_loss: 0.06524 valid_loss: 0.07527 test_loss: 0.08202 \n",
      "[103/500] train_loss: 0.06388 valid_loss: 0.07580 test_loss: 0.08371 \n",
      "[104/500] train_loss: 0.06498 valid_loss: 0.07795 test_loss: 0.08570 \n",
      "[105/500] train_loss: 0.06558 valid_loss: 0.07637 test_loss: 0.08351 \n",
      "[106/500] train_loss: 0.06555 valid_loss: 0.07460 test_loss: 0.08173 \n",
      "验证损失减少 (0.074911 --> 0.074597). 正在保存模型...\n",
      "[107/500] train_loss: 0.06344 valid_loss: 0.07396 test_loss: 0.08274 \n",
      "验证损失减少 (0.074597 --> 0.073958). 正在保存模型...\n",
      "[108/500] train_loss: 0.06394 valid_loss: 0.07374 test_loss: 0.08109 \n",
      "验证损失减少 (0.073958 --> 0.073740). 正在保存模型...\n",
      "[109/500] train_loss: 0.06409 valid_loss: 0.07524 test_loss: 0.08291 \n",
      "[110/500] train_loss: 0.06330 valid_loss: 0.07594 test_loss: 0.08075 \n",
      "[111/500] train_loss: 0.06456 valid_loss: 0.07457 test_loss: 0.08150 \n",
      "[112/500] train_loss: 0.06433 valid_loss: 0.07686 test_loss: 0.08583 \n",
      "[113/500] train_loss: 0.06356 valid_loss: 0.07581 test_loss: 0.08190 \n",
      "[114/500] train_loss: 0.06318 valid_loss: 0.07469 test_loss: 0.08233 \n",
      "[115/500] train_loss: 0.06184 valid_loss: 0.07846 test_loss: 0.08563 \n",
      "[116/500] train_loss: 0.06223 valid_loss: 0.07513 test_loss: 0.08245 \n",
      "[117/500] train_loss: 0.06464 valid_loss: 0.07487 test_loss: 0.08263 \n",
      "[118/500] train_loss: 0.06321 valid_loss: 0.07416 test_loss: 0.08196 \n",
      "[119/500] train_loss: 0.06162 valid_loss: 0.07491 test_loss: 0.08317 \n",
      "[120/500] train_loss: 0.06176 valid_loss: 0.07273 test_loss: 0.08167 \n",
      "验证损失减少 (0.073740 --> 0.072729). 正在保存模型...\n",
      "[121/500] train_loss: 0.06317 valid_loss: 0.07519 test_loss: 0.08194 \n",
      "[122/500] train_loss: 0.06245 valid_loss: 0.07618 test_loss: 0.08237 \n",
      "[123/500] train_loss: 0.06182 valid_loss: 0.07557 test_loss: 0.08133 \n",
      "[124/500] train_loss: 0.06104 valid_loss: 0.07482 test_loss: 0.08110 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[125/500] train_loss: 0.06132 valid_loss: 0.07433 test_loss: 0.08096 \n",
      "[126/500] train_loss: 0.06190 valid_loss: 0.07451 test_loss: 0.08135 \n",
      "[127/500] train_loss: 0.06320 valid_loss: 0.07486 test_loss: 0.08225 \n",
      "[128/500] train_loss: 0.06147 valid_loss: 0.07451 test_loss: 0.08069 \n",
      "[129/500] train_loss: 0.06149 valid_loss: 0.07927 test_loss: 0.08194 \n",
      "[130/500] train_loss: 0.06335 valid_loss: 0.07718 test_loss: 0.08187 \n",
      "[131/500] train_loss: 0.06085 valid_loss: 0.07767 test_loss: 0.08482 \n",
      "[132/500] train_loss: 0.06032 valid_loss: 0.07483 test_loss: 0.08155 \n",
      "[133/500] train_loss: 0.06062 valid_loss: 0.07677 test_loss: 0.08236 \n",
      "[134/500] train_loss: 0.06077 valid_loss: 0.07611 test_loss: 0.08198 \n",
      "[135/500] train_loss: 0.05995 valid_loss: 0.07567 test_loss: 0.08140 \n",
      "[136/500] train_loss: 0.06183 valid_loss: 0.07592 test_loss: 0.08002 \n",
      "[137/500] train_loss: 0.06005 valid_loss: 0.07792 test_loss: 0.08191 \n",
      "[138/500] train_loss: 0.06037 valid_loss: 0.07580 test_loss: 0.08267 \n",
      "[139/500] train_loss: 0.06022 valid_loss: 0.07687 test_loss: 0.08009 \n",
      "[140/500] train_loss: 0.05975 valid_loss: 0.07391 test_loss: 0.08276 \n",
      "[141/500] train_loss: 0.06098 valid_loss: 0.07272 test_loss: 0.08066 \n",
      "验证损失减少 (0.072729 --> 0.072716). 正在保存模型...\n",
      "[142/500] train_loss: 0.06020 valid_loss: 0.07358 test_loss: 0.08144 \n",
      "[143/500] train_loss: 0.05848 valid_loss: 0.07826 test_loss: 0.08261 \n",
      "[144/500] train_loss: 0.05942 valid_loss: 0.07558 test_loss: 0.08161 \n",
      "[145/500] train_loss: 0.06115 valid_loss: 0.07423 test_loss: 0.08051 \n",
      "[146/500] train_loss: 0.05920 valid_loss: 0.07399 test_loss: 0.08203 \n",
      "[147/500] train_loss: 0.05804 valid_loss: 0.07416 test_loss: 0.08139 \n",
      "[148/500] train_loss: 0.05897 valid_loss: 0.07767 test_loss: 0.08158 \n",
      "[149/500] train_loss: 0.05998 valid_loss: 0.07382 test_loss: 0.08122 \n",
      "[150/500] train_loss: 0.05983 valid_loss: 0.07494 test_loss: 0.08169 \n",
      "[151/500] train_loss: 0.05905 valid_loss: 0.07484 test_loss: 0.08161 \n",
      "[152/500] train_loss: 0.05912 valid_loss: 0.07397 test_loss: 0.08097 \n",
      "[153/500] train_loss: 0.05858 valid_loss: 0.07452 test_loss: 0.08089 \n",
      "[154/500] train_loss: 0.05897 valid_loss: 0.07316 test_loss: 0.08033 \n",
      "[155/500] train_loss: 0.05658 valid_loss: 0.07176 test_loss: 0.08020 \n",
      "验证损失减少 (0.072716 --> 0.071759). 正在保存模型...\n",
      "[156/500] train_loss: 0.05903 valid_loss: 0.07574 test_loss: 0.08187 \n",
      "[157/500] train_loss: 0.05815 valid_loss: 0.07510 test_loss: 0.08046 \n",
      "[158/500] train_loss: 0.05789 valid_loss: 0.07447 test_loss: 0.07998 \n",
      "[159/500] train_loss: 0.05780 valid_loss: 0.07231 test_loss: 0.07928 \n",
      "[160/500] train_loss: 0.05831 valid_loss: 0.07488 test_loss: 0.08081 \n",
      "[161/500] train_loss: 0.05929 valid_loss: 0.07398 test_loss: 0.08252 \n",
      "[162/500] train_loss: 0.05798 valid_loss: 0.07425 test_loss: 0.08118 \n",
      "[163/500] train_loss: 0.05724 valid_loss: 0.07462 test_loss: 0.07986 \n",
      "[164/500] train_loss: 0.05908 valid_loss: 0.07026 test_loss: 0.07915 \n",
      "验证损失减少 (0.071759 --> 0.070262). 正在保存模型...\n",
      "[165/500] train_loss: 0.05725 valid_loss: 0.07179 test_loss: 0.08027 \n",
      "[166/500] train_loss: 0.05700 valid_loss: 0.07291 test_loss: 0.08109 \n",
      "[167/500] train_loss: 0.05621 valid_loss: 0.07166 test_loss: 0.07988 \n",
      "[168/500] train_loss: 0.05583 valid_loss: 0.07294 test_loss: 0.08084 \n",
      "[169/500] train_loss: 0.05645 valid_loss: 0.07438 test_loss: 0.08244 \n",
      "[170/500] train_loss: 0.05777 valid_loss: 0.07385 test_loss: 0.08080 \n",
      "[171/500] train_loss: 0.05630 valid_loss: 0.07297 test_loss: 0.08133 \n",
      "[172/500] train_loss: 0.05651 valid_loss: 0.07335 test_loss: 0.08063 \n",
      "[173/500] train_loss: 0.05628 valid_loss: 0.07334 test_loss: 0.08026 \n",
      "[174/500] train_loss: 0.05821 valid_loss: 0.07299 test_loss: 0.08098 \n",
      "[175/500] train_loss: 0.05713 valid_loss: 0.07509 test_loss: 0.08218 \n",
      "[176/500] train_loss: 0.05696 valid_loss: 0.07430 test_loss: 0.08032 \n",
      "[177/500] train_loss: 0.05782 valid_loss: 0.07437 test_loss: 0.08132 \n",
      "[178/500] train_loss: 0.05640 valid_loss: 0.07349 test_loss: 0.07943 \n",
      "[179/500] train_loss: 0.05527 valid_loss: 0.07270 test_loss: 0.08127 \n",
      "[180/500] train_loss: 0.05601 valid_loss: 0.07317 test_loss: 0.08102 \n",
      "[181/500] train_loss: 0.05481 valid_loss: 0.07391 test_loss: 0.08050 \n",
      "[182/500] train_loss: 0.05532 valid_loss: 0.07226 test_loss: 0.08103 \n",
      "[183/500] train_loss: 0.05619 valid_loss: 0.07328 test_loss: 0.08122 \n",
      "[184/500] train_loss: 0.05535 valid_loss: 0.07336 test_loss: 0.07989 \n",
      "[185/500] train_loss: 0.05491 valid_loss: 0.07272 test_loss: 0.07983 \n",
      "[186/500] train_loss: 0.05474 valid_loss: 0.07182 test_loss: 0.07937 \n",
      "[187/500] train_loss: 0.05663 valid_loss: 0.07272 test_loss: 0.07920 \n",
      "[188/500] train_loss: 0.05500 valid_loss: 0.07084 test_loss: 0.07927 \n",
      "[189/500] train_loss: 0.05580 valid_loss: 0.07206 test_loss: 0.07972 \n",
      "[190/500] train_loss: 0.05561 valid_loss: 0.07402 test_loss: 0.07900 \n",
      "[191/500] train_loss: 0.05548 valid_loss: 0.07182 test_loss: 0.07982 \n",
      "[192/500] train_loss: 0.05583 valid_loss: 0.07222 test_loss: 0.08042 \n",
      "[193/500] train_loss: 0.05356 valid_loss: 0.07247 test_loss: 0.07991 \n",
      "[194/500] train_loss: 0.05407 valid_loss: 0.07231 test_loss: 0.08084 \n",
      "[195/500] train_loss: 0.05465 valid_loss: 0.07492 test_loss: 0.07868 \n",
      "[196/500] train_loss: 0.05409 valid_loss: 0.07257 test_loss: 0.07891 \n",
      "[197/500] train_loss: 0.05469 valid_loss: 0.07440 test_loss: 0.08134 \n",
      "[198/500] train_loss: 0.05406 valid_loss: 0.07229 test_loss: 0.07957 \n",
      "[199/500] train_loss: 0.05432 valid_loss: 0.06940 test_loss: 0.08053 \n",
      "验证损失减少 (0.070262 --> 0.069397). 正在保存模型...\n",
      "[200/500] train_loss: 0.05349 valid_loss: 0.07159 test_loss: 0.08024 \n",
      "[201/500] train_loss: 0.05278 valid_loss: 0.07233 test_loss: 0.08114 \n",
      "[202/500] train_loss: 0.05384 valid_loss: 0.07323 test_loss: 0.08061 \n",
      "[203/500] train_loss: 0.05423 valid_loss: 0.07618 test_loss: 0.08078 \n",
      "[204/500] train_loss: 0.05352 valid_loss: 0.07492 test_loss: 0.07969 \n",
      "[205/500] train_loss: 0.05286 valid_loss: 0.07360 test_loss: 0.08079 \n",
      "[206/500] train_loss: 0.05531 valid_loss: 0.07055 test_loss: 0.07996 \n",
      "[207/500] train_loss: 0.05377 valid_loss: 0.07159 test_loss: 0.08029 \n",
      "[208/500] train_loss: 0.05430 valid_loss: 0.07247 test_loss: 0.08049 \n",
      "[209/500] train_loss: 0.05286 valid_loss: 0.07323 test_loss: 0.08089 \n",
      "[210/500] train_loss: 0.05373 valid_loss: 0.07383 test_loss: 0.08205 \n",
      "[211/500] train_loss: 0.05346 valid_loss: 0.07523 test_loss: 0.07965 \n",
      "[212/500] train_loss: 0.05355 valid_loss: 0.07102 test_loss: 0.07961 \n",
      "[213/500] train_loss: 0.05370 valid_loss: 0.07593 test_loss: 0.08098 \n",
      "[214/500] train_loss: 0.05445 valid_loss: 0.07535 test_loss: 0.08024 \n",
      "[215/500] train_loss: 0.05370 valid_loss: 0.07197 test_loss: 0.07901 \n",
      "[216/500] train_loss: 0.05177 valid_loss: 0.07174 test_loss: 0.08050 \n",
      "[217/500] train_loss: 0.05226 valid_loss: 0.07083 test_loss: 0.07822 \n",
      "[218/500] train_loss: 0.05240 valid_loss: 0.07000 test_loss: 0.07943 \n",
      "[219/500] train_loss: 0.05305 valid_loss: 0.07204 test_loss: 0.08029 \n",
      "[220/500] train_loss: 0.05278 valid_loss: 0.07150 test_loss: 0.08019 \n",
      "[221/500] train_loss: 0.05133 valid_loss: 0.07167 test_loss: 0.08040 \n",
      "[222/500] train_loss: 0.05265 valid_loss: 0.07143 test_loss: 0.07988 \n",
      "[223/500] train_loss: 0.05415 valid_loss: 0.07421 test_loss: 0.08185 \n",
      "[224/500] train_loss: 0.05337 valid_loss: 0.07200 test_loss: 0.08223 \n",
      "[225/500] train_loss: 0.05268 valid_loss: 0.07092 test_loss: 0.07949 \n",
      "[226/500] train_loss: 0.05191 valid_loss: 0.07203 test_loss: 0.07946 \n",
      "[227/500] train_loss: 0.05229 valid_loss: 0.07091 test_loss: 0.08149 \n",
      "[228/500] train_loss: 0.05099 valid_loss: 0.07365 test_loss: 0.08030 \n",
      "[229/500] train_loss: 0.05194 valid_loss: 0.07203 test_loss: 0.08011 \n",
      "[230/500] train_loss: 0.05177 valid_loss: 0.07034 test_loss: 0.07982 \n",
      "[231/500] train_loss: 0.05246 valid_loss: 0.07215 test_loss: 0.08004 \n",
      "[232/500] train_loss: 0.05136 valid_loss: 0.07385 test_loss: 0.08031 \n",
      "[233/500] train_loss: 0.05088 valid_loss: 0.07273 test_loss: 0.08344 \n",
      "[234/500] train_loss: 0.05271 valid_loss: 0.07361 test_loss: 0.08060 \n",
      "[235/500] train_loss: 0.05034 valid_loss: 0.07507 test_loss: 0.08074 \n",
      "[236/500] train_loss: 0.05235 valid_loss: 0.07224 test_loss: 0.08110 \n",
      "[237/500] train_loss: 0.05102 valid_loss: 0.07142 test_loss: 0.07955 \n",
      "[238/500] train_loss: 0.05083 valid_loss: 0.07274 test_loss: 0.08109 \n",
      "[239/500] train_loss: 0.05143 valid_loss: 0.07417 test_loss: 0.08000 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[240/500] train_loss: 0.05014 valid_loss: 0.07099 test_loss: 0.07920 \n",
      "[241/500] train_loss: 0.05067 valid_loss: 0.07396 test_loss: 0.08233 \n",
      "[242/500] train_loss: 0.05142 valid_loss: 0.07295 test_loss: 0.08010 \n",
      "[243/500] train_loss: 0.05187 valid_loss: 0.07298 test_loss: 0.08162 \n",
      "[244/500] train_loss: 0.04968 valid_loss: 0.07275 test_loss: 0.08126 \n",
      "[245/500] train_loss: 0.05156 valid_loss: 0.07149 test_loss: 0.08184 \n",
      "[246/500] train_loss: 0.05188 valid_loss: 0.07376 test_loss: 0.08046 \n",
      "[247/500] train_loss: 0.05173 valid_loss: 0.07131 test_loss: 0.08045 \n",
      "[248/500] train_loss: 0.05102 valid_loss: 0.07001 test_loss: 0.07958 \n",
      "[249/500] train_loss: 0.05065 valid_loss: 0.06941 test_loss: 0.07942 \n",
      "[250/500] train_loss: 0.05111 valid_loss: 0.07199 test_loss: 0.08037 \n",
      "[251/500] train_loss: 0.04977 valid_loss: 0.06898 test_loss: 0.08000 \n",
      "验证损失减少 (0.069397 --> 0.068981). 正在保存模型...\n",
      "[252/500] train_loss: 0.05039 valid_loss: 0.07229 test_loss: 0.08047 \n",
      "[253/500] train_loss: 0.05039 valid_loss: 0.07194 test_loss: 0.08146 \n",
      "[254/500] train_loss: 0.05067 valid_loss: 0.07008 test_loss: 0.07943 \n",
      "[255/500] train_loss: 0.05035 valid_loss: 0.07010 test_loss: 0.08147 \n",
      "[256/500] train_loss: 0.05121 valid_loss: 0.07087 test_loss: 0.08250 \n",
      "[257/500] train_loss: 0.05036 valid_loss: 0.07007 test_loss: 0.08018 \n",
      "[258/500] train_loss: 0.04967 valid_loss: 0.06903 test_loss: 0.08094 \n",
      "[259/500] train_loss: 0.05025 valid_loss: 0.07167 test_loss: 0.08050 \n",
      "[260/500] train_loss: 0.04931 valid_loss: 0.07088 test_loss: 0.08117 \n",
      "[261/500] train_loss: 0.04973 valid_loss: 0.06979 test_loss: 0.08068 \n",
      "[262/500] train_loss: 0.04987 valid_loss: 0.07103 test_loss: 0.08104 \n",
      "[263/500] train_loss: 0.04985 valid_loss: 0.07182 test_loss: 0.08023 \n",
      "[264/500] train_loss: 0.04984 valid_loss: 0.06964 test_loss: 0.07974 \n",
      "[265/500] train_loss: 0.04980 valid_loss: 0.07146 test_loss: 0.08214 \n",
      "[266/500] train_loss: 0.05134 valid_loss: 0.07024 test_loss: 0.08088 \n",
      "[267/500] train_loss: 0.05007 valid_loss: 0.07118 test_loss: 0.08086 \n",
      "[268/500] train_loss: 0.04908 valid_loss: 0.07069 test_loss: 0.08139 \n",
      "[269/500] train_loss: 0.04982 valid_loss: 0.07207 test_loss: 0.08245 \n",
      "[270/500] train_loss: 0.04946 valid_loss: 0.07183 test_loss: 0.08075 \n",
      "[271/500] train_loss: 0.04816 valid_loss: 0.07203 test_loss: 0.08156 \n",
      "[272/500] train_loss: 0.04816 valid_loss: 0.07042 test_loss: 0.08029 \n",
      "[273/500] train_loss: 0.04848 valid_loss: 0.07120 test_loss: 0.08139 \n",
      "[274/500] train_loss: 0.05020 valid_loss: 0.07046 test_loss: 0.08090 \n",
      "[275/500] train_loss: 0.04784 valid_loss: 0.07162 test_loss: 0.08187 \n",
      "[276/500] train_loss: 0.04846 valid_loss: 0.07042 test_loss: 0.08230 \n",
      "[277/500] train_loss: 0.04968 valid_loss: 0.07070 test_loss: 0.08103 \n",
      "[278/500] train_loss: 0.04931 valid_loss: 0.07066 test_loss: 0.08098 \n",
      "[279/500] train_loss: 0.04970 valid_loss: 0.07129 test_loss: 0.08066 \n",
      "[280/500] train_loss: 0.04825 valid_loss: 0.07157 test_loss: 0.08237 \n",
      "[281/500] train_loss: 0.04755 valid_loss: 0.07195 test_loss: 0.08117 \n",
      "[282/500] train_loss: 0.04773 valid_loss: 0.07029 test_loss: 0.08041 \n",
      "[283/500] train_loss: 0.04712 valid_loss: 0.07057 test_loss: 0.07951 \n",
      "[284/500] train_loss: 0.04917 valid_loss: 0.07186 test_loss: 0.08069 \n",
      "[285/500] train_loss: 0.04891 valid_loss: 0.07143 test_loss: 0.08033 \n",
      "[286/500] train_loss: 0.04805 valid_loss: 0.07428 test_loss: 0.08057 \n",
      "[287/500] train_loss: 0.04887 valid_loss: 0.07060 test_loss: 0.07902 \n",
      "[288/500] train_loss: 0.04860 valid_loss: 0.07132 test_loss: 0.08063 \n",
      "[289/500] train_loss: 0.04952 valid_loss: 0.07058 test_loss: 0.08018 \n",
      "[290/500] train_loss: 0.04845 valid_loss: 0.07665 test_loss: 0.08142 \n",
      "[291/500] train_loss: 0.04875 valid_loss: 0.07640 test_loss: 0.08136 \n",
      "[292/500] train_loss: 0.04964 valid_loss: 0.07222 test_loss: 0.08194 \n",
      "[293/500] train_loss: 0.04626 valid_loss: 0.07059 test_loss: 0.07950 \n",
      "[294/500] train_loss: 0.04783 valid_loss: 0.07055 test_loss: 0.08052 \n",
      "[295/500] train_loss: 0.04796 valid_loss: 0.06936 test_loss: 0.07998 \n",
      "[296/500] train_loss: 0.04789 valid_loss: 0.07080 test_loss: 0.08100 \n",
      "[297/500] train_loss: 0.04920 valid_loss: 0.07036 test_loss: 0.08118 \n",
      "[298/500] train_loss: 0.04754 valid_loss: 0.07175 test_loss: 0.08188 \n",
      "[299/500] train_loss: 0.04966 valid_loss: 0.06957 test_loss: 0.08044 \n",
      "[300/500] train_loss: 0.04743 valid_loss: 0.06955 test_loss: 0.08025 \n",
      "[301/500] train_loss: 0.04702 valid_loss: 0.07022 test_loss: 0.08107 \n",
      "[302/500] train_loss: 0.04814 valid_loss: 0.06926 test_loss: 0.08125 \n",
      "[303/500] train_loss: 0.04723 valid_loss: 0.07042 test_loss: 0.08058 \n",
      "[304/500] train_loss: 0.04672 valid_loss: 0.07139 test_loss: 0.08059 \n",
      "[305/500] train_loss: 0.04757 valid_loss: 0.07288 test_loss: 0.08056 \n",
      "[306/500] train_loss: 0.04699 valid_loss: 0.07178 test_loss: 0.08025 \n",
      "[307/500] train_loss: 0.04795 valid_loss: 0.07061 test_loss: 0.07950 \n",
      "[308/500] train_loss: 0.04777 valid_loss: 0.07199 test_loss: 0.08118 \n",
      "[309/500] train_loss: 0.04710 valid_loss: 0.07124 test_loss: 0.08103 \n",
      "[310/500] train_loss: 0.04690 valid_loss: 0.06974 test_loss: 0.08008 \n",
      "[311/500] train_loss: 0.04750 valid_loss: 0.07080 test_loss: 0.08049 \n",
      "[312/500] train_loss: 0.04657 valid_loss: 0.07052 test_loss: 0.07943 \n",
      "[313/500] train_loss: 0.04804 valid_loss: 0.07055 test_loss: 0.08128 \n",
      "[314/500] train_loss: 0.04789 valid_loss: 0.06951 test_loss: 0.08039 \n",
      "[315/500] train_loss: 0.04658 valid_loss: 0.07101 test_loss: 0.08136 \n",
      "[316/500] train_loss: 0.04671 valid_loss: 0.07137 test_loss: 0.07989 \n",
      "[317/500] train_loss: 0.04715 valid_loss: 0.07003 test_loss: 0.07991 \n",
      "[318/500] train_loss: 0.04607 valid_loss: 0.07173 test_loss: 0.08152 \n",
      "[319/500] train_loss: 0.04699 valid_loss: 0.06828 test_loss: 0.08009 \n",
      "验证损失减少 (0.068981 --> 0.068280). 正在保存模型...\n",
      "[320/500] train_loss: 0.04796 valid_loss: 0.07220 test_loss: 0.08061 \n",
      "[321/500] train_loss: 0.04794 valid_loss: 0.07089 test_loss: 0.07900 \n",
      "[322/500] train_loss: 0.04540 valid_loss: 0.07124 test_loss: 0.08141 \n",
      "[323/500] train_loss: 0.04646 valid_loss: 0.07248 test_loss: 0.08044 \n",
      "[324/500] train_loss: 0.04621 valid_loss: 0.07074 test_loss: 0.08038 \n",
      "[325/500] train_loss: 0.04568 valid_loss: 0.07085 test_loss: 0.08052 \n",
      "[326/500] train_loss: 0.04636 valid_loss: 0.07019 test_loss: 0.08169 \n",
      "[327/500] train_loss: 0.04550 valid_loss: 0.07053 test_loss: 0.08136 \n",
      "[328/500] train_loss: 0.04644 valid_loss: 0.06961 test_loss: 0.08246 \n",
      "[329/500] train_loss: 0.04536 valid_loss: 0.07014 test_loss: 0.07964 \n",
      "[330/500] train_loss: 0.04478 valid_loss: 0.07069 test_loss: 0.08024 \n",
      "[331/500] train_loss: 0.04621 valid_loss: 0.07024 test_loss: 0.08002 \n",
      "[332/500] train_loss: 0.04658 valid_loss: 0.07056 test_loss: 0.08001 \n",
      "[333/500] train_loss: 0.04616 valid_loss: 0.07012 test_loss: 0.08014 \n",
      "[334/500] train_loss: 0.04779 valid_loss: 0.06998 test_loss: 0.07902 \n",
      "[335/500] train_loss: 0.04602 valid_loss: 0.07123 test_loss: 0.08081 \n",
      "[336/500] train_loss: 0.04597 valid_loss: 0.07166 test_loss: 0.08148 \n",
      "[337/500] train_loss: 0.04640 valid_loss: 0.07010 test_loss: 0.08104 \n",
      "[338/500] train_loss: 0.04530 valid_loss: 0.07122 test_loss: 0.08094 \n",
      "[339/500] train_loss: 0.04666 valid_loss: 0.06985 test_loss: 0.08032 \n",
      "[340/500] train_loss: 0.04590 valid_loss: 0.07156 test_loss: 0.08050 \n",
      "[341/500] train_loss: 0.04636 valid_loss: 0.07176 test_loss: 0.07877 \n",
      "[342/500] train_loss: 0.04559 valid_loss: 0.07132 test_loss: 0.08056 \n",
      "[343/500] train_loss: 0.04537 valid_loss: 0.06998 test_loss: 0.08134 \n",
      "[344/500] train_loss: 0.04464 valid_loss: 0.07260 test_loss: 0.08198 \n",
      "[345/500] train_loss: 0.04573 valid_loss: 0.06984 test_loss: 0.07984 \n",
      "[346/500] train_loss: 0.04693 valid_loss: 0.06921 test_loss: 0.08055 \n",
      "[347/500] train_loss: 0.04531 valid_loss: 0.07132 test_loss: 0.07941 \n",
      "[348/500] train_loss: 0.04609 valid_loss: 0.07278 test_loss: 0.08174 \n",
      "[349/500] train_loss: 0.04528 valid_loss: 0.06851 test_loss: 0.07996 \n",
      "[350/500] train_loss: 0.04455 valid_loss: 0.07003 test_loss: 0.08063 \n",
      "[351/500] train_loss: 0.04508 valid_loss: 0.07076 test_loss: 0.08209 \n",
      "[352/500] train_loss: 0.04640 valid_loss: 0.07069 test_loss: 0.08145 \n",
      "[353/500] train_loss: 0.04683 valid_loss: 0.06915 test_loss: 0.08079 \n",
      "[354/500] train_loss: 0.04534 valid_loss: 0.06870 test_loss: 0.08086 \n",
      "[355/500] train_loss: 0.04622 valid_loss: 0.06879 test_loss: 0.07896 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[356/500] train_loss: 0.04482 valid_loss: 0.07051 test_loss: 0.08100 \n",
      "[357/500] train_loss: 0.04456 valid_loss: 0.07155 test_loss: 0.08108 \n",
      "[358/500] train_loss: 0.04638 valid_loss: 0.07077 test_loss: 0.08049 \n",
      "[359/500] train_loss: 0.04494 valid_loss: 0.07116 test_loss: 0.07948 \n",
      "[360/500] train_loss: 0.04465 valid_loss: 0.07600 test_loss: 0.08086 \n",
      "[361/500] train_loss: 0.04428 valid_loss: 0.07017 test_loss: 0.08201 \n",
      "[362/500] train_loss: 0.04474 valid_loss: 0.07021 test_loss: 0.07996 \n",
      "[363/500] train_loss: 0.04458 valid_loss: 0.06883 test_loss: 0.08005 \n",
      "[364/500] train_loss: 0.04517 valid_loss: 0.07111 test_loss: 0.08142 \n",
      "[365/500] train_loss: 0.04416 valid_loss: 0.07036 test_loss: 0.08198 \n",
      "[366/500] train_loss: 0.04494 valid_loss: 0.07047 test_loss: 0.08209 \n",
      "[367/500] train_loss: 0.04326 valid_loss: 0.06927 test_loss: 0.08152 \n",
      "[368/500] train_loss: 0.04631 valid_loss: 0.07038 test_loss: 0.08189 \n",
      "[369/500] train_loss: 0.04453 valid_loss: 0.07517 test_loss: 0.08333 \n",
      "[370/500] train_loss: 0.04436 valid_loss: 0.06927 test_loss: 0.08186 \n",
      "[371/500] train_loss: 0.04452 valid_loss: 0.06910 test_loss: 0.08138 \n",
      "[372/500] train_loss: 0.04455 valid_loss: 0.07047 test_loss: 0.08144 \n",
      "[373/500] train_loss: 0.04468 valid_loss: 0.06920 test_loss: 0.08216 \n",
      "[374/500] train_loss: 0.04431 valid_loss: 0.06851 test_loss: 0.08145 \n",
      "[375/500] train_loss: 0.04474 valid_loss: 0.06870 test_loss: 0.08139 \n",
      "[376/500] train_loss: 0.04387 valid_loss: 0.07095 test_loss: 0.08207 \n",
      "[377/500] train_loss: 0.04410 valid_loss: 0.06928 test_loss: 0.08143 \n",
      "[378/500] train_loss: 0.04430 valid_loss: 0.06932 test_loss: 0.08045 \n",
      "[379/500] train_loss: 0.04523 valid_loss: 0.06904 test_loss: 0.08130 \n",
      "[380/500] train_loss: 0.04362 valid_loss: 0.07059 test_loss: 0.08313 \n",
      "[381/500] train_loss: 0.04388 valid_loss: 0.07139 test_loss: 0.08241 \n",
      "[382/500] train_loss: 0.04465 valid_loss: 0.07298 test_loss: 0.08314 \n",
      "[383/500] train_loss: 0.04375 valid_loss: 0.07096 test_loss: 0.08127 \n",
      "[384/500] train_loss: 0.04444 valid_loss: 0.06962 test_loss: 0.08084 \n",
      "[385/500] train_loss: 0.04393 valid_loss: 0.07109 test_loss: 0.08092 \n",
      "[386/500] train_loss: 0.04451 valid_loss: 0.07082 test_loss: 0.08059 \n",
      "[387/500] train_loss: 0.04261 valid_loss: 0.06936 test_loss: 0.08104 \n",
      "[388/500] train_loss: 0.04382 valid_loss: 0.06975 test_loss: 0.08155 \n",
      "[389/500] train_loss: 0.04410 valid_loss: 0.07121 test_loss: 0.08125 \n",
      "[390/500] train_loss: 0.04291 valid_loss: 0.07134 test_loss: 0.08116 \n",
      "[391/500] train_loss: 0.04359 valid_loss: 0.07055 test_loss: 0.08029 \n",
      "[392/500] train_loss: 0.04276 valid_loss: 0.07069 test_loss: 0.08088 \n",
      "[393/500] train_loss: 0.04389 valid_loss: 0.07296 test_loss: 0.08201 \n",
      "[394/500] train_loss: 0.04335 valid_loss: 0.07112 test_loss: 0.08045 \n",
      "[395/500] train_loss: 0.04373 valid_loss: 0.07287 test_loss: 0.08167 \n",
      "[396/500] train_loss: 0.04352 valid_loss: 0.07033 test_loss: 0.08191 \n",
      "[397/500] train_loss: 0.04265 valid_loss: 0.07243 test_loss: 0.08356 \n",
      "[398/500] train_loss: 0.04328 valid_loss: 0.06954 test_loss: 0.08058 \n",
      "[399/500] train_loss: 0.04403 valid_loss: 0.06962 test_loss: 0.07981 \n",
      "[400/500] train_loss: 0.04394 valid_loss: 0.06889 test_loss: 0.07991 \n",
      "[401/500] train_loss: 0.04281 valid_loss: 0.06914 test_loss: 0.07858 \n",
      "[402/500] train_loss: 0.04462 valid_loss: 0.07012 test_loss: 0.07981 \n",
      "[403/500] train_loss: 0.04443 valid_loss: 0.07104 test_loss: 0.08125 \n",
      "[404/500] train_loss: 0.04312 valid_loss: 0.07034 test_loss: 0.08056 \n",
      "[405/500] train_loss: 0.04278 valid_loss: 0.07228 test_loss: 0.08175 \n",
      "[406/500] train_loss: 0.04374 valid_loss: 0.07174 test_loss: 0.08233 \n",
      "[407/500] train_loss: 0.04337 valid_loss: 0.07146 test_loss: 0.08150 \n",
      "[408/500] train_loss: 0.04323 valid_loss: 0.07140 test_loss: 0.08123 \n",
      "[409/500] train_loss: 0.04261 valid_loss: 0.07027 test_loss: 0.08153 \n",
      "[410/500] train_loss: 0.04287 valid_loss: 0.07131 test_loss: 0.08247 \n",
      "[411/500] train_loss: 0.04420 valid_loss: 0.07134 test_loss: 0.08223 \n",
      "[412/500] train_loss: 0.04446 valid_loss: 0.07154 test_loss: 0.08221 \n",
      "[413/500] train_loss: 0.04299 valid_loss: 0.07183 test_loss: 0.08331 \n",
      "[414/500] train_loss: 0.04249 valid_loss: 0.07024 test_loss: 0.08151 \n",
      "[415/500] train_loss: 0.04336 valid_loss: 0.07110 test_loss: 0.08320 \n",
      "[416/500] train_loss: 0.04169 valid_loss: 0.07036 test_loss: 0.08202 \n",
      "[417/500] train_loss: 0.04186 valid_loss: 0.07151 test_loss: 0.08417 \n",
      "[418/500] train_loss: 0.04318 valid_loss: 0.06985 test_loss: 0.08154 \n",
      "[419/500] train_loss: 0.04144 valid_loss: 0.06862 test_loss: 0.08020 \n",
      "[420/500] train_loss: 0.04345 valid_loss: 0.06979 test_loss: 0.08156 \n",
      "[421/500] train_loss: 0.04172 valid_loss: 0.07228 test_loss: 0.08159 \n",
      "[422/500] train_loss: 0.04287 valid_loss: 0.07086 test_loss: 0.08287 \n",
      "[423/500] train_loss: 0.04364 valid_loss: 0.07162 test_loss: 0.08065 \n",
      "[424/500] train_loss: 0.04310 valid_loss: 0.07215 test_loss: 0.08071 \n",
      "[425/500] train_loss: 0.04256 valid_loss: 0.07022 test_loss: 0.08050 \n",
      "[426/500] train_loss: 0.04234 valid_loss: 0.07153 test_loss: 0.08069 \n",
      "[427/500] train_loss: 0.04269 valid_loss: 0.06908 test_loss: 0.07949 \n",
      "[428/500] train_loss: 0.04251 valid_loss: 0.07115 test_loss: 0.08214 \n",
      "[429/500] train_loss: 0.04310 valid_loss: 0.07202 test_loss: 0.08099 \n",
      "[430/500] train_loss: 0.04206 valid_loss: 0.07030 test_loss: 0.08026 \n",
      "[431/500] train_loss: 0.04160 valid_loss: 0.07235 test_loss: 0.08237 \n",
      "[432/500] train_loss: 0.04213 valid_loss: 0.06995 test_loss: 0.08059 \n",
      "[433/500] train_loss: 0.04210 valid_loss: 0.07074 test_loss: 0.08085 \n",
      "[434/500] train_loss: 0.04228 valid_loss: 0.07075 test_loss: 0.08134 \n",
      "[435/500] train_loss: 0.04103 valid_loss: 0.07073 test_loss: 0.08006 \n",
      "[436/500] train_loss: 0.04060 valid_loss: 0.06924 test_loss: 0.07988 \n",
      "[437/500] train_loss: 0.04158 valid_loss: 0.06957 test_loss: 0.08070 \n",
      "[438/500] train_loss: 0.04208 valid_loss: 0.06898 test_loss: 0.07938 \n",
      "[439/500] train_loss: 0.04178 valid_loss: 0.07091 test_loss: 0.08028 \n",
      "[440/500] train_loss: 0.04196 valid_loss: 0.07121 test_loss: 0.08188 \n",
      "[441/500] train_loss: 0.04161 valid_loss: 0.07139 test_loss: 0.08150 \n",
      "[442/500] train_loss: 0.04230 valid_loss: 0.06938 test_loss: 0.08111 \n",
      "[443/500] train_loss: 0.04136 valid_loss: 0.07058 test_loss: 0.08168 \n",
      "[444/500] train_loss: 0.04165 valid_loss: 0.07213 test_loss: 0.08204 \n",
      "[445/500] train_loss: 0.04255 valid_loss: 0.07001 test_loss: 0.08030 \n",
      "[446/500] train_loss: 0.04010 valid_loss: 0.06964 test_loss: 0.08063 \n",
      "[447/500] train_loss: 0.04146 valid_loss: 0.07097 test_loss: 0.08233 \n",
      "[448/500] train_loss: 0.04310 valid_loss: 0.07363 test_loss: 0.08344 \n",
      "[449/500] train_loss: 0.04249 valid_loss: 0.07171 test_loss: 0.08293 \n",
      "[450/500] train_loss: 0.04148 valid_loss: 0.07082 test_loss: 0.08286 \n",
      "[451/500] train_loss: 0.04196 valid_loss: 0.07333 test_loss: 0.08249 \n",
      "[452/500] train_loss: 0.04198 valid_loss: 0.07134 test_loss: 0.08069 \n",
      "[453/500] train_loss: 0.04210 valid_loss: 0.07152 test_loss: 0.08311 \n",
      "[454/500] train_loss: 0.04122 valid_loss: 0.07104 test_loss: 0.08402 \n",
      "[455/500] train_loss: 0.04180 valid_loss: 0.07084 test_loss: 0.08245 \n",
      "[456/500] train_loss: 0.04154 valid_loss: 0.07089 test_loss: 0.08240 \n",
      "[457/500] train_loss: 0.04348 valid_loss: 0.07223 test_loss: 0.08273 \n",
      "[458/500] train_loss: 0.04073 valid_loss: 0.07268 test_loss: 0.08098 \n",
      "[459/500] train_loss: 0.04192 valid_loss: 0.07135 test_loss: 0.08170 \n",
      "[460/500] train_loss: 0.04138 valid_loss: 0.07255 test_loss: 0.08253 \n",
      "[461/500] train_loss: 0.04131 valid_loss: 0.07166 test_loss: 0.08081 \n",
      "[462/500] train_loss: 0.04077 valid_loss: 0.07218 test_loss: 0.08104 \n",
      "[463/500] train_loss: 0.04121 valid_loss: 0.07289 test_loss: 0.08098 \n",
      "[464/500] train_loss: 0.04142 valid_loss: 0.07181 test_loss: 0.08156 \n",
      "[465/500] train_loss: 0.03936 valid_loss: 0.07052 test_loss: 0.08047 \n",
      "[466/500] train_loss: 0.04299 valid_loss: 0.07235 test_loss: 0.08088 \n",
      "[467/500] train_loss: 0.04159 valid_loss: 0.07166 test_loss: 0.08100 \n",
      "[468/500] train_loss: 0.04149 valid_loss: 0.07102 test_loss: 0.08217 \n",
      "[469/500] train_loss: 0.04193 valid_loss: 0.07383 test_loss: 0.08357 \n",
      "[470/500] train_loss: 0.04201 valid_loss: 0.07140 test_loss: 0.08369 \n",
      "[471/500] train_loss: 0.04121 valid_loss: 0.06935 test_loss: 0.08118 \n",
      "[472/500] train_loss: 0.04081 valid_loss: 0.07038 test_loss: 0.08216 \n",
      "[473/500] train_loss: 0.04148 valid_loss: 0.07136 test_loss: 0.08168 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[474/500] train_loss: 0.04161 valid_loss: 0.07135 test_loss: 0.08215 \n",
      "[475/500] train_loss: 0.04143 valid_loss: 0.06974 test_loss: 0.08262 \n",
      "[476/500] train_loss: 0.04147 valid_loss: 0.07142 test_loss: 0.08330 \n",
      "[477/500] train_loss: 0.04060 valid_loss: 0.07078 test_loss: 0.08212 \n",
      "[478/500] train_loss: 0.04027 valid_loss: 0.07150 test_loss: 0.08224 \n",
      "[479/500] train_loss: 0.04095 valid_loss: 0.07110 test_loss: 0.08248 \n",
      "[480/500] train_loss: 0.04124 valid_loss: 0.07074 test_loss: 0.08449 \n",
      "[481/500] train_loss: 0.04094 valid_loss: 0.07128 test_loss: 0.08311 \n",
      "[482/500] train_loss: 0.03907 valid_loss: 0.07225 test_loss: 0.08309 \n",
      "[483/500] train_loss: 0.04102 valid_loss: 0.07045 test_loss: 0.08119 \n",
      "[484/500] train_loss: 0.04105 valid_loss: 0.07064 test_loss: 0.08112 \n",
      "[485/500] train_loss: 0.04210 valid_loss: 0.07058 test_loss: 0.08316 \n",
      "[486/500] train_loss: 0.04028 valid_loss: 0.07163 test_loss: 0.08376 \n",
      "[487/500] train_loss: 0.04197 valid_loss: 0.07129 test_loss: 0.08367 \n",
      "[488/500] train_loss: 0.04025 valid_loss: 0.07246 test_loss: 0.08311 \n",
      "[489/500] train_loss: 0.03962 valid_loss: 0.07185 test_loss: 0.08341 \n",
      "[490/500] train_loss: 0.04082 valid_loss: 0.07078 test_loss: 0.08214 \n",
      "[491/500] train_loss: 0.04129 valid_loss: 0.07261 test_loss: 0.08339 \n",
      "[492/500] train_loss: 0.04009 valid_loss: 0.07243 test_loss: 0.08328 \n",
      "[493/500] train_loss: 0.04003 valid_loss: 0.07180 test_loss: 0.08261 \n",
      "[494/500] train_loss: 0.04049 valid_loss: 0.07009 test_loss: 0.08252 \n",
      "[495/500] train_loss: 0.03946 valid_loss: 0.07157 test_loss: 0.08179 \n",
      "[496/500] train_loss: 0.04165 valid_loss: 0.07124 test_loss: 0.08113 \n",
      "[497/500] train_loss: 0.04076 valid_loss: 0.07167 test_loss: 0.08326 \n",
      "[498/500] train_loss: 0.03944 valid_loss: 0.07161 test_loss: 0.08181 \n",
      "[499/500] train_loss: 0.03920 valid_loss: 0.07187 test_loss: 0.08384 \n",
      "[500/500] train_loss: 0.04129 valid_loss: 0.07143 test_loss: 0.08311 \n",
      "TRAINING MODEL 16\n",
      "[  1/500] train_loss: 0.38499 valid_loss: 0.27781 test_loss: 0.28273 \n",
      "验证损失减少 (inf --> 0.277807). 正在保存模型...\n",
      "[  2/500] train_loss: 0.20840 valid_loss: 0.20138 test_loss: 0.20226 \n",
      "验证损失减少 (0.277807 --> 0.201378). 正在保存模型...\n",
      "[  3/500] train_loss: 0.16011 valid_loss: 0.16754 test_loss: 0.17215 \n",
      "验证损失减少 (0.201378 --> 0.167540). 正在保存模型...\n",
      "[  4/500] train_loss: 0.14074 valid_loss: 0.14135 test_loss: 0.14954 \n",
      "验证损失减少 (0.167540 --> 0.141352). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13071 valid_loss: 0.13312 test_loss: 0.14132 \n",
      "验证损失减少 (0.141352 --> 0.133119). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12644 valid_loss: 0.13040 test_loss: 0.13776 \n",
      "验证损失减少 (0.133119 --> 0.130404). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11735 valid_loss: 0.12565 test_loss: 0.13550 \n",
      "验证损失减少 (0.130404 --> 0.125650). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11509 valid_loss: 0.11596 test_loss: 0.12650 \n",
      "验证损失减少 (0.125650 --> 0.115964). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11153 valid_loss: 0.11684 test_loss: 0.12593 \n",
      "[ 10/500] train_loss: 0.10931 valid_loss: 0.11565 test_loss: 0.12236 \n",
      "验证损失减少 (0.115964 --> 0.115655). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10888 valid_loss: 0.11480 test_loss: 0.12421 \n",
      "验证损失减少 (0.115655 --> 0.114795). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10842 valid_loss: 0.11107 test_loss: 0.12136 \n",
      "验证损失减少 (0.114795 --> 0.111068). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10560 valid_loss: 0.10806 test_loss: 0.11946 \n",
      "验证损失减少 (0.111068 --> 0.108065). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10507 valid_loss: 0.10715 test_loss: 0.11772 \n",
      "验证损失减少 (0.108065 --> 0.107152). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10110 valid_loss: 0.10480 test_loss: 0.11543 \n",
      "验证损失减少 (0.107152 --> 0.104797). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10054 valid_loss: 0.10188 test_loss: 0.11171 \n",
      "验证损失减少 (0.104797 --> 0.101877). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09789 valid_loss: 0.10058 test_loss: 0.11206 \n",
      "验证损失减少 (0.101877 --> 0.100580). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09985 valid_loss: 0.10138 test_loss: 0.11344 \n",
      "[ 19/500] train_loss: 0.09694 valid_loss: 0.09959 test_loss: 0.11044 \n",
      "验证损失减少 (0.100580 --> 0.099587). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09396 valid_loss: 0.09921 test_loss: 0.11245 \n",
      "验证损失减少 (0.099587 --> 0.099210). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09499 valid_loss: 0.09731 test_loss: 0.10882 \n",
      "验证损失减少 (0.099210 --> 0.097306). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09324 valid_loss: 0.09561 test_loss: 0.10709 \n",
      "验证损失减少 (0.097306 --> 0.095614). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.08985 valid_loss: 0.09401 test_loss: 0.10366 \n",
      "验证损失减少 (0.095614 --> 0.094007). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09134 valid_loss: 0.09305 test_loss: 0.10335 \n",
      "验证损失减少 (0.094007 --> 0.093049). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.09121 valid_loss: 0.09408 test_loss: 0.10464 \n",
      "[ 26/500] train_loss: 0.08901 valid_loss: 0.09146 test_loss: 0.10149 \n",
      "验证损失减少 (0.093049 --> 0.091463). 正在保存模型...\n",
      "[ 27/500] train_loss: 0.08663 valid_loss: 0.09111 test_loss: 0.10042 \n",
      "验证损失减少 (0.091463 --> 0.091110). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08615 valid_loss: 0.08835 test_loss: 0.09973 \n",
      "验证损失减少 (0.091110 --> 0.088352). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08687 valid_loss: 0.08963 test_loss: 0.09867 \n",
      "[ 30/500] train_loss: 0.08703 valid_loss: 0.09012 test_loss: 0.10071 \n",
      "[ 31/500] train_loss: 0.08673 valid_loss: 0.08871 test_loss: 0.09699 \n",
      "[ 32/500] train_loss: 0.08414 valid_loss: 0.08746 test_loss: 0.09551 \n",
      "验证损失减少 (0.088352 --> 0.087465). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08556 valid_loss: 0.08774 test_loss: 0.09573 \n",
      "[ 34/500] train_loss: 0.08447 valid_loss: 0.08591 test_loss: 0.09553 \n",
      "验证损失减少 (0.087465 --> 0.085906). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08660 valid_loss: 0.08599 test_loss: 0.09577 \n",
      "[ 36/500] train_loss: 0.08435 valid_loss: 0.08570 test_loss: 0.09543 \n",
      "验证损失减少 (0.085906 --> 0.085701). 正在保存模型...\n",
      "[ 37/500] train_loss: 0.08212 valid_loss: 0.08582 test_loss: 0.09538 \n",
      "[ 38/500] train_loss: 0.07865 valid_loss: 0.08694 test_loss: 0.09817 \n",
      "[ 39/500] train_loss: 0.08214 valid_loss: 0.08453 test_loss: 0.09521 \n",
      "验证损失减少 (0.085701 --> 0.084532). 正在保存模型...\n",
      "[ 40/500] train_loss: 0.08124 valid_loss: 0.08474 test_loss: 0.09445 \n",
      "[ 41/500] train_loss: 0.08138 valid_loss: 0.08559 test_loss: 0.09276 \n",
      "[ 42/500] train_loss: 0.08072 valid_loss: 0.08277 test_loss: 0.09380 \n",
      "验证损失减少 (0.084532 --> 0.082774). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.07784 valid_loss: 0.08351 test_loss: 0.09437 \n",
      "[ 44/500] train_loss: 0.08130 valid_loss: 0.08522 test_loss: 0.09577 \n",
      "[ 45/500] train_loss: 0.07954 valid_loss: 0.08163 test_loss: 0.09239 \n",
      "验证损失减少 (0.082774 --> 0.081628). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07781 valid_loss: 0.08107 test_loss: 0.09142 \n",
      "验证损失减少 (0.081628 --> 0.081067). 正在保存模型...\n",
      "[ 47/500] train_loss: 0.07970 valid_loss: 0.08282 test_loss: 0.09315 \n",
      "[ 48/500] train_loss: 0.07812 valid_loss: 0.08197 test_loss: 0.09219 \n",
      "[ 49/500] train_loss: 0.07856 valid_loss: 0.08217 test_loss: 0.09138 \n",
      "[ 50/500] train_loss: 0.07805 valid_loss: 0.08083 test_loss: 0.09021 \n",
      "验证损失减少 (0.081067 --> 0.080829). 正在保存模型...\n",
      "[ 51/500] train_loss: 0.07789 valid_loss: 0.08242 test_loss: 0.09192 \n",
      "[ 52/500] train_loss: 0.07497 valid_loss: 0.08082 test_loss: 0.09019 \n",
      "验证损失减少 (0.080829 --> 0.080817). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07607 valid_loss: 0.08021 test_loss: 0.08950 \n",
      "验证损失减少 (0.080817 --> 0.080205). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07666 valid_loss: 0.08076 test_loss: 0.09136 \n",
      "[ 55/500] train_loss: 0.07678 valid_loss: 0.07951 test_loss: 0.08882 \n",
      "验证损失减少 (0.080205 --> 0.079512). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07651 valid_loss: 0.07986 test_loss: 0.08879 \n",
      "[ 57/500] train_loss: 0.07482 valid_loss: 0.07997 test_loss: 0.08855 \n",
      "[ 58/500] train_loss: 0.07725 valid_loss: 0.08308 test_loss: 0.09060 \n",
      "[ 59/500] train_loss: 0.07390 valid_loss: 0.08132 test_loss: 0.09093 \n",
      "[ 60/500] train_loss: 0.07476 valid_loss: 0.08082 test_loss: 0.09083 \n",
      "[ 61/500] train_loss: 0.07490 valid_loss: 0.08021 test_loss: 0.08901 \n",
      "[ 62/500] train_loss: 0.07465 valid_loss: 0.07888 test_loss: 0.08675 \n",
      "验证损失减少 (0.079512 --> 0.078883). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.07388 valid_loss: 0.07878 test_loss: 0.08892 \n",
      "验证损失减少 (0.078883 --> 0.078778). 正在保存模型...\n",
      "[ 64/500] train_loss: 0.07235 valid_loss: 0.07729 test_loss: 0.08748 \n",
      "验证损失减少 (0.078778 --> 0.077288). 正在保存模型...\n",
      "[ 65/500] train_loss: 0.07200 valid_loss: 0.07849 test_loss: 0.08674 \n",
      "[ 66/500] train_loss: 0.07312 valid_loss: 0.07667 test_loss: 0.08624 \n",
      "验证损失减少 (0.077288 --> 0.076673). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 67/500] train_loss: 0.07315 valid_loss: 0.07894 test_loss: 0.08659 \n",
      "[ 68/500] train_loss: 0.07130 valid_loss: 0.07985 test_loss: 0.08681 \n",
      "[ 69/500] train_loss: 0.07168 valid_loss: 0.07708 test_loss: 0.08643 \n",
      "[ 70/500] train_loss: 0.07154 valid_loss: 0.07665 test_loss: 0.08569 \n",
      "验证损失减少 (0.076673 --> 0.076653). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.07219 valid_loss: 0.07840 test_loss: 0.08739 \n",
      "[ 72/500] train_loss: 0.07086 valid_loss: 0.07671 test_loss: 0.08614 \n",
      "[ 73/500] train_loss: 0.07147 valid_loss: 0.07765 test_loss: 0.08711 \n",
      "[ 74/500] train_loss: 0.07085 valid_loss: 0.07726 test_loss: 0.08661 \n",
      "[ 75/500] train_loss: 0.07027 valid_loss: 0.07615 test_loss: 0.08596 \n",
      "验证损失减少 (0.076653 --> 0.076146). 正在保存模型...\n",
      "[ 76/500] train_loss: 0.06769 valid_loss: 0.07727 test_loss: 0.08550 \n",
      "[ 77/500] train_loss: 0.06821 valid_loss: 0.07718 test_loss: 0.08517 \n",
      "[ 78/500] train_loss: 0.07034 valid_loss: 0.07630 test_loss: 0.08719 \n",
      "[ 79/500] train_loss: 0.06832 valid_loss: 0.07771 test_loss: 0.08609 \n",
      "[ 80/500] train_loss: 0.06931 valid_loss: 0.07606 test_loss: 0.08624 \n",
      "验证损失减少 (0.076146 --> 0.076057). 正在保存模型...\n",
      "[ 81/500] train_loss: 0.06967 valid_loss: 0.07506 test_loss: 0.08396 \n",
      "验证损失减少 (0.076057 --> 0.075062). 正在保存模型...\n",
      "[ 82/500] train_loss: 0.06818 valid_loss: 0.07492 test_loss: 0.08317 \n",
      "验证损失减少 (0.075062 --> 0.074922). 正在保存模型...\n",
      "[ 83/500] train_loss: 0.06819 valid_loss: 0.07666 test_loss: 0.08397 \n",
      "[ 84/500] train_loss: 0.06736 valid_loss: 0.07622 test_loss: 0.08440 \n",
      "[ 85/500] train_loss: 0.06912 valid_loss: 0.07604 test_loss: 0.08664 \n",
      "[ 86/500] train_loss: 0.06720 valid_loss: 0.07561 test_loss: 0.08557 \n",
      "[ 87/500] train_loss: 0.06733 valid_loss: 0.07524 test_loss: 0.08500 \n",
      "[ 88/500] train_loss: 0.06910 valid_loss: 0.07784 test_loss: 0.08733 \n",
      "[ 89/500] train_loss: 0.06708 valid_loss: 0.07440 test_loss: 0.08532 \n",
      "验证损失减少 (0.074922 --> 0.074400). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.06715 valid_loss: 0.07446 test_loss: 0.08288 \n",
      "[ 91/500] train_loss: 0.06482 valid_loss: 0.07450 test_loss: 0.08386 \n",
      "[ 92/500] train_loss: 0.06610 valid_loss: 0.07467 test_loss: 0.08356 \n",
      "[ 93/500] train_loss: 0.06750 valid_loss: 0.07366 test_loss: 0.08328 \n",
      "验证损失减少 (0.074400 --> 0.073655). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.06684 valid_loss: 0.07315 test_loss: 0.08327 \n",
      "验证损失减少 (0.073655 --> 0.073154). 正在保存模型...\n",
      "[ 95/500] train_loss: 0.06547 valid_loss: 0.07410 test_loss: 0.08420 \n",
      "[ 96/500] train_loss: 0.06575 valid_loss: 0.07366 test_loss: 0.08386 \n",
      "[ 97/500] train_loss: 0.06544 valid_loss: 0.07522 test_loss: 0.08346 \n",
      "[ 98/500] train_loss: 0.06521 valid_loss: 0.07358 test_loss: 0.08226 \n",
      "[ 99/500] train_loss: 0.06422 valid_loss: 0.07524 test_loss: 0.08416 \n",
      "[100/500] train_loss: 0.06364 valid_loss: 0.07340 test_loss: 0.08200 \n",
      "[101/500] train_loss: 0.06574 valid_loss: 0.07457 test_loss: 0.08397 \n",
      "[102/500] train_loss: 0.06420 valid_loss: 0.07515 test_loss: 0.08349 \n",
      "[103/500] train_loss: 0.06299 valid_loss: 0.07401 test_loss: 0.08410 \n",
      "[104/500] train_loss: 0.06562 valid_loss: 0.07458 test_loss: 0.08280 \n",
      "[105/500] train_loss: 0.06437 valid_loss: 0.07444 test_loss: 0.08334 \n",
      "[106/500] train_loss: 0.06469 valid_loss: 0.07232 test_loss: 0.08192 \n",
      "验证损失减少 (0.073154 --> 0.072319). 正在保存模型...\n",
      "[107/500] train_loss: 0.06691 valid_loss: 0.07238 test_loss: 0.08277 \n",
      "[108/500] train_loss: 0.06188 valid_loss: 0.07356 test_loss: 0.08406 \n",
      "[109/500] train_loss: 0.06512 valid_loss: 0.07152 test_loss: 0.08253 \n",
      "验证损失减少 (0.072319 --> 0.071519). 正在保存模型...\n",
      "[110/500] train_loss: 0.06428 valid_loss: 0.07444 test_loss: 0.08278 \n",
      "[111/500] train_loss: 0.06167 valid_loss: 0.07282 test_loss: 0.08191 \n",
      "[112/500] train_loss: 0.06312 valid_loss: 0.07210 test_loss: 0.08169 \n",
      "[113/500] train_loss: 0.06483 valid_loss: 0.07309 test_loss: 0.08306 \n",
      "[114/500] train_loss: 0.06298 valid_loss: 0.07280 test_loss: 0.08131 \n",
      "[115/500] train_loss: 0.06541 valid_loss: 0.07380 test_loss: 0.08243 \n",
      "[116/500] train_loss: 0.06292 valid_loss: 0.07260 test_loss: 0.08336 \n",
      "[117/500] train_loss: 0.06182 valid_loss: 0.07448 test_loss: 0.08338 \n",
      "[118/500] train_loss: 0.06199 valid_loss: 0.07135 test_loss: 0.08101 \n",
      "验证损失减少 (0.071519 --> 0.071353). 正在保存模型...\n",
      "[119/500] train_loss: 0.06260 valid_loss: 0.07219 test_loss: 0.08105 \n",
      "[120/500] train_loss: 0.06191 valid_loss: 0.07201 test_loss: 0.08023 \n",
      "[121/500] train_loss: 0.06333 valid_loss: 0.07349 test_loss: 0.08265 \n",
      "[122/500] train_loss: 0.06209 valid_loss: 0.07304 test_loss: 0.08259 \n",
      "[123/500] train_loss: 0.06165 valid_loss: 0.07345 test_loss: 0.08064 \n",
      "[124/500] train_loss: 0.06127 valid_loss: 0.07260 test_loss: 0.08197 \n",
      "[125/500] train_loss: 0.06110 valid_loss: 0.07356 test_loss: 0.08032 \n",
      "[126/500] train_loss: 0.06216 valid_loss: 0.07297 test_loss: 0.08034 \n",
      "[127/500] train_loss: 0.06218 valid_loss: 0.07199 test_loss: 0.08314 \n",
      "[128/500] train_loss: 0.06181 valid_loss: 0.07221 test_loss: 0.08165 \n",
      "[129/500] train_loss: 0.06090 valid_loss: 0.07222 test_loss: 0.08097 \n",
      "[130/500] train_loss: 0.06125 valid_loss: 0.07176 test_loss: 0.08122 \n",
      "[131/500] train_loss: 0.06114 valid_loss: 0.07233 test_loss: 0.08145 \n",
      "[132/500] train_loss: 0.05988 valid_loss: 0.07297 test_loss: 0.08185 \n",
      "[133/500] train_loss: 0.05859 valid_loss: 0.07283 test_loss: 0.08108 \n",
      "[134/500] train_loss: 0.06081 valid_loss: 0.07254 test_loss: 0.08030 \n",
      "[135/500] train_loss: 0.06097 valid_loss: 0.07255 test_loss: 0.08185 \n",
      "[136/500] train_loss: 0.05899 valid_loss: 0.07356 test_loss: 0.08103 \n",
      "[137/500] train_loss: 0.05998 valid_loss: 0.07354 test_loss: 0.08069 \n",
      "[138/500] train_loss: 0.05900 valid_loss: 0.07361 test_loss: 0.08312 \n",
      "[139/500] train_loss: 0.05845 valid_loss: 0.07395 test_loss: 0.08156 \n",
      "[140/500] train_loss: 0.05900 valid_loss: 0.07282 test_loss: 0.08256 \n",
      "[141/500] train_loss: 0.05918 valid_loss: 0.07422 test_loss: 0.08098 \n",
      "[142/500] train_loss: 0.05897 valid_loss: 0.07350 test_loss: 0.08017 \n",
      "[143/500] train_loss: 0.05899 valid_loss: 0.07385 test_loss: 0.08209 \n",
      "[144/500] train_loss: 0.05855 valid_loss: 0.07249 test_loss: 0.08071 \n",
      "[145/500] train_loss: 0.05901 valid_loss: 0.07228 test_loss: 0.07883 \n",
      "[146/500] train_loss: 0.05783 valid_loss: 0.07199 test_loss: 0.08058 \n",
      "[147/500] train_loss: 0.05793 valid_loss: 0.07153 test_loss: 0.08049 \n",
      "[148/500] train_loss: 0.05951 valid_loss: 0.07117 test_loss: 0.07964 \n",
      "验证损失减少 (0.071353 --> 0.071168). 正在保存模型...\n",
      "[149/500] train_loss: 0.05949 valid_loss: 0.07238 test_loss: 0.08016 \n",
      "[150/500] train_loss: 0.05969 valid_loss: 0.07194 test_loss: 0.08009 \n",
      "[151/500] train_loss: 0.05931 valid_loss: 0.07275 test_loss: 0.08090 \n",
      "[152/500] train_loss: 0.06106 valid_loss: 0.07179 test_loss: 0.07998 \n",
      "[153/500] train_loss: 0.05673 valid_loss: 0.07316 test_loss: 0.08170 \n",
      "[154/500] train_loss: 0.05881 valid_loss: 0.07260 test_loss: 0.08063 \n",
      "[155/500] train_loss: 0.05887 valid_loss: 0.07127 test_loss: 0.08083 \n",
      "[156/500] train_loss: 0.05684 valid_loss: 0.07321 test_loss: 0.08288 \n",
      "[157/500] train_loss: 0.05799 valid_loss: 0.07161 test_loss: 0.07980 \n",
      "[158/500] train_loss: 0.05648 valid_loss: 0.07363 test_loss: 0.07988 \n",
      "[159/500] train_loss: 0.05885 valid_loss: 0.07240 test_loss: 0.08056 \n",
      "[160/500] train_loss: 0.05855 valid_loss: 0.07303 test_loss: 0.08047 \n",
      "[161/500] train_loss: 0.05861 valid_loss: 0.07329 test_loss: 0.07947 \n",
      "[162/500] train_loss: 0.05665 valid_loss: 0.07070 test_loss: 0.07874 \n",
      "验证损失减少 (0.071168 --> 0.070701). 正在保存模型...\n",
      "[163/500] train_loss: 0.05754 valid_loss: 0.07274 test_loss: 0.07932 \n",
      "[164/500] train_loss: 0.05700 valid_loss: 0.07128 test_loss: 0.07796 \n",
      "[165/500] train_loss: 0.05640 valid_loss: 0.07225 test_loss: 0.07960 \n",
      "[166/500] train_loss: 0.05655 valid_loss: 0.07391 test_loss: 0.07955 \n",
      "[167/500] train_loss: 0.05760 valid_loss: 0.07194 test_loss: 0.07915 \n",
      "[168/500] train_loss: 0.05578 valid_loss: 0.07243 test_loss: 0.07960 \n",
      "[169/500] train_loss: 0.05659 valid_loss: 0.07125 test_loss: 0.07854 \n",
      "[170/500] train_loss: 0.05583 valid_loss: 0.07232 test_loss: 0.07832 \n",
      "[171/500] train_loss: 0.05696 valid_loss: 0.07235 test_loss: 0.07858 \n",
      "[172/500] train_loss: 0.05640 valid_loss: 0.07114 test_loss: 0.07842 \n",
      "[173/500] train_loss: 0.05587 valid_loss: 0.07019 test_loss: 0.07888 \n",
      "验证损失减少 (0.070701 --> 0.070194). 正在保存模型...\n",
      "[174/500] train_loss: 0.05465 valid_loss: 0.07064 test_loss: 0.08014 \n",
      "[175/500] train_loss: 0.05521 valid_loss: 0.07250 test_loss: 0.07824 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176/500] train_loss: 0.05464 valid_loss: 0.07178 test_loss: 0.07879 \n",
      "[177/500] train_loss: 0.05640 valid_loss: 0.07333 test_loss: 0.07862 \n",
      "[178/500] train_loss: 0.05732 valid_loss: 0.07246 test_loss: 0.08016 \n",
      "[179/500] train_loss: 0.05537 valid_loss: 0.07184 test_loss: 0.07907 \n",
      "[180/500] train_loss: 0.05507 valid_loss: 0.07305 test_loss: 0.07916 \n",
      "[181/500] train_loss: 0.05585 valid_loss: 0.07163 test_loss: 0.07861 \n",
      "[182/500] train_loss: 0.05525 valid_loss: 0.07132 test_loss: 0.07962 \n",
      "[183/500] train_loss: 0.05730 valid_loss: 0.07116 test_loss: 0.07784 \n",
      "[184/500] train_loss: 0.05619 valid_loss: 0.07007 test_loss: 0.07747 \n",
      "验证损失减少 (0.070194 --> 0.070072). 正在保存模型...\n",
      "[185/500] train_loss: 0.05536 valid_loss: 0.07103 test_loss: 0.07806 \n",
      "[186/500] train_loss: 0.05532 valid_loss: 0.07175 test_loss: 0.07872 \n",
      "[187/500] train_loss: 0.05651 valid_loss: 0.07107 test_loss: 0.07873 \n",
      "[188/500] train_loss: 0.05542 valid_loss: 0.07384 test_loss: 0.07878 \n",
      "[189/500] train_loss: 0.05571 valid_loss: 0.07223 test_loss: 0.08017 \n",
      "[190/500] train_loss: 0.05542 valid_loss: 0.07092 test_loss: 0.07894 \n",
      "[191/500] train_loss: 0.05608 valid_loss: 0.07108 test_loss: 0.07838 \n",
      "[192/500] train_loss: 0.05490 valid_loss: 0.07189 test_loss: 0.07931 \n",
      "[193/500] train_loss: 0.05324 valid_loss: 0.07154 test_loss: 0.08042 \n",
      "[194/500] train_loss: 0.05388 valid_loss: 0.07150 test_loss: 0.08011 \n",
      "[195/500] train_loss: 0.05304 valid_loss: 0.07256 test_loss: 0.07899 \n",
      "[196/500] train_loss: 0.05318 valid_loss: 0.07110 test_loss: 0.07929 \n",
      "[197/500] train_loss: 0.05409 valid_loss: 0.07181 test_loss: 0.07963 \n",
      "[198/500] train_loss: 0.05481 valid_loss: 0.07239 test_loss: 0.07930 \n",
      "[199/500] train_loss: 0.05416 valid_loss: 0.07326 test_loss: 0.07866 \n",
      "[200/500] train_loss: 0.05280 valid_loss: 0.07150 test_loss: 0.07845 \n",
      "[201/500] train_loss: 0.05395 valid_loss: 0.07171 test_loss: 0.08063 \n",
      "[202/500] train_loss: 0.05377 valid_loss: 0.07604 test_loss: 0.08189 \n",
      "[203/500] train_loss: 0.05290 valid_loss: 0.07292 test_loss: 0.07930 \n",
      "[204/500] train_loss: 0.05335 valid_loss: 0.07160 test_loss: 0.07857 \n",
      "[205/500] train_loss: 0.05448 valid_loss: 0.07173 test_loss: 0.07888 \n",
      "[206/500] train_loss: 0.05429 valid_loss: 0.07386 test_loss: 0.07946 \n",
      "[207/500] train_loss: 0.05342 valid_loss: 0.07178 test_loss: 0.07893 \n",
      "[208/500] train_loss: 0.05429 valid_loss: 0.07179 test_loss: 0.07894 \n",
      "[209/500] train_loss: 0.05333 valid_loss: 0.07082 test_loss: 0.08045 \n",
      "[210/500] train_loss: 0.05410 valid_loss: 0.07185 test_loss: 0.08153 \n",
      "[211/500] train_loss: 0.05457 valid_loss: 0.06986 test_loss: 0.07920 \n",
      "验证损失减少 (0.070072 --> 0.069856). 正在保存模型...\n",
      "[212/500] train_loss: 0.05437 valid_loss: 0.07178 test_loss: 0.07924 \n",
      "[213/500] train_loss: 0.05316 valid_loss: 0.07113 test_loss: 0.07942 \n",
      "[214/500] train_loss: 0.05345 valid_loss: 0.07236 test_loss: 0.07889 \n",
      "[215/500] train_loss: 0.05163 valid_loss: 0.07182 test_loss: 0.08065 \n",
      "[216/500] train_loss: 0.05114 valid_loss: 0.07047 test_loss: 0.07972 \n",
      "[217/500] train_loss: 0.05268 valid_loss: 0.07126 test_loss: 0.07974 \n",
      "[218/500] train_loss: 0.05199 valid_loss: 0.06970 test_loss: 0.07910 \n",
      "验证损失减少 (0.069856 --> 0.069698). 正在保存模型...\n",
      "[219/500] train_loss: 0.05246 valid_loss: 0.07015 test_loss: 0.07951 \n",
      "[220/500] train_loss: 0.05191 valid_loss: 0.07079 test_loss: 0.08168 \n",
      "[221/500] train_loss: 0.05161 valid_loss: 0.07041 test_loss: 0.07950 \n",
      "[222/500] train_loss: 0.05249 valid_loss: 0.06892 test_loss: 0.08059 \n",
      "验证损失减少 (0.069698 --> 0.068918). 正在保存模型...\n",
      "[223/500] train_loss: 0.05330 valid_loss: 0.06972 test_loss: 0.07899 \n",
      "[224/500] train_loss: 0.05251 valid_loss: 0.07037 test_loss: 0.07896 \n",
      "[225/500] train_loss: 0.05154 valid_loss: 0.07200 test_loss: 0.07946 \n",
      "[226/500] train_loss: 0.05077 valid_loss: 0.07085 test_loss: 0.07962 \n",
      "[227/500] train_loss: 0.05235 valid_loss: 0.07091 test_loss: 0.08003 \n",
      "[228/500] train_loss: 0.05245 valid_loss: 0.07109 test_loss: 0.07921 \n",
      "[229/500] train_loss: 0.05149 valid_loss: 0.07034 test_loss: 0.07869 \n",
      "[230/500] train_loss: 0.05153 valid_loss: 0.07051 test_loss: 0.08073 \n",
      "[231/500] train_loss: 0.05028 valid_loss: 0.06954 test_loss: 0.08007 \n",
      "[232/500] train_loss: 0.05034 valid_loss: 0.06999 test_loss: 0.07934 \n",
      "[233/500] train_loss: 0.05094 valid_loss: 0.07349 test_loss: 0.08007 \n",
      "[234/500] train_loss: 0.05223 valid_loss: 0.07132 test_loss: 0.08030 \n",
      "[235/500] train_loss: 0.05208 valid_loss: 0.07041 test_loss: 0.07781 \n",
      "[236/500] train_loss: 0.05203 valid_loss: 0.07092 test_loss: 0.07937 \n",
      "[237/500] train_loss: 0.05105 valid_loss: 0.07322 test_loss: 0.07802 \n",
      "[238/500] train_loss: 0.05116 valid_loss: 0.07243 test_loss: 0.08014 \n",
      "[239/500] train_loss: 0.05148 valid_loss: 0.07131 test_loss: 0.08040 \n",
      "[240/500] train_loss: 0.05073 valid_loss: 0.07244 test_loss: 0.07897 \n",
      "[241/500] train_loss: 0.04967 valid_loss: 0.06947 test_loss: 0.07808 \n",
      "[242/500] train_loss: 0.05201 valid_loss: 0.07269 test_loss: 0.07840 \n",
      "[243/500] train_loss: 0.05121 valid_loss: 0.07262 test_loss: 0.07992 \n",
      "[244/500] train_loss: 0.05065 valid_loss: 0.07205 test_loss: 0.07986 \n",
      "[245/500] train_loss: 0.05161 valid_loss: 0.07114 test_loss: 0.07867 \n",
      "[246/500] train_loss: 0.05133 valid_loss: 0.07068 test_loss: 0.07944 \n",
      "[247/500] train_loss: 0.05095 valid_loss: 0.06993 test_loss: 0.07953 \n",
      "[248/500] train_loss: 0.05130 valid_loss: 0.06942 test_loss: 0.07972 \n",
      "[249/500] train_loss: 0.05079 valid_loss: 0.07101 test_loss: 0.07914 \n",
      "[250/500] train_loss: 0.05067 valid_loss: 0.07044 test_loss: 0.07912 \n",
      "[251/500] train_loss: 0.05113 valid_loss: 0.07367 test_loss: 0.07948 \n",
      "[252/500] train_loss: 0.04933 valid_loss: 0.07082 test_loss: 0.07881 \n",
      "[253/500] train_loss: 0.04937 valid_loss: 0.07116 test_loss: 0.08057 \n",
      "[254/500] train_loss: 0.04889 valid_loss: 0.07119 test_loss: 0.08006 \n",
      "[255/500] train_loss: 0.04946 valid_loss: 0.07010 test_loss: 0.07884 \n",
      "[256/500] train_loss: 0.04988 valid_loss: 0.06941 test_loss: 0.07928 \n",
      "[257/500] train_loss: 0.05059 valid_loss: 0.07088 test_loss: 0.08045 \n",
      "[258/500] train_loss: 0.05040 valid_loss: 0.06902 test_loss: 0.07902 \n",
      "[259/500] train_loss: 0.04954 valid_loss: 0.07034 test_loss: 0.07908 \n",
      "[260/500] train_loss: 0.04797 valid_loss: 0.07139 test_loss: 0.07948 \n",
      "[261/500] train_loss: 0.05017 valid_loss: 0.07149 test_loss: 0.07940 \n",
      "[262/500] train_loss: 0.04900 valid_loss: 0.06928 test_loss: 0.07802 \n",
      "[263/500] train_loss: 0.04861 valid_loss: 0.07109 test_loss: 0.08017 \n",
      "[264/500] train_loss: 0.04878 valid_loss: 0.07043 test_loss: 0.07890 \n",
      "[265/500] train_loss: 0.05043 valid_loss: 0.07047 test_loss: 0.07849 \n",
      "[266/500] train_loss: 0.05009 valid_loss: 0.06984 test_loss: 0.08069 \n",
      "[267/500] train_loss: 0.05074 valid_loss: 0.07303 test_loss: 0.07944 \n",
      "[268/500] train_loss: 0.04947 valid_loss: 0.07144 test_loss: 0.08049 \n",
      "[269/500] train_loss: 0.04931 valid_loss: 0.07088 test_loss: 0.08009 \n",
      "[270/500] train_loss: 0.04885 valid_loss: 0.07175 test_loss: 0.08043 \n",
      "[271/500] train_loss: 0.04826 valid_loss: 0.07075 test_loss: 0.07882 \n",
      "[272/500] train_loss: 0.04994 valid_loss: 0.07056 test_loss: 0.07902 \n",
      "[273/500] train_loss: 0.05006 valid_loss: 0.07070 test_loss: 0.07854 \n",
      "[274/500] train_loss: 0.04893 valid_loss: 0.07113 test_loss: 0.07922 \n",
      "[275/500] train_loss: 0.04921 valid_loss: 0.07108 test_loss: 0.07958 \n",
      "[276/500] train_loss: 0.04987 valid_loss: 0.07083 test_loss: 0.07868 \n",
      "[277/500] train_loss: 0.04883 valid_loss: 0.06977 test_loss: 0.07831 \n",
      "[278/500] train_loss: 0.04976 valid_loss: 0.06984 test_loss: 0.07728 \n",
      "[279/500] train_loss: 0.04781 valid_loss: 0.07333 test_loss: 0.08121 \n",
      "[280/500] train_loss: 0.05007 valid_loss: 0.07144 test_loss: 0.07859 \n",
      "[281/500] train_loss: 0.04758 valid_loss: 0.07236 test_loss: 0.08051 \n",
      "[282/500] train_loss: 0.04662 valid_loss: 0.07312 test_loss: 0.07942 \n",
      "[283/500] train_loss: 0.04899 valid_loss: 0.07121 test_loss: 0.07849 \n",
      "[284/500] train_loss: 0.04691 valid_loss: 0.07204 test_loss: 0.08003 \n",
      "[285/500] train_loss: 0.04865 valid_loss: 0.06986 test_loss: 0.07846 \n",
      "[286/500] train_loss: 0.04758 valid_loss: 0.07187 test_loss: 0.07962 \n",
      "[287/500] train_loss: 0.04999 valid_loss: 0.07295 test_loss: 0.07903 \n",
      "[288/500] train_loss: 0.04725 valid_loss: 0.07057 test_loss: 0.08009 \n",
      "[289/500] train_loss: 0.04668 valid_loss: 0.06980 test_loss: 0.07972 \n",
      "[290/500] train_loss: 0.04738 valid_loss: 0.07118 test_loss: 0.07907 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291/500] train_loss: 0.04763 valid_loss: 0.07049 test_loss: 0.08062 \n",
      "[292/500] train_loss: 0.04766 valid_loss: 0.07025 test_loss: 0.07897 \n",
      "[293/500] train_loss: 0.04794 valid_loss: 0.07085 test_loss: 0.08149 \n",
      "[294/500] train_loss: 0.04602 valid_loss: 0.07194 test_loss: 0.08021 \n",
      "[295/500] train_loss: 0.04875 valid_loss: 0.07084 test_loss: 0.07934 \n",
      "[296/500] train_loss: 0.04875 valid_loss: 0.07058 test_loss: 0.08012 \n",
      "[297/500] train_loss: 0.04724 valid_loss: 0.07218 test_loss: 0.08058 \n",
      "[298/500] train_loss: 0.04681 valid_loss: 0.07146 test_loss: 0.08054 \n",
      "[299/500] train_loss: 0.04624 valid_loss: 0.07347 test_loss: 0.08167 \n",
      "[300/500] train_loss: 0.04768 valid_loss: 0.07192 test_loss: 0.07992 \n",
      "[301/500] train_loss: 0.04730 valid_loss: 0.07214 test_loss: 0.08105 \n",
      "[302/500] train_loss: 0.04638 valid_loss: 0.07026 test_loss: 0.07978 \n",
      "[303/500] train_loss: 0.04690 valid_loss: 0.07008 test_loss: 0.08016 \n",
      "[304/500] train_loss: 0.04763 valid_loss: 0.07106 test_loss: 0.08111 \n",
      "[305/500] train_loss: 0.04742 valid_loss: 0.07100 test_loss: 0.07839 \n",
      "[306/500] train_loss: 0.04707 valid_loss: 0.06938 test_loss: 0.07838 \n",
      "[307/500] train_loss: 0.04679 valid_loss: 0.07018 test_loss: 0.07876 \n",
      "[308/500] train_loss: 0.04718 valid_loss: 0.07065 test_loss: 0.08001 \n",
      "[309/500] train_loss: 0.04788 valid_loss: 0.07002 test_loss: 0.08059 \n",
      "[310/500] train_loss: 0.04726 valid_loss: 0.07122 test_loss: 0.07996 \n",
      "[311/500] train_loss: 0.04618 valid_loss: 0.07047 test_loss: 0.08052 \n",
      "[312/500] train_loss: 0.04664 valid_loss: 0.06979 test_loss: 0.08213 \n",
      "[313/500] train_loss: 0.04564 valid_loss: 0.07153 test_loss: 0.08041 \n",
      "[314/500] train_loss: 0.04694 valid_loss: 0.07046 test_loss: 0.07922 \n",
      "[315/500] train_loss: 0.04584 valid_loss: 0.07213 test_loss: 0.08152 \n",
      "[316/500] train_loss: 0.04580 valid_loss: 0.07203 test_loss: 0.07995 \n",
      "[317/500] train_loss: 0.04681 valid_loss: 0.07147 test_loss: 0.08039 \n",
      "[318/500] train_loss: 0.04543 valid_loss: 0.07160 test_loss: 0.07897 \n",
      "[319/500] train_loss: 0.04666 valid_loss: 0.07078 test_loss: 0.07927 \n",
      "[320/500] train_loss: 0.04713 valid_loss: 0.07356 test_loss: 0.08009 \n",
      "[321/500] train_loss: 0.04597 valid_loss: 0.07174 test_loss: 0.07908 \n",
      "[322/500] train_loss: 0.04669 valid_loss: 0.07140 test_loss: 0.08080 \n",
      "[323/500] train_loss: 0.04688 valid_loss: 0.07135 test_loss: 0.08211 \n",
      "[324/500] train_loss: 0.04643 valid_loss: 0.07319 test_loss: 0.08259 \n",
      "[325/500] train_loss: 0.04543 valid_loss: 0.07140 test_loss: 0.08010 \n",
      "[326/500] train_loss: 0.04691 valid_loss: 0.07056 test_loss: 0.08116 \n",
      "[327/500] train_loss: 0.04695 valid_loss: 0.07089 test_loss: 0.08039 \n",
      "[328/500] train_loss: 0.04470 valid_loss: 0.06976 test_loss: 0.08105 \n",
      "[329/500] train_loss: 0.04633 valid_loss: 0.06997 test_loss: 0.07981 \n",
      "[330/500] train_loss: 0.04677 valid_loss: 0.07200 test_loss: 0.08008 \n",
      "[331/500] train_loss: 0.04567 valid_loss: 0.07015 test_loss: 0.08061 \n",
      "[332/500] train_loss: 0.04717 valid_loss: 0.07156 test_loss: 0.07989 \n",
      "[333/500] train_loss: 0.04596 valid_loss: 0.06988 test_loss: 0.07850 \n",
      "[334/500] train_loss: 0.04489 valid_loss: 0.07121 test_loss: 0.07920 \n",
      "[335/500] train_loss: 0.04505 valid_loss: 0.07034 test_loss: 0.07947 \n",
      "[336/500] train_loss: 0.04524 valid_loss: 0.07197 test_loss: 0.08085 \n",
      "[337/500] train_loss: 0.04642 valid_loss: 0.07074 test_loss: 0.08141 \n",
      "[338/500] train_loss: 0.04526 valid_loss: 0.06994 test_loss: 0.08000 \n",
      "[339/500] train_loss: 0.04522 valid_loss: 0.07033 test_loss: 0.07955 \n",
      "[340/500] train_loss: 0.04666 valid_loss: 0.07026 test_loss: 0.08046 \n",
      "[341/500] train_loss: 0.04547 valid_loss: 0.07000 test_loss: 0.08237 \n",
      "[342/500] train_loss: 0.04497 valid_loss: 0.07173 test_loss: 0.08096 \n",
      "[343/500] train_loss: 0.04691 valid_loss: 0.07117 test_loss: 0.07927 \n",
      "[344/500] train_loss: 0.04550 valid_loss: 0.07114 test_loss: 0.08002 \n",
      "[345/500] train_loss: 0.04616 valid_loss: 0.07228 test_loss: 0.08093 \n",
      "[346/500] train_loss: 0.04425 valid_loss: 0.07203 test_loss: 0.07988 \n",
      "[347/500] train_loss: 0.04566 valid_loss: 0.07221 test_loss: 0.08068 \n",
      "[348/500] train_loss: 0.04590 valid_loss: 0.07155 test_loss: 0.08048 \n",
      "[349/500] train_loss: 0.04459 valid_loss: 0.07217 test_loss: 0.08230 \n",
      "[350/500] train_loss: 0.04538 valid_loss: 0.07104 test_loss: 0.08180 \n",
      "[351/500] train_loss: 0.04558 valid_loss: 0.07243 test_loss: 0.08099 \n",
      "[352/500] train_loss: 0.04616 valid_loss: 0.07110 test_loss: 0.08103 \n",
      "[353/500] train_loss: 0.04516 valid_loss: 0.07009 test_loss: 0.08151 \n",
      "[354/500] train_loss: 0.04501 valid_loss: 0.07327 test_loss: 0.08181 \n",
      "[355/500] train_loss: 0.04671 valid_loss: 0.07046 test_loss: 0.07886 \n",
      "[356/500] train_loss: 0.04413 valid_loss: 0.07182 test_loss: 0.08030 \n",
      "[357/500] train_loss: 0.04562 valid_loss: 0.07124 test_loss: 0.08055 \n",
      "[358/500] train_loss: 0.04529 valid_loss: 0.06966 test_loss: 0.07944 \n",
      "[359/500] train_loss: 0.04468 valid_loss: 0.06979 test_loss: 0.08121 \n",
      "[360/500] train_loss: 0.04505 valid_loss: 0.07062 test_loss: 0.07953 \n",
      "[361/500] train_loss: 0.04486 valid_loss: 0.07403 test_loss: 0.07959 \n",
      "[362/500] train_loss: 0.04578 valid_loss: 0.07115 test_loss: 0.07947 \n",
      "[363/500] train_loss: 0.04585 valid_loss: 0.07197 test_loss: 0.08137 \n",
      "[364/500] train_loss: 0.04646 valid_loss: 0.07231 test_loss: 0.07972 \n",
      "[365/500] train_loss: 0.04397 valid_loss: 0.07178 test_loss: 0.08176 \n",
      "[366/500] train_loss: 0.04466 valid_loss: 0.07146 test_loss: 0.08107 \n",
      "[367/500] train_loss: 0.04436 valid_loss: 0.07184 test_loss: 0.08096 \n",
      "[368/500] train_loss: 0.04507 valid_loss: 0.07072 test_loss: 0.08056 \n",
      "[369/500] train_loss: 0.04497 valid_loss: 0.06961 test_loss: 0.07888 \n",
      "[370/500] train_loss: 0.04298 valid_loss: 0.07195 test_loss: 0.08205 \n",
      "[371/500] train_loss: 0.04408 valid_loss: 0.07112 test_loss: 0.08142 \n",
      "[372/500] train_loss: 0.04351 valid_loss: 0.07168 test_loss: 0.08107 \n",
      "[373/500] train_loss: 0.04539 valid_loss: 0.07155 test_loss: 0.08114 \n",
      "[374/500] train_loss: 0.04314 valid_loss: 0.07157 test_loss: 0.08065 \n",
      "[375/500] train_loss: 0.04461 valid_loss: 0.07142 test_loss: 0.08125 \n",
      "[376/500] train_loss: 0.04476 valid_loss: 0.06949 test_loss: 0.07985 \n",
      "[377/500] train_loss: 0.04382 valid_loss: 0.06999 test_loss: 0.08054 \n",
      "[378/500] train_loss: 0.04429 valid_loss: 0.07077 test_loss: 0.08234 \n",
      "[379/500] train_loss: 0.04461 valid_loss: 0.06889 test_loss: 0.08076 \n",
      "验证损失减少 (0.068918 --> 0.068885). 正在保存模型...\n",
      "[380/500] train_loss: 0.04459 valid_loss: 0.07042 test_loss: 0.08209 \n",
      "[381/500] train_loss: 0.04434 valid_loss: 0.07103 test_loss: 0.08123 \n",
      "[382/500] train_loss: 0.04327 valid_loss: 0.07075 test_loss: 0.08128 \n",
      "[383/500] train_loss: 0.04419 valid_loss: 0.07033 test_loss: 0.08224 \n",
      "[384/500] train_loss: 0.04463 valid_loss: 0.07237 test_loss: 0.08089 \n",
      "[385/500] train_loss: 0.04404 valid_loss: 0.07388 test_loss: 0.08339 \n",
      "[386/500] train_loss: 0.04323 valid_loss: 0.07071 test_loss: 0.08111 \n",
      "[387/500] train_loss: 0.04448 valid_loss: 0.07221 test_loss: 0.08134 \n",
      "[388/500] train_loss: 0.04386 valid_loss: 0.07154 test_loss: 0.08084 \n",
      "[389/500] train_loss: 0.04371 valid_loss: 0.07183 test_loss: 0.07973 \n",
      "[390/500] train_loss: 0.04382 valid_loss: 0.07081 test_loss: 0.08032 \n",
      "[391/500] train_loss: 0.04327 valid_loss: 0.07203 test_loss: 0.07970 \n",
      "[392/500] train_loss: 0.04419 valid_loss: 0.07205 test_loss: 0.08078 \n",
      "[393/500] train_loss: 0.04370 valid_loss: 0.07305 test_loss: 0.07992 \n",
      "[394/500] train_loss: 0.04318 valid_loss: 0.07200 test_loss: 0.08125 \n",
      "[395/500] train_loss: 0.04319 valid_loss: 0.07088 test_loss: 0.08096 \n",
      "[396/500] train_loss: 0.04355 valid_loss: 0.07187 test_loss: 0.08170 \n",
      "[397/500] train_loss: 0.04281 valid_loss: 0.07123 test_loss: 0.08026 \n",
      "[398/500] train_loss: 0.04233 valid_loss: 0.07350 test_loss: 0.08060 \n",
      "[399/500] train_loss: 0.04205 valid_loss: 0.07215 test_loss: 0.08062 \n",
      "[400/500] train_loss: 0.04328 valid_loss: 0.07485 test_loss: 0.08014 \n",
      "[401/500] train_loss: 0.04332 valid_loss: 0.07100 test_loss: 0.08184 \n",
      "[402/500] train_loss: 0.04318 valid_loss: 0.07114 test_loss: 0.08184 \n",
      "[403/500] train_loss: 0.04295 valid_loss: 0.07156 test_loss: 0.08031 \n",
      "[404/500] train_loss: 0.04386 valid_loss: 0.07077 test_loss: 0.07897 \n",
      "[405/500] train_loss: 0.04271 valid_loss: 0.07108 test_loss: 0.08103 \n",
      "[406/500] train_loss: 0.04225 valid_loss: 0.07248 test_loss: 0.08017 \n",
      "[407/500] train_loss: 0.04359 valid_loss: 0.07185 test_loss: 0.08030 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[408/500] train_loss: 0.04271 valid_loss: 0.07244 test_loss: 0.08093 \n",
      "[409/500] train_loss: 0.04333 valid_loss: 0.07293 test_loss: 0.07966 \n",
      "[410/500] train_loss: 0.04442 valid_loss: 0.07214 test_loss: 0.08000 \n",
      "[411/500] train_loss: 0.04411 valid_loss: 0.07392 test_loss: 0.08143 \n",
      "[412/500] train_loss: 0.04263 valid_loss: 0.07291 test_loss: 0.08163 \n",
      "[413/500] train_loss: 0.04274 valid_loss: 0.07124 test_loss: 0.08210 \n",
      "[414/500] train_loss: 0.04206 valid_loss: 0.07276 test_loss: 0.08125 \n",
      "[415/500] train_loss: 0.04222 valid_loss: 0.07340 test_loss: 0.08192 \n",
      "[416/500] train_loss: 0.04443 valid_loss: 0.07188 test_loss: 0.07950 \n",
      "[417/500] train_loss: 0.04334 valid_loss: 0.07395 test_loss: 0.08233 \n",
      "[418/500] train_loss: 0.04275 valid_loss: 0.07159 test_loss: 0.07973 \n",
      "[419/500] train_loss: 0.04294 valid_loss: 0.07119 test_loss: 0.08005 \n",
      "[420/500] train_loss: 0.04322 valid_loss: 0.07141 test_loss: 0.07996 \n",
      "[421/500] train_loss: 0.04285 valid_loss: 0.07221 test_loss: 0.07980 \n",
      "[422/500] train_loss: 0.04226 valid_loss: 0.07064 test_loss: 0.08012 \n",
      "[423/500] train_loss: 0.04149 valid_loss: 0.07400 test_loss: 0.08049 \n",
      "[424/500] train_loss: 0.04341 valid_loss: 0.07850 test_loss: 0.08126 \n",
      "[425/500] train_loss: 0.04313 valid_loss: 0.07100 test_loss: 0.08140 \n",
      "[426/500] train_loss: 0.04339 valid_loss: 0.07032 test_loss: 0.08110 \n",
      "[427/500] train_loss: 0.04229 valid_loss: 0.07482 test_loss: 0.08440 \n",
      "[428/500] train_loss: 0.04267 valid_loss: 0.06990 test_loss: 0.08237 \n",
      "[429/500] train_loss: 0.04201 valid_loss: 0.07138 test_loss: 0.08245 \n",
      "[430/500] train_loss: 0.04181 valid_loss: 0.07224 test_loss: 0.08087 \n",
      "[431/500] train_loss: 0.04216 valid_loss: 0.07193 test_loss: 0.08141 \n",
      "[432/500] train_loss: 0.04187 valid_loss: 0.07200 test_loss: 0.08263 \n",
      "[433/500] train_loss: 0.04345 valid_loss: 0.07180 test_loss: 0.08092 \n",
      "[434/500] train_loss: 0.04240 valid_loss: 0.07172 test_loss: 0.08056 \n",
      "[435/500] train_loss: 0.04121 valid_loss: 0.07227 test_loss: 0.07990 \n",
      "[436/500] train_loss: 0.04146 valid_loss: 0.07207 test_loss: 0.08103 \n",
      "[437/500] train_loss: 0.04247 valid_loss: 0.07363 test_loss: 0.08128 \n",
      "[438/500] train_loss: 0.04274 valid_loss: 0.07261 test_loss: 0.07909 \n",
      "[439/500] train_loss: 0.04127 valid_loss: 0.07134 test_loss: 0.07921 \n",
      "[440/500] train_loss: 0.04300 valid_loss: 0.07093 test_loss: 0.07956 \n",
      "[441/500] train_loss: 0.04159 valid_loss: 0.07413 test_loss: 0.08128 \n",
      "[442/500] train_loss: 0.04240 valid_loss: 0.07392 test_loss: 0.08100 \n",
      "[443/500] train_loss: 0.04143 valid_loss: 0.07239 test_loss: 0.08084 \n",
      "[444/500] train_loss: 0.04144 valid_loss: 0.07330 test_loss: 0.08092 \n",
      "[445/500] train_loss: 0.04200 valid_loss: 0.07244 test_loss: 0.07995 \n",
      "[446/500] train_loss: 0.04172 valid_loss: 0.07226 test_loss: 0.07952 \n",
      "[447/500] train_loss: 0.04219 valid_loss: 0.07158 test_loss: 0.07988 \n",
      "[448/500] train_loss: 0.04117 valid_loss: 0.07313 test_loss: 0.08371 \n",
      "[449/500] train_loss: 0.04234 valid_loss: 0.07264 test_loss: 0.08242 \n",
      "[450/500] train_loss: 0.04286 valid_loss: 0.07069 test_loss: 0.08152 \n",
      "[451/500] train_loss: 0.04177 valid_loss: 0.07229 test_loss: 0.08172 \n",
      "[452/500] train_loss: 0.04163 valid_loss: 0.07315 test_loss: 0.08182 \n",
      "[453/500] train_loss: 0.04181 valid_loss: 0.07196 test_loss: 0.08034 \n",
      "[454/500] train_loss: 0.04161 valid_loss: 0.07251 test_loss: 0.08264 \n",
      "[455/500] train_loss: 0.04197 valid_loss: 0.07253 test_loss: 0.08262 \n",
      "[456/500] train_loss: 0.04114 valid_loss: 0.07215 test_loss: 0.08264 \n",
      "[457/500] train_loss: 0.04123 valid_loss: 0.07324 test_loss: 0.08138 \n",
      "[458/500] train_loss: 0.04111 valid_loss: 0.07285 test_loss: 0.08289 \n",
      "[459/500] train_loss: 0.04127 valid_loss: 0.07060 test_loss: 0.08235 \n",
      "[460/500] train_loss: 0.04181 valid_loss: 0.07101 test_loss: 0.08218 \n",
      "[461/500] train_loss: 0.04069 valid_loss: 0.07136 test_loss: 0.08239 \n",
      "[462/500] train_loss: 0.04110 valid_loss: 0.07072 test_loss: 0.08224 \n",
      "[463/500] train_loss: 0.04162 valid_loss: 0.07271 test_loss: 0.08155 \n",
      "[464/500] train_loss: 0.04112 valid_loss: 0.07045 test_loss: 0.08204 \n",
      "[465/500] train_loss: 0.04248 valid_loss: 0.07283 test_loss: 0.08350 \n",
      "[466/500] train_loss: 0.04177 valid_loss: 0.07228 test_loss: 0.08204 \n",
      "[467/500] train_loss: 0.04154 valid_loss: 0.07135 test_loss: 0.08106 \n",
      "[468/500] train_loss: 0.04125 valid_loss: 0.07134 test_loss: 0.08121 \n",
      "[469/500] train_loss: 0.04115 valid_loss: 0.07250 test_loss: 0.08087 \n",
      "[470/500] train_loss: 0.04007 valid_loss: 0.07190 test_loss: 0.08200 \n",
      "[471/500] train_loss: 0.04081 valid_loss: 0.07286 test_loss: 0.07986 \n",
      "[472/500] train_loss: 0.04070 valid_loss: 0.07219 test_loss: 0.08078 \n",
      "[473/500] train_loss: 0.04143 valid_loss: 0.07060 test_loss: 0.08012 \n",
      "[474/500] train_loss: 0.04067 valid_loss: 0.07220 test_loss: 0.08297 \n",
      "[475/500] train_loss: 0.04086 valid_loss: 0.07116 test_loss: 0.08188 \n",
      "[476/500] train_loss: 0.04017 valid_loss: 0.07143 test_loss: 0.08143 \n",
      "[477/500] train_loss: 0.04165 valid_loss: 0.07115 test_loss: 0.08191 \n",
      "[478/500] train_loss: 0.04051 valid_loss: 0.07242 test_loss: 0.08163 \n",
      "[479/500] train_loss: 0.04145 valid_loss: 0.07274 test_loss: 0.08218 \n",
      "[480/500] train_loss: 0.04109 valid_loss: 0.07148 test_loss: 0.08116 \n",
      "[481/500] train_loss: 0.04110 valid_loss: 0.07323 test_loss: 0.08116 \n",
      "[482/500] train_loss: 0.04281 valid_loss: 0.07320 test_loss: 0.08210 \n",
      "[483/500] train_loss: 0.04095 valid_loss: 0.07228 test_loss: 0.08227 \n",
      "[484/500] train_loss: 0.04004 valid_loss: 0.07375 test_loss: 0.08090 \n",
      "[485/500] train_loss: 0.03864 valid_loss: 0.07294 test_loss: 0.08303 \n",
      "[486/500] train_loss: 0.04149 valid_loss: 0.07308 test_loss: 0.08128 \n",
      "[487/500] train_loss: 0.04002 valid_loss: 0.07355 test_loss: 0.08150 \n",
      "[488/500] train_loss: 0.04062 valid_loss: 0.07247 test_loss: 0.08145 \n",
      "[489/500] train_loss: 0.04172 valid_loss: 0.07117 test_loss: 0.08183 \n",
      "[490/500] train_loss: 0.04171 valid_loss: 0.07200 test_loss: 0.08320 \n",
      "[491/500] train_loss: 0.04046 valid_loss: 0.07195 test_loss: 0.08290 \n",
      "[492/500] train_loss: 0.04090 valid_loss: 0.07133 test_loss: 0.08292 \n",
      "[493/500] train_loss: 0.04039 valid_loss: 0.07252 test_loss: 0.08330 \n",
      "[494/500] train_loss: 0.04000 valid_loss: 0.07268 test_loss: 0.08399 \n",
      "[495/500] train_loss: 0.04210 valid_loss: 0.07163 test_loss: 0.08180 \n",
      "[496/500] train_loss: 0.03984 valid_loss: 0.07195 test_loss: 0.08173 \n",
      "[497/500] train_loss: 0.04133 valid_loss: 0.07204 test_loss: 0.08118 \n",
      "[498/500] train_loss: 0.04112 valid_loss: 0.07364 test_loss: 0.08286 \n",
      "[499/500] train_loss: 0.03975 valid_loss: 0.07308 test_loss: 0.08298 \n",
      "[500/500] train_loss: 0.04198 valid_loss: 0.07107 test_loss: 0.08097 \n",
      "TRAINING MODEL 17\n",
      "[  1/500] train_loss: 0.33927 valid_loss: 0.26420 test_loss: 0.27109 \n",
      "验证损失减少 (inf --> 0.264198). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19364 valid_loss: 0.18788 test_loss: 0.19162 \n",
      "验证损失减少 (0.264198 --> 0.187881). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15118 valid_loss: 0.15478 test_loss: 0.15979 \n",
      "验证损失减少 (0.187881 --> 0.154783). 正在保存模型...\n",
      "[  4/500] train_loss: 0.14024 valid_loss: 0.14291 test_loss: 0.15049 \n",
      "验证损失减少 (0.154783 --> 0.142910). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13196 valid_loss: 0.13662 test_loss: 0.14085 \n",
      "验证损失减少 (0.142910 --> 0.136623). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12312 valid_loss: 0.12859 test_loss: 0.13536 \n",
      "验证损失减少 (0.136623 --> 0.128592). 正在保存模型...\n",
      "[  7/500] train_loss: 0.12210 valid_loss: 0.12583 test_loss: 0.13191 \n",
      "验证损失减少 (0.128592 --> 0.125830). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11771 valid_loss: 0.12024 test_loss: 0.12478 \n",
      "验证损失减少 (0.125830 --> 0.120241). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11514 valid_loss: 0.11674 test_loss: 0.12319 \n",
      "验证损失减少 (0.120241 --> 0.116740). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11121 valid_loss: 0.11229 test_loss: 0.11941 \n",
      "验证损失减少 (0.116740 --> 0.112290). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.10734 valid_loss: 0.11211 test_loss: 0.11935 \n",
      "验证损失减少 (0.112290 --> 0.112109). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10489 valid_loss: 0.10731 test_loss: 0.11625 \n",
      "验证损失减少 (0.112109 --> 0.107312). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10383 valid_loss: 0.10907 test_loss: 0.11588 \n",
      "[ 14/500] train_loss: 0.10079 valid_loss: 0.10700 test_loss: 0.11579 \n",
      "验证损失减少 (0.107312 --> 0.106997). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.09988 valid_loss: 0.10488 test_loss: 0.11261 \n",
      "验证损失减少 (0.106997 --> 0.104884). 正在保存模型...\n",
      "[ 16/500] train_loss: 0.10136 valid_loss: 0.10409 test_loss: 0.11190 \n",
      "验证损失减少 (0.104884 --> 0.104089). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17/500] train_loss: 0.09863 valid_loss: 0.10397 test_loss: 0.10864 \n",
      "验证损失减少 (0.104089 --> 0.103970). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09695 valid_loss: 0.09989 test_loss: 0.10763 \n",
      "验证损失减少 (0.103970 --> 0.099893). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09503 valid_loss: 0.10119 test_loss: 0.11034 \n",
      "[ 20/500] train_loss: 0.09321 valid_loss: 0.09789 test_loss: 0.10628 \n",
      "验证损失减少 (0.099893 --> 0.097894). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09183 valid_loss: 0.09803 test_loss: 0.10574 \n",
      "[ 22/500] train_loss: 0.09459 valid_loss: 0.09434 test_loss: 0.10641 \n",
      "验证损失减少 (0.097894 --> 0.094343). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09529 valid_loss: 0.09754 test_loss: 0.10611 \n",
      "[ 24/500] train_loss: 0.09139 valid_loss: 0.09594 test_loss: 0.10227 \n",
      "[ 25/500] train_loss: 0.09221 valid_loss: 0.09514 test_loss: 0.10163 \n",
      "[ 26/500] train_loss: 0.08916 valid_loss: 0.09612 test_loss: 0.10244 \n",
      "[ 27/500] train_loss: 0.08981 valid_loss: 0.09099 test_loss: 0.10147 \n",
      "验证损失减少 (0.094343 --> 0.090991). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08961 valid_loss: 0.09006 test_loss: 0.10170 \n",
      "验证损失减少 (0.090991 --> 0.090057). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08548 valid_loss: 0.09184 test_loss: 0.10083 \n",
      "[ 30/500] train_loss: 0.08951 valid_loss: 0.09008 test_loss: 0.09906 \n",
      "[ 31/500] train_loss: 0.08582 valid_loss: 0.09062 test_loss: 0.09712 \n",
      "[ 32/500] train_loss: 0.08746 valid_loss: 0.08874 test_loss: 0.09813 \n",
      "验证损失减少 (0.090057 --> 0.088742). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08669 valid_loss: 0.08908 test_loss: 0.09604 \n",
      "[ 34/500] train_loss: 0.08245 valid_loss: 0.08839 test_loss: 0.09835 \n",
      "验证损失减少 (0.088742 --> 0.088388). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08409 valid_loss: 0.08777 test_loss: 0.09969 \n",
      "验证损失减少 (0.088388 --> 0.087775). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08294 valid_loss: 0.08880 test_loss: 0.09823 \n",
      "[ 37/500] train_loss: 0.08165 valid_loss: 0.08676 test_loss: 0.09707 \n",
      "验证损失减少 (0.087775 --> 0.086757). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.08296 valid_loss: 0.08567 test_loss: 0.09467 \n",
      "验证损失减少 (0.086757 --> 0.085671). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.08142 valid_loss: 0.08569 test_loss: 0.09541 \n",
      "[ 40/500] train_loss: 0.08133 valid_loss: 0.08783 test_loss: 0.09544 \n",
      "[ 41/500] train_loss: 0.07969 valid_loss: 0.08723 test_loss: 0.09301 \n",
      "[ 42/500] train_loss: 0.08143 valid_loss: 0.08574 test_loss: 0.09325 \n",
      "[ 43/500] train_loss: 0.08076 valid_loss: 0.08567 test_loss: 0.09158 \n",
      "[ 44/500] train_loss: 0.07977 valid_loss: 0.08418 test_loss: 0.09205 \n",
      "验证损失减少 (0.085671 --> 0.084179). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.07902 valid_loss: 0.08557 test_loss: 0.09482 \n",
      "[ 46/500] train_loss: 0.07740 valid_loss: 0.08554 test_loss: 0.09235 \n",
      "[ 47/500] train_loss: 0.07719 valid_loss: 0.08552 test_loss: 0.09230 \n",
      "[ 48/500] train_loss: 0.07748 valid_loss: 0.08546 test_loss: 0.09258 \n",
      "[ 49/500] train_loss: 0.07769 valid_loss: 0.08358 test_loss: 0.09110 \n",
      "验证损失减少 (0.084179 --> 0.083584). 正在保存模型...\n",
      "[ 50/500] train_loss: 0.07733 valid_loss: 0.08476 test_loss: 0.08997 \n",
      "[ 51/500] train_loss: 0.07721 valid_loss: 0.08245 test_loss: 0.08916 \n",
      "验证损失减少 (0.083584 --> 0.082450). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07766 valid_loss: 0.08414 test_loss: 0.08902 \n",
      "[ 53/500] train_loss: 0.07844 valid_loss: 0.08195 test_loss: 0.08896 \n",
      "验证损失减少 (0.082450 --> 0.081953). 正在保存模型...\n",
      "[ 54/500] train_loss: 0.07560 valid_loss: 0.08405 test_loss: 0.08991 \n",
      "[ 55/500] train_loss: 0.07495 valid_loss: 0.08360 test_loss: 0.08895 \n",
      "[ 56/500] train_loss: 0.07432 valid_loss: 0.08362 test_loss: 0.09048 \n",
      "[ 57/500] train_loss: 0.07463 valid_loss: 0.08209 test_loss: 0.09016 \n",
      "[ 58/500] train_loss: 0.07579 valid_loss: 0.08282 test_loss: 0.09009 \n",
      "[ 59/500] train_loss: 0.07400 valid_loss: 0.08363 test_loss: 0.08824 \n",
      "[ 60/500] train_loss: 0.07450 valid_loss: 0.08183 test_loss: 0.08873 \n",
      "验证损失减少 (0.081953 --> 0.081833). 正在保存模型...\n",
      "[ 61/500] train_loss: 0.07367 valid_loss: 0.07902 test_loss: 0.08820 \n",
      "验证损失减少 (0.081833 --> 0.079019). 正在保存模型...\n",
      "[ 62/500] train_loss: 0.07382 valid_loss: 0.08344 test_loss: 0.08741 \n",
      "[ 63/500] train_loss: 0.07268 valid_loss: 0.08006 test_loss: 0.08863 \n",
      "[ 64/500] train_loss: 0.07403 valid_loss: 0.08134 test_loss: 0.08948 \n",
      "[ 65/500] train_loss: 0.07207 valid_loss: 0.08121 test_loss: 0.08820 \n",
      "[ 66/500] train_loss: 0.07263 valid_loss: 0.07950 test_loss: 0.08763 \n",
      "[ 67/500] train_loss: 0.07079 valid_loss: 0.08097 test_loss: 0.08841 \n",
      "[ 68/500] train_loss: 0.07274 valid_loss: 0.07952 test_loss: 0.08819 \n",
      "[ 69/500] train_loss: 0.07124 valid_loss: 0.08023 test_loss: 0.08812 \n",
      "[ 70/500] train_loss: 0.06909 valid_loss: 0.08026 test_loss: 0.08750 \n",
      "[ 71/500] train_loss: 0.07121 valid_loss: 0.07978 test_loss: 0.08700 \n",
      "[ 72/500] train_loss: 0.07134 valid_loss: 0.08085 test_loss: 0.08732 \n",
      "[ 73/500] train_loss: 0.07011 valid_loss: 0.08187 test_loss: 0.08686 \n",
      "[ 74/500] train_loss: 0.07037 valid_loss: 0.08035 test_loss: 0.08742 \n",
      "[ 75/500] train_loss: 0.06891 valid_loss: 0.08157 test_loss: 0.08616 \n",
      "[ 76/500] train_loss: 0.07021 valid_loss: 0.07815 test_loss: 0.08708 \n",
      "验证损失减少 (0.079019 --> 0.078147). 正在保存模型...\n",
      "[ 77/500] train_loss: 0.06996 valid_loss: 0.08015 test_loss: 0.08600 \n",
      "[ 78/500] train_loss: 0.06921 valid_loss: 0.07750 test_loss: 0.08577 \n",
      "验证损失减少 (0.078147 --> 0.077498). 正在保存模型...\n",
      "[ 79/500] train_loss: 0.06854 valid_loss: 0.07897 test_loss: 0.08674 \n",
      "[ 80/500] train_loss: 0.06829 valid_loss: 0.07841 test_loss: 0.08526 \n",
      "[ 81/500] train_loss: 0.06871 valid_loss: 0.07756 test_loss: 0.08427 \n",
      "[ 82/500] train_loss: 0.06928 valid_loss: 0.08012 test_loss: 0.08468 \n",
      "[ 83/500] train_loss: 0.06864 valid_loss: 0.07923 test_loss: 0.08582 \n",
      "[ 84/500] train_loss: 0.06756 valid_loss: 0.07776 test_loss: 0.08421 \n",
      "[ 85/500] train_loss: 0.06790 valid_loss: 0.07881 test_loss: 0.08390 \n",
      "[ 86/500] train_loss: 0.06853 valid_loss: 0.07777 test_loss: 0.08561 \n",
      "[ 87/500] train_loss: 0.07014 valid_loss: 0.07793 test_loss: 0.08488 \n",
      "[ 88/500] train_loss: 0.06769 valid_loss: 0.07801 test_loss: 0.08449 \n",
      "[ 89/500] train_loss: 0.06645 valid_loss: 0.07681 test_loss: 0.08458 \n",
      "验证损失减少 (0.077498 --> 0.076807). 正在保存模型...\n",
      "[ 90/500] train_loss: 0.06622 valid_loss: 0.07885 test_loss: 0.08614 \n",
      "[ 91/500] train_loss: 0.06747 valid_loss: 0.07931 test_loss: 0.08442 \n",
      "[ 92/500] train_loss: 0.06512 valid_loss: 0.07952 test_loss: 0.08472 \n",
      "[ 93/500] train_loss: 0.06834 valid_loss: 0.07770 test_loss: 0.08383 \n",
      "[ 94/500] train_loss: 0.06678 valid_loss: 0.07726 test_loss: 0.08245 \n",
      "[ 95/500] train_loss: 0.06609 valid_loss: 0.08002 test_loss: 0.08561 \n",
      "[ 96/500] train_loss: 0.06766 valid_loss: 0.07774 test_loss: 0.08570 \n",
      "[ 97/500] train_loss: 0.06690 valid_loss: 0.07859 test_loss: 0.08412 \n",
      "[ 98/500] train_loss: 0.06434 valid_loss: 0.07817 test_loss: 0.08401 \n",
      "[ 99/500] train_loss: 0.06521 valid_loss: 0.07908 test_loss: 0.08554 \n",
      "[100/500] train_loss: 0.06592 valid_loss: 0.07914 test_loss: 0.08474 \n",
      "[101/500] train_loss: 0.06806 valid_loss: 0.07687 test_loss: 0.08442 \n",
      "[102/500] train_loss: 0.06601 valid_loss: 0.07801 test_loss: 0.08500 \n",
      "[103/500] train_loss: 0.06550 valid_loss: 0.07864 test_loss: 0.08610 \n",
      "[104/500] train_loss: 0.06551 valid_loss: 0.07786 test_loss: 0.08555 \n",
      "[105/500] train_loss: 0.06635 valid_loss: 0.07788 test_loss: 0.08469 \n",
      "[106/500] train_loss: 0.06419 valid_loss: 0.07841 test_loss: 0.08467 \n",
      "[107/500] train_loss: 0.06488 valid_loss: 0.07572 test_loss: 0.08328 \n",
      "验证损失减少 (0.076807 --> 0.075718). 正在保存模型...\n",
      "[108/500] train_loss: 0.06497 valid_loss: 0.07751 test_loss: 0.08422 \n",
      "[109/500] train_loss: 0.06395 valid_loss: 0.07891 test_loss: 0.08340 \n",
      "[110/500] train_loss: 0.06377 valid_loss: 0.07727 test_loss: 0.08316 \n",
      "[111/500] train_loss: 0.06463 valid_loss: 0.07649 test_loss: 0.08585 \n",
      "[112/500] train_loss: 0.06361 valid_loss: 0.07625 test_loss: 0.08310 \n",
      "[113/500] train_loss: 0.06428 valid_loss: 0.07622 test_loss: 0.08307 \n",
      "[114/500] train_loss: 0.06495 valid_loss: 0.07656 test_loss: 0.08273 \n",
      "[115/500] train_loss: 0.06430 valid_loss: 0.07630 test_loss: 0.08348 \n",
      "[116/500] train_loss: 0.06326 valid_loss: 0.07645 test_loss: 0.08327 \n",
      "[117/500] train_loss: 0.06287 valid_loss: 0.07688 test_loss: 0.08230 \n",
      "[118/500] train_loss: 0.06353 valid_loss: 0.07491 test_loss: 0.08171 \n",
      "验证损失减少 (0.075718 --> 0.074908). 正在保存模型...\n",
      "[119/500] train_loss: 0.06301 valid_loss: 0.07617 test_loss: 0.08452 \n",
      "[120/500] train_loss: 0.06250 valid_loss: 0.08104 test_loss: 0.08398 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121/500] train_loss: 0.06340 valid_loss: 0.07580 test_loss: 0.08139 \n",
      "[122/500] train_loss: 0.06177 valid_loss: 0.07526 test_loss: 0.08218 \n",
      "[123/500] train_loss: 0.06228 valid_loss: 0.07928 test_loss: 0.08263 \n",
      "[124/500] train_loss: 0.06291 valid_loss: 0.07757 test_loss: 0.08207 \n",
      "[125/500] train_loss: 0.06164 valid_loss: 0.07555 test_loss: 0.08218 \n",
      "[126/500] train_loss: 0.06141 valid_loss: 0.07550 test_loss: 0.08193 \n",
      "[127/500] train_loss: 0.06361 valid_loss: 0.07595 test_loss: 0.08286 \n",
      "[128/500] train_loss: 0.06159 valid_loss: 0.07807 test_loss: 0.08464 \n",
      "[129/500] train_loss: 0.06256 valid_loss: 0.07637 test_loss: 0.08454 \n",
      "[130/500] train_loss: 0.06079 valid_loss: 0.07496 test_loss: 0.08213 \n",
      "[131/500] train_loss: 0.06175 valid_loss: 0.07731 test_loss: 0.08265 \n",
      "[132/500] train_loss: 0.06082 valid_loss: 0.07827 test_loss: 0.08213 \n",
      "[133/500] train_loss: 0.06243 valid_loss: 0.07825 test_loss: 0.08320 \n",
      "[134/500] train_loss: 0.06125 valid_loss: 0.07531 test_loss: 0.08193 \n",
      "[135/500] train_loss: 0.06180 valid_loss: 0.07661 test_loss: 0.08284 \n",
      "[136/500] train_loss: 0.06204 valid_loss: 0.07625 test_loss: 0.08214 \n",
      "[137/500] train_loss: 0.06070 valid_loss: 0.07730 test_loss: 0.08269 \n",
      "[138/500] train_loss: 0.06045 valid_loss: 0.07692 test_loss: 0.08342 \n",
      "[139/500] train_loss: 0.05932 valid_loss: 0.07458 test_loss: 0.08205 \n",
      "验证损失减少 (0.074908 --> 0.074579). 正在保存模型...\n",
      "[140/500] train_loss: 0.06012 valid_loss: 0.07545 test_loss: 0.08230 \n",
      "[141/500] train_loss: 0.06024 valid_loss: 0.07597 test_loss: 0.08170 \n",
      "[142/500] train_loss: 0.05997 valid_loss: 0.07569 test_loss: 0.08135 \n",
      "[143/500] train_loss: 0.05945 valid_loss: 0.07616 test_loss: 0.08187 \n",
      "[144/500] train_loss: 0.05960 valid_loss: 0.07761 test_loss: 0.08144 \n",
      "[145/500] train_loss: 0.05989 valid_loss: 0.07589 test_loss: 0.08293 \n",
      "[146/500] train_loss: 0.05819 valid_loss: 0.07616 test_loss: 0.08171 \n",
      "[147/500] train_loss: 0.05893 valid_loss: 0.07484 test_loss: 0.08116 \n",
      "[148/500] train_loss: 0.06062 valid_loss: 0.07539 test_loss: 0.08354 \n",
      "[149/500] train_loss: 0.05959 valid_loss: 0.07707 test_loss: 0.08296 \n",
      "[150/500] train_loss: 0.05849 valid_loss: 0.07656 test_loss: 0.08106 \n",
      "[151/500] train_loss: 0.05832 valid_loss: 0.07580 test_loss: 0.08257 \n",
      "[152/500] train_loss: 0.05968 valid_loss: 0.07405 test_loss: 0.08221 \n",
      "验证损失减少 (0.074579 --> 0.074055). 正在保存模型...\n",
      "[153/500] train_loss: 0.05978 valid_loss: 0.07349 test_loss: 0.08187 \n",
      "验证损失减少 (0.074055 --> 0.073494). 正在保存模型...\n",
      "[154/500] train_loss: 0.05863 valid_loss: 0.07459 test_loss: 0.08107 \n",
      "[155/500] train_loss: 0.05781 valid_loss: 0.07493 test_loss: 0.08207 \n",
      "[156/500] train_loss: 0.05919 valid_loss: 0.07548 test_loss: 0.08167 \n",
      "[157/500] train_loss: 0.05777 valid_loss: 0.07746 test_loss: 0.08054 \n",
      "[158/500] train_loss: 0.05955 valid_loss: 0.07483 test_loss: 0.08112 \n",
      "[159/500] train_loss: 0.05989 valid_loss: 0.07453 test_loss: 0.08033 \n",
      "[160/500] train_loss: 0.05545 valid_loss: 0.07594 test_loss: 0.08106 \n",
      "[161/500] train_loss: 0.05697 valid_loss: 0.07567 test_loss: 0.08080 \n",
      "[162/500] train_loss: 0.05786 valid_loss: 0.07543 test_loss: 0.08127 \n",
      "[163/500] train_loss: 0.05887 valid_loss: 0.07342 test_loss: 0.08118 \n",
      "验证损失减少 (0.073494 --> 0.073417). 正在保存模型...\n",
      "[164/500] train_loss: 0.05797 valid_loss: 0.07532 test_loss: 0.08132 \n",
      "[165/500] train_loss: 0.05826 valid_loss: 0.07592 test_loss: 0.08151 \n",
      "[166/500] train_loss: 0.05792 valid_loss: 0.07511 test_loss: 0.08207 \n",
      "[167/500] train_loss: 0.05636 valid_loss: 0.07459 test_loss: 0.08155 \n",
      "[168/500] train_loss: 0.05818 valid_loss: 0.07474 test_loss: 0.08075 \n",
      "[169/500] train_loss: 0.05672 valid_loss: 0.07364 test_loss: 0.08085 \n",
      "[170/500] train_loss: 0.05852 valid_loss: 0.07812 test_loss: 0.08246 \n",
      "[171/500] train_loss: 0.05607 valid_loss: 0.07595 test_loss: 0.08230 \n",
      "[172/500] train_loss: 0.05730 valid_loss: 0.07397 test_loss: 0.07965 \n",
      "[173/500] train_loss: 0.05608 valid_loss: 0.07489 test_loss: 0.08001 \n",
      "[174/500] train_loss: 0.05601 valid_loss: 0.07725 test_loss: 0.08070 \n",
      "[175/500] train_loss: 0.05603 valid_loss: 0.07358 test_loss: 0.07931 \n",
      "[176/500] train_loss: 0.05788 valid_loss: 0.07413 test_loss: 0.08018 \n",
      "[177/500] train_loss: 0.05504 valid_loss: 0.07376 test_loss: 0.08084 \n",
      "[178/500] train_loss: 0.05679 valid_loss: 0.07498 test_loss: 0.08094 \n",
      "[179/500] train_loss: 0.05661 valid_loss: 0.07459 test_loss: 0.08035 \n",
      "[180/500] train_loss: 0.05625 valid_loss: 0.07788 test_loss: 0.08025 \n",
      "[181/500] train_loss: 0.05612 valid_loss: 0.07304 test_loss: 0.08050 \n",
      "验证损失减少 (0.073417 --> 0.073035). 正在保存模型...\n",
      "[182/500] train_loss: 0.05658 valid_loss: 0.07454 test_loss: 0.07987 \n",
      "[183/500] train_loss: 0.05578 valid_loss: 0.07455 test_loss: 0.08164 \n",
      "[184/500] train_loss: 0.05544 valid_loss: 0.07451 test_loss: 0.07999 \n",
      "[185/500] train_loss: 0.05514 valid_loss: 0.07651 test_loss: 0.08189 \n",
      "[186/500] train_loss: 0.05435 valid_loss: 0.07272 test_loss: 0.08022 \n",
      "验证损失减少 (0.073035 --> 0.072724). 正在保存模型...\n",
      "[187/500] train_loss: 0.05547 valid_loss: 0.07436 test_loss: 0.08143 \n",
      "[188/500] train_loss: 0.05584 valid_loss: 0.07383 test_loss: 0.08046 \n",
      "[189/500] train_loss: 0.05659 valid_loss: 0.07390 test_loss: 0.07924 \n",
      "[190/500] train_loss: 0.05519 valid_loss: 0.07393 test_loss: 0.08019 \n",
      "[191/500] train_loss: 0.05343 valid_loss: 0.07260 test_loss: 0.08057 \n",
      "验证损失减少 (0.072724 --> 0.072597). 正在保存模型...\n",
      "[192/500] train_loss: 0.05429 valid_loss: 0.07613 test_loss: 0.08049 \n",
      "[193/500] train_loss: 0.05494 valid_loss: 0.07362 test_loss: 0.07988 \n",
      "[194/500] train_loss: 0.05565 valid_loss: 0.07440 test_loss: 0.08036 \n",
      "[195/500] train_loss: 0.05424 valid_loss: 0.07496 test_loss: 0.07984 \n",
      "[196/500] train_loss: 0.05492 valid_loss: 0.07356 test_loss: 0.08107 \n",
      "[197/500] train_loss: 0.05436 valid_loss: 0.07484 test_loss: 0.08054 \n",
      "[198/500] train_loss: 0.05312 valid_loss: 0.07398 test_loss: 0.07974 \n",
      "[199/500] train_loss: 0.05325 valid_loss: 0.07447 test_loss: 0.07976 \n",
      "[200/500] train_loss: 0.05404 valid_loss: 0.07381 test_loss: 0.08010 \n",
      "[201/500] train_loss: 0.05403 valid_loss: 0.07606 test_loss: 0.08086 \n",
      "[202/500] train_loss: 0.05513 valid_loss: 0.07583 test_loss: 0.07915 \n",
      "[203/500] train_loss: 0.05465 valid_loss: 0.07575 test_loss: 0.07890 \n",
      "[204/500] train_loss: 0.05208 valid_loss: 0.07610 test_loss: 0.08093 \n",
      "[205/500] train_loss: 0.05406 valid_loss: 0.07708 test_loss: 0.08122 \n",
      "[206/500] train_loss: 0.05323 valid_loss: 0.07357 test_loss: 0.08006 \n",
      "[207/500] train_loss: 0.05269 valid_loss: 0.07287 test_loss: 0.08211 \n",
      "[208/500] train_loss: 0.05288 valid_loss: 0.07515 test_loss: 0.08009 \n",
      "[209/500] train_loss: 0.05388 valid_loss: 0.07577 test_loss: 0.08084 \n",
      "[210/500] train_loss: 0.05450 valid_loss: 0.07531 test_loss: 0.08143 \n",
      "[211/500] train_loss: 0.05420 valid_loss: 0.07546 test_loss: 0.08094 \n",
      "[212/500] train_loss: 0.05340 valid_loss: 0.07484 test_loss: 0.08043 \n",
      "[213/500] train_loss: 0.05231 valid_loss: 0.07827 test_loss: 0.08362 \n",
      "[214/500] train_loss: 0.05293 valid_loss: 0.07544 test_loss: 0.08109 \n",
      "[215/500] train_loss: 0.05445 valid_loss: 0.07321 test_loss: 0.07928 \n",
      "[216/500] train_loss: 0.05296 valid_loss: 0.07526 test_loss: 0.08146 \n",
      "[217/500] train_loss: 0.05328 valid_loss: 0.07492 test_loss: 0.08088 \n",
      "[218/500] train_loss: 0.05317 valid_loss: 0.07467 test_loss: 0.08146 \n",
      "[219/500] train_loss: 0.05339 valid_loss: 0.07547 test_loss: 0.08059 \n",
      "[220/500] train_loss: 0.05243 valid_loss: 0.07554 test_loss: 0.08072 \n",
      "[221/500] train_loss: 0.05216 valid_loss: 0.07531 test_loss: 0.08099 \n",
      "[222/500] train_loss: 0.05344 valid_loss: 0.07544 test_loss: 0.08198 \n",
      "[223/500] train_loss: 0.05342 valid_loss: 0.07458 test_loss: 0.08155 \n",
      "[224/500] train_loss: 0.05157 valid_loss: 0.07322 test_loss: 0.07966 \n",
      "[225/500] train_loss: 0.05344 valid_loss: 0.07276 test_loss: 0.08113 \n",
      "[226/500] train_loss: 0.05189 valid_loss: 0.07444 test_loss: 0.08101 \n",
      "[227/500] train_loss: 0.05280 valid_loss: 0.07369 test_loss: 0.07985 \n",
      "[228/500] train_loss: 0.05250 valid_loss: 0.07226 test_loss: 0.08016 \n",
      "验证损失减少 (0.072597 --> 0.072260). 正在保存模型...\n",
      "[229/500] train_loss: 0.05062 valid_loss: 0.07550 test_loss: 0.08101 \n",
      "[230/500] train_loss: 0.05273 valid_loss: 0.07591 test_loss: 0.08136 \n",
      "[231/500] train_loss: 0.05295 valid_loss: 0.07544 test_loss: 0.08240 \n",
      "[232/500] train_loss: 0.05158 valid_loss: 0.07515 test_loss: 0.08117 \n",
      "[233/500] train_loss: 0.05203 valid_loss: 0.07390 test_loss: 0.08147 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[234/500] train_loss: 0.05151 valid_loss: 0.07284 test_loss: 0.07967 \n",
      "[235/500] train_loss: 0.05176 valid_loss: 0.07298 test_loss: 0.08120 \n",
      "[236/500] train_loss: 0.05163 valid_loss: 0.07504 test_loss: 0.08114 \n",
      "[237/500] train_loss: 0.05203 valid_loss: 0.07433 test_loss: 0.08140 \n",
      "[238/500] train_loss: 0.05070 valid_loss: 0.07394 test_loss: 0.08151 \n",
      "[239/500] train_loss: 0.05240 valid_loss: 0.07391 test_loss: 0.08144 \n",
      "[240/500] train_loss: 0.05270 valid_loss: 0.07351 test_loss: 0.08119 \n",
      "[241/500] train_loss: 0.05111 valid_loss: 0.07409 test_loss: 0.08121 \n",
      "[242/500] train_loss: 0.05035 valid_loss: 0.07336 test_loss: 0.08080 \n",
      "[243/500] train_loss: 0.05042 valid_loss: 0.07627 test_loss: 0.08115 \n",
      "[244/500] train_loss: 0.05121 valid_loss: 0.07355 test_loss: 0.08078 \n",
      "[245/500] train_loss: 0.05054 valid_loss: 0.07389 test_loss: 0.07994 \n",
      "[246/500] train_loss: 0.05085 valid_loss: 0.07646 test_loss: 0.08202 \n",
      "[247/500] train_loss: 0.05052 valid_loss: 0.07508 test_loss: 0.08227 \n",
      "[248/500] train_loss: 0.05226 valid_loss: 0.07605 test_loss: 0.08038 \n",
      "[249/500] train_loss: 0.05158 valid_loss: 0.07526 test_loss: 0.08170 \n",
      "[250/500] train_loss: 0.05034 valid_loss: 0.07500 test_loss: 0.08193 \n",
      "[251/500] train_loss: 0.05103 valid_loss: 0.07566 test_loss: 0.08096 \n",
      "[252/500] train_loss: 0.05228 valid_loss: 0.07427 test_loss: 0.08039 \n",
      "[253/500] train_loss: 0.05081 valid_loss: 0.07448 test_loss: 0.07991 \n",
      "[254/500] train_loss: 0.05082 valid_loss: 0.07655 test_loss: 0.08121 \n",
      "[255/500] train_loss: 0.05079 valid_loss: 0.07528 test_loss: 0.08027 \n",
      "[256/500] train_loss: 0.04987 valid_loss: 0.07511 test_loss: 0.08182 \n",
      "[257/500] train_loss: 0.05139 valid_loss: 0.07609 test_loss: 0.08030 \n",
      "[258/500] train_loss: 0.05060 valid_loss: 0.07466 test_loss: 0.08274 \n",
      "[259/500] train_loss: 0.05013 valid_loss: 0.07330 test_loss: 0.08004 \n",
      "[260/500] train_loss: 0.04949 valid_loss: 0.07462 test_loss: 0.08001 \n",
      "[261/500] train_loss: 0.04989 valid_loss: 0.07557 test_loss: 0.08002 \n",
      "[262/500] train_loss: 0.05084 valid_loss: 0.07462 test_loss: 0.08084 \n",
      "[263/500] train_loss: 0.05001 valid_loss: 0.07374 test_loss: 0.08094 \n",
      "[264/500] train_loss: 0.04891 valid_loss: 0.07520 test_loss: 0.07999 \n",
      "[265/500] train_loss: 0.04999 valid_loss: 0.07603 test_loss: 0.08142 \n",
      "[266/500] train_loss: 0.04882 valid_loss: 0.07540 test_loss: 0.08200 \n",
      "[267/500] train_loss: 0.04866 valid_loss: 0.07533 test_loss: 0.08024 \n",
      "[268/500] train_loss: 0.04996 valid_loss: 0.07633 test_loss: 0.08021 \n",
      "[269/500] train_loss: 0.05021 valid_loss: 0.07573 test_loss: 0.08089 \n",
      "[270/500] train_loss: 0.05121 valid_loss: 0.07446 test_loss: 0.08000 \n",
      "[271/500] train_loss: 0.04990 valid_loss: 0.07357 test_loss: 0.08064 \n",
      "[272/500] train_loss: 0.04891 valid_loss: 0.07444 test_loss: 0.08050 \n",
      "[273/500] train_loss: 0.04911 valid_loss: 0.07450 test_loss: 0.08030 \n",
      "[274/500] train_loss: 0.04978 valid_loss: 0.07539 test_loss: 0.07978 \n",
      "[275/500] train_loss: 0.05020 valid_loss: 0.07653 test_loss: 0.08067 \n",
      "[276/500] train_loss: 0.05028 valid_loss: 0.07362 test_loss: 0.08193 \n",
      "[277/500] train_loss: 0.04924 valid_loss: 0.07430 test_loss: 0.08013 \n",
      "[278/500] train_loss: 0.04914 valid_loss: 0.07465 test_loss: 0.08013 \n",
      "[279/500] train_loss: 0.04917 valid_loss: 0.07421 test_loss: 0.07995 \n",
      "[280/500] train_loss: 0.04906 valid_loss: 0.07537 test_loss: 0.08149 \n",
      "[281/500] train_loss: 0.04853 valid_loss: 0.07500 test_loss: 0.08120 \n",
      "[282/500] train_loss: 0.04770 valid_loss: 0.07504 test_loss: 0.08032 \n",
      "[283/500] train_loss: 0.04881 valid_loss: 0.07485 test_loss: 0.08075 \n",
      "[284/500] train_loss: 0.04960 valid_loss: 0.07577 test_loss: 0.08180 \n",
      "[285/500] train_loss: 0.04870 valid_loss: 0.07481 test_loss: 0.08195 \n",
      "[286/500] train_loss: 0.05045 valid_loss: 0.07655 test_loss: 0.08086 \n",
      "[287/500] train_loss: 0.04784 valid_loss: 0.07565 test_loss: 0.08101 \n",
      "[288/500] train_loss: 0.04978 valid_loss: 0.07420 test_loss: 0.07930 \n",
      "[289/500] train_loss: 0.04690 valid_loss: 0.07455 test_loss: 0.07985 \n",
      "[290/500] train_loss: 0.04837 valid_loss: 0.07507 test_loss: 0.08060 \n",
      "[291/500] train_loss: 0.04953 valid_loss: 0.07491 test_loss: 0.08072 \n",
      "[292/500] train_loss: 0.04839 valid_loss: 0.07358 test_loss: 0.08180 \n",
      "[293/500] train_loss: 0.04658 valid_loss: 0.07437 test_loss: 0.08223 \n",
      "[294/500] train_loss: 0.04744 valid_loss: 0.07455 test_loss: 0.08298 \n",
      "[295/500] train_loss: 0.05022 valid_loss: 0.07383 test_loss: 0.08098 \n",
      "[296/500] train_loss: 0.04905 valid_loss: 0.07438 test_loss: 0.08164 \n",
      "[297/500] train_loss: 0.04902 valid_loss: 0.07621 test_loss: 0.08156 \n",
      "[298/500] train_loss: 0.04769 valid_loss: 0.07689 test_loss: 0.08179 \n",
      "[299/500] train_loss: 0.04846 valid_loss: 0.07295 test_loss: 0.08089 \n",
      "[300/500] train_loss: 0.04872 valid_loss: 0.07393 test_loss: 0.07997 \n",
      "[301/500] train_loss: 0.04889 valid_loss: 0.07319 test_loss: 0.08053 \n",
      "[302/500] train_loss: 0.04758 valid_loss: 0.07311 test_loss: 0.08140 \n",
      "[303/500] train_loss: 0.04768 valid_loss: 0.07350 test_loss: 0.08108 \n",
      "[304/500] train_loss: 0.04750 valid_loss: 0.07437 test_loss: 0.08206 \n",
      "[305/500] train_loss: 0.04806 valid_loss: 0.07530 test_loss: 0.08026 \n",
      "[306/500] train_loss: 0.04920 valid_loss: 0.07390 test_loss: 0.07957 \n",
      "[307/500] train_loss: 0.04828 valid_loss: 0.07248 test_loss: 0.08065 \n",
      "[308/500] train_loss: 0.04774 valid_loss: 0.07275 test_loss: 0.07982 \n",
      "[309/500] train_loss: 0.04703 valid_loss: 0.07350 test_loss: 0.08033 \n",
      "[310/500] train_loss: 0.04545 valid_loss: 0.07286 test_loss: 0.08000 \n",
      "[311/500] train_loss: 0.04761 valid_loss: 0.07444 test_loss: 0.08001 \n",
      "[312/500] train_loss: 0.04738 valid_loss: 0.07629 test_loss: 0.08125 \n",
      "[313/500] train_loss: 0.04621 valid_loss: 0.07453 test_loss: 0.08204 \n",
      "[314/500] train_loss: 0.04712 valid_loss: 0.08019 test_loss: 0.08013 \n",
      "[315/500] train_loss: 0.04833 valid_loss: 0.07604 test_loss: 0.07961 \n",
      "[316/500] train_loss: 0.04708 valid_loss: 0.07438 test_loss: 0.08058 \n",
      "[317/500] train_loss: 0.04782 valid_loss: 0.07379 test_loss: 0.08056 \n",
      "[318/500] train_loss: 0.04753 valid_loss: 0.07663 test_loss: 0.08202 \n",
      "[319/500] train_loss: 0.04743 valid_loss: 0.07416 test_loss: 0.08087 \n",
      "[320/500] train_loss: 0.04729 valid_loss: 0.07324 test_loss: 0.07966 \n",
      "[321/500] train_loss: 0.04708 valid_loss: 0.07655 test_loss: 0.08154 \n",
      "[322/500] train_loss: 0.04746 valid_loss: 0.07506 test_loss: 0.08052 \n",
      "[323/500] train_loss: 0.04680 valid_loss: 0.07335 test_loss: 0.08039 \n",
      "[324/500] train_loss: 0.04654 valid_loss: 0.07659 test_loss: 0.08080 \n",
      "[325/500] train_loss: 0.04644 valid_loss: 0.07604 test_loss: 0.08114 \n",
      "[326/500] train_loss: 0.04690 valid_loss: 0.07379 test_loss: 0.08123 \n",
      "[327/500] train_loss: 0.04677 valid_loss: 0.07601 test_loss: 0.08232 \n",
      "[328/500] train_loss: 0.04650 valid_loss: 0.07641 test_loss: 0.08256 \n",
      "[329/500] train_loss: 0.04649 valid_loss: 0.07556 test_loss: 0.08109 \n",
      "[330/500] train_loss: 0.04616 valid_loss: 0.07353 test_loss: 0.08085 \n",
      "[331/500] train_loss: 0.04686 valid_loss: 0.07423 test_loss: 0.08130 \n",
      "[332/500] train_loss: 0.04727 valid_loss: 0.07448 test_loss: 0.08247 \n",
      "[333/500] train_loss: 0.04742 valid_loss: 0.07370 test_loss: 0.08039 \n",
      "[334/500] train_loss: 0.04565 valid_loss: 0.07639 test_loss: 0.08320 \n",
      "[335/500] train_loss: 0.04745 valid_loss: 0.07563 test_loss: 0.08191 \n",
      "[336/500] train_loss: 0.04649 valid_loss: 0.07528 test_loss: 0.08256 \n",
      "[337/500] train_loss: 0.04632 valid_loss: 0.07376 test_loss: 0.08092 \n",
      "[338/500] train_loss: 0.04613 valid_loss: 0.07748 test_loss: 0.08230 \n",
      "[339/500] train_loss: 0.04537 valid_loss: 0.07392 test_loss: 0.08176 \n",
      "[340/500] train_loss: 0.04602 valid_loss: 0.07597 test_loss: 0.08157 \n",
      "[341/500] train_loss: 0.04652 valid_loss: 0.07336 test_loss: 0.08198 \n",
      "[342/500] train_loss: 0.04512 valid_loss: 0.07347 test_loss: 0.08130 \n",
      "[343/500] train_loss: 0.04586 valid_loss: 0.07695 test_loss: 0.08224 \n",
      "[344/500] train_loss: 0.04588 valid_loss: 0.07330 test_loss: 0.08156 \n",
      "[345/500] train_loss: 0.04700 valid_loss: 0.07195 test_loss: 0.08176 \n",
      "验证损失减少 (0.072260 --> 0.071949). 正在保存模型...\n",
      "[346/500] train_loss: 0.04645 valid_loss: 0.07249 test_loss: 0.08075 \n",
      "[347/500] train_loss: 0.04647 valid_loss: 0.07379 test_loss: 0.08153 \n",
      "[348/500] train_loss: 0.04630 valid_loss: 0.07385 test_loss: 0.08174 \n",
      "[349/500] train_loss: 0.04563 valid_loss: 0.07422 test_loss: 0.08202 \n",
      "[350/500] train_loss: 0.04663 valid_loss: 0.07442 test_loss: 0.08175 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[351/500] train_loss: 0.04648 valid_loss: 0.07298 test_loss: 0.08110 \n",
      "[352/500] train_loss: 0.04583 valid_loss: 0.07414 test_loss: 0.08149 \n",
      "[353/500] train_loss: 0.04532 valid_loss: 0.07434 test_loss: 0.08127 \n",
      "[354/500] train_loss: 0.04446 valid_loss: 0.07519 test_loss: 0.08111 \n",
      "[355/500] train_loss: 0.04630 valid_loss: 0.07496 test_loss: 0.08173 \n",
      "[356/500] train_loss: 0.04558 valid_loss: 0.07418 test_loss: 0.08109 \n",
      "[357/500] train_loss: 0.04558 valid_loss: 0.07471 test_loss: 0.08144 \n",
      "[358/500] train_loss: 0.04542 valid_loss: 0.07646 test_loss: 0.08097 \n",
      "[359/500] train_loss: 0.04418 valid_loss: 0.07394 test_loss: 0.08389 \n",
      "[360/500] train_loss: 0.04455 valid_loss: 0.07401 test_loss: 0.08169 \n",
      "[361/500] train_loss: 0.04464 valid_loss: 0.07477 test_loss: 0.08315 \n",
      "[362/500] train_loss: 0.04548 valid_loss: 0.07378 test_loss: 0.08279 \n",
      "[363/500] train_loss: 0.04593 valid_loss: 0.07484 test_loss: 0.08215 \n",
      "[364/500] train_loss: 0.04447 valid_loss: 0.07382 test_loss: 0.08433 \n",
      "[365/500] train_loss: 0.04478 valid_loss: 0.07338 test_loss: 0.08077 \n",
      "[366/500] train_loss: 0.04526 valid_loss: 0.07588 test_loss: 0.08194 \n",
      "[367/500] train_loss: 0.04355 valid_loss: 0.07477 test_loss: 0.08204 \n",
      "[368/500] train_loss: 0.04392 valid_loss: 0.07430 test_loss: 0.08119 \n",
      "[369/500] train_loss: 0.04381 valid_loss: 0.07634 test_loss: 0.08171 \n",
      "[370/500] train_loss: 0.04519 valid_loss: 0.07602 test_loss: 0.08062 \n",
      "[371/500] train_loss: 0.04523 valid_loss: 0.07744 test_loss: 0.08296 \n",
      "[372/500] train_loss: 0.04626 valid_loss: 0.07727 test_loss: 0.08231 \n",
      "[373/500] train_loss: 0.04529 valid_loss: 0.07514 test_loss: 0.08304 \n",
      "[374/500] train_loss: 0.04398 valid_loss: 0.07416 test_loss: 0.08433 \n",
      "[375/500] train_loss: 0.04579 valid_loss: 0.07683 test_loss: 0.08314 \n",
      "[376/500] train_loss: 0.04377 valid_loss: 0.07559 test_loss: 0.08169 \n",
      "[377/500] train_loss: 0.04508 valid_loss: 0.07547 test_loss: 0.08186 \n",
      "[378/500] train_loss: 0.04485 valid_loss: 0.07590 test_loss: 0.08234 \n",
      "[379/500] train_loss: 0.04507 valid_loss: 0.07515 test_loss: 0.08137 \n",
      "[380/500] train_loss: 0.04284 valid_loss: 0.07364 test_loss: 0.08099 \n",
      "[381/500] train_loss: 0.04427 valid_loss: 0.07522 test_loss: 0.08299 \n",
      "[382/500] train_loss: 0.04482 valid_loss: 0.07283 test_loss: 0.08144 \n",
      "[383/500] train_loss: 0.04405 valid_loss: 0.07289 test_loss: 0.08029 \n",
      "[384/500] train_loss: 0.04552 valid_loss: 0.07572 test_loss: 0.08200 \n",
      "[385/500] train_loss: 0.04459 valid_loss: 0.07351 test_loss: 0.08214 \n",
      "[386/500] train_loss: 0.04292 valid_loss: 0.07262 test_loss: 0.08076 \n",
      "[387/500] train_loss: 0.04395 valid_loss: 0.07506 test_loss: 0.08237 \n",
      "[388/500] train_loss: 0.04477 valid_loss: 0.07476 test_loss: 0.08216 \n",
      "[389/500] train_loss: 0.04354 valid_loss: 0.07342 test_loss: 0.08158 \n",
      "[390/500] train_loss: 0.04473 valid_loss: 0.07471 test_loss: 0.08165 \n",
      "[391/500] train_loss: 0.04529 valid_loss: 0.07300 test_loss: 0.08332 \n",
      "[392/500] train_loss: 0.04304 valid_loss: 0.07339 test_loss: 0.08230 \n",
      "[393/500] train_loss: 0.04430 valid_loss: 0.07400 test_loss: 0.08116 \n",
      "[394/500] train_loss: 0.04351 valid_loss: 0.07290 test_loss: 0.08168 \n",
      "[395/500] train_loss: 0.04343 valid_loss: 0.07215 test_loss: 0.08108 \n",
      "[396/500] train_loss: 0.04432 valid_loss: 0.07307 test_loss: 0.08108 \n",
      "[397/500] train_loss: 0.04374 valid_loss: 0.07444 test_loss: 0.08406 \n",
      "[398/500] train_loss: 0.04438 valid_loss: 0.07433 test_loss: 0.08180 \n",
      "[399/500] train_loss: 0.04450 valid_loss: 0.07536 test_loss: 0.08117 \n",
      "[400/500] train_loss: 0.04416 valid_loss: 0.07555 test_loss: 0.08247 \n",
      "[401/500] train_loss: 0.04400 valid_loss: 0.07361 test_loss: 0.08204 \n",
      "[402/500] train_loss: 0.04376 valid_loss: 0.07326 test_loss: 0.08289 \n",
      "[403/500] train_loss: 0.04339 valid_loss: 0.07242 test_loss: 0.07926 \n",
      "[404/500] train_loss: 0.04427 valid_loss: 0.07532 test_loss: 0.08077 \n",
      "[405/500] train_loss: 0.04331 valid_loss: 0.07375 test_loss: 0.08174 \n",
      "[406/500] train_loss: 0.04367 valid_loss: 0.07333 test_loss: 0.08204 \n",
      "[407/500] train_loss: 0.04490 valid_loss: 0.07953 test_loss: 0.08111 \n",
      "[408/500] train_loss: 0.04361 valid_loss: 0.07540 test_loss: 0.08099 \n",
      "[409/500] train_loss: 0.04342 valid_loss: 0.07610 test_loss: 0.08240 \n",
      "[410/500] train_loss: 0.04274 valid_loss: 0.07413 test_loss: 0.08179 \n",
      "[411/500] train_loss: 0.04284 valid_loss: 0.07468 test_loss: 0.08062 \n",
      "[412/500] train_loss: 0.04374 valid_loss: 0.07358 test_loss: 0.08070 \n",
      "[413/500] train_loss: 0.04329 valid_loss: 0.07357 test_loss: 0.08193 \n",
      "[414/500] train_loss: 0.04273 valid_loss: 0.07450 test_loss: 0.08372 \n",
      "[415/500] train_loss: 0.04315 valid_loss: 0.07388 test_loss: 0.08184 \n",
      "[416/500] train_loss: 0.04397 valid_loss: 0.07311 test_loss: 0.08255 \n",
      "[417/500] train_loss: 0.04312 valid_loss: 0.07243 test_loss: 0.08030 \n",
      "[418/500] train_loss: 0.04224 valid_loss: 0.07273 test_loss: 0.08112 \n",
      "[419/500] train_loss: 0.04315 valid_loss: 0.07546 test_loss: 0.08084 \n",
      "[420/500] train_loss: 0.04246 valid_loss: 0.07402 test_loss: 0.08247 \n",
      "[421/500] train_loss: 0.04250 valid_loss: 0.07279 test_loss: 0.08126 \n",
      "[422/500] train_loss: 0.04293 valid_loss: 0.07395 test_loss: 0.08152 \n",
      "[423/500] train_loss: 0.04504 valid_loss: 0.07467 test_loss: 0.08098 \n",
      "[424/500] train_loss: 0.04272 valid_loss: 0.07419 test_loss: 0.08123 \n",
      "[425/500] train_loss: 0.04262 valid_loss: 0.07433 test_loss: 0.08115 \n",
      "[426/500] train_loss: 0.04094 valid_loss: 0.07491 test_loss: 0.08074 \n",
      "[427/500] train_loss: 0.04287 valid_loss: 0.07295 test_loss: 0.08134 \n",
      "[428/500] train_loss: 0.04419 valid_loss: 0.07434 test_loss: 0.08070 \n",
      "[429/500] train_loss: 0.04284 valid_loss: 0.07420 test_loss: 0.08216 \n",
      "[430/500] train_loss: 0.04319 valid_loss: 0.07433 test_loss: 0.08388 \n",
      "[431/500] train_loss: 0.04139 valid_loss: 0.07358 test_loss: 0.08182 \n",
      "[432/500] train_loss: 0.04168 valid_loss: 0.07447 test_loss: 0.08174 \n",
      "[433/500] train_loss: 0.04331 valid_loss: 0.07356 test_loss: 0.08178 \n",
      "[434/500] train_loss: 0.04268 valid_loss: 0.07426 test_loss: 0.08185 \n",
      "[435/500] train_loss: 0.04279 valid_loss: 0.07438 test_loss: 0.08195 \n",
      "[436/500] train_loss: 0.04348 valid_loss: 0.07591 test_loss: 0.08345 \n",
      "[437/500] train_loss: 0.04307 valid_loss: 0.07471 test_loss: 0.08222 \n",
      "[438/500] train_loss: 0.04203 valid_loss: 0.07583 test_loss: 0.08231 \n",
      "[439/500] train_loss: 0.04238 valid_loss: 0.07297 test_loss: 0.08091 \n",
      "[440/500] train_loss: 0.04398 valid_loss: 0.07418 test_loss: 0.08174 \n",
      "[441/500] train_loss: 0.04501 valid_loss: 0.07312 test_loss: 0.08170 \n",
      "[442/500] train_loss: 0.04249 valid_loss: 0.07581 test_loss: 0.08162 \n",
      "[443/500] train_loss: 0.04159 valid_loss: 0.07254 test_loss: 0.08063 \n",
      "[444/500] train_loss: 0.04235 valid_loss: 0.07386 test_loss: 0.08055 \n",
      "[445/500] train_loss: 0.04130 valid_loss: 0.07391 test_loss: 0.08145 \n",
      "[446/500] train_loss: 0.04188 valid_loss: 0.07497 test_loss: 0.08221 \n",
      "[447/500] train_loss: 0.04357 valid_loss: 0.07404 test_loss: 0.08208 \n",
      "[448/500] train_loss: 0.04269 valid_loss: 0.07217 test_loss: 0.08095 \n",
      "[449/500] train_loss: 0.04271 valid_loss: 0.07400 test_loss: 0.08110 \n",
      "[450/500] train_loss: 0.04315 valid_loss: 0.07264 test_loss: 0.08151 \n",
      "[451/500] train_loss: 0.04270 valid_loss: 0.07455 test_loss: 0.08080 \n",
      "[452/500] train_loss: 0.04277 valid_loss: 0.07209 test_loss: 0.08089 \n",
      "[453/500] train_loss: 0.04279 valid_loss: 0.07310 test_loss: 0.08112 \n",
      "[454/500] train_loss: 0.04264 valid_loss: 0.07263 test_loss: 0.08183 \n",
      "[455/500] train_loss: 0.04206 valid_loss: 0.07373 test_loss: 0.08334 \n",
      "[456/500] train_loss: 0.04157 valid_loss: 0.07400 test_loss: 0.08411 \n",
      "[457/500] train_loss: 0.04315 valid_loss: 0.07433 test_loss: 0.08123 \n",
      "[458/500] train_loss: 0.04033 valid_loss: 0.07451 test_loss: 0.08275 \n",
      "[459/500] train_loss: 0.04325 valid_loss: 0.07449 test_loss: 0.08334 \n",
      "[460/500] train_loss: 0.04265 valid_loss: 0.07460 test_loss: 0.08216 \n",
      "[461/500] train_loss: 0.04189 valid_loss: 0.07482 test_loss: 0.08259 \n",
      "[462/500] train_loss: 0.04185 valid_loss: 0.07454 test_loss: 0.08423 \n",
      "[463/500] train_loss: 0.04140 valid_loss: 0.07524 test_loss: 0.08295 \n",
      "[464/500] train_loss: 0.04138 valid_loss: 0.07791 test_loss: 0.08236 \n",
      "[465/500] train_loss: 0.04203 valid_loss: 0.07620 test_loss: 0.08547 \n",
      "[466/500] train_loss: 0.04089 valid_loss: 0.07566 test_loss: 0.08208 \n",
      "[467/500] train_loss: 0.04132 valid_loss: 0.07526 test_loss: 0.08205 \n",
      "[468/500] train_loss: 0.04133 valid_loss: 0.07395 test_loss: 0.08217 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[469/500] train_loss: 0.04199 valid_loss: 0.07333 test_loss: 0.08199 \n",
      "[470/500] train_loss: 0.04203 valid_loss: 0.07445 test_loss: 0.08335 \n",
      "[471/500] train_loss: 0.04191 valid_loss: 0.07342 test_loss: 0.08261 \n",
      "[472/500] train_loss: 0.04118 valid_loss: 0.07458 test_loss: 0.08179 \n",
      "[473/500] train_loss: 0.04073 valid_loss: 0.07476 test_loss: 0.08114 \n",
      "[474/500] train_loss: 0.04198 valid_loss: 0.07600 test_loss: 0.08263 \n",
      "[475/500] train_loss: 0.04129 valid_loss: 0.07317 test_loss: 0.08213 \n",
      "[476/500] train_loss: 0.04156 valid_loss: 0.07360 test_loss: 0.08089 \n",
      "[477/500] train_loss: 0.04131 valid_loss: 0.07595 test_loss: 0.08258 \n",
      "[478/500] train_loss: 0.04233 valid_loss: 0.07459 test_loss: 0.08132 \n",
      "[479/500] train_loss: 0.04160 valid_loss: 0.07378 test_loss: 0.08158 \n",
      "[480/500] train_loss: 0.04300 valid_loss: 0.07421 test_loss: 0.08103 \n",
      "[481/500] train_loss: 0.04039 valid_loss: 0.07422 test_loss: 0.08146 \n",
      "[482/500] train_loss: 0.04097 valid_loss: 0.07389 test_loss: 0.08190 \n",
      "[483/500] train_loss: 0.04114 valid_loss: 0.07455 test_loss: 0.08230 \n",
      "[484/500] train_loss: 0.04103 valid_loss: 0.07474 test_loss: 0.08122 \n",
      "[485/500] train_loss: 0.04059 valid_loss: 0.07541 test_loss: 0.08067 \n",
      "[486/500] train_loss: 0.04203 valid_loss: 0.07558 test_loss: 0.08200 \n",
      "[487/500] train_loss: 0.04117 valid_loss: 0.07505 test_loss: 0.08201 \n",
      "[488/500] train_loss: 0.04213 valid_loss: 0.07258 test_loss: 0.08136 \n",
      "[489/500] train_loss: 0.04241 valid_loss: 0.07615 test_loss: 0.08474 \n",
      "[490/500] train_loss: 0.04137 valid_loss: 0.07523 test_loss: 0.08277 \n",
      "[491/500] train_loss: 0.04245 valid_loss: 0.07354 test_loss: 0.08169 \n",
      "[492/500] train_loss: 0.04119 valid_loss: 0.07727 test_loss: 0.08241 \n",
      "[493/500] train_loss: 0.04129 valid_loss: 0.07579 test_loss: 0.08164 \n",
      "[494/500] train_loss: 0.04232 valid_loss: 0.07406 test_loss: 0.08466 \n",
      "[495/500] train_loss: 0.04076 valid_loss: 0.07638 test_loss: 0.08089 \n",
      "[496/500] train_loss: 0.04064 valid_loss: 0.07591 test_loss: 0.08203 \n",
      "[497/500] train_loss: 0.04117 valid_loss: 0.07724 test_loss: 0.08322 \n",
      "[498/500] train_loss: 0.04134 valid_loss: 0.07507 test_loss: 0.08107 \n",
      "[499/500] train_loss: 0.04085 valid_loss: 0.07558 test_loss: 0.08229 \n",
      "[500/500] train_loss: 0.04117 valid_loss: 0.07608 test_loss: 0.08095 \n",
      "TRAINING MODEL 18\n",
      "[  1/500] train_loss: 0.37400 valid_loss: 0.26290 test_loss: 0.27020 \n",
      "验证损失减少 (inf --> 0.262901). 正在保存模型...\n",
      "[  2/500] train_loss: 0.19824 valid_loss: 0.18712 test_loss: 0.19430 \n",
      "验证损失减少 (0.262901 --> 0.187119). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15654 valid_loss: 0.15601 test_loss: 0.16301 \n",
      "验证损失减少 (0.187119 --> 0.156007). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13976 valid_loss: 0.14918 test_loss: 0.15744 \n",
      "验证损失减少 (0.156007 --> 0.149183). 正在保存模型...\n",
      "[  5/500] train_loss: 0.13318 valid_loss: 0.13172 test_loss: 0.14038 \n",
      "验证损失减少 (0.149183 --> 0.131719). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12410 valid_loss: 0.12954 test_loss: 0.13901 \n",
      "验证损失减少 (0.131719 --> 0.129544). 正在保存模型...\n",
      "[  7/500] train_loss: 0.12130 valid_loss: 0.12431 test_loss: 0.13294 \n",
      "验证损失减少 (0.129544 --> 0.124305). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11766 valid_loss: 0.11872 test_loss: 0.12805 \n",
      "验证损失减少 (0.124305 --> 0.118722). 正在保存模型...\n",
      "[  9/500] train_loss: 0.11658 valid_loss: 0.11863 test_loss: 0.12533 \n",
      "验证损失减少 (0.118722 --> 0.118632). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.11171 valid_loss: 0.11311 test_loss: 0.12272 \n",
      "验证损失减少 (0.118632 --> 0.113110). 正在保存模型...\n",
      "[ 11/500] train_loss: 0.11055 valid_loss: 0.11117 test_loss: 0.12028 \n",
      "验证损失减少 (0.113110 --> 0.111173). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10839 valid_loss: 0.11076 test_loss: 0.12055 \n",
      "验证损失减少 (0.111173 --> 0.110760). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10580 valid_loss: 0.10640 test_loss: 0.11752 \n",
      "验证损失减少 (0.110760 --> 0.106396). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10276 valid_loss: 0.10695 test_loss: 0.11587 \n",
      "[ 15/500] train_loss: 0.10261 valid_loss: 0.10816 test_loss: 0.11439 \n",
      "[ 16/500] train_loss: 0.09968 valid_loss: 0.10263 test_loss: 0.11382 \n",
      "验证损失减少 (0.106396 --> 0.102634). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09722 valid_loss: 0.10117 test_loss: 0.11327 \n",
      "验证损失减少 (0.102634 --> 0.101168). 正在保存模型...\n",
      "[ 18/500] train_loss: 0.09827 valid_loss: 0.10163 test_loss: 0.11026 \n",
      "[ 19/500] train_loss: 0.09754 valid_loss: 0.10012 test_loss: 0.10811 \n",
      "验证损失减少 (0.101168 --> 0.100120). 正在保存模型...\n",
      "[ 20/500] train_loss: 0.09592 valid_loss: 0.09996 test_loss: 0.10869 \n",
      "验证损失减少 (0.100120 --> 0.099960). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09693 valid_loss: 0.09672 test_loss: 0.10669 \n",
      "验证损失减少 (0.099960 --> 0.096723). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09355 valid_loss: 0.09605 test_loss: 0.10634 \n",
      "验证损失减少 (0.096723 --> 0.096053). 正在保存模型...\n",
      "[ 23/500] train_loss: 0.09198 valid_loss: 0.09421 test_loss: 0.10441 \n",
      "验证损失减少 (0.096053 --> 0.094210). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.08975 valid_loss: 0.09653 test_loss: 0.10511 \n",
      "[ 25/500] train_loss: 0.09267 valid_loss: 0.09343 test_loss: 0.10327 \n",
      "验证损失减少 (0.094210 --> 0.093428). 正在保存模型...\n",
      "[ 26/500] train_loss: 0.09094 valid_loss: 0.09444 test_loss: 0.10327 \n",
      "[ 27/500] train_loss: 0.09101 valid_loss: 0.09432 test_loss: 0.10359 \n",
      "[ 28/500] train_loss: 0.08792 valid_loss: 0.09480 test_loss: 0.10277 \n",
      "[ 29/500] train_loss: 0.08949 valid_loss: 0.09312 test_loss: 0.10109 \n",
      "验证损失减少 (0.093428 --> 0.093122). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08711 valid_loss: 0.09159 test_loss: 0.10022 \n",
      "验证损失减少 (0.093122 --> 0.091585). 正在保存模型...\n",
      "[ 31/500] train_loss: 0.08800 valid_loss: 0.08968 test_loss: 0.09949 \n",
      "验证损失减少 (0.091585 --> 0.089680). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08491 valid_loss: 0.08874 test_loss: 0.09800 \n",
      "验证损失减少 (0.089680 --> 0.088743). 正在保存模型...\n",
      "[ 33/500] train_loss: 0.08349 valid_loss: 0.08745 test_loss: 0.09758 \n",
      "验证损失减少 (0.088743 --> 0.087454). 正在保存模型...\n",
      "[ 34/500] train_loss: 0.08390 valid_loss: 0.08689 test_loss: 0.09795 \n",
      "验证损失减少 (0.087454 --> 0.086894). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08492 valid_loss: 0.08675 test_loss: 0.09891 \n",
      "验证损失减少 (0.086894 --> 0.086747). 正在保存模型...\n",
      "[ 36/500] train_loss: 0.08281 valid_loss: 0.08767 test_loss: 0.09812 \n",
      "[ 37/500] train_loss: 0.08301 valid_loss: 0.08721 test_loss: 0.09591 \n",
      "[ 38/500] train_loss: 0.08448 valid_loss: 0.08649 test_loss: 0.09541 \n",
      "验证损失减少 (0.086747 --> 0.086492). 正在保存模型...\n",
      "[ 39/500] train_loss: 0.08113 valid_loss: 0.08662 test_loss: 0.09809 \n",
      "[ 40/500] train_loss: 0.08314 valid_loss: 0.08496 test_loss: 0.09701 \n",
      "验证损失减少 (0.086492 --> 0.084960). 正在保存模型...\n",
      "[ 41/500] train_loss: 0.08063 valid_loss: 0.08625 test_loss: 0.09459 \n",
      "[ 42/500] train_loss: 0.07904 valid_loss: 0.08723 test_loss: 0.09430 \n",
      "[ 43/500] train_loss: 0.08202 valid_loss: 0.08682 test_loss: 0.09557 \n",
      "[ 44/500] train_loss: 0.08177 valid_loss: 0.08420 test_loss: 0.09567 \n",
      "验证损失减少 (0.084960 --> 0.084200). 正在保存模型...\n",
      "[ 45/500] train_loss: 0.07864 valid_loss: 0.08366 test_loss: 0.09521 \n",
      "验证损失减少 (0.084200 --> 0.083660). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07780 valid_loss: 0.08477 test_loss: 0.09348 \n",
      "[ 47/500] train_loss: 0.07992 valid_loss: 0.08348 test_loss: 0.09302 \n",
      "验证损失减少 (0.083660 --> 0.083484). 正在保存模型...\n",
      "[ 48/500] train_loss: 0.08015 valid_loss: 0.08364 test_loss: 0.09335 \n",
      "[ 49/500] train_loss: 0.08047 valid_loss: 0.08451 test_loss: 0.09203 \n",
      "[ 50/500] train_loss: 0.07695 valid_loss: 0.08434 test_loss: 0.09386 \n",
      "[ 51/500] train_loss: 0.07693 valid_loss: 0.08225 test_loss: 0.09199 \n",
      "验证损失减少 (0.083484 --> 0.082247). 正在保存模型...\n",
      "[ 52/500] train_loss: 0.07728 valid_loss: 0.08234 test_loss: 0.09228 \n",
      "[ 53/500] train_loss: 0.07766 valid_loss: 0.08251 test_loss: 0.09232 \n",
      "[ 54/500] train_loss: 0.07708 valid_loss: 0.08206 test_loss: 0.09081 \n",
      "验证损失减少 (0.082247 --> 0.082056). 正在保存模型...\n",
      "[ 55/500] train_loss: 0.07423 valid_loss: 0.08131 test_loss: 0.09094 \n",
      "验证损失减少 (0.082056 --> 0.081308). 正在保存模型...\n",
      "[ 56/500] train_loss: 0.07427 valid_loss: 0.08302 test_loss: 0.09048 \n",
      "[ 57/500] train_loss: 0.07489 valid_loss: 0.08171 test_loss: 0.09083 \n",
      "[ 58/500] train_loss: 0.07501 valid_loss: 0.08373 test_loss: 0.08942 \n",
      "[ 59/500] train_loss: 0.07514 valid_loss: 0.08155 test_loss: 0.08969 \n",
      "[ 60/500] train_loss: 0.07590 valid_loss: 0.08202 test_loss: 0.09105 \n",
      "[ 61/500] train_loss: 0.07520 valid_loss: 0.08305 test_loss: 0.09016 \n",
      "[ 62/500] train_loss: 0.07362 valid_loss: 0.08137 test_loss: 0.08768 \n",
      "[ 63/500] train_loss: 0.07308 valid_loss: 0.07970 test_loss: 0.08857 \n",
      "验证损失减少 (0.081308 --> 0.079702). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 64/500] train_loss: 0.07203 valid_loss: 0.08121 test_loss: 0.08997 \n",
      "[ 65/500] train_loss: 0.07486 valid_loss: 0.07993 test_loss: 0.08853 \n",
      "[ 66/500] train_loss: 0.07310 valid_loss: 0.08013 test_loss: 0.09101 \n",
      "[ 67/500] train_loss: 0.07280 valid_loss: 0.08024 test_loss: 0.08959 \n",
      "[ 68/500] train_loss: 0.07054 valid_loss: 0.07936 test_loss: 0.08775 \n",
      "验证损失减少 (0.079702 --> 0.079360). 正在保存模型...\n",
      "[ 69/500] train_loss: 0.07310 valid_loss: 0.07867 test_loss: 0.08843 \n",
      "验证损失减少 (0.079360 --> 0.078667). 正在保存模型...\n",
      "[ 70/500] train_loss: 0.07036 valid_loss: 0.08071 test_loss: 0.08809 \n",
      "[ 71/500] train_loss: 0.07233 valid_loss: 0.08004 test_loss: 0.08859 \n",
      "[ 72/500] train_loss: 0.07162 valid_loss: 0.07931 test_loss: 0.08811 \n",
      "[ 73/500] train_loss: 0.07121 valid_loss: 0.07978 test_loss: 0.08863 \n",
      "[ 74/500] train_loss: 0.06921 valid_loss: 0.07969 test_loss: 0.08798 \n",
      "[ 75/500] train_loss: 0.07102 valid_loss: 0.08120 test_loss: 0.08775 \n",
      "[ 76/500] train_loss: 0.07164 valid_loss: 0.08090 test_loss: 0.08875 \n",
      "[ 77/500] train_loss: 0.07003 valid_loss: 0.08106 test_loss: 0.08729 \n",
      "[ 78/500] train_loss: 0.06836 valid_loss: 0.07915 test_loss: 0.08675 \n",
      "[ 79/500] train_loss: 0.07084 valid_loss: 0.07769 test_loss: 0.08621 \n",
      "验证损失减少 (0.078667 --> 0.077690). 正在保存模型...\n",
      "[ 80/500] train_loss: 0.06867 valid_loss: 0.07941 test_loss: 0.08732 \n",
      "[ 81/500] train_loss: 0.06965 valid_loss: 0.07846 test_loss: 0.08700 \n",
      "[ 82/500] train_loss: 0.06923 valid_loss: 0.07892 test_loss: 0.08610 \n",
      "[ 83/500] train_loss: 0.06768 valid_loss: 0.07953 test_loss: 0.08725 \n",
      "[ 84/500] train_loss: 0.06913 valid_loss: 0.07875 test_loss: 0.08670 \n",
      "[ 85/500] train_loss: 0.07014 valid_loss: 0.08375 test_loss: 0.08765 \n",
      "[ 86/500] train_loss: 0.06781 valid_loss: 0.07955 test_loss: 0.08723 \n",
      "[ 87/500] train_loss: 0.06879 valid_loss: 0.07765 test_loss: 0.08613 \n",
      "验证损失减少 (0.077690 --> 0.077653). 正在保存模型...\n",
      "[ 88/500] train_loss: 0.06923 valid_loss: 0.07783 test_loss: 0.08604 \n",
      "[ 89/500] train_loss: 0.06771 valid_loss: 0.07825 test_loss: 0.08582 \n",
      "[ 90/500] train_loss: 0.06747 valid_loss: 0.07795 test_loss: 0.08582 \n",
      "[ 91/500] train_loss: 0.06650 valid_loss: 0.07747 test_loss: 0.08572 \n",
      "验证损失减少 (0.077653 --> 0.077471). 正在保存模型...\n",
      "[ 92/500] train_loss: 0.06748 valid_loss: 0.07987 test_loss: 0.08494 \n",
      "[ 93/500] train_loss: 0.06745 valid_loss: 0.07590 test_loss: 0.08516 \n",
      "验证损失减少 (0.077471 --> 0.075903). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.06696 valid_loss: 0.07669 test_loss: 0.08469 \n",
      "[ 95/500] train_loss: 0.06676 valid_loss: 0.07768 test_loss: 0.08451 \n",
      "[ 96/500] train_loss: 0.06495 valid_loss: 0.07580 test_loss: 0.08484 \n",
      "验证损失减少 (0.075903 --> 0.075799). 正在保存模型...\n",
      "[ 97/500] train_loss: 0.06604 valid_loss: 0.07955 test_loss: 0.08546 \n",
      "[ 98/500] train_loss: 0.06811 valid_loss: 0.07782 test_loss: 0.08484 \n",
      "[ 99/500] train_loss: 0.06651 valid_loss: 0.07696 test_loss: 0.08454 \n",
      "[100/500] train_loss: 0.06508 valid_loss: 0.07570 test_loss: 0.08499 \n",
      "验证损失减少 (0.075799 --> 0.075699). 正在保存模型...\n",
      "[101/500] train_loss: 0.06585 valid_loss: 0.07533 test_loss: 0.08411 \n",
      "验证损失减少 (0.075699 --> 0.075328). 正在保存模型...\n",
      "[102/500] train_loss: 0.06645 valid_loss: 0.07792 test_loss: 0.08454 \n",
      "[103/500] train_loss: 0.06663 valid_loss: 0.07643 test_loss: 0.08505 \n",
      "[104/500] train_loss: 0.06617 valid_loss: 0.08050 test_loss: 0.08557 \n",
      "[105/500] train_loss: 0.06452 valid_loss: 0.07722 test_loss: 0.08483 \n",
      "[106/500] train_loss: 0.06531 valid_loss: 0.07994 test_loss: 0.08501 \n",
      "[107/500] train_loss: 0.06390 valid_loss: 0.07561 test_loss: 0.08415 \n",
      "[108/500] train_loss: 0.06592 valid_loss: 0.07412 test_loss: 0.08283 \n",
      "验证损失减少 (0.075328 --> 0.074117). 正在保存模型...\n",
      "[109/500] train_loss: 0.06299 valid_loss: 0.07735 test_loss: 0.08404 \n",
      "[110/500] train_loss: 0.06482 valid_loss: 0.07746 test_loss: 0.08449 \n",
      "[111/500] train_loss: 0.06412 valid_loss: 0.07599 test_loss: 0.08619 \n",
      "[112/500] train_loss: 0.06449 valid_loss: 0.07493 test_loss: 0.08509 \n",
      "[113/500] train_loss: 0.06364 valid_loss: 0.07926 test_loss: 0.08352 \n",
      "[114/500] train_loss: 0.06561 valid_loss: 0.07662 test_loss: 0.08528 \n",
      "[115/500] train_loss: 0.06215 valid_loss: 0.07626 test_loss: 0.08318 \n",
      "[116/500] train_loss: 0.06256 valid_loss: 0.07644 test_loss: 0.08267 \n",
      "[117/500] train_loss: 0.06465 valid_loss: 0.07575 test_loss: 0.08376 \n",
      "[118/500] train_loss: 0.06306 valid_loss: 0.07644 test_loss: 0.08286 \n",
      "[119/500] train_loss: 0.06248 valid_loss: 0.07601 test_loss: 0.08376 \n",
      "[120/500] train_loss: 0.06321 valid_loss: 0.07566 test_loss: 0.08433 \n",
      "[121/500] train_loss: 0.06433 valid_loss: 0.07652 test_loss: 0.08568 \n",
      "[122/500] train_loss: 0.06196 valid_loss: 0.07545 test_loss: 0.08294 \n",
      "[123/500] train_loss: 0.06384 valid_loss: 0.07843 test_loss: 0.08351 \n",
      "[124/500] train_loss: 0.06244 valid_loss: 0.07357 test_loss: 0.08293 \n",
      "验证损失减少 (0.074117 --> 0.073567). 正在保存模型...\n",
      "[125/500] train_loss: 0.06007 valid_loss: 0.07380 test_loss: 0.08305 \n",
      "[126/500] train_loss: 0.06452 valid_loss: 0.07570 test_loss: 0.08279 \n",
      "[127/500] train_loss: 0.06255 valid_loss: 0.07367 test_loss: 0.08409 \n",
      "[128/500] train_loss: 0.06017 valid_loss: 0.07454 test_loss: 0.08373 \n",
      "[129/500] train_loss: 0.06264 valid_loss: 0.07492 test_loss: 0.08309 \n",
      "[130/500] train_loss: 0.06192 valid_loss: 0.07400 test_loss: 0.08165 \n",
      "[131/500] train_loss: 0.06206 valid_loss: 0.07519 test_loss: 0.08356 \n",
      "[132/500] train_loss: 0.06071 valid_loss: 0.07524 test_loss: 0.08116 \n",
      "[133/500] train_loss: 0.06080 valid_loss: 0.07397 test_loss: 0.08229 \n",
      "[134/500] train_loss: 0.06203 valid_loss: 0.07521 test_loss: 0.08234 \n",
      "[135/500] train_loss: 0.06098 valid_loss: 0.07430 test_loss: 0.08447 \n",
      "[136/500] train_loss: 0.06027 valid_loss: 0.07457 test_loss: 0.08287 \n",
      "[137/500] train_loss: 0.05939 valid_loss: 0.07374 test_loss: 0.08344 \n",
      "[138/500] train_loss: 0.06162 valid_loss: 0.07529 test_loss: 0.08245 \n",
      "[139/500] train_loss: 0.06260 valid_loss: 0.07396 test_loss: 0.08210 \n",
      "[140/500] train_loss: 0.05967 valid_loss: 0.07554 test_loss: 0.08201 \n",
      "[141/500] train_loss: 0.06064 valid_loss: 0.07558 test_loss: 0.08304 \n",
      "[142/500] train_loss: 0.05941 valid_loss: 0.07405 test_loss: 0.08148 \n",
      "[143/500] train_loss: 0.06273 valid_loss: 0.07359 test_loss: 0.08202 \n",
      "[144/500] train_loss: 0.06064 valid_loss: 0.07817 test_loss: 0.08316 \n",
      "[145/500] train_loss: 0.06015 valid_loss: 0.07618 test_loss: 0.08172 \n",
      "[146/500] train_loss: 0.06097 valid_loss: 0.07497 test_loss: 0.08069 \n",
      "[147/500] train_loss: 0.05923 valid_loss: 0.07321 test_loss: 0.08115 \n",
      "验证损失减少 (0.073567 --> 0.073210). 正在保存模型...\n",
      "[148/500] train_loss: 0.05942 valid_loss: 0.07424 test_loss: 0.08350 \n",
      "[149/500] train_loss: 0.05996 valid_loss: 0.07501 test_loss: 0.08160 \n",
      "[150/500] train_loss: 0.05963 valid_loss: 0.07432 test_loss: 0.08192 \n",
      "[151/500] train_loss: 0.06025 valid_loss: 0.07384 test_loss: 0.08081 \n",
      "[152/500] train_loss: 0.05888 valid_loss: 0.07366 test_loss: 0.08159 \n",
      "[153/500] train_loss: 0.05831 valid_loss: 0.07509 test_loss: 0.08111 \n",
      "[154/500] train_loss: 0.05906 valid_loss: 0.07397 test_loss: 0.08141 \n",
      "[155/500] train_loss: 0.05936 valid_loss: 0.07558 test_loss: 0.07997 \n",
      "[156/500] train_loss: 0.05879 valid_loss: 0.07443 test_loss: 0.08221 \n",
      "[157/500] train_loss: 0.05746 valid_loss: 0.07724 test_loss: 0.08323 \n",
      "[158/500] train_loss: 0.05869 valid_loss: 0.07639 test_loss: 0.08389 \n",
      "[159/500] train_loss: 0.05772 valid_loss: 0.07458 test_loss: 0.08337 \n",
      "[160/500] train_loss: 0.06015 valid_loss: 0.07664 test_loss: 0.08652 \n",
      "[161/500] train_loss: 0.05935 valid_loss: 0.07470 test_loss: 0.08109 \n",
      "[162/500] train_loss: 0.05754 valid_loss: 0.07464 test_loss: 0.08171 \n",
      "[163/500] train_loss: 0.06027 valid_loss: 0.07828 test_loss: 0.08416 \n",
      "[164/500] train_loss: 0.05863 valid_loss: 0.07412 test_loss: 0.08315 \n",
      "[165/500] train_loss: 0.05828 valid_loss: 0.07430 test_loss: 0.08301 \n",
      "[166/500] train_loss: 0.05832 valid_loss: 0.07697 test_loss: 0.08290 \n",
      "[167/500] train_loss: 0.05859 valid_loss: 0.07486 test_loss: 0.08249 \n",
      "[168/500] train_loss: 0.05839 valid_loss: 0.07386 test_loss: 0.08117 \n",
      "[169/500] train_loss: 0.05777 valid_loss: 0.07311 test_loss: 0.08217 \n",
      "验证损失减少 (0.073210 --> 0.073114). 正在保存模型...\n",
      "[170/500] train_loss: 0.05750 valid_loss: 0.07392 test_loss: 0.08198 \n",
      "[171/500] train_loss: 0.05569 valid_loss: 0.07345 test_loss: 0.08246 \n",
      "[172/500] train_loss: 0.05790 valid_loss: 0.07373 test_loss: 0.08236 \n",
      "[173/500] train_loss: 0.05666 valid_loss: 0.07252 test_loss: 0.08179 \n",
      "验证损失减少 (0.073114 --> 0.072525). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174/500] train_loss: 0.05732 valid_loss: 0.07395 test_loss: 0.08386 \n",
      "[175/500] train_loss: 0.05671 valid_loss: 0.07304 test_loss: 0.08162 \n",
      "[176/500] train_loss: 0.05722 valid_loss: 0.07297 test_loss: 0.08198 \n",
      "[177/500] train_loss: 0.05641 valid_loss: 0.07277 test_loss: 0.08169 \n",
      "[178/500] train_loss: 0.05663 valid_loss: 0.07278 test_loss: 0.08122 \n",
      "[179/500] train_loss: 0.05639 valid_loss: 0.07265 test_loss: 0.08218 \n",
      "[180/500] train_loss: 0.05740 valid_loss: 0.07260 test_loss: 0.08080 \n",
      "[181/500] train_loss: 0.05656 valid_loss: 0.07275 test_loss: 0.08226 \n",
      "[182/500] train_loss: 0.05700 valid_loss: 0.07594 test_loss: 0.08184 \n",
      "[183/500] train_loss: 0.05680 valid_loss: 0.07246 test_loss: 0.08156 \n",
      "验证损失减少 (0.072525 --> 0.072459). 正在保存模型...\n",
      "[184/500] train_loss: 0.05558 valid_loss: 0.07244 test_loss: 0.08098 \n",
      "验证损失减少 (0.072459 --> 0.072440). 正在保存模型...\n",
      "[185/500] train_loss: 0.05664 valid_loss: 0.07160 test_loss: 0.08153 \n",
      "验证损失减少 (0.072440 --> 0.071602). 正在保存模型...\n",
      "[186/500] train_loss: 0.05597 valid_loss: 0.07277 test_loss: 0.08144 \n",
      "[187/500] train_loss: 0.05631 valid_loss: 0.07289 test_loss: 0.08133 \n",
      "[188/500] train_loss: 0.05540 valid_loss: 0.07329 test_loss: 0.08268 \n",
      "[189/500] train_loss: 0.05561 valid_loss: 0.07207 test_loss: 0.08032 \n",
      "[190/500] train_loss: 0.05408 valid_loss: 0.07329 test_loss: 0.08064 \n",
      "[191/500] train_loss: 0.05445 valid_loss: 0.07260 test_loss: 0.08059 \n",
      "[192/500] train_loss: 0.05520 valid_loss: 0.07208 test_loss: 0.08109 \n",
      "[193/500] train_loss: 0.05522 valid_loss: 0.07250 test_loss: 0.08097 \n",
      "[194/500] train_loss: 0.05504 valid_loss: 0.07194 test_loss: 0.08083 \n",
      "[195/500] train_loss: 0.05447 valid_loss: 0.07149 test_loss: 0.08058 \n",
      "验证损失减少 (0.071602 --> 0.071487). 正在保存模型...\n",
      "[196/500] train_loss: 0.05526 valid_loss: 0.07250 test_loss: 0.08043 \n",
      "[197/500] train_loss: 0.05381 valid_loss: 0.07248 test_loss: 0.08218 \n",
      "[198/500] train_loss: 0.05474 valid_loss: 0.07329 test_loss: 0.08268 \n",
      "[199/500] train_loss: 0.05567 valid_loss: 0.07184 test_loss: 0.08311 \n",
      "[200/500] train_loss: 0.05454 valid_loss: 0.07079 test_loss: 0.08079 \n",
      "验证损失减少 (0.071487 --> 0.070788). 正在保存模型...\n",
      "[201/500] train_loss: 0.05433 valid_loss: 0.07202 test_loss: 0.08114 \n",
      "[202/500] train_loss: 0.05442 valid_loss: 0.07227 test_loss: 0.07978 \n",
      "[203/500] train_loss: 0.05471 valid_loss: 0.07298 test_loss: 0.08181 \n",
      "[204/500] train_loss: 0.05554 valid_loss: 0.07103 test_loss: 0.08061 \n",
      "[205/500] train_loss: 0.05336 valid_loss: 0.07311 test_loss: 0.08166 \n",
      "[206/500] train_loss: 0.05362 valid_loss: 0.07147 test_loss: 0.08096 \n",
      "[207/500] train_loss: 0.05542 valid_loss: 0.07052 test_loss: 0.08145 \n",
      "验证损失减少 (0.070788 --> 0.070525). 正在保存模型...\n",
      "[208/500] train_loss: 0.05511 valid_loss: 0.07111 test_loss: 0.07957 \n",
      "[209/500] train_loss: 0.05337 valid_loss: 0.07174 test_loss: 0.08170 \n",
      "[210/500] train_loss: 0.05310 valid_loss: 0.07256 test_loss: 0.08132 \n",
      "[211/500] train_loss: 0.05396 valid_loss: 0.07254 test_loss: 0.08085 \n",
      "[212/500] train_loss: 0.05528 valid_loss: 0.07329 test_loss: 0.08121 \n",
      "[213/500] train_loss: 0.05445 valid_loss: 0.07104 test_loss: 0.08128 \n",
      "[214/500] train_loss: 0.05386 valid_loss: 0.07070 test_loss: 0.08058 \n",
      "[215/500] train_loss: 0.05500 valid_loss: 0.07149 test_loss: 0.08057 \n",
      "[216/500] train_loss: 0.05255 valid_loss: 0.07242 test_loss: 0.08062 \n",
      "[217/500] train_loss: 0.05367 valid_loss: 0.07257 test_loss: 0.08127 \n",
      "[218/500] train_loss: 0.05239 valid_loss: 0.07303 test_loss: 0.08120 \n",
      "[219/500] train_loss: 0.05447 valid_loss: 0.07277 test_loss: 0.08193 \n",
      "[220/500] train_loss: 0.05349 valid_loss: 0.07167 test_loss: 0.08044 \n",
      "[221/500] train_loss: 0.05415 valid_loss: 0.07445 test_loss: 0.08247 \n",
      "[222/500] train_loss: 0.05230 valid_loss: 0.07338 test_loss: 0.08279 \n",
      "[223/500] train_loss: 0.05356 valid_loss: 0.07450 test_loss: 0.08181 \n",
      "[224/500] train_loss: 0.05288 valid_loss: 0.07118 test_loss: 0.08008 \n",
      "[225/500] train_loss: 0.05325 valid_loss: 0.07360 test_loss: 0.08235 \n",
      "[226/500] train_loss: 0.05294 valid_loss: 0.07158 test_loss: 0.08224 \n",
      "[227/500] train_loss: 0.05184 valid_loss: 0.07196 test_loss: 0.08132 \n",
      "[228/500] train_loss: 0.05231 valid_loss: 0.07106 test_loss: 0.08144 \n",
      "[229/500] train_loss: 0.05400 valid_loss: 0.07265 test_loss: 0.08190 \n",
      "[230/500] train_loss: 0.05232 valid_loss: 0.07115 test_loss: 0.08328 \n",
      "[231/500] train_loss: 0.05238 valid_loss: 0.07355 test_loss: 0.08271 \n",
      "[232/500] train_loss: 0.05308 valid_loss: 0.07170 test_loss: 0.08161 \n",
      "[233/500] train_loss: 0.05433 valid_loss: 0.07274 test_loss: 0.08143 \n",
      "[234/500] train_loss: 0.05269 valid_loss: 0.07275 test_loss: 0.08132 \n",
      "[235/500] train_loss: 0.05389 valid_loss: 0.07149 test_loss: 0.08106 \n",
      "[236/500] train_loss: 0.05310 valid_loss: 0.07311 test_loss: 0.08221 \n",
      "[237/500] train_loss: 0.05154 valid_loss: 0.07126 test_loss: 0.08121 \n",
      "[238/500] train_loss: 0.05283 valid_loss: 0.07127 test_loss: 0.08068 \n",
      "[239/500] train_loss: 0.05193 valid_loss: 0.07151 test_loss: 0.08055 \n",
      "[240/500] train_loss: 0.05175 valid_loss: 0.07071 test_loss: 0.08072 \n",
      "[241/500] train_loss: 0.05174 valid_loss: 0.07116 test_loss: 0.08039 \n",
      "[242/500] train_loss: 0.05351 valid_loss: 0.07112 test_loss: 0.08183 \n",
      "[243/500] train_loss: 0.05234 valid_loss: 0.07174 test_loss: 0.08078 \n",
      "[244/500] train_loss: 0.05085 valid_loss: 0.07247 test_loss: 0.08164 \n",
      "[245/500] train_loss: 0.05380 valid_loss: 0.07343 test_loss: 0.08123 \n",
      "[246/500] train_loss: 0.05287 valid_loss: 0.07406 test_loss: 0.08277 \n",
      "[247/500] train_loss: 0.05099 valid_loss: 0.07231 test_loss: 0.08136 \n",
      "[248/500] train_loss: 0.05271 valid_loss: 0.07128 test_loss: 0.08099 \n",
      "[249/500] train_loss: 0.05186 valid_loss: 0.07204 test_loss: 0.08329 \n",
      "[250/500] train_loss: 0.05093 valid_loss: 0.07250 test_loss: 0.08214 \n",
      "[251/500] train_loss: 0.05233 valid_loss: 0.07255 test_loss: 0.08350 \n",
      "[252/500] train_loss: 0.05207 valid_loss: 0.07269 test_loss: 0.08229 \n",
      "[253/500] train_loss: 0.05198 valid_loss: 0.07185 test_loss: 0.08341 \n",
      "[254/500] train_loss: 0.05120 valid_loss: 0.07181 test_loss: 0.08252 \n",
      "[255/500] train_loss: 0.05235 valid_loss: 0.07213 test_loss: 0.08318 \n",
      "[256/500] train_loss: 0.05125 valid_loss: 0.07378 test_loss: 0.08176 \n",
      "[257/500] train_loss: 0.05124 valid_loss: 0.07314 test_loss: 0.08300 \n",
      "[258/500] train_loss: 0.05070 valid_loss: 0.07180 test_loss: 0.08139 \n",
      "[259/500] train_loss: 0.04945 valid_loss: 0.07185 test_loss: 0.07978 \n",
      "[260/500] train_loss: 0.05044 valid_loss: 0.07216 test_loss: 0.08332 \n",
      "[261/500] train_loss: 0.05150 valid_loss: 0.07202 test_loss: 0.08238 \n",
      "[262/500] train_loss: 0.05174 valid_loss: 0.07429 test_loss: 0.08307 \n",
      "[263/500] train_loss: 0.05032 valid_loss: 0.07545 test_loss: 0.08430 \n",
      "[264/500] train_loss: 0.04933 valid_loss: 0.07440 test_loss: 0.08212 \n",
      "[265/500] train_loss: 0.04963 valid_loss: 0.07251 test_loss: 0.08423 \n",
      "[266/500] train_loss: 0.04934 valid_loss: 0.07184 test_loss: 0.08174 \n",
      "[267/500] train_loss: 0.04982 valid_loss: 0.07286 test_loss: 0.08307 \n",
      "[268/500] train_loss: 0.05192 valid_loss: 0.07327 test_loss: 0.08355 \n",
      "[269/500] train_loss: 0.04862 valid_loss: 0.07280 test_loss: 0.08304 \n",
      "[270/500] train_loss: 0.05101 valid_loss: 0.07374 test_loss: 0.08059 \n",
      "[271/500] train_loss: 0.05091 valid_loss: 0.07272 test_loss: 0.08202 \n",
      "[272/500] train_loss: 0.04983 valid_loss: 0.07266 test_loss: 0.08235 \n",
      "[273/500] train_loss: 0.05037 valid_loss: 0.07135 test_loss: 0.08232 \n",
      "[274/500] train_loss: 0.04980 valid_loss: 0.07100 test_loss: 0.08220 \n",
      "[275/500] train_loss: 0.04970 valid_loss: 0.07621 test_loss: 0.08171 \n",
      "[276/500] train_loss: 0.05052 valid_loss: 0.07512 test_loss: 0.08336 \n",
      "[277/500] train_loss: 0.04936 valid_loss: 0.07075 test_loss: 0.08162 \n",
      "[278/500] train_loss: 0.05043 valid_loss: 0.07375 test_loss: 0.08403 \n",
      "[279/500] train_loss: 0.05004 valid_loss: 0.07084 test_loss: 0.08268 \n",
      "[280/500] train_loss: 0.05074 valid_loss: 0.07140 test_loss: 0.08341 \n",
      "[281/500] train_loss: 0.04868 valid_loss: 0.07181 test_loss: 0.08526 \n",
      "[282/500] train_loss: 0.04965 valid_loss: 0.07123 test_loss: 0.08364 \n",
      "[283/500] train_loss: 0.04965 valid_loss: 0.07209 test_loss: 0.08378 \n",
      "[284/500] train_loss: 0.04979 valid_loss: 0.07421 test_loss: 0.08350 \n",
      "[285/500] train_loss: 0.04843 valid_loss: 0.07295 test_loss: 0.08312 \n",
      "[286/500] train_loss: 0.04852 valid_loss: 0.07566 test_loss: 0.08555 \n",
      "[287/500] train_loss: 0.04895 valid_loss: 0.07606 test_loss: 0.08267 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[288/500] train_loss: 0.04989 valid_loss: 0.07211 test_loss: 0.08231 \n",
      "[289/500] train_loss: 0.04843 valid_loss: 0.07292 test_loss: 0.08405 \n",
      "[290/500] train_loss: 0.04855 valid_loss: 0.07305 test_loss: 0.08206 \n",
      "[291/500] train_loss: 0.04822 valid_loss: 0.07178 test_loss: 0.08280 \n",
      "[292/500] train_loss: 0.04839 valid_loss: 0.07383 test_loss: 0.08371 \n",
      "[293/500] train_loss: 0.04944 valid_loss: 0.07204 test_loss: 0.08358 \n",
      "[294/500] train_loss: 0.04903 valid_loss: 0.07295 test_loss: 0.08205 \n",
      "[295/500] train_loss: 0.04931 valid_loss: 0.07373 test_loss: 0.08325 \n",
      "[296/500] train_loss: 0.05025 valid_loss: 0.07338 test_loss: 0.08359 \n",
      "[297/500] train_loss: 0.04825 valid_loss: 0.07379 test_loss: 0.08447 \n",
      "[298/500] train_loss: 0.04814 valid_loss: 0.07157 test_loss: 0.08189 \n",
      "[299/500] train_loss: 0.04851 valid_loss: 0.07156 test_loss: 0.08336 \n",
      "[300/500] train_loss: 0.04810 valid_loss: 0.07149 test_loss: 0.08254 \n",
      "[301/500] train_loss: 0.04733 valid_loss: 0.07396 test_loss: 0.08230 \n",
      "[302/500] train_loss: 0.04962 valid_loss: 0.07295 test_loss: 0.08321 \n",
      "[303/500] train_loss: 0.04857 valid_loss: 0.07202 test_loss: 0.08248 \n",
      "[304/500] train_loss: 0.04901 valid_loss: 0.07361 test_loss: 0.08429 \n",
      "[305/500] train_loss: 0.04790 valid_loss: 0.07404 test_loss: 0.08409 \n",
      "[306/500] train_loss: 0.04819 valid_loss: 0.07277 test_loss: 0.08215 \n",
      "[307/500] train_loss: 0.04907 valid_loss: 0.07353 test_loss: 0.08237 \n",
      "[308/500] train_loss: 0.04841 valid_loss: 0.07220 test_loss: 0.08197 \n",
      "[309/500] train_loss: 0.04825 valid_loss: 0.07327 test_loss: 0.08283 \n",
      "[310/500] train_loss: 0.04806 valid_loss: 0.07410 test_loss: 0.08288 \n",
      "[311/500] train_loss: 0.04950 valid_loss: 0.07398 test_loss: 0.08302 \n",
      "[312/500] train_loss: 0.04893 valid_loss: 0.07377 test_loss: 0.08278 \n",
      "[313/500] train_loss: 0.04762 valid_loss: 0.07362 test_loss: 0.08251 \n",
      "[314/500] train_loss: 0.04719 valid_loss: 0.07508 test_loss: 0.08673 \n",
      "[315/500] train_loss: 0.04975 valid_loss: 0.07422 test_loss: 0.08452 \n",
      "[316/500] train_loss: 0.04949 valid_loss: 0.07379 test_loss: 0.08364 \n",
      "[317/500] train_loss: 0.04772 valid_loss: 0.07495 test_loss: 0.08288 \n",
      "[318/500] train_loss: 0.04887 valid_loss: 0.07307 test_loss: 0.08268 \n",
      "[319/500] train_loss: 0.04814 valid_loss: 0.07353 test_loss: 0.08286 \n",
      "[320/500] train_loss: 0.04761 valid_loss: 0.07298 test_loss: 0.08357 \n",
      "[321/500] train_loss: 0.04926 valid_loss: 0.07269 test_loss: 0.08418 \n",
      "[322/500] train_loss: 0.04746 valid_loss: 0.07352 test_loss: 0.08275 \n",
      "[323/500] train_loss: 0.04720 valid_loss: 0.07333 test_loss: 0.08248 \n",
      "[324/500] train_loss: 0.04752 valid_loss: 0.07274 test_loss: 0.08295 \n",
      "[325/500] train_loss: 0.04802 valid_loss: 0.07272 test_loss: 0.08419 \n",
      "[326/500] train_loss: 0.04748 valid_loss: 0.07198 test_loss: 0.08464 \n",
      "[327/500] train_loss: 0.04753 valid_loss: 0.07180 test_loss: 0.08213 \n",
      "[328/500] train_loss: 0.04697 valid_loss: 0.07277 test_loss: 0.08223 \n",
      "[329/500] train_loss: 0.04753 valid_loss: 0.07286 test_loss: 0.08332 \n",
      "[330/500] train_loss: 0.04576 valid_loss: 0.07255 test_loss: 0.08343 \n",
      "[331/500] train_loss: 0.04680 valid_loss: 0.07180 test_loss: 0.08232 \n",
      "[332/500] train_loss: 0.04775 valid_loss: 0.07184 test_loss: 0.08297 \n",
      "[333/500] train_loss: 0.04752 valid_loss: 0.07209 test_loss: 0.08307 \n",
      "[334/500] train_loss: 0.04749 valid_loss: 0.07276 test_loss: 0.08264 \n",
      "[335/500] train_loss: 0.04745 valid_loss: 0.07467 test_loss: 0.08242 \n",
      "[336/500] train_loss: 0.04740 valid_loss: 0.07247 test_loss: 0.08084 \n",
      "[337/500] train_loss: 0.04780 valid_loss: 0.07341 test_loss: 0.08225 \n",
      "[338/500] train_loss: 0.04643 valid_loss: 0.07181 test_loss: 0.08199 \n",
      "[339/500] train_loss: 0.04614 valid_loss: 0.07433 test_loss: 0.08230 \n",
      "[340/500] train_loss: 0.04743 valid_loss: 0.07209 test_loss: 0.08234 \n",
      "[341/500] train_loss: 0.04639 valid_loss: 0.07181 test_loss: 0.08153 \n",
      "[342/500] train_loss: 0.04655 valid_loss: 0.07297 test_loss: 0.08209 \n",
      "[343/500] train_loss: 0.04640 valid_loss: 0.07193 test_loss: 0.08187 \n",
      "[344/500] train_loss: 0.04678 valid_loss: 0.07351 test_loss: 0.08252 \n",
      "[345/500] train_loss: 0.04731 valid_loss: 0.07247 test_loss: 0.08350 \n",
      "[346/500] train_loss: 0.04774 valid_loss: 0.07365 test_loss: 0.08404 \n",
      "[347/500] train_loss: 0.04643 valid_loss: 0.07358 test_loss: 0.08382 \n",
      "[348/500] train_loss: 0.04610 valid_loss: 0.07126 test_loss: 0.08115 \n",
      "[349/500] train_loss: 0.04637 valid_loss: 0.07251 test_loss: 0.08270 \n",
      "[350/500] train_loss: 0.04579 valid_loss: 0.07340 test_loss: 0.08171 \n",
      "[351/500] train_loss: 0.04539 valid_loss: 0.07270 test_loss: 0.08294 \n",
      "[352/500] train_loss: 0.04769 valid_loss: 0.07564 test_loss: 0.08453 \n",
      "[353/500] train_loss: 0.04696 valid_loss: 0.07268 test_loss: 0.08221 \n",
      "[354/500] train_loss: 0.04699 valid_loss: 0.07233 test_loss: 0.08404 \n",
      "[355/500] train_loss: 0.04587 valid_loss: 0.07378 test_loss: 0.08438 \n",
      "[356/500] train_loss: 0.04509 valid_loss: 0.07296 test_loss: 0.08408 \n",
      "[357/500] train_loss: 0.04577 valid_loss: 0.07502 test_loss: 0.08541 \n",
      "[358/500] train_loss: 0.04562 valid_loss: 0.07337 test_loss: 0.08292 \n",
      "[359/500] train_loss: 0.04763 valid_loss: 0.07586 test_loss: 0.08249 \n",
      "[360/500] train_loss: 0.04614 valid_loss: 0.07390 test_loss: 0.08294 \n",
      "[361/500] train_loss: 0.04504 valid_loss: 0.07403 test_loss: 0.08138 \n",
      "[362/500] train_loss: 0.04533 valid_loss: 0.07448 test_loss: 0.08288 \n",
      "[363/500] train_loss: 0.04582 valid_loss: 0.07302 test_loss: 0.08178 \n",
      "[364/500] train_loss: 0.04472 valid_loss: 0.07595 test_loss: 0.08190 \n",
      "[365/500] train_loss: 0.04605 valid_loss: 0.07286 test_loss: 0.08074 \n",
      "[366/500] train_loss: 0.04616 valid_loss: 0.07279 test_loss: 0.08309 \n",
      "[367/500] train_loss: 0.04520 valid_loss: 0.07342 test_loss: 0.08345 \n",
      "[368/500] train_loss: 0.04594 valid_loss: 0.07280 test_loss: 0.08383 \n",
      "[369/500] train_loss: 0.04541 valid_loss: 0.07278 test_loss: 0.08356 \n",
      "[370/500] train_loss: 0.04623 valid_loss: 0.07359 test_loss: 0.08319 \n",
      "[371/500] train_loss: 0.04590 valid_loss: 0.07343 test_loss: 0.08299 \n",
      "[372/500] train_loss: 0.04433 valid_loss: 0.07440 test_loss: 0.08441 \n",
      "[373/500] train_loss: 0.04483 valid_loss: 0.07163 test_loss: 0.08239 \n",
      "[374/500] train_loss: 0.04624 valid_loss: 0.07286 test_loss: 0.08210 \n",
      "[375/500] train_loss: 0.04488 valid_loss: 0.07357 test_loss: 0.08175 \n",
      "[376/500] train_loss: 0.04599 valid_loss: 0.07280 test_loss: 0.08400 \n",
      "[377/500] train_loss: 0.04470 valid_loss: 0.07305 test_loss: 0.08446 \n",
      "[378/500] train_loss: 0.04432 valid_loss: 0.07135 test_loss: 0.08251 \n",
      "[379/500] train_loss: 0.04535 valid_loss: 0.07283 test_loss: 0.08157 \n",
      "[380/500] train_loss: 0.04542 valid_loss: 0.07295 test_loss: 0.08254 \n",
      "[381/500] train_loss: 0.04551 valid_loss: 0.07274 test_loss: 0.08319 \n",
      "[382/500] train_loss: 0.04533 valid_loss: 0.07165 test_loss: 0.08279 \n",
      "[383/500] train_loss: 0.04545 valid_loss: 0.07307 test_loss: 0.08369 \n",
      "[384/500] train_loss: 0.04507 valid_loss: 0.07287 test_loss: 0.08168 \n",
      "[385/500] train_loss: 0.04435 valid_loss: 0.07277 test_loss: 0.08413 \n",
      "[386/500] train_loss: 0.04600 valid_loss: 0.07347 test_loss: 0.08399 \n",
      "[387/500] train_loss: 0.04441 valid_loss: 0.07360 test_loss: 0.08469 \n",
      "[388/500] train_loss: 0.04503 valid_loss: 0.07310 test_loss: 0.08495 \n",
      "[389/500] train_loss: 0.04460 valid_loss: 0.07147 test_loss: 0.08150 \n",
      "[390/500] train_loss: 0.04425 valid_loss: 0.07209 test_loss: 0.08375 \n",
      "[391/500] train_loss: 0.04457 valid_loss: 0.07122 test_loss: 0.08390 \n",
      "[392/500] train_loss: 0.04453 valid_loss: 0.07109 test_loss: 0.08277 \n",
      "[393/500] train_loss: 0.04387 valid_loss: 0.07303 test_loss: 0.08463 \n",
      "[394/500] train_loss: 0.04448 valid_loss: 0.07101 test_loss: 0.08458 \n",
      "[395/500] train_loss: 0.04476 valid_loss: 0.07282 test_loss: 0.08406 \n",
      "[396/500] train_loss: 0.04561 valid_loss: 0.07100 test_loss: 0.08274 \n",
      "[397/500] train_loss: 0.04439 valid_loss: 0.07318 test_loss: 0.08346 \n",
      "[398/500] train_loss: 0.04509 valid_loss: 0.07202 test_loss: 0.08408 \n",
      "[399/500] train_loss: 0.04430 valid_loss: 0.07417 test_loss: 0.08330 \n",
      "[400/500] train_loss: 0.04401 valid_loss: 0.07216 test_loss: 0.08249 \n",
      "[401/500] train_loss: 0.04390 valid_loss: 0.07362 test_loss: 0.08547 \n",
      "[402/500] train_loss: 0.04472 valid_loss: 0.07496 test_loss: 0.08441 \n",
      "[403/500] train_loss: 0.04582 valid_loss: 0.07338 test_loss: 0.08524 \n",
      "[404/500] train_loss: 0.04371 valid_loss: 0.07388 test_loss: 0.08308 \n",
      "[405/500] train_loss: 0.04481 valid_loss: 0.07114 test_loss: 0.08203 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[406/500] train_loss: 0.04380 valid_loss: 0.07356 test_loss: 0.08283 \n",
      "[407/500] train_loss: 0.04307 valid_loss: 0.07282 test_loss: 0.08267 \n",
      "[408/500] train_loss: 0.04490 valid_loss: 0.07406 test_loss: 0.08385 \n",
      "[409/500] train_loss: 0.04387 valid_loss: 0.07168 test_loss: 0.08164 \n",
      "[410/500] train_loss: 0.04396 valid_loss: 0.07370 test_loss: 0.08289 \n",
      "[411/500] train_loss: 0.04394 valid_loss: 0.07141 test_loss: 0.08131 \n",
      "[412/500] train_loss: 0.04346 valid_loss: 0.07246 test_loss: 0.08271 \n",
      "[413/500] train_loss: 0.04369 valid_loss: 0.07309 test_loss: 0.08238 \n",
      "[414/500] train_loss: 0.04239 valid_loss: 0.07294 test_loss: 0.08268 \n",
      "[415/500] train_loss: 0.04434 valid_loss: 0.07247 test_loss: 0.08237 \n",
      "[416/500] train_loss: 0.04425 valid_loss: 0.07312 test_loss: 0.08393 \n",
      "[417/500] train_loss: 0.04485 valid_loss: 0.07362 test_loss: 0.08238 \n",
      "[418/500] train_loss: 0.04537 valid_loss: 0.07346 test_loss: 0.08214 \n",
      "[419/500] train_loss: 0.04355 valid_loss: 0.07389 test_loss: 0.08423 \n",
      "[420/500] train_loss: 0.04309 valid_loss: 0.07355 test_loss: 0.08298 \n",
      "[421/500] train_loss: 0.04386 valid_loss: 0.07440 test_loss: 0.08481 \n",
      "[422/500] train_loss: 0.04355 valid_loss: 0.07365 test_loss: 0.08341 \n",
      "[423/500] train_loss: 0.04469 valid_loss: 0.07393 test_loss: 0.08463 \n",
      "[424/500] train_loss: 0.04211 valid_loss: 0.07467 test_loss: 0.08352 \n",
      "[425/500] train_loss: 0.04396 valid_loss: 0.07372 test_loss: 0.08214 \n",
      "[426/500] train_loss: 0.04384 valid_loss: 0.07352 test_loss: 0.08359 \n",
      "[427/500] train_loss: 0.04392 valid_loss: 0.07306 test_loss: 0.08457 \n",
      "[428/500] train_loss: 0.04354 valid_loss: 0.07417 test_loss: 0.08404 \n",
      "[429/500] train_loss: 0.04353 valid_loss: 0.07441 test_loss: 0.08288 \n",
      "[430/500] train_loss: 0.04376 valid_loss: 0.07320 test_loss: 0.08403 \n",
      "[431/500] train_loss: 0.04308 valid_loss: 0.07373 test_loss: 0.08443 \n",
      "[432/500] train_loss: 0.04358 valid_loss: 0.07415 test_loss: 0.08454 \n",
      "[433/500] train_loss: 0.04330 valid_loss: 0.07478 test_loss: 0.08391 \n",
      "[434/500] train_loss: 0.04417 valid_loss: 0.07326 test_loss: 0.08534 \n",
      "[435/500] train_loss: 0.04377 valid_loss: 0.07226 test_loss: 0.08262 \n",
      "[436/500] train_loss: 0.04303 valid_loss: 0.07443 test_loss: 0.08392 \n",
      "[437/500] train_loss: 0.04294 valid_loss: 0.07295 test_loss: 0.08452 \n",
      "[438/500] train_loss: 0.04294 valid_loss: 0.07493 test_loss: 0.08386 \n",
      "[439/500] train_loss: 0.04363 valid_loss: 0.07449 test_loss: 0.08596 \n",
      "[440/500] train_loss: 0.04368 valid_loss: 0.07294 test_loss: 0.08394 \n",
      "[441/500] train_loss: 0.04242 valid_loss: 0.07532 test_loss: 0.08284 \n",
      "[442/500] train_loss: 0.04224 valid_loss: 0.07415 test_loss: 0.08680 \n",
      "[443/500] train_loss: 0.04269 valid_loss: 0.07435 test_loss: 0.08546 \n",
      "[444/500] train_loss: 0.04310 valid_loss: 0.07632 test_loss: 0.08840 \n",
      "[445/500] train_loss: 0.04366 valid_loss: 0.07486 test_loss: 0.08514 \n",
      "[446/500] train_loss: 0.04362 valid_loss: 0.07296 test_loss: 0.08476 \n",
      "[447/500] train_loss: 0.04203 valid_loss: 0.07447 test_loss: 0.08402 \n",
      "[448/500] train_loss: 0.04223 valid_loss: 0.07396 test_loss: 0.08433 \n",
      "[449/500] train_loss: 0.04272 valid_loss: 0.07247 test_loss: 0.08418 \n",
      "[450/500] train_loss: 0.04246 valid_loss: 0.07452 test_loss: 0.08523 \n",
      "[451/500] train_loss: 0.04236 valid_loss: 0.07335 test_loss: 0.08215 \n",
      "[452/500] train_loss: 0.04286 valid_loss: 0.07390 test_loss: 0.08366 \n",
      "[453/500] train_loss: 0.04280 valid_loss: 0.07360 test_loss: 0.08462 \n",
      "[454/500] train_loss: 0.04346 valid_loss: 0.07380 test_loss: 0.08306 \n",
      "[455/500] train_loss: 0.04292 valid_loss: 0.07476 test_loss: 0.08474 \n",
      "[456/500] train_loss: 0.04321 valid_loss: 0.07412 test_loss: 0.08500 \n",
      "[457/500] train_loss: 0.04300 valid_loss: 0.07305 test_loss: 0.08488 \n",
      "[458/500] train_loss: 0.04225 valid_loss: 0.07449 test_loss: 0.08477 \n",
      "[459/500] train_loss: 0.04262 valid_loss: 0.07434 test_loss: 0.08531 \n",
      "[460/500] train_loss: 0.04231 valid_loss: 0.07393 test_loss: 0.08494 \n",
      "[461/500] train_loss: 0.04266 valid_loss: 0.07214 test_loss: 0.08382 \n",
      "[462/500] train_loss: 0.04343 valid_loss: 0.07319 test_loss: 0.08543 \n",
      "[463/500] train_loss: 0.04249 valid_loss: 0.07386 test_loss: 0.08581 \n",
      "[464/500] train_loss: 0.04257 valid_loss: 0.07310 test_loss: 0.08488 \n",
      "[465/500] train_loss: 0.04276 valid_loss: 0.07233 test_loss: 0.08488 \n",
      "[466/500] train_loss: 0.04310 valid_loss: 0.07365 test_loss: 0.08518 \n",
      "[467/500] train_loss: 0.04335 valid_loss: 0.07171 test_loss: 0.08405 \n",
      "[468/500] train_loss: 0.04112 valid_loss: 0.07412 test_loss: 0.08625 \n",
      "[469/500] train_loss: 0.04122 valid_loss: 0.07385 test_loss: 0.08366 \n",
      "[470/500] train_loss: 0.04153 valid_loss: 0.07253 test_loss: 0.08430 \n",
      "[471/500] train_loss: 0.04222 valid_loss: 0.07353 test_loss: 0.08545 \n",
      "[472/500] train_loss: 0.04228 valid_loss: 0.07291 test_loss: 0.08388 \n",
      "[473/500] train_loss: 0.04313 valid_loss: 0.07324 test_loss: 0.08291 \n",
      "[474/500] train_loss: 0.04208 valid_loss: 0.07457 test_loss: 0.08331 \n",
      "[475/500] train_loss: 0.04147 valid_loss: 0.07499 test_loss: 0.08639 \n",
      "[476/500] train_loss: 0.04264 valid_loss: 0.07250 test_loss: 0.08402 \n",
      "[477/500] train_loss: 0.04185 valid_loss: 0.07360 test_loss: 0.08523 \n",
      "[478/500] train_loss: 0.04092 valid_loss: 0.07325 test_loss: 0.08344 \n",
      "[479/500] train_loss: 0.04326 valid_loss: 0.07348 test_loss: 0.08473 \n",
      "[480/500] train_loss: 0.04219 valid_loss: 0.07299 test_loss: 0.08613 \n",
      "[481/500] train_loss: 0.04158 valid_loss: 0.07209 test_loss: 0.08484 \n",
      "[482/500] train_loss: 0.04262 valid_loss: 0.07172 test_loss: 0.08408 \n",
      "[483/500] train_loss: 0.04215 valid_loss: 0.07206 test_loss: 0.08430 \n",
      "[484/500] train_loss: 0.04245 valid_loss: 0.07247 test_loss: 0.08382 \n",
      "[485/500] train_loss: 0.04213 valid_loss: 0.07490 test_loss: 0.08537 \n",
      "[486/500] train_loss: 0.04324 valid_loss: 0.07296 test_loss: 0.08440 \n",
      "[487/500] train_loss: 0.04231 valid_loss: 0.07299 test_loss: 0.08409 \n",
      "[488/500] train_loss: 0.04190 valid_loss: 0.07308 test_loss: 0.08491 \n",
      "[489/500] train_loss: 0.04051 valid_loss: 0.07465 test_loss: 0.08412 \n",
      "[490/500] train_loss: 0.04220 valid_loss: 0.07399 test_loss: 0.08334 \n",
      "[491/500] train_loss: 0.04221 valid_loss: 0.07308 test_loss: 0.08484 \n",
      "[492/500] train_loss: 0.04113 valid_loss: 0.07301 test_loss: 0.08559 \n",
      "[493/500] train_loss: 0.04145 valid_loss: 0.07399 test_loss: 0.08524 \n",
      "[494/500] train_loss: 0.04226 valid_loss: 0.07309 test_loss: 0.08496 \n",
      "[495/500] train_loss: 0.04076 valid_loss: 0.07492 test_loss: 0.08559 \n",
      "[496/500] train_loss: 0.04207 valid_loss: 0.07370 test_loss: 0.08433 \n",
      "[497/500] train_loss: 0.04235 valid_loss: 0.07411 test_loss: 0.08552 \n",
      "[498/500] train_loss: 0.04104 valid_loss: 0.07402 test_loss: 0.08492 \n",
      "[499/500] train_loss: 0.04148 valid_loss: 0.07465 test_loss: 0.08449 \n",
      "[500/500] train_loss: 0.04103 valid_loss: 0.07313 test_loss: 0.08377 \n",
      "TRAINING MODEL 19\n",
      "[  1/500] train_loss: 0.34341 valid_loss: 0.24364 test_loss: 0.24946 \n",
      "验证损失减少 (inf --> 0.243636). 正在保存模型...\n",
      "[  2/500] train_loss: 0.18603 valid_loss: 0.17715 test_loss: 0.18205 \n",
      "验证损失减少 (0.243636 --> 0.177148). 正在保存模型...\n",
      "[  3/500] train_loss: 0.15012 valid_loss: 0.14800 test_loss: 0.15621 \n",
      "验证损失减少 (0.177148 --> 0.148001). 正在保存模型...\n",
      "[  4/500] train_loss: 0.13677 valid_loss: 0.13743 test_loss: 0.14599 \n",
      "验证损失减少 (0.148001 --> 0.137426). 正在保存模型...\n",
      "[  5/500] train_loss: 0.12555 valid_loss: 0.13000 test_loss: 0.13828 \n",
      "验证损失减少 (0.137426 --> 0.130002). 正在保存模型...\n",
      "[  6/500] train_loss: 0.12146 valid_loss: 0.12361 test_loss: 0.13271 \n",
      "验证损失减少 (0.130002 --> 0.123608). 正在保存模型...\n",
      "[  7/500] train_loss: 0.11746 valid_loss: 0.11987 test_loss: 0.12780 \n",
      "验证损失减少 (0.123608 --> 0.119867). 正在保存模型...\n",
      "[  8/500] train_loss: 0.11582 valid_loss: 0.12133 test_loss: 0.12923 \n",
      "[  9/500] train_loss: 0.11251 valid_loss: 0.11586 test_loss: 0.12459 \n",
      "验证损失减少 (0.119867 --> 0.115864). 正在保存模型...\n",
      "[ 10/500] train_loss: 0.10831 valid_loss: 0.11748 test_loss: 0.12598 \n",
      "[ 11/500] train_loss: 0.11072 valid_loss: 0.11123 test_loss: 0.12008 \n",
      "验证损失减少 (0.115864 --> 0.111234). 正在保存模型...\n",
      "[ 12/500] train_loss: 0.10701 valid_loss: 0.10944 test_loss: 0.12031 \n",
      "验证损失减少 (0.111234 --> 0.109439). 正在保存模型...\n",
      "[ 13/500] train_loss: 0.10590 valid_loss: 0.10825 test_loss: 0.11841 \n",
      "验证损失减少 (0.109439 --> 0.108251). 正在保存模型...\n",
      "[ 14/500] train_loss: 0.10269 valid_loss: 0.10714 test_loss: 0.11817 \n",
      "验证损失减少 (0.108251 --> 0.107138). 正在保存模型...\n",
      "[ 15/500] train_loss: 0.10087 valid_loss: 0.10394 test_loss: 0.11444 \n",
      "验证损失减少 (0.107138 --> 0.103942). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16/500] train_loss: 0.10069 valid_loss: 0.10033 test_loss: 0.11203 \n",
      "验证损失减少 (0.103942 --> 0.100327). 正在保存模型...\n",
      "[ 17/500] train_loss: 0.09854 valid_loss: 0.10345 test_loss: 0.11400 \n",
      "[ 18/500] train_loss: 0.09650 valid_loss: 0.09751 test_loss: 0.10894 \n",
      "验证损失减少 (0.100327 --> 0.097512). 正在保存模型...\n",
      "[ 19/500] train_loss: 0.09482 valid_loss: 0.09844 test_loss: 0.10958 \n",
      "[ 20/500] train_loss: 0.09453 valid_loss: 0.09663 test_loss: 0.10897 \n",
      "验证损失减少 (0.097512 --> 0.096634). 正在保存模型...\n",
      "[ 21/500] train_loss: 0.09663 valid_loss: 0.09528 test_loss: 0.10692 \n",
      "验证损失减少 (0.096634 --> 0.095283). 正在保存模型...\n",
      "[ 22/500] train_loss: 0.09216 valid_loss: 0.09694 test_loss: 0.10721 \n",
      "[ 23/500] train_loss: 0.09152 valid_loss: 0.09383 test_loss: 0.10553 \n",
      "验证损失减少 (0.095283 --> 0.093835). 正在保存模型...\n",
      "[ 24/500] train_loss: 0.09262 valid_loss: 0.09336 test_loss: 0.10319 \n",
      "验证损失减少 (0.093835 --> 0.093363). 正在保存模型...\n",
      "[ 25/500] train_loss: 0.09284 valid_loss: 0.09608 test_loss: 0.10588 \n",
      "[ 26/500] train_loss: 0.08772 valid_loss: 0.09573 test_loss: 0.10481 \n",
      "[ 27/500] train_loss: 0.08595 valid_loss: 0.09205 test_loss: 0.10162 \n",
      "验证损失减少 (0.093363 --> 0.092051). 正在保存模型...\n",
      "[ 28/500] train_loss: 0.08754 valid_loss: 0.09140 test_loss: 0.10330 \n",
      "验证损失减少 (0.092051 --> 0.091399). 正在保存模型...\n",
      "[ 29/500] train_loss: 0.08773 valid_loss: 0.09126 test_loss: 0.10095 \n",
      "验证损失减少 (0.091399 --> 0.091264). 正在保存模型...\n",
      "[ 30/500] train_loss: 0.08910 valid_loss: 0.09390 test_loss: 0.10278 \n",
      "[ 31/500] train_loss: 0.08369 valid_loss: 0.08916 test_loss: 0.09861 \n",
      "验证损失减少 (0.091264 --> 0.089162). 正在保存模型...\n",
      "[ 32/500] train_loss: 0.08394 valid_loss: 0.09185 test_loss: 0.09930 \n",
      "[ 33/500] train_loss: 0.08837 valid_loss: 0.09017 test_loss: 0.09732 \n",
      "[ 34/500] train_loss: 0.08358 valid_loss: 0.08702 test_loss: 0.09683 \n",
      "验证损失减少 (0.089162 --> 0.087023). 正在保存模型...\n",
      "[ 35/500] train_loss: 0.08384 valid_loss: 0.08836 test_loss: 0.09738 \n",
      "[ 36/500] train_loss: 0.08450 valid_loss: 0.08773 test_loss: 0.09663 \n",
      "[ 37/500] train_loss: 0.08522 valid_loss: 0.08530 test_loss: 0.09478 \n",
      "验证损失减少 (0.087023 --> 0.085302). 正在保存模型...\n",
      "[ 38/500] train_loss: 0.08461 valid_loss: 0.08741 test_loss: 0.09699 \n",
      "[ 39/500] train_loss: 0.08243 valid_loss: 0.08622 test_loss: 0.09589 \n",
      "[ 40/500] train_loss: 0.08177 valid_loss: 0.08589 test_loss: 0.09555 \n",
      "[ 41/500] train_loss: 0.08229 valid_loss: 0.08576 test_loss: 0.09406 \n",
      "[ 42/500] train_loss: 0.08180 valid_loss: 0.08374 test_loss: 0.09284 \n",
      "验证损失减少 (0.085302 --> 0.083739). 正在保存模型...\n",
      "[ 43/500] train_loss: 0.07997 valid_loss: 0.08415 test_loss: 0.09280 \n",
      "[ 44/500] train_loss: 0.08017 valid_loss: 0.08480 test_loss: 0.09607 \n",
      "[ 45/500] train_loss: 0.07928 valid_loss: 0.08164 test_loss: 0.09186 \n",
      "验证损失减少 (0.083739 --> 0.081639). 正在保存模型...\n",
      "[ 46/500] train_loss: 0.07843 valid_loss: 0.08416 test_loss: 0.09350 \n",
      "[ 47/500] train_loss: 0.07886 valid_loss: 0.08474 test_loss: 0.09455 \n",
      "[ 48/500] train_loss: 0.07923 valid_loss: 0.08237 test_loss: 0.09203 \n",
      "[ 49/500] train_loss: 0.07796 valid_loss: 0.08490 test_loss: 0.09192 \n",
      "[ 50/500] train_loss: 0.07792 valid_loss: 0.08316 test_loss: 0.09216 \n",
      "[ 51/500] train_loss: 0.07671 valid_loss: 0.08445 test_loss: 0.09062 \n",
      "[ 52/500] train_loss: 0.07763 valid_loss: 0.08081 test_loss: 0.09037 \n",
      "验证损失减少 (0.081639 --> 0.080809). 正在保存模型...\n",
      "[ 53/500] train_loss: 0.07569 valid_loss: 0.08191 test_loss: 0.09058 \n",
      "[ 54/500] train_loss: 0.07677 valid_loss: 0.08222 test_loss: 0.09256 \n",
      "[ 55/500] train_loss: 0.07679 valid_loss: 0.08101 test_loss: 0.09162 \n",
      "[ 56/500] train_loss: 0.07445 valid_loss: 0.08238 test_loss: 0.09130 \n",
      "[ 57/500] train_loss: 0.07621 valid_loss: 0.08291 test_loss: 0.09149 \n",
      "[ 58/500] train_loss: 0.07425 valid_loss: 0.08112 test_loss: 0.08989 \n",
      "[ 59/500] train_loss: 0.07624 valid_loss: 0.08133 test_loss: 0.08931 \n",
      "[ 60/500] train_loss: 0.07532 valid_loss: 0.08121 test_loss: 0.09122 \n",
      "[ 61/500] train_loss: 0.07352 valid_loss: 0.08102 test_loss: 0.09015 \n",
      "[ 62/500] train_loss: 0.07354 valid_loss: 0.07971 test_loss: 0.08968 \n",
      "验证损失减少 (0.080809 --> 0.079708). 正在保存模型...\n",
      "[ 63/500] train_loss: 0.07351 valid_loss: 0.08072 test_loss: 0.08925 \n",
      "[ 64/500] train_loss: 0.07427 valid_loss: 0.08109 test_loss: 0.08898 \n",
      "[ 65/500] train_loss: 0.07359 valid_loss: 0.08065 test_loss: 0.08973 \n",
      "[ 66/500] train_loss: 0.07360 valid_loss: 0.07892 test_loss: 0.08870 \n",
      "验证损失减少 (0.079708 --> 0.078917). 正在保存模型...\n",
      "[ 67/500] train_loss: 0.07233 valid_loss: 0.08281 test_loss: 0.08891 \n",
      "[ 68/500] train_loss: 0.07223 valid_loss: 0.08279 test_loss: 0.08877 \n",
      "[ 69/500] train_loss: 0.07254 valid_loss: 0.08096 test_loss: 0.08889 \n",
      "[ 70/500] train_loss: 0.07221 valid_loss: 0.07806 test_loss: 0.08646 \n",
      "验证损失减少 (0.078917 --> 0.078061). 正在保存模型...\n",
      "[ 71/500] train_loss: 0.07173 valid_loss: 0.07985 test_loss: 0.08810 \n",
      "[ 72/500] train_loss: 0.07308 valid_loss: 0.07916 test_loss: 0.08701 \n",
      "[ 73/500] train_loss: 0.07236 valid_loss: 0.07750 test_loss: 0.08815 \n",
      "验证损失减少 (0.078061 --> 0.077496). 正在保存模型...\n",
      "[ 74/500] train_loss: 0.07169 valid_loss: 0.08045 test_loss: 0.08770 \n",
      "[ 75/500] train_loss: 0.07026 valid_loss: 0.08145 test_loss: 0.08975 \n",
      "[ 76/500] train_loss: 0.07310 valid_loss: 0.07961 test_loss: 0.08696 \n",
      "[ 77/500] train_loss: 0.06968 valid_loss: 0.07752 test_loss: 0.08535 \n",
      "[ 78/500] train_loss: 0.07029 valid_loss: 0.07970 test_loss: 0.08676 \n",
      "[ 79/500] train_loss: 0.07009 valid_loss: 0.07787 test_loss: 0.08618 \n",
      "[ 80/500] train_loss: 0.06953 valid_loss: 0.07943 test_loss: 0.08659 \n",
      "[ 81/500] train_loss: 0.07011 valid_loss: 0.07789 test_loss: 0.08691 \n",
      "[ 82/500] train_loss: 0.06984 valid_loss: 0.07945 test_loss: 0.08615 \n",
      "[ 83/500] train_loss: 0.07045 valid_loss: 0.07857 test_loss: 0.08689 \n",
      "[ 84/500] train_loss: 0.06810 valid_loss: 0.07751 test_loss: 0.08603 \n",
      "[ 85/500] train_loss: 0.06911 valid_loss: 0.07666 test_loss: 0.08584 \n",
      "验证损失减少 (0.077496 --> 0.076665). 正在保存模型...\n",
      "[ 86/500] train_loss: 0.06775 valid_loss: 0.07722 test_loss: 0.08545 \n",
      "[ 87/500] train_loss: 0.06693 valid_loss: 0.07775 test_loss: 0.08509 \n",
      "[ 88/500] train_loss: 0.06958 valid_loss: 0.07676 test_loss: 0.08613 \n",
      "[ 89/500] train_loss: 0.06886 valid_loss: 0.07894 test_loss: 0.08883 \n",
      "[ 90/500] train_loss: 0.06900 valid_loss: 0.07732 test_loss: 0.08523 \n",
      "[ 91/500] train_loss: 0.06832 valid_loss: 0.07758 test_loss: 0.08493 \n",
      "[ 92/500] train_loss: 0.07000 valid_loss: 0.07976 test_loss: 0.08504 \n",
      "[ 93/500] train_loss: 0.06971 valid_loss: 0.07596 test_loss: 0.08489 \n",
      "验证损失减少 (0.076665 --> 0.075959). 正在保存模型...\n",
      "[ 94/500] train_loss: 0.06964 valid_loss: 0.07679 test_loss: 0.08493 \n",
      "[ 95/500] train_loss: 0.06715 valid_loss: 0.07701 test_loss: 0.08419 \n",
      "[ 96/500] train_loss: 0.06647 valid_loss: 0.07922 test_loss: 0.08559 \n",
      "[ 97/500] train_loss: 0.06590 valid_loss: 0.07498 test_loss: 0.08435 \n",
      "验证损失减少 (0.075959 --> 0.074979). 正在保存模型...\n",
      "[ 98/500] train_loss: 0.06885 valid_loss: 0.07692 test_loss: 0.08690 \n",
      "[ 99/500] train_loss: 0.06675 valid_loss: 0.07741 test_loss: 0.08511 \n",
      "[100/500] train_loss: 0.06773 valid_loss: 0.07775 test_loss: 0.08408 \n",
      "[101/500] train_loss: 0.06891 valid_loss: 0.07652 test_loss: 0.08391 \n",
      "[102/500] train_loss: 0.06579 valid_loss: 0.07522 test_loss: 0.08358 \n",
      "[103/500] train_loss: 0.06598 valid_loss: 0.07767 test_loss: 0.08397 \n",
      "[104/500] train_loss: 0.06426 valid_loss: 0.07802 test_loss: 0.08582 \n",
      "[105/500] train_loss: 0.06510 valid_loss: 0.07865 test_loss: 0.08324 \n",
      "[106/500] train_loss: 0.06551 valid_loss: 0.07599 test_loss: 0.08401 \n",
      "[107/500] train_loss: 0.06519 valid_loss: 0.07583 test_loss: 0.08418 \n",
      "[108/500] train_loss: 0.06575 valid_loss: 0.07693 test_loss: 0.08303 \n",
      "[109/500] train_loss: 0.06537 valid_loss: 0.07554 test_loss: 0.08302 \n",
      "[110/500] train_loss: 0.06660 valid_loss: 0.07651 test_loss: 0.08509 \n",
      "[111/500] train_loss: 0.06697 valid_loss: 0.07557 test_loss: 0.08323 \n",
      "[112/500] train_loss: 0.06601 valid_loss: 0.07780 test_loss: 0.08518 \n",
      "[113/500] train_loss: 0.06453 valid_loss: 0.07722 test_loss: 0.08509 \n",
      "[114/500] train_loss: 0.06357 valid_loss: 0.07535 test_loss: 0.08217 \n",
      "[115/500] train_loss: 0.06417 valid_loss: 0.07518 test_loss: 0.08257 \n",
      "[116/500] train_loss: 0.06597 valid_loss: 0.07609 test_loss: 0.08396 \n",
      "[117/500] train_loss: 0.06449 valid_loss: 0.07538 test_loss: 0.08246 \n",
      "[118/500] train_loss: 0.06168 valid_loss: 0.07395 test_loss: 0.08130 \n",
      "验证损失减少 (0.074979 --> 0.073952). 正在保存模型...\n",
      "[119/500] train_loss: 0.06390 valid_loss: 0.07594 test_loss: 0.08346 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/500] train_loss: 0.06404 valid_loss: 0.07494 test_loss: 0.08301 \n",
      "[121/500] train_loss: 0.06121 valid_loss: 0.07658 test_loss: 0.08256 \n",
      "[122/500] train_loss: 0.06317 valid_loss: 0.07725 test_loss: 0.08357 \n",
      "[123/500] train_loss: 0.06289 valid_loss: 0.07762 test_loss: 0.08584 \n",
      "[124/500] train_loss: 0.06398 valid_loss: 0.07691 test_loss: 0.08365 \n",
      "[125/500] train_loss: 0.06191 valid_loss: 0.07465 test_loss: 0.08344 \n",
      "[126/500] train_loss: 0.06241 valid_loss: 0.07462 test_loss: 0.08334 \n",
      "[127/500] train_loss: 0.06288 valid_loss: 0.07472 test_loss: 0.08279 \n",
      "[128/500] train_loss: 0.06163 valid_loss: 0.07525 test_loss: 0.08349 \n",
      "[129/500] train_loss: 0.06302 valid_loss: 0.07346 test_loss: 0.08168 \n",
      "验证损失减少 (0.073952 --> 0.073460). 正在保存模型...\n",
      "[130/500] train_loss: 0.06419 valid_loss: 0.07539 test_loss: 0.08263 \n",
      "[131/500] train_loss: 0.06193 valid_loss: 0.07632 test_loss: 0.08251 \n",
      "[132/500] train_loss: 0.06262 valid_loss: 0.07525 test_loss: 0.08375 \n",
      "[133/500] train_loss: 0.06086 valid_loss: 0.07456 test_loss: 0.08382 \n",
      "[134/500] train_loss: 0.06380 valid_loss: 0.07397 test_loss: 0.08306 \n",
      "[135/500] train_loss: 0.06056 valid_loss: 0.07371 test_loss: 0.08206 \n",
      "[136/500] train_loss: 0.06186 valid_loss: 0.07368 test_loss: 0.08238 \n",
      "[137/500] train_loss: 0.06129 valid_loss: 0.07614 test_loss: 0.08311 \n",
      "[138/500] train_loss: 0.06164 valid_loss: 0.07575 test_loss: 0.08201 \n",
      "[139/500] train_loss: 0.06080 valid_loss: 0.07361 test_loss: 0.08069 \n",
      "[140/500] train_loss: 0.06119 valid_loss: 0.07550 test_loss: 0.08247 \n",
      "[141/500] train_loss: 0.06075 valid_loss: 0.07282 test_loss: 0.08275 \n",
      "验证损失减少 (0.073460 --> 0.072819). 正在保存模型...\n",
      "[142/500] train_loss: 0.06157 valid_loss: 0.07215 test_loss: 0.08243 \n",
      "验证损失减少 (0.072819 --> 0.072153). 正在保存模型...\n",
      "[143/500] train_loss: 0.06083 valid_loss: 0.07437 test_loss: 0.08250 \n",
      "[144/500] train_loss: 0.06148 valid_loss: 0.07206 test_loss: 0.08249 \n",
      "验证损失减少 (0.072153 --> 0.072058). 正在保存模型...\n",
      "[145/500] train_loss: 0.06205 valid_loss: 0.07423 test_loss: 0.08279 \n",
      "[146/500] train_loss: 0.05980 valid_loss: 0.07384 test_loss: 0.08379 \n",
      "[147/500] train_loss: 0.06011 valid_loss: 0.07443 test_loss: 0.08387 \n",
      "[148/500] train_loss: 0.06136 valid_loss: 0.07479 test_loss: 0.08279 \n",
      "[149/500] train_loss: 0.06121 valid_loss: 0.07380 test_loss: 0.08205 \n",
      "[150/500] train_loss: 0.06011 valid_loss: 0.07439 test_loss: 0.08229 \n",
      "[151/500] train_loss: 0.06052 valid_loss: 0.07240 test_loss: 0.08235 \n",
      "[152/500] train_loss: 0.06061 valid_loss: 0.07499 test_loss: 0.08272 \n",
      "[153/500] train_loss: 0.06037 valid_loss: 0.07511 test_loss: 0.08107 \n",
      "[154/500] train_loss: 0.05968 valid_loss: 0.07386 test_loss: 0.08106 \n",
      "[155/500] train_loss: 0.05669 valid_loss: 0.07550 test_loss: 0.08305 \n",
      "[156/500] train_loss: 0.06037 valid_loss: 0.07299 test_loss: 0.08252 \n",
      "[157/500] train_loss: 0.05925 valid_loss: 0.07561 test_loss: 0.08397 \n",
      "[158/500] train_loss: 0.05837 valid_loss: 0.07364 test_loss: 0.08226 \n",
      "[159/500] train_loss: 0.05935 valid_loss: 0.07644 test_loss: 0.08306 \n",
      "[160/500] train_loss: 0.05898 valid_loss: 0.07267 test_loss: 0.08112 \n",
      "[161/500] train_loss: 0.05936 valid_loss: 0.07278 test_loss: 0.08182 \n",
      "[162/500] train_loss: 0.05927 valid_loss: 0.07336 test_loss: 0.08068 \n",
      "[163/500] train_loss: 0.05869 valid_loss: 0.07754 test_loss: 0.08328 \n",
      "[164/500] train_loss: 0.05907 valid_loss: 0.07539 test_loss: 0.08218 \n",
      "[165/500] train_loss: 0.05856 valid_loss: 0.07633 test_loss: 0.08121 \n",
      "[166/500] train_loss: 0.05819 valid_loss: 0.07289 test_loss: 0.08134 \n",
      "[167/500] train_loss: 0.05739 valid_loss: 0.07376 test_loss: 0.08280 \n",
      "[168/500] train_loss: 0.05963 valid_loss: 0.07364 test_loss: 0.08204 \n",
      "[169/500] train_loss: 0.05898 valid_loss: 0.07238 test_loss: 0.08151 \n",
      "[170/500] train_loss: 0.05880 valid_loss: 0.07483 test_loss: 0.08275 \n",
      "[171/500] train_loss: 0.05834 valid_loss: 0.07421 test_loss: 0.08255 \n",
      "[172/500] train_loss: 0.05729 valid_loss: 0.07241 test_loss: 0.08086 \n",
      "[173/500] train_loss: 0.05726 valid_loss: 0.07569 test_loss: 0.08178 \n",
      "[174/500] train_loss: 0.05764 valid_loss: 0.07307 test_loss: 0.08239 \n",
      "[175/500] train_loss: 0.05885 valid_loss: 0.07372 test_loss: 0.08223 \n",
      "[176/500] train_loss: 0.05746 valid_loss: 0.07319 test_loss: 0.08123 \n",
      "[177/500] train_loss: 0.05677 valid_loss: 0.07383 test_loss: 0.08049 \n",
      "[178/500] train_loss: 0.05783 valid_loss: 0.07189 test_loss: 0.07988 \n",
      "验证损失减少 (0.072058 --> 0.071895). 正在保存模型...\n",
      "[179/500] train_loss: 0.05571 valid_loss: 0.07364 test_loss: 0.08139 \n",
      "[180/500] train_loss: 0.05664 valid_loss: 0.07210 test_loss: 0.08249 \n",
      "[181/500] train_loss: 0.05637 valid_loss: 0.07159 test_loss: 0.07968 \n",
      "验证损失减少 (0.071895 --> 0.071592). 正在保存模型...\n",
      "[182/500] train_loss: 0.05685 valid_loss: 0.07330 test_loss: 0.08270 \n",
      "[183/500] train_loss: 0.05667 valid_loss: 0.07566 test_loss: 0.08303 \n",
      "[184/500] train_loss: 0.05713 valid_loss: 0.07266 test_loss: 0.08292 \n",
      "[185/500] train_loss: 0.05589 valid_loss: 0.07339 test_loss: 0.08317 \n",
      "[186/500] train_loss: 0.05754 valid_loss: 0.07297 test_loss: 0.08165 \n",
      "[187/500] train_loss: 0.05613 valid_loss: 0.07523 test_loss: 0.08150 \n",
      "[188/500] train_loss: 0.05691 valid_loss: 0.07449 test_loss: 0.08091 \n",
      "[189/500] train_loss: 0.05696 valid_loss: 0.07371 test_loss: 0.08083 \n",
      "[190/500] train_loss: 0.05645 valid_loss: 0.07295 test_loss: 0.08132 \n",
      "[191/500] train_loss: 0.05646 valid_loss: 0.07273 test_loss: 0.08052 \n",
      "[192/500] train_loss: 0.05652 valid_loss: 0.07288 test_loss: 0.08056 \n",
      "[193/500] train_loss: 0.05516 valid_loss: 0.07277 test_loss: 0.08197 \n",
      "[194/500] train_loss: 0.05723 valid_loss: 0.07184 test_loss: 0.08067 \n",
      "[195/500] train_loss: 0.05533 valid_loss: 0.07433 test_loss: 0.08199 \n",
      "[196/500] train_loss: 0.05479 valid_loss: 0.07232 test_loss: 0.08161 \n",
      "[197/500] train_loss: 0.05540 valid_loss: 0.07284 test_loss: 0.08207 \n",
      "[198/500] train_loss: 0.05325 valid_loss: 0.07146 test_loss: 0.08062 \n",
      "验证损失减少 (0.071592 --> 0.071456). 正在保存模型...\n",
      "[199/500] train_loss: 0.05513 valid_loss: 0.07411 test_loss: 0.08109 \n",
      "[200/500] train_loss: 0.05600 valid_loss: 0.07322 test_loss: 0.08116 \n",
      "[201/500] train_loss: 0.05402 valid_loss: 0.07440 test_loss: 0.08507 \n",
      "[202/500] train_loss: 0.05525 valid_loss: 0.07360 test_loss: 0.08208 \n",
      "[203/500] train_loss: 0.05544 valid_loss: 0.07348 test_loss: 0.08123 \n",
      "[204/500] train_loss: 0.05587 valid_loss: 0.07286 test_loss: 0.08202 \n",
      "[205/500] train_loss: 0.05522 valid_loss: 0.07177 test_loss: 0.08114 \n",
      "[206/500] train_loss: 0.05569 valid_loss: 0.07132 test_loss: 0.08035 \n",
      "验证损失减少 (0.071456 --> 0.071317). 正在保存模型...\n",
      "[207/500] train_loss: 0.05563 valid_loss: 0.07522 test_loss: 0.08330 \n",
      "[208/500] train_loss: 0.05498 valid_loss: 0.07341 test_loss: 0.08332 \n",
      "[209/500] train_loss: 0.05445 valid_loss: 0.07266 test_loss: 0.08245 \n",
      "[210/500] train_loss: 0.05482 valid_loss: 0.07286 test_loss: 0.08117 \n",
      "[211/500] train_loss: 0.05398 valid_loss: 0.07413 test_loss: 0.08382 \n",
      "[212/500] train_loss: 0.05425 valid_loss: 0.07310 test_loss: 0.08275 \n",
      "[213/500] train_loss: 0.05400 valid_loss: 0.07152 test_loss: 0.08103 \n",
      "[214/500] train_loss: 0.05411 valid_loss: 0.07358 test_loss: 0.08172 \n",
      "[215/500] train_loss: 0.05385 valid_loss: 0.07222 test_loss: 0.08150 \n",
      "[216/500] train_loss: 0.05504 valid_loss: 0.07239 test_loss: 0.08270 \n",
      "[217/500] train_loss: 0.05376 valid_loss: 0.07231 test_loss: 0.08202 \n",
      "[218/500] train_loss: 0.05376 valid_loss: 0.07315 test_loss: 0.08215 \n",
      "[219/500] train_loss: 0.05330 valid_loss: 0.07196 test_loss: 0.08206 \n",
      "[220/500] train_loss: 0.05345 valid_loss: 0.07192 test_loss: 0.08136 \n",
      "[221/500] train_loss: 0.05466 valid_loss: 0.07254 test_loss: 0.08095 \n",
      "[222/500] train_loss: 0.05381 valid_loss: 0.07319 test_loss: 0.08200 \n",
      "[223/500] train_loss: 0.05452 valid_loss: 0.07491 test_loss: 0.08212 \n",
      "[224/500] train_loss: 0.05274 valid_loss: 0.07360 test_loss: 0.08213 \n",
      "[225/500] train_loss: 0.05367 valid_loss: 0.07374 test_loss: 0.08185 \n",
      "[226/500] train_loss: 0.05285 valid_loss: 0.07228 test_loss: 0.08079 \n",
      "[227/500] train_loss: 0.05154 valid_loss: 0.07294 test_loss: 0.08240 \n",
      "[228/500] train_loss: 0.05354 valid_loss: 0.07222 test_loss: 0.08240 \n",
      "[229/500] train_loss: 0.05303 valid_loss: 0.07189 test_loss: 0.08234 \n",
      "[230/500] train_loss: 0.05166 valid_loss: 0.07406 test_loss: 0.08194 \n",
      "[231/500] train_loss: 0.05354 valid_loss: 0.07185 test_loss: 0.08102 \n",
      "[232/500] train_loss: 0.05347 valid_loss: 0.07288 test_loss: 0.08234 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[233/500] train_loss: 0.05375 valid_loss: 0.07235 test_loss: 0.08222 \n",
      "[234/500] train_loss: 0.05258 valid_loss: 0.07272 test_loss: 0.08229 \n",
      "[235/500] train_loss: 0.05409 valid_loss: 0.07302 test_loss: 0.08231 \n",
      "[236/500] train_loss: 0.05361 valid_loss: 0.07206 test_loss: 0.08074 \n",
      "[237/500] train_loss: 0.05326 valid_loss: 0.07395 test_loss: 0.08248 \n",
      "[238/500] train_loss: 0.05319 valid_loss: 0.07292 test_loss: 0.08050 \n",
      "[239/500] train_loss: 0.05204 valid_loss: 0.07433 test_loss: 0.08475 \n",
      "[240/500] train_loss: 0.05494 valid_loss: 0.07631 test_loss: 0.08226 \n",
      "[241/500] train_loss: 0.05282 valid_loss: 0.07376 test_loss: 0.08112 \n",
      "[242/500] train_loss: 0.05268 valid_loss: 0.07303 test_loss: 0.08125 \n",
      "[243/500] train_loss: 0.05304 valid_loss: 0.07288 test_loss: 0.08241 \n",
      "[244/500] train_loss: 0.05216 valid_loss: 0.07307 test_loss: 0.08132 \n",
      "[245/500] train_loss: 0.05202 valid_loss: 0.07317 test_loss: 0.08140 \n",
      "[246/500] train_loss: 0.05259 valid_loss: 0.07348 test_loss: 0.08276 \n",
      "[247/500] train_loss: 0.05267 valid_loss: 0.07310 test_loss: 0.08319 \n",
      "[248/500] train_loss: 0.05017 valid_loss: 0.07202 test_loss: 0.08186 \n",
      "[249/500] train_loss: 0.05237 valid_loss: 0.07267 test_loss: 0.08257 \n",
      "[250/500] train_loss: 0.05189 valid_loss: 0.07325 test_loss: 0.08222 \n",
      "[251/500] train_loss: 0.05215 valid_loss: 0.07380 test_loss: 0.08319 \n",
      "[252/500] train_loss: 0.05088 valid_loss: 0.07347 test_loss: 0.08610 \n",
      "[253/500] train_loss: 0.05225 valid_loss: 0.07359 test_loss: 0.08263 \n",
      "[254/500] train_loss: 0.05119 valid_loss: 0.07204 test_loss: 0.08260 \n",
      "[255/500] train_loss: 0.05170 valid_loss: 0.07203 test_loss: 0.08208 \n",
      "[256/500] train_loss: 0.05186 valid_loss: 0.07196 test_loss: 0.08167 \n",
      "[257/500] train_loss: 0.05215 valid_loss: 0.07421 test_loss: 0.08250 \n",
      "[258/500] train_loss: 0.05134 valid_loss: 0.07127 test_loss: 0.08273 \n",
      "验证损失减少 (0.071317 --> 0.071268). 正在保存模型...\n",
      "[259/500] train_loss: 0.04959 valid_loss: 0.07189 test_loss: 0.08373 \n",
      "[260/500] train_loss: 0.05079 valid_loss: 0.07340 test_loss: 0.08313 \n",
      "[261/500] train_loss: 0.05157 valid_loss: 0.07293 test_loss: 0.08188 \n",
      "[262/500] train_loss: 0.05132 valid_loss: 0.07117 test_loss: 0.08161 \n",
      "验证损失减少 (0.071268 --> 0.071174). 正在保存模型...\n",
      "[263/500] train_loss: 0.05155 valid_loss: 0.07187 test_loss: 0.08141 \n",
      "[264/500] train_loss: 0.05091 valid_loss: 0.07492 test_loss: 0.08271 \n",
      "[265/500] train_loss: 0.05141 valid_loss: 0.07310 test_loss: 0.08255 \n",
      "[266/500] train_loss: 0.05072 valid_loss: 0.07353 test_loss: 0.08255 \n",
      "[267/500] train_loss: 0.04956 valid_loss: 0.07209 test_loss: 0.08327 \n",
      "[268/500] train_loss: 0.05147 valid_loss: 0.07298 test_loss: 0.08311 \n",
      "[269/500] train_loss: 0.05006 valid_loss: 0.07371 test_loss: 0.08273 \n",
      "[270/500] train_loss: 0.04934 valid_loss: 0.07359 test_loss: 0.08210 \n",
      "[271/500] train_loss: 0.05042 valid_loss: 0.07390 test_loss: 0.08161 \n",
      "[272/500] train_loss: 0.05036 valid_loss: 0.07111 test_loss: 0.08245 \n",
      "验证损失减少 (0.071174 --> 0.071108). 正在保存模型...\n",
      "[273/500] train_loss: 0.05033 valid_loss: 0.07280 test_loss: 0.08154 \n",
      "[274/500] train_loss: 0.04966 valid_loss: 0.07338 test_loss: 0.08123 \n",
      "[275/500] train_loss: 0.04895 valid_loss: 0.07314 test_loss: 0.08152 \n",
      "[276/500] train_loss: 0.04879 valid_loss: 0.07363 test_loss: 0.08127 \n",
      "[277/500] train_loss: 0.04917 valid_loss: 0.07349 test_loss: 0.08269 \n",
      "[278/500] train_loss: 0.04998 valid_loss: 0.07282 test_loss: 0.08170 \n",
      "[279/500] train_loss: 0.04945 valid_loss: 0.07331 test_loss: 0.08204 \n",
      "[280/500] train_loss: 0.05068 valid_loss: 0.07194 test_loss: 0.08248 \n",
      "[281/500] train_loss: 0.04962 valid_loss: 0.07305 test_loss: 0.08207 \n",
      "[282/500] train_loss: 0.05039 valid_loss: 0.07230 test_loss: 0.08202 \n",
      "[283/500] train_loss: 0.04936 valid_loss: 0.07394 test_loss: 0.08261 \n",
      "[284/500] train_loss: 0.04996 valid_loss: 0.07501 test_loss: 0.08256 \n",
      "[285/500] train_loss: 0.05047 valid_loss: 0.07349 test_loss: 0.08232 \n",
      "[286/500] train_loss: 0.04954 valid_loss: 0.07321 test_loss: 0.08278 \n",
      "[287/500] train_loss: 0.05052 valid_loss: 0.07248 test_loss: 0.08122 \n",
      "[288/500] train_loss: 0.04902 valid_loss: 0.07473 test_loss: 0.08247 \n",
      "[289/500] train_loss: 0.04947 valid_loss: 0.07482 test_loss: 0.08234 \n",
      "[290/500] train_loss: 0.05026 valid_loss: 0.07239 test_loss: 0.08108 \n",
      "[291/500] train_loss: 0.05096 valid_loss: 0.07213 test_loss: 0.08186 \n",
      "[292/500] train_loss: 0.04871 valid_loss: 0.07480 test_loss: 0.08349 \n",
      "[293/500] train_loss: 0.04949 valid_loss: 0.07294 test_loss: 0.08244 \n",
      "[294/500] train_loss: 0.04870 valid_loss: 0.07220 test_loss: 0.08154 \n",
      "[295/500] train_loss: 0.04858 valid_loss: 0.07224 test_loss: 0.08236 \n",
      "[296/500] train_loss: 0.04875 valid_loss: 0.07282 test_loss: 0.08258 \n",
      "[297/500] train_loss: 0.04967 valid_loss: 0.07220 test_loss: 0.08320 \n",
      "[298/500] train_loss: 0.04741 valid_loss: 0.07341 test_loss: 0.08340 \n",
      "[299/500] train_loss: 0.04894 valid_loss: 0.07250 test_loss: 0.08161 \n",
      "[300/500] train_loss: 0.04866 valid_loss: 0.07244 test_loss: 0.08232 \n",
      "[301/500] train_loss: 0.04927 valid_loss: 0.07392 test_loss: 0.08254 \n",
      "[302/500] train_loss: 0.04973 valid_loss: 0.07436 test_loss: 0.08190 \n",
      "[303/500] train_loss: 0.04903 valid_loss: 0.07386 test_loss: 0.08248 \n",
      "[304/500] train_loss: 0.04835 valid_loss: 0.07459 test_loss: 0.08301 \n",
      "[305/500] train_loss: 0.04767 valid_loss: 0.07459 test_loss: 0.08201 \n",
      "[306/500] train_loss: 0.04854 valid_loss: 0.07476 test_loss: 0.08371 \n",
      "[307/500] train_loss: 0.04826 valid_loss: 0.07318 test_loss: 0.08208 \n",
      "[308/500] train_loss: 0.04807 valid_loss: 0.07533 test_loss: 0.08364 \n",
      "[309/500] train_loss: 0.04830 valid_loss: 0.07304 test_loss: 0.08263 \n",
      "[310/500] train_loss: 0.04750 valid_loss: 0.07301 test_loss: 0.08131 \n",
      "[311/500] train_loss: 0.04821 valid_loss: 0.07392 test_loss: 0.08277 \n",
      "[312/500] train_loss: 0.04817 valid_loss: 0.07191 test_loss: 0.08088 \n",
      "[313/500] train_loss: 0.04899 valid_loss: 0.07309 test_loss: 0.08246 \n",
      "[314/500] train_loss: 0.04869 valid_loss: 0.07472 test_loss: 0.08053 \n",
      "[315/500] train_loss: 0.04854 valid_loss: 0.07489 test_loss: 0.08216 \n",
      "[316/500] train_loss: 0.04834 valid_loss: 0.07322 test_loss: 0.08128 \n",
      "[317/500] train_loss: 0.04832 valid_loss: 0.07306 test_loss: 0.08206 \n",
      "[318/500] train_loss: 0.04764 valid_loss: 0.07371 test_loss: 0.08243 \n",
      "[319/500] train_loss: 0.04772 valid_loss: 0.07336 test_loss: 0.08249 \n",
      "[320/500] train_loss: 0.04764 valid_loss: 0.07303 test_loss: 0.08250 \n",
      "[321/500] train_loss: 0.04821 valid_loss: 0.07281 test_loss: 0.08200 \n",
      "[322/500] train_loss: 0.04823 valid_loss: 0.07217 test_loss: 0.08112 \n",
      "[323/500] train_loss: 0.04873 valid_loss: 0.07229 test_loss: 0.08192 \n",
      "[324/500] train_loss: 0.04849 valid_loss: 0.07310 test_loss: 0.08231 \n",
      "[325/500] train_loss: 0.04662 valid_loss: 0.07391 test_loss: 0.08176 \n",
      "[326/500] train_loss: 0.04815 valid_loss: 0.07412 test_loss: 0.08133 \n",
      "[327/500] train_loss: 0.04845 valid_loss: 0.07378 test_loss: 0.08189 \n",
      "[328/500] train_loss: 0.04600 valid_loss: 0.07329 test_loss: 0.08200 \n",
      "[329/500] train_loss: 0.04582 valid_loss: 0.07264 test_loss: 0.08163 \n",
      "[330/500] train_loss: 0.04615 valid_loss: 0.07491 test_loss: 0.08376 \n",
      "[331/500] train_loss: 0.04737 valid_loss: 0.07335 test_loss: 0.08241 \n",
      "[332/500] train_loss: 0.04799 valid_loss: 0.07276 test_loss: 0.08326 \n",
      "[333/500] train_loss: 0.04753 valid_loss: 0.07374 test_loss: 0.08322 \n",
      "[334/500] train_loss: 0.04647 valid_loss: 0.07367 test_loss: 0.08233 \n",
      "[335/500] train_loss: 0.04714 valid_loss: 0.07263 test_loss: 0.08227 \n",
      "[336/500] train_loss: 0.04683 valid_loss: 0.07317 test_loss: 0.08364 \n",
      "[337/500] train_loss: 0.04770 valid_loss: 0.07402 test_loss: 0.08200 \n",
      "[338/500] train_loss: 0.04648 valid_loss: 0.07647 test_loss: 0.08315 \n",
      "[339/500] train_loss: 0.04705 valid_loss: 0.07378 test_loss: 0.08149 \n",
      "[340/500] train_loss: 0.04628 valid_loss: 0.07310 test_loss: 0.08227 \n",
      "[341/500] train_loss: 0.04793 valid_loss: 0.07352 test_loss: 0.08038 \n",
      "[342/500] train_loss: 0.04756 valid_loss: 0.07459 test_loss: 0.08207 \n",
      "[343/500] train_loss: 0.04639 valid_loss: 0.07431 test_loss: 0.08268 \n",
      "[344/500] train_loss: 0.04582 valid_loss: 0.07362 test_loss: 0.08185 \n",
      "[345/500] train_loss: 0.04592 valid_loss: 0.07466 test_loss: 0.08324 \n",
      "[346/500] train_loss: 0.04597 valid_loss: 0.07332 test_loss: 0.08123 \n",
      "[347/500] train_loss: 0.04749 valid_loss: 0.07378 test_loss: 0.08236 \n",
      "[348/500] train_loss: 0.04669 valid_loss: 0.07360 test_loss: 0.08164 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[349/500] train_loss: 0.04631 valid_loss: 0.07400 test_loss: 0.08186 \n",
      "[350/500] train_loss: 0.04769 valid_loss: 0.07408 test_loss: 0.08198 \n",
      "[351/500] train_loss: 0.04571 valid_loss: 0.07513 test_loss: 0.08291 \n",
      "[352/500] train_loss: 0.04808 valid_loss: 0.07278 test_loss: 0.08206 \n",
      "[353/500] train_loss: 0.04573 valid_loss: 0.07409 test_loss: 0.08171 \n",
      "[354/500] train_loss: 0.04584 valid_loss: 0.07456 test_loss: 0.08136 \n",
      "[355/500] train_loss: 0.04580 valid_loss: 0.07482 test_loss: 0.08341 \n",
      "[356/500] train_loss: 0.04612 valid_loss: 0.07413 test_loss: 0.08253 \n",
      "[357/500] train_loss: 0.04628 valid_loss: 0.07479 test_loss: 0.08127 \n",
      "[358/500] train_loss: 0.04574 valid_loss: 0.07544 test_loss: 0.08270 \n",
      "[359/500] train_loss: 0.04603 valid_loss: 0.07513 test_loss: 0.08259 \n",
      "[360/500] train_loss: 0.04658 valid_loss: 0.07362 test_loss: 0.08031 \n",
      "[361/500] train_loss: 0.04556 valid_loss: 0.07475 test_loss: 0.08309 \n",
      "[362/500] train_loss: 0.04592 valid_loss: 0.07520 test_loss: 0.08163 \n",
      "[363/500] train_loss: 0.04561 valid_loss: 0.07422 test_loss: 0.08266 \n",
      "[364/500] train_loss: 0.04565 valid_loss: 0.07467 test_loss: 0.08309 \n",
      "[365/500] train_loss: 0.04573 valid_loss: 0.07460 test_loss: 0.08319 \n",
      "[366/500] train_loss: 0.04575 valid_loss: 0.07534 test_loss: 0.08334 \n",
      "[367/500] train_loss: 0.04562 valid_loss: 0.07400 test_loss: 0.08219 \n",
      "[368/500] train_loss: 0.04585 valid_loss: 0.07508 test_loss: 0.08237 \n",
      "[369/500] train_loss: 0.04522 valid_loss: 0.07605 test_loss: 0.08266 \n",
      "[370/500] train_loss: 0.04521 valid_loss: 0.07417 test_loss: 0.08324 \n",
      "[371/500] train_loss: 0.04526 valid_loss: 0.07424 test_loss: 0.08238 \n",
      "[372/500] train_loss: 0.04471 valid_loss: 0.07626 test_loss: 0.08423 \n",
      "[373/500] train_loss: 0.04592 valid_loss: 0.07442 test_loss: 0.08249 \n",
      "[374/500] train_loss: 0.04545 valid_loss: 0.07420 test_loss: 0.08345 \n",
      "[375/500] train_loss: 0.04511 valid_loss: 0.07367 test_loss: 0.08177 \n",
      "[376/500] train_loss: 0.04552 valid_loss: 0.07656 test_loss: 0.08238 \n",
      "[377/500] train_loss: 0.04500 valid_loss: 0.07817 test_loss: 0.08397 \n",
      "[378/500] train_loss: 0.04568 valid_loss: 0.07562 test_loss: 0.08276 \n",
      "[379/500] train_loss: 0.04584 valid_loss: 0.07275 test_loss: 0.08323 \n",
      "[380/500] train_loss: 0.04499 valid_loss: 0.07350 test_loss: 0.08170 \n",
      "[381/500] train_loss: 0.04604 valid_loss: 0.07250 test_loss: 0.08225 \n",
      "[382/500] train_loss: 0.04356 valid_loss: 0.07387 test_loss: 0.08253 \n",
      "[383/500] train_loss: 0.04408 valid_loss: 0.07312 test_loss: 0.08331 \n",
      "[384/500] train_loss: 0.04571 valid_loss: 0.07417 test_loss: 0.08285 \n",
      "[385/500] train_loss: 0.04577 valid_loss: 0.07441 test_loss: 0.08334 \n",
      "[386/500] train_loss: 0.04510 valid_loss: 0.07402 test_loss: 0.08415 \n",
      "[387/500] train_loss: 0.04399 valid_loss: 0.07534 test_loss: 0.08447 \n",
      "[388/500] train_loss: 0.04525 valid_loss: 0.07460 test_loss: 0.08314 \n",
      "[389/500] train_loss: 0.04468 valid_loss: 0.07506 test_loss: 0.08424 \n",
      "[390/500] train_loss: 0.04443 valid_loss: 0.07487 test_loss: 0.08409 \n",
      "[391/500] train_loss: 0.04407 valid_loss: 0.07516 test_loss: 0.08302 \n",
      "[392/500] train_loss: 0.04541 valid_loss: 0.07420 test_loss: 0.08353 \n",
      "[393/500] train_loss: 0.04404 valid_loss: 0.07411 test_loss: 0.08187 \n",
      "[394/500] train_loss: 0.04481 valid_loss: 0.07372 test_loss: 0.08447 \n",
      "[395/500] train_loss: 0.04320 valid_loss: 0.07411 test_loss: 0.08339 \n",
      "[396/500] train_loss: 0.04490 valid_loss: 0.07522 test_loss: 0.08464 \n",
      "[397/500] train_loss: 0.04568 valid_loss: 0.07469 test_loss: 0.08402 \n",
      "[398/500] train_loss: 0.04346 valid_loss: 0.07473 test_loss: 0.08373 \n",
      "[399/500] train_loss: 0.04391 valid_loss: 0.07335 test_loss: 0.08321 \n",
      "[400/500] train_loss: 0.04469 valid_loss: 0.07383 test_loss: 0.08474 \n",
      "[401/500] train_loss: 0.04421 valid_loss: 0.07446 test_loss: 0.08333 \n",
      "[402/500] train_loss: 0.04382 valid_loss: 0.07407 test_loss: 0.08314 \n",
      "[403/500] train_loss: 0.04532 valid_loss: 0.07425 test_loss: 0.08300 \n",
      "[404/500] train_loss: 0.04352 valid_loss: 0.07485 test_loss: 0.08456 \n",
      "[405/500] train_loss: 0.04393 valid_loss: 0.07293 test_loss: 0.08282 \n",
      "[406/500] train_loss: 0.04447 valid_loss: 0.07363 test_loss: 0.08451 \n",
      "[407/500] train_loss: 0.04394 valid_loss: 0.07326 test_loss: 0.08255 \n",
      "[408/500] train_loss: 0.04484 valid_loss: 0.07324 test_loss: 0.08196 \n",
      "[409/500] train_loss: 0.04313 valid_loss: 0.07360 test_loss: 0.08204 \n",
      "[410/500] train_loss: 0.04336 valid_loss: 0.07340 test_loss: 0.08177 \n",
      "[411/500] train_loss: 0.04462 valid_loss: 0.07407 test_loss: 0.08161 \n",
      "[412/500] train_loss: 0.04426 valid_loss: 0.07359 test_loss: 0.08298 \n",
      "[413/500] train_loss: 0.04476 valid_loss: 0.07513 test_loss: 0.08276 \n",
      "[414/500] train_loss: 0.04367 valid_loss: 0.07586 test_loss: 0.08240 \n",
      "[415/500] train_loss: 0.04374 valid_loss: 0.07556 test_loss: 0.08272 \n",
      "[416/500] train_loss: 0.04391 valid_loss: 0.07442 test_loss: 0.08416 \n",
      "[417/500] train_loss: 0.04446 valid_loss: 0.07595 test_loss: 0.08228 \n",
      "[418/500] train_loss: 0.04384 valid_loss: 0.07411 test_loss: 0.08342 \n",
      "[419/500] train_loss: 0.04270 valid_loss: 0.07498 test_loss: 0.08504 \n",
      "[420/500] train_loss: 0.04288 valid_loss: 0.07306 test_loss: 0.08357 \n",
      "[421/500] train_loss: 0.04463 valid_loss: 0.07519 test_loss: 0.08353 \n",
      "[422/500] train_loss: 0.04346 valid_loss: 0.07438 test_loss: 0.08376 \n",
      "[423/500] train_loss: 0.04308 valid_loss: 0.07208 test_loss: 0.08328 \n",
      "[424/500] train_loss: 0.04232 valid_loss: 0.07352 test_loss: 0.08153 \n",
      "[425/500] train_loss: 0.04273 valid_loss: 0.07292 test_loss: 0.08279 \n",
      "[426/500] train_loss: 0.04462 valid_loss: 0.07345 test_loss: 0.08287 \n",
      "[427/500] train_loss: 0.04500 valid_loss: 0.07491 test_loss: 0.08368 \n",
      "[428/500] train_loss: 0.04441 valid_loss: 0.07385 test_loss: 0.08317 \n",
      "[429/500] train_loss: 0.04359 valid_loss: 0.07339 test_loss: 0.08377 \n",
      "[430/500] train_loss: 0.04296 valid_loss: 0.07333 test_loss: 0.08321 \n",
      "[431/500] train_loss: 0.04418 valid_loss: 0.07536 test_loss: 0.08540 \n",
      "[432/500] train_loss: 0.04429 valid_loss: 0.07225 test_loss: 0.08385 \n",
      "[433/500] train_loss: 0.04357 valid_loss: 0.07369 test_loss: 0.08368 \n",
      "[434/500] train_loss: 0.04171 valid_loss: 0.07438 test_loss: 0.08279 \n",
      "[435/500] train_loss: 0.04239 valid_loss: 0.07360 test_loss: 0.08306 \n",
      "[436/500] train_loss: 0.04289 valid_loss: 0.07218 test_loss: 0.08361 \n",
      "[437/500] train_loss: 0.04339 valid_loss: 0.07363 test_loss: 0.08411 \n",
      "[438/500] train_loss: 0.04321 valid_loss: 0.07309 test_loss: 0.08500 \n",
      "[439/500] train_loss: 0.04218 valid_loss: 0.07260 test_loss: 0.08357 \n",
      "[440/500] train_loss: 0.04318 valid_loss: 0.07188 test_loss: 0.08299 \n",
      "[441/500] train_loss: 0.04361 valid_loss: 0.07245 test_loss: 0.08254 \n",
      "[442/500] train_loss: 0.04313 valid_loss: 0.07441 test_loss: 0.08276 \n",
      "[443/500] train_loss: 0.04234 valid_loss: 0.07354 test_loss: 0.08361 \n",
      "[444/500] train_loss: 0.04360 valid_loss: 0.07325 test_loss: 0.08331 \n",
      "[445/500] train_loss: 0.04316 valid_loss: 0.07156 test_loss: 0.08290 \n",
      "[446/500] train_loss: 0.04346 valid_loss: 0.07402 test_loss: 0.08316 \n",
      "[447/500] train_loss: 0.04275 valid_loss: 0.07266 test_loss: 0.08281 \n",
      "[448/500] train_loss: 0.04300 valid_loss: 0.07475 test_loss: 0.08295 \n",
      "[449/500] train_loss: 0.04303 valid_loss: 0.07301 test_loss: 0.08165 \n",
      "[450/500] train_loss: 0.04320 valid_loss: 0.07440 test_loss: 0.08199 \n",
      "[451/500] train_loss: 0.04215 valid_loss: 0.07430 test_loss: 0.08336 \n",
      "[452/500] train_loss: 0.04293 valid_loss: 0.07958 test_loss: 0.08442 \n",
      "[453/500] train_loss: 0.04394 valid_loss: 0.07284 test_loss: 0.08234 \n",
      "[454/500] train_loss: 0.04266 valid_loss: 0.07231 test_loss: 0.08237 \n",
      "[455/500] train_loss: 0.04275 valid_loss: 0.07299 test_loss: 0.08300 \n",
      "[456/500] train_loss: 0.04288 valid_loss: 0.07206 test_loss: 0.08295 \n",
      "[457/500] train_loss: 0.04246 valid_loss: 0.07199 test_loss: 0.08385 \n",
      "[458/500] train_loss: 0.04161 valid_loss: 0.07494 test_loss: 0.08439 \n",
      "[459/500] train_loss: 0.04167 valid_loss: 0.07563 test_loss: 0.08444 \n",
      "[460/500] train_loss: 0.04132 valid_loss: 0.07365 test_loss: 0.08463 \n",
      "[461/500] train_loss: 0.04211 valid_loss: 0.07363 test_loss: 0.08407 \n",
      "[462/500] train_loss: 0.04257 valid_loss: 0.07302 test_loss: 0.08257 \n",
      "[463/500] train_loss: 0.04279 valid_loss: 0.07458 test_loss: 0.08352 \n",
      "[464/500] train_loss: 0.04281 valid_loss: 0.07459 test_loss: 0.08365 \n",
      "[465/500] train_loss: 0.04272 valid_loss: 0.07431 test_loss: 0.08270 \n",
      "[466/500] train_loss: 0.04179 valid_loss: 0.07427 test_loss: 0.08371 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[467/500] train_loss: 0.04158 valid_loss: 0.07304 test_loss: 0.08467 \n",
      "[468/500] train_loss: 0.04199 valid_loss: 0.07433 test_loss: 0.08410 \n",
      "[469/500] train_loss: 0.04352 valid_loss: 0.07349 test_loss: 0.08324 \n",
      "[470/500] train_loss: 0.04236 valid_loss: 0.07266 test_loss: 0.08206 \n",
      "[471/500] train_loss: 0.04304 valid_loss: 0.07461 test_loss: 0.08408 \n",
      "[472/500] train_loss: 0.04309 valid_loss: 0.07532 test_loss: 0.08392 \n",
      "[473/500] train_loss: 0.04141 valid_loss: 0.07322 test_loss: 0.08332 \n",
      "[474/500] train_loss: 0.04192 valid_loss: 0.07291 test_loss: 0.08308 \n",
      "[475/500] train_loss: 0.04194 valid_loss: 0.07366 test_loss: 0.08441 \n",
      "[476/500] train_loss: 0.04260 valid_loss: 0.07399 test_loss: 0.08235 \n",
      "[477/500] train_loss: 0.04303 valid_loss: 0.07419 test_loss: 0.08221 \n",
      "[478/500] train_loss: 0.04179 valid_loss: 0.07520 test_loss: 0.08364 \n",
      "[479/500] train_loss: 0.04079 valid_loss: 0.07402 test_loss: 0.08270 \n",
      "[480/500] train_loss: 0.04282 valid_loss: 0.07645 test_loss: 0.08376 \n",
      "[481/500] train_loss: 0.04101 valid_loss: 0.07495 test_loss: 0.08431 \n",
      "[482/500] train_loss: 0.04145 valid_loss: 0.07594 test_loss: 0.08360 \n",
      "[483/500] train_loss: 0.04283 valid_loss: 0.07480 test_loss: 0.08455 \n",
      "[484/500] train_loss: 0.04071 valid_loss: 0.07348 test_loss: 0.08571 \n",
      "[485/500] train_loss: 0.04122 valid_loss: 0.07399 test_loss: 0.08308 \n",
      "[486/500] train_loss: 0.04109 valid_loss: 0.07389 test_loss: 0.08327 \n",
      "[487/500] train_loss: 0.04153 valid_loss: 0.07412 test_loss: 0.08335 \n",
      "[488/500] train_loss: 0.04141 valid_loss: 0.07493 test_loss: 0.08477 \n",
      "[489/500] train_loss: 0.04151 valid_loss: 0.07606 test_loss: 0.08357 \n",
      "[490/500] train_loss: 0.04329 valid_loss: 0.07440 test_loss: 0.08185 \n",
      "[491/500] train_loss: 0.04111 valid_loss: 0.07616 test_loss: 0.08343 \n",
      "[492/500] train_loss: 0.04126 valid_loss: 0.07432 test_loss: 0.08526 \n",
      "[493/500] train_loss: 0.04187 valid_loss: 0.07721 test_loss: 0.08475 \n",
      "[494/500] train_loss: 0.04056 valid_loss: 0.07446 test_loss: 0.08391 \n",
      "[495/500] train_loss: 0.04190 valid_loss: 0.07251 test_loss: 0.08360 \n",
      "[496/500] train_loss: 0.04174 valid_loss: 0.07398 test_loss: 0.08456 \n",
      "[497/500] train_loss: 0.04126 valid_loss: 0.07309 test_loss: 0.08363 \n",
      "[498/500] train_loss: 0.04072 valid_loss: 0.07506 test_loss: 0.08356 \n",
      "[499/500] train_loss: 0.04216 valid_loss: 0.07630 test_loss: 0.08436 \n",
      "[500/500] train_loss: 0.04166 valid_loss: 0.07620 test_loss: 0.08389 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 500\n",
    "\n",
    "train_loader = dl_train_seen\n",
    "valid_loader = dl_valid_seen\n",
    "test_loader = dl_test_seen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "#for i in range(1):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    #model = LSTMModel(input_size, hidden_size, output_size).cuda()\n",
    "    model = PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_seen_01_01%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78da7f",
   "metadata": {},
   "source": [
    "这段代码的作用是训练模型。首先，设置了批量大小（batch_size）和训练轮数（n_epochs）。然后，分别设置了训练数据集（train_loader）、验证数据集（valid_loader）和测试数据集（test_loader）的加载器。接下来，通过循环来训练多个模型（在这里只训练一个模型）。\n",
    "\n",
    "在循环内部，首先打印出当前正在训练的模型的信息。然后，实例化了一个PTPNet模型，并将其移动到GPU上进行加速。接着，定义了优化器（optimizer）和损失函数（criterion）。最后，指定了保存模型的文件名（fn），并调用train_model函数来训练模型，并将训练过程中的损失值保存在train_loss、valid_loss和test_loss变量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25cb3e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1dee835f2b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, color='blue', label='train loss')\n",
    "plt.plot(valid_loss, color='green', label='valid loss')\n",
    "plt.plot(test_loss, color='red', label='test loss')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "703529d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x215adb6ad60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, color='blue', label='train loss')\n",
    "plt.plot(valid_loss, color='green', label='valid loss')\n",
    "plt.plot(test_loss, color='red', label='test loss')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66881feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/200] train_loss: 0.38802 valid_loss: 0.26677 test_loss: 0.23387 \n",
      "验证损失减少 (inf --> 0.233868). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20869 valid_loss: 0.18862 test_loss: 0.14938 \n",
      "验证损失减少 (0.266766 --> 0.149376). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16021 valid_loss: 0.16388 test_loss: 0.13240 \n",
      "验证损失减少 (0.188623 --> 0.132398). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14411 valid_loss: 0.14412 test_loss: 0.12451 \n",
      "验证损失减少 (0.163882 --> 0.124511). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13400 valid_loss: 0.13952 test_loss: 0.12297 \n",
      "验证损失减少 (0.144123 --> 0.122969). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12639 valid_loss: 0.13324 test_loss: 0.11513 \n",
      "验证损失减少 (0.139519 --> 0.115131). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12574 valid_loss: 0.12512 test_loss: 0.11161 \n",
      "验证损失减少 (0.133238 --> 0.111611). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11850 valid_loss: 0.12272 test_loss: 0.11025 \n",
      "验证损失减少 (0.125123 --> 0.110247). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11884 valid_loss: 0.11871 test_loss: 0.10656 \n",
      "验证损失减少 (0.122716 --> 0.106557). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11440 valid_loss: 0.11605 test_loss: 0.10677 \n",
      "验证损失减少 (0.118714 --> 0.106767). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11244 valid_loss: 0.11416 test_loss: 0.12336 \n",
      "[ 12/200] train_loss: 0.10699 valid_loss: 0.11635 test_loss: 0.10686 \n",
      "验证损失减少 (0.116052 --> 0.106857). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10692 valid_loss: 0.11024 test_loss: 0.10406 \n",
      "验证损失减少 (0.116348 --> 0.104065). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10599 valid_loss: 0.11176 test_loss: 0.10357 \n",
      "验证损失减少 (0.110238 --> 0.103566). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10316 valid_loss: 0.10792 test_loss: 0.10965 \n",
      "验证损失减少 (0.111765 --> 0.109646). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10350 valid_loss: 0.10577 test_loss: 0.09832 \n",
      "验证损失减少 (0.107919 --> 0.098316). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10137 valid_loss: 0.10487 test_loss: 0.10818 \n",
      "[ 18/200] train_loss: 0.09997 valid_loss: 0.10517 test_loss: 0.11449 \n",
      "[ 19/200] train_loss: 0.09910 valid_loss: 0.10517 test_loss: 0.10486 \n",
      "验证损失减少 (0.105770 --> 0.104857). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09522 valid_loss: 0.10403 test_loss: 0.10307 \n",
      "验证损失减少 (0.105168 --> 0.103070). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09618 valid_loss: 0.10062 test_loss: 0.09798 \n",
      "验证损失减少 (0.104025 --> 0.097981). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09741 valid_loss: 0.09809 test_loss: 0.12992 \n",
      "[ 23/200] train_loss: 0.09428 valid_loss: 0.10108 test_loss: 0.15898 \n",
      "[ 24/200] train_loss: 0.09397 valid_loss: 0.09911 test_loss: 0.10652 \n",
      "[ 25/200] train_loss: 0.09376 valid_loss: 0.09857 test_loss: 0.12301 \n",
      "[ 26/200] train_loss: 0.09014 valid_loss: 0.09637 test_loss: 0.12948 \n",
      "[ 27/200] train_loss: 0.09120 valid_loss: 0.09385 test_loss: 0.15512 \n",
      "[ 28/200] train_loss: 0.08850 valid_loss: 0.09564 test_loss: 0.10175 \n",
      "[ 29/200] train_loss: 0.08863 valid_loss: 0.09476 test_loss: 0.11229 \n",
      "[ 30/200] train_loss: 0.08890 valid_loss: 0.09313 test_loss: 0.10614 \n",
      "[ 31/200] train_loss: 0.08868 valid_loss: 0.09335 test_loss: 0.09747 \n",
      "验证损失减少 (0.100624 --> 0.097473). 正在保存模型...\n",
      "[ 32/200] train_loss: 0.08865 valid_loss: 0.09242 test_loss: 0.10838 \n",
      "[ 33/200] train_loss: 0.08614 valid_loss: 0.09233 test_loss: 0.11202 \n",
      "[ 34/200] train_loss: 0.08563 valid_loss: 0.09125 test_loss: 0.11203 \n",
      "[ 35/200] train_loss: 0.08592 valid_loss: 0.09429 test_loss: 0.09663 \n",
      "[ 36/200] train_loss: 0.08311 valid_loss: 0.09171 test_loss: 0.09577 \n",
      "[ 37/200] train_loss: 0.08292 valid_loss: 0.09061 test_loss: 0.09750 \n",
      "[ 38/200] train_loss: 0.08464 valid_loss: 0.08929 test_loss: 0.09348 \n",
      "[ 39/200] train_loss: 0.08423 valid_loss: 0.09134 test_loss: 0.09240 \n",
      "验证损失减少 (0.093345 --> 0.092395). 正在保存模型...\n",
      "[ 40/200] train_loss: 0.08258 valid_loss: 0.09155 test_loss: 0.09377 \n",
      "[ 41/200] train_loss: 0.08383 valid_loss: 0.08935 test_loss: 0.09212 \n",
      "[ 42/200] train_loss: 0.08276 valid_loss: 0.09160 test_loss: 0.09802 \n",
      "[ 43/200] train_loss: 0.08242 valid_loss: 0.08825 test_loss: 0.10309 \n",
      "[ 44/200] train_loss: 0.08120 valid_loss: 0.08793 test_loss: 0.09370 \n",
      "[ 45/200] train_loss: 0.08023 valid_loss: 0.08912 test_loss: 0.09879 \n",
      "[ 46/200] train_loss: 0.08014 valid_loss: 0.08671 test_loss: 0.13803 \n",
      "[ 47/200] train_loss: 0.07951 valid_loss: 0.08656 test_loss: 0.15424 \n",
      "[ 48/200] train_loss: 0.07990 valid_loss: 0.08665 test_loss: 0.10651 \n",
      "[ 49/200] train_loss: 0.07835 valid_loss: 0.08602 test_loss: 0.11580 \n",
      "[ 50/200] train_loss: 0.07840 valid_loss: 0.08576 test_loss: 0.11063 \n",
      "[ 51/200] train_loss: 0.07943 valid_loss: 0.08623 test_loss: 0.09318 \n",
      "[ 52/200] train_loss: 0.07867 valid_loss: 0.08506 test_loss: 0.09549 \n",
      "[ 53/200] train_loss: 0.07708 valid_loss: 0.08577 test_loss: 0.25342 \n",
      "[ 54/200] train_loss: 0.07730 valid_loss: 0.08453 test_loss: 0.09761 \n",
      "[ 55/200] train_loss: 0.07523 valid_loss: 0.08526 test_loss: 0.19547 \n",
      "[ 56/200] train_loss: 0.07745 valid_loss: 0.08419 test_loss: 0.09464 \n",
      "[ 57/200] train_loss: 0.07501 valid_loss: 0.08499 test_loss: 0.09280 \n",
      "[ 58/200] train_loss: 0.07427 valid_loss: 0.08542 test_loss: 0.08973 \n",
      "验证损失减少 (0.091344 --> 0.089734). 正在保存模型...\n",
      "[ 59/200] train_loss: 0.07517 valid_loss: 0.08492 test_loss: 0.09218 \n",
      "[ 60/200] train_loss: 0.07641 valid_loss: 0.08321 test_loss: 0.09353 \n",
      "[ 61/200] train_loss: 0.07419 valid_loss: 0.08332 test_loss: 0.09029 \n",
      "[ 62/200] train_loss: 0.07389 valid_loss: 0.08411 test_loss: 0.09478 \n",
      "[ 63/200] train_loss: 0.07583 valid_loss: 0.08461 test_loss: 0.09379 \n",
      "[ 64/200] train_loss: 0.07358 valid_loss: 0.08400 test_loss: 0.09168 \n",
      "[ 65/200] train_loss: 0.07519 valid_loss: 0.08321 test_loss: 0.09008 \n",
      "[ 66/200] train_loss: 0.07436 valid_loss: 0.08307 test_loss: 0.09036 \n",
      "[ 67/200] train_loss: 0.07477 valid_loss: 0.08078 test_loss: 0.09049 \n",
      "[ 68/200] train_loss: 0.07380 valid_loss: 0.08268 test_loss: 0.11378 \n",
      "[ 69/200] train_loss: 0.07425 valid_loss: 0.08142 test_loss: 0.09083 \n",
      "[ 70/200] train_loss: 0.07380 valid_loss: 0.08058 test_loss: 0.24100 \n",
      "[ 71/200] train_loss: 0.07180 valid_loss: 0.08063 test_loss: 0.11074 \n",
      "[ 72/200] train_loss: 0.07236 valid_loss: 0.08144 test_loss: 0.08992 \n",
      "[ 73/200] train_loss: 0.07119 valid_loss: 0.08071 test_loss: 0.08936 \n",
      "[ 74/200] train_loss: 0.07143 valid_loss: 0.08163 test_loss: 0.08689 \n",
      "[ 75/200] train_loss: 0.07202 valid_loss: 0.07792 test_loss: 0.12539 \n",
      "[ 76/200] train_loss: 0.07128 valid_loss: 0.08151 test_loss: 0.12599 \n",
      "[ 77/200] train_loss: 0.07197 valid_loss: 0.08247 test_loss: 0.08623 \n",
      "[ 78/200] train_loss: 0.07031 valid_loss: 0.08046 test_loss: 0.59205 \n",
      "[ 79/200] train_loss: 0.07063 valid_loss: 0.08124 test_loss: 0.08977 \n",
      "[ 80/200] train_loss: 0.07052 valid_loss: 0.08242 test_loss: 0.16129 \n",
      "[ 81/200] train_loss: 0.07015 valid_loss: 0.07970 test_loss: 0.10484 \n",
      "[ 82/200] train_loss: 0.06900 valid_loss: 0.07995 test_loss: 0.19176 \n",
      "[ 83/200] train_loss: 0.06968 valid_loss: 0.07829 test_loss: 0.10824 \n",
      "[ 84/200] train_loss: 0.06989 valid_loss: 0.07960 test_loss: 0.10323 \n",
      "[ 85/200] train_loss: 0.06969 valid_loss: 0.07852 test_loss: 0.08892 \n",
      "[ 86/200] train_loss: 0.06881 valid_loss: 0.07897 test_loss: 0.10969 \n",
      "[ 87/200] train_loss: 0.06618 valid_loss: 0.07949 test_loss: 0.09198 \n",
      "[ 88/200] train_loss: 0.06783 valid_loss: 0.07927 test_loss: 0.12208 \n",
      "[ 89/200] train_loss: 0.06792 valid_loss: 0.08435 test_loss: 0.09143 \n",
      "[ 90/200] train_loss: 0.06771 valid_loss: 0.07954 test_loss: 0.08802 \n",
      "[ 91/200] train_loss: 0.06769 valid_loss: 0.07893 test_loss: 0.08847 \n",
      "[ 92/200] train_loss: 0.06679 valid_loss: 0.08432 test_loss: 0.09184 \n",
      "[ 93/200] train_loss: 0.06755 valid_loss: 0.07896 test_loss: 0.08862 \n",
      "[ 94/200] train_loss: 0.06736 valid_loss: 0.07963 test_loss: 0.08901 \n",
      "[ 95/200] train_loss: 0.06875 valid_loss: 0.07900 test_loss: 0.08778 \n",
      "[ 96/200] train_loss: 0.06630 valid_loss: 0.08368 test_loss: 0.09240 \n",
      "[ 97/200] train_loss: 0.06683 valid_loss: 0.07971 test_loss: 0.08968 \n",
      "[ 98/200] train_loss: 0.06668 valid_loss: 0.08019 test_loss: 0.08940 \n",
      "[ 99/200] train_loss: 0.06625 valid_loss: 0.07878 test_loss: 0.09231 \n",
      "[100/200] train_loss: 0.06593 valid_loss: 0.08311 test_loss: 0.09407 \n",
      "[101/200] train_loss: 0.06685 valid_loss: 0.07966 test_loss: 0.09040 \n",
      "[102/200] train_loss: 0.06745 valid_loss: 0.07743 test_loss: 0.27201 \n",
      "[103/200] train_loss: 0.06678 valid_loss: 0.07666 test_loss: 0.09243 \n",
      "[104/200] train_loss: 0.06658 valid_loss: 0.08064 test_loss: 0.08967 \n",
      "[105/200] train_loss: 0.06462 valid_loss: 0.08014 test_loss: 0.08859 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106/200] train_loss: 0.06388 valid_loss: 0.07875 test_loss: 0.09077 \n",
      "[107/200] train_loss: 0.06559 valid_loss: 0.08090 test_loss: 0.09187 \n",
      "[108/200] train_loss: 0.06639 valid_loss: 0.07751 test_loss: 0.08780 \n",
      "[109/200] train_loss: 0.06506 valid_loss: 0.07881 test_loss: 0.21528 \n",
      "[110/200] train_loss: 0.06605 valid_loss: 0.07851 test_loss: 0.08732 \n",
      "[111/200] train_loss: 0.06489 valid_loss: 0.08017 test_loss: 0.09313 \n",
      "[112/200] train_loss: 0.06364 valid_loss: 0.08110 test_loss: 0.12814 \n",
      "[113/200] train_loss: 0.06386 valid_loss: 0.07826 test_loss: 0.09078 \n",
      "[114/200] train_loss: 0.06350 valid_loss: 0.07782 test_loss: 0.09218 \n",
      "[115/200] train_loss: 0.06239 valid_loss: 0.07785 test_loss: 0.09830 \n",
      "[116/200] train_loss: 0.06474 valid_loss: 0.07849 test_loss: 0.09800 \n",
      "[117/200] train_loss: 0.06343 valid_loss: 0.07817 test_loss: 0.08903 \n",
      "[118/200] train_loss: 0.06626 valid_loss: 0.07750 test_loss: 0.11227 \n",
      "[119/200] train_loss: 0.06263 valid_loss: 0.07897 test_loss: 0.11026 \n",
      "[120/200] train_loss: 0.06236 valid_loss: 0.07894 test_loss: 0.09125 \n",
      "[121/200] train_loss: 0.06514 valid_loss: 0.07932 test_loss: 0.09047 \n",
      "[122/200] train_loss: 0.06310 valid_loss: 0.07806 test_loss: 0.08966 \n",
      "[123/200] train_loss: 0.06383 valid_loss: 0.08239 test_loss: 0.09536 \n",
      "[124/200] train_loss: 0.06259 valid_loss: 0.07919 test_loss: 0.09316 \n",
      "[125/200] train_loss: 0.06352 valid_loss: 0.07936 test_loss: 0.09124 \n",
      "[126/200] train_loss: 0.06205 valid_loss: 0.08061 test_loss: 0.09138 \n",
      "[127/200] train_loss: 0.06150 valid_loss: 0.07825 test_loss: 0.09085 \n",
      "[128/200] train_loss: 0.06135 valid_loss: 0.07987 test_loss: 0.09205 \n",
      "[129/200] train_loss: 0.06157 valid_loss: 0.07813 test_loss: 0.09098 \n",
      "[130/200] train_loss: 0.06248 valid_loss: 0.08343 test_loss: 0.09676 \n",
      "[131/200] train_loss: 0.06025 valid_loss: 0.07818 test_loss: 0.09029 \n",
      "[132/200] train_loss: 0.06112 valid_loss: 0.08277 test_loss: 0.09399 \n",
      "[133/200] train_loss: 0.06302 valid_loss: 0.07942 test_loss: 0.09057 \n",
      "[134/200] train_loss: 0.06223 valid_loss: 0.07483 test_loss: 0.09019 \n",
      "[135/200] train_loss: 0.06131 valid_loss: 0.07663 test_loss: 0.09298 \n",
      "[136/200] train_loss: 0.06108 valid_loss: 0.08033 test_loss: 0.09570 \n",
      "[137/200] train_loss: 0.06033 valid_loss: 0.07815 test_loss: 0.09264 \n",
      "[138/200] train_loss: 0.06044 valid_loss: 0.07698 test_loss: 0.09037 \n",
      "[139/200] train_loss: 0.06073 valid_loss: 0.08022 test_loss: 0.08952 \n",
      "[140/200] train_loss: 0.06273 valid_loss: 0.07824 test_loss: 0.09556 \n",
      "[141/200] train_loss: 0.06076 valid_loss: 0.07608 test_loss: 0.10013 \n",
      "[142/200] train_loss: 0.06260 valid_loss: 0.07456 test_loss: 0.18282 \n",
      "[143/200] train_loss: 0.05839 valid_loss: 0.07803 test_loss: 0.09062 \n",
      "[144/200] train_loss: 0.06023 valid_loss: 0.07753 test_loss: 0.09337 \n",
      "[145/200] train_loss: 0.05996 valid_loss: 0.08074 test_loss: 0.09472 \n",
      "[146/200] train_loss: 0.06087 valid_loss: 0.07707 test_loss: 0.09255 \n",
      "[147/200] train_loss: 0.06177 valid_loss: 0.07670 test_loss: 0.08907 \n",
      "[148/200] train_loss: 0.05955 valid_loss: 0.07882 test_loss: 0.09119 \n",
      "[149/200] train_loss: 0.05816 valid_loss: 0.07966 test_loss: 0.09795 \n",
      "[150/200] train_loss: 0.06059 valid_loss: 0.08013 test_loss: 0.09363 \n",
      "[151/200] train_loss: 0.06105 valid_loss: 0.07537 test_loss: 0.09113 \n",
      "[152/200] train_loss: 0.05754 valid_loss: 0.07558 test_loss: 0.09240 \n",
      "[153/200] train_loss: 0.05838 valid_loss: 0.07575 test_loss: 0.09162 \n",
      "[154/200] train_loss: 0.06058 valid_loss: 0.07652 test_loss: 0.09208 \n",
      "[155/200] train_loss: 0.05755 valid_loss: 0.07567 test_loss: 0.09000 \n",
      "[156/200] train_loss: 0.05754 valid_loss: 0.07544 test_loss: 0.09061 \n",
      "[157/200] train_loss: 0.05894 valid_loss: 0.07883 test_loss: 0.09250 \n",
      "[158/200] train_loss: 0.05728 valid_loss: 0.07779 test_loss: 0.09179 \n",
      "[159/200] train_loss: 0.05711 valid_loss: 0.08022 test_loss: 0.09073 \n",
      "[160/200] train_loss: 0.05852 valid_loss: 0.07671 test_loss: 0.09013 \n",
      "[161/200] train_loss: 0.05879 valid_loss: 0.08123 test_loss: 0.09360 \n",
      "[162/200] train_loss: 0.05894 valid_loss: 0.07609 test_loss: 0.08776 \n",
      "[163/200] train_loss: 0.05958 valid_loss: 0.07453 test_loss: 0.09169 \n",
      "[164/200] train_loss: 0.05775 valid_loss: 0.07596 test_loss: 0.09270 \n",
      "[165/200] train_loss: 0.05676 valid_loss: 0.08053 test_loss: 0.09228 \n",
      "[166/200] train_loss: 0.05776 valid_loss: 0.08520 test_loss: 0.09367 \n",
      "[167/200] train_loss: 0.05819 valid_loss: 0.07713 test_loss: 0.08879 \n",
      "[168/200] train_loss: 0.05959 valid_loss: 0.08096 test_loss: 0.10074 \n",
      "[169/200] train_loss: 0.05644 valid_loss: 0.07620 test_loss: 0.09018 \n",
      "[170/200] train_loss: 0.05604 valid_loss: 0.07912 test_loss: 0.09363 \n",
      "[171/200] train_loss: 0.05556 valid_loss: 0.07596 test_loss: 0.09068 \n",
      "[172/200] train_loss: 0.05761 valid_loss: 0.07347 test_loss: 0.09124 \n",
      "[173/200] train_loss: 0.05805 valid_loss: 0.07760 test_loss: 0.09603 \n",
      "[174/200] train_loss: 0.05798 valid_loss: 0.07635 test_loss: 0.20007 \n",
      "[175/200] train_loss: 0.05768 valid_loss: 0.07673 test_loss: 0.09245 \n",
      "[176/200] train_loss: 0.05514 valid_loss: 0.08738 test_loss: 0.10395 \n",
      "[177/200] train_loss: 0.05654 valid_loss: 0.07952 test_loss: 0.09956 \n",
      "[178/200] train_loss: 0.05604 valid_loss: 0.09402 test_loss: 0.13060 \n",
      "[179/200] train_loss: 0.05569 valid_loss: 0.09295 test_loss: 0.11368 \n",
      "[180/200] train_loss: 0.05759 valid_loss: 0.07825 test_loss: 0.08882 \n",
      "[181/200] train_loss: 0.05521 valid_loss: 0.07453 test_loss: 0.08761 \n",
      "[182/200] train_loss: 0.05580 valid_loss: 0.08889 test_loss: 0.10593 \n",
      "[183/200] train_loss: 0.05572 valid_loss: 0.07609 test_loss: 0.09196 \n",
      "[184/200] train_loss: 0.05563 valid_loss: 0.08049 test_loss: 0.09754 \n",
      "[185/200] train_loss: 0.05623 valid_loss: 0.07636 test_loss: 0.09342 \n",
      "[186/200] train_loss: 0.05688 valid_loss: 0.08769 test_loss: 0.10564 \n",
      "[187/200] train_loss: 0.05515 valid_loss: 0.07716 test_loss: 0.09335 \n",
      "[188/200] train_loss: 0.05598 valid_loss: 0.08325 test_loss: 0.10022 \n",
      "[189/200] train_loss: 0.05528 valid_loss: 0.08526 test_loss: 0.09515 \n",
      "[190/200] train_loss: 0.05350 valid_loss: 0.07986 test_loss: 0.09344 \n",
      "[191/200] train_loss: 0.05525 valid_loss: 0.08936 test_loss: 0.12169 \n",
      "[192/200] train_loss: 0.05648 valid_loss: 0.08412 test_loss: 0.14491 \n",
      "[193/200] train_loss: 0.05547 valid_loss: 0.08246 test_loss: 0.10608 \n",
      "[194/200] train_loss: 0.05619 valid_loss: 0.08762 test_loss: 0.16064 \n",
      "[195/200] train_loss: 0.05453 valid_loss: 0.07480 test_loss: 0.09500 \n",
      "[196/200] train_loss: 0.05495 valid_loss: 0.07703 test_loss: 0.09200 \n",
      "[197/200] train_loss: 0.05617 valid_loss: 0.08012 test_loss: 0.09766 \n",
      "[198/200] train_loss: 0.05544 valid_loss: 0.07712 test_loss: 0.09582 \n",
      "[199/200] train_loss: 0.05433 valid_loss: 0.08361 test_loss: 0.10899 \n",
      "[200/200] train_loss: 0.05318 valid_loss: 0.07993 test_loss: 0.09566 \n",
      "TRAINING MODEL 1\n",
      "[  1/200] train_loss: 0.39356 valid_loss: 0.27285 test_loss: 0.23661 \n",
      "验证损失减少 (inf --> 0.236606). 正在保存模型...\n",
      "[  2/200] train_loss: 0.21583 valid_loss: 0.19699 test_loss: 0.15754 \n",
      "验证损失减少 (0.272854 --> 0.157537). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16755 valid_loss: 0.17267 test_loss: 0.14075 \n",
      "验证损失减少 (0.196988 --> 0.140753). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14910 valid_loss: 0.15074 test_loss: 0.12219 \n",
      "验证损失减少 (0.172671 --> 0.122191). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13532 valid_loss: 0.14609 test_loss: 0.12706 \n",
      "验证损失减少 (0.150742 --> 0.127056). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13157 valid_loss: 0.13690 test_loss: 0.12006 \n",
      "验证损失减少 (0.146086 --> 0.120060). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12592 valid_loss: 0.12954 test_loss: 0.11803 \n",
      "验证损失减少 (0.136898 --> 0.118034). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12003 valid_loss: 0.12823 test_loss: 0.11328 \n",
      "验证损失减少 (0.129543 --> 0.113279). 正在保存模型...\n",
      "[  9/200] train_loss: 0.12103 valid_loss: 0.12928 test_loss: 0.11730 \n",
      "验证损失减少 (0.128228 --> 0.117300). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11690 valid_loss: 0.12267 test_loss: 0.11399 \n",
      "验证损失减少 (0.129280 --> 0.113991). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11646 valid_loss: 0.11865 test_loss: 0.11174 \n",
      "验证损失减少 (0.122667 --> 0.111743). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.11612 valid_loss: 0.11995 test_loss: 0.11651 \n",
      "验证损失减少 (0.118651 --> 0.116505). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10947 valid_loss: 0.11556 test_loss: 0.11040 \n",
      "验证损失减少 (0.119952 --> 0.110396). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.11131 valid_loss: 0.11556 test_loss: 0.10974 \n",
      "验证损失减少 (0.115563 --> 0.109736). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15/200] train_loss: 0.10872 valid_loss: 0.11263 test_loss: 0.10982 \n",
      "验证损失减少 (0.115559 --> 0.109816). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10427 valid_loss: 0.11260 test_loss: 0.10890 \n",
      "验证损失减少 (0.112627 --> 0.108904). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10380 valid_loss: 0.11683 test_loss: 0.11139 \n",
      "验证损失减少 (0.112596 --> 0.111388). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.10182 valid_loss: 0.11024 test_loss: 0.10605 \n",
      "验证损失减少 (0.116832 --> 0.106049). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.10139 valid_loss: 0.10594 test_loss: 0.10310 \n",
      "验证损失减少 (0.110238 --> 0.103102). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09985 valid_loss: 0.10582 test_loss: 0.10679 \n",
      "[ 21/200] train_loss: 0.09937 valid_loss: 0.10748 test_loss: 0.10504 \n",
      "验证损失减少 (0.105942 --> 0.105038). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09827 valid_loss: 0.10426 test_loss: 0.10709 \n",
      "验证损失减少 (0.107479 --> 0.107092). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09934 valid_loss: 0.10484 test_loss: 0.13807 \n",
      "[ 24/200] train_loss: 0.09788 valid_loss: 0.10386 test_loss: 0.15913 \n",
      "[ 25/200] train_loss: 0.09551 valid_loss: 0.10026 test_loss: 0.11634 \n",
      "[ 26/200] train_loss: 0.09607 valid_loss: 0.10120 test_loss: 0.12193 \n",
      "[ 27/200] train_loss: 0.09433 valid_loss: 0.10359 test_loss: 0.10591 \n",
      "[ 28/200] train_loss: 0.09456 valid_loss: 0.09790 test_loss: 0.11466 \n",
      "[ 29/200] train_loss: 0.09330 valid_loss: 0.10147 test_loss: 0.10721 \n",
      "[ 30/200] train_loss: 0.09010 valid_loss: 0.09962 test_loss: 0.11612 \n",
      "[ 31/200] train_loss: 0.09329 valid_loss: 0.09791 test_loss: 0.12967 \n",
      "[ 32/200] train_loss: 0.09188 valid_loss: 0.09783 test_loss: 0.15068 \n",
      "[ 33/200] train_loss: 0.09413 valid_loss: 0.09568 test_loss: 0.20396 \n",
      "[ 34/200] train_loss: 0.09180 valid_loss: 0.09608 test_loss: 0.10449 \n",
      "[ 35/200] train_loss: 0.08969 valid_loss: 0.09723 test_loss: 0.10425 \n",
      "验证损失减少 (0.104257 --> 0.104251). 正在保存模型...\n",
      "[ 36/200] train_loss: 0.08632 valid_loss: 0.09377 test_loss: 0.10547 \n",
      "[ 37/200] train_loss: 0.08554 valid_loss: 0.09508 test_loss: 0.12519 \n",
      "[ 38/200] train_loss: 0.08666 valid_loss: 0.09374 test_loss: 0.11712 \n",
      "[ 39/200] train_loss: 0.08705 valid_loss: 0.09226 test_loss: 0.12583 \n",
      "[ 40/200] train_loss: 0.08632 valid_loss: 0.09465 test_loss: 0.10503 \n",
      "[ 41/200] train_loss: 0.08600 valid_loss: 0.09205 test_loss: 0.12062 \n",
      "[ 42/200] train_loss: 0.08746 valid_loss: 0.09213 test_loss: 0.10182 \n",
      "[ 43/200] train_loss: 0.08398 valid_loss: 0.09166 test_loss: 0.11574 \n",
      "[ 44/200] train_loss: 0.08585 valid_loss: 0.09149 test_loss: 0.12987 \n",
      "[ 45/200] train_loss: 0.08390 valid_loss: 0.09284 test_loss: 0.09755 \n",
      "[ 46/200] train_loss: 0.08302 valid_loss: 0.09125 test_loss: 0.10291 \n",
      "[ 47/200] train_loss: 0.08458 valid_loss: 0.08935 test_loss: 0.09643 \n",
      "验证损失减少 (0.097226 --> 0.096434). 正在保存模型...\n",
      "[ 48/200] train_loss: 0.08490 valid_loss: 0.09027 test_loss: 0.10392 \n",
      "[ 49/200] train_loss: 0.08248 valid_loss: 0.08796 test_loss: 0.13867 \n",
      "[ 50/200] train_loss: 0.08162 valid_loss: 0.08811 test_loss: 0.09961 \n",
      "[ 51/200] train_loss: 0.08221 valid_loss: 0.09044 test_loss: 0.10009 \n",
      "[ 52/200] train_loss: 0.07995 valid_loss: 0.09019 test_loss: 0.09527 \n",
      "[ 53/200] train_loss: 0.07898 valid_loss: 0.08871 test_loss: 0.11335 \n",
      "[ 54/200] train_loss: 0.08013 valid_loss: 0.08987 test_loss: 0.09914 \n",
      "[ 55/200] train_loss: 0.07828 valid_loss: 0.08782 test_loss: 0.09281 \n",
      "[ 56/200] train_loss: 0.08060 valid_loss: 0.08908 test_loss: 0.09642 \n",
      "[ 57/200] train_loss: 0.07937 valid_loss: 0.08510 test_loss: 0.12630 \n",
      "[ 58/200] train_loss: 0.08012 valid_loss: 0.08894 test_loss: 0.09966 \n",
      "[ 59/200] train_loss: 0.07743 valid_loss: 0.08822 test_loss: 0.09843 \n",
      "[ 60/200] train_loss: 0.07862 valid_loss: 0.08534 test_loss: 0.24087 \n",
      "[ 61/200] train_loss: 0.07784 valid_loss: 0.08518 test_loss: 0.10512 \n",
      "[ 62/200] train_loss: 0.07953 valid_loss: 0.09015 test_loss: 0.10664 \n",
      "[ 63/200] train_loss: 0.07785 valid_loss: 0.08483 test_loss: 0.10105 \n",
      "[ 64/200] train_loss: 0.07785 valid_loss: 0.08607 test_loss: 0.09666 \n",
      "[ 65/200] train_loss: 0.07866 valid_loss: 0.08746 test_loss: 0.09990 \n",
      "[ 66/200] train_loss: 0.07811 valid_loss: 0.08500 test_loss: 0.09842 \n",
      "[ 67/200] train_loss: 0.07851 valid_loss: 0.08542 test_loss: 0.13469 \n",
      "[ 68/200] train_loss: 0.07641 valid_loss: 0.08439 test_loss: 0.09480 \n",
      "[ 69/200] train_loss: 0.07591 valid_loss: 0.08243 test_loss: 0.10066 \n",
      "[ 70/200] train_loss: 0.07491 valid_loss: 0.08575 test_loss: 0.10905 \n",
      "[ 71/200] train_loss: 0.07599 valid_loss: 0.08298 test_loss: 0.13158 \n",
      "[ 72/200] train_loss: 0.07605 valid_loss: 0.08355 test_loss: 0.09992 \n",
      "[ 73/200] train_loss: 0.07451 valid_loss: 0.08361 test_loss: 0.09432 \n",
      "[ 74/200] train_loss: 0.07360 valid_loss: 0.08480 test_loss: 0.09659 \n",
      "[ 75/200] train_loss: 0.07357 valid_loss: 0.08258 test_loss: 0.09379 \n",
      "[ 76/200] train_loss: 0.07452 valid_loss: 0.08633 test_loss: 0.09531 \n",
      "[ 77/200] train_loss: 0.07445 valid_loss: 0.08389 test_loss: 0.28466 \n",
      "[ 78/200] train_loss: 0.07262 valid_loss: 0.08182 test_loss: 0.21102 \n",
      "[ 79/200] train_loss: 0.07379 valid_loss: 0.08284 test_loss: 0.10102 \n",
      "[ 80/200] train_loss: 0.07538 valid_loss: 0.08175 test_loss: 0.39856 \n",
      "[ 81/200] train_loss: 0.07249 valid_loss: 0.08193 test_loss: 0.13894 \n",
      "[ 82/200] train_loss: 0.07253 valid_loss: 0.08520 test_loss: 0.11108 \n",
      "[ 83/200] train_loss: 0.07328 valid_loss: 0.08217 test_loss: 0.11615 \n",
      "[ 84/200] train_loss: 0.07440 valid_loss: 0.08357 test_loss: 0.15281 \n",
      "[ 85/200] train_loss: 0.07399 valid_loss: 0.08220 test_loss: 0.10724 \n",
      "[ 86/200] train_loss: 0.07224 valid_loss: 0.07945 test_loss: 0.13260 \n",
      "[ 87/200] train_loss: 0.07010 valid_loss: 0.08192 test_loss: 0.09710 \n",
      "[ 88/200] train_loss: 0.07098 valid_loss: 0.08042 test_loss: 0.15530 \n",
      "[ 89/200] train_loss: 0.07118 valid_loss: 0.08037 test_loss: 0.68219 \n",
      "[ 90/200] train_loss: 0.07220 valid_loss: 0.08146 test_loss: 0.12014 \n",
      "[ 91/200] train_loss: 0.07138 valid_loss: 0.08128 test_loss: 0.09772 \n",
      "[ 92/200] train_loss: 0.07068 valid_loss: 0.08087 test_loss: 0.36729 \n",
      "[ 93/200] train_loss: 0.07142 valid_loss: 0.07920 test_loss: 0.25106 \n",
      "[ 94/200] train_loss: 0.07004 valid_loss: 0.08083 test_loss: 0.09654 \n",
      "[ 95/200] train_loss: 0.06951 valid_loss: 0.08131 test_loss: 0.38780 \n",
      "[ 96/200] train_loss: 0.06937 valid_loss: 0.08016 test_loss: 0.17414 \n",
      "[ 97/200] train_loss: 0.07075 valid_loss: 0.08021 test_loss: 0.21728 \n",
      "[ 98/200] train_loss: 0.06828 valid_loss: 0.08058 test_loss: 0.13113 \n",
      "[ 99/200] train_loss: 0.06922 valid_loss: 0.07920 test_loss: 0.19041 \n",
      "[100/200] train_loss: 0.07287 valid_loss: 0.07909 test_loss: 0.09373 \n",
      "[101/200] train_loss: 0.07148 valid_loss: 0.08007 test_loss: 0.14567 \n",
      "[102/200] train_loss: 0.06764 valid_loss: 0.07939 test_loss: 0.11996 \n",
      "[103/200] train_loss: 0.06969 valid_loss: 0.07939 test_loss: 0.17111 \n",
      "[104/200] train_loss: 0.06962 valid_loss: 0.07895 test_loss: 0.10396 \n",
      "[105/200] train_loss: 0.06703 valid_loss: 0.07817 test_loss: 0.34520 \n",
      "[106/200] train_loss: 0.06896 valid_loss: 0.07965 test_loss: 0.09799 \n",
      "[107/200] train_loss: 0.06742 valid_loss: 0.07945 test_loss: 0.26286 \n",
      "[108/200] train_loss: 0.06902 valid_loss: 0.08025 test_loss: 0.09879 \n",
      "[109/200] train_loss: 0.06759 valid_loss: 0.07793 test_loss: 0.10838 \n",
      "[110/200] train_loss: 0.06948 valid_loss: 0.07921 test_loss: 0.11009 \n",
      "[111/200] train_loss: 0.06748 valid_loss: 0.07768 test_loss: 0.10746 \n",
      "[112/200] train_loss: 0.06845 valid_loss: 0.08008 test_loss: 0.09819 \n",
      "[113/200] train_loss: 0.07018 valid_loss: 0.07901 test_loss: 0.09856 \n",
      "[114/200] train_loss: 0.06773 valid_loss: 0.07781 test_loss: 0.09362 \n",
      "[115/200] train_loss: 0.06811 valid_loss: 0.07919 test_loss: 0.10331 \n",
      "[116/200] train_loss: 0.06831 valid_loss: 0.07879 test_loss: 0.10261 \n",
      "[117/200] train_loss: 0.06783 valid_loss: 0.07795 test_loss: 0.09368 \n",
      "[118/200] train_loss: 0.06823 valid_loss: 0.07825 test_loss: 0.11037 \n",
      "[119/200] train_loss: 0.06581 valid_loss: 0.07818 test_loss: 0.14236 \n",
      "[120/200] train_loss: 0.06652 valid_loss: 0.07912 test_loss: 0.10630 \n",
      "[121/200] train_loss: 0.06551 valid_loss: 0.07765 test_loss: 0.12219 \n",
      "[122/200] train_loss: 0.06741 valid_loss: 0.07734 test_loss: 0.14257 \n",
      "[123/200] train_loss: 0.06624 valid_loss: 0.07982 test_loss: 0.09950 \n",
      "[124/200] train_loss: 0.06594 valid_loss: 0.08150 test_loss: 0.14030 \n",
      "[125/200] train_loss: 0.06704 valid_loss: 0.07843 test_loss: 0.09683 \n",
      "[126/200] train_loss: 0.06512 valid_loss: 0.07945 test_loss: 0.18747 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[127/200] train_loss: 0.06641 valid_loss: 0.07984 test_loss: 0.10042 \n",
      "[128/200] train_loss: 0.06393 valid_loss: 0.07794 test_loss: 0.31683 \n",
      "[129/200] train_loss: 0.06382 valid_loss: 0.08093 test_loss: 0.10058 \n",
      "[130/200] train_loss: 0.06469 valid_loss: 0.08335 test_loss: 0.10348 \n",
      "[131/200] train_loss: 0.06566 valid_loss: 0.08024 test_loss: 0.09547 \n",
      "[132/200] train_loss: 0.06505 valid_loss: 0.08090 test_loss: 0.09557 \n",
      "[133/200] train_loss: 0.06636 valid_loss: 0.08088 test_loss: 0.09364 \n",
      "[134/200] train_loss: 0.06382 valid_loss: 0.07864 test_loss: 0.09583 \n",
      "[135/200] train_loss: 0.06310 valid_loss: 0.08144 test_loss: 0.09610 \n",
      "[136/200] train_loss: 0.06425 valid_loss: 0.08308 test_loss: 0.09676 \n",
      "[137/200] train_loss: 0.06438 valid_loss: 0.08368 test_loss: 0.09733 \n",
      "[138/200] train_loss: 0.06381 valid_loss: 0.07994 test_loss: 0.10044 \n",
      "[139/200] train_loss: 0.06405 valid_loss: 0.08084 test_loss: 0.10211 \n",
      "[140/200] train_loss: 0.06384 valid_loss: 0.08077 test_loss: 0.09723 \n",
      "[141/200] train_loss: 0.06361 valid_loss: 0.08082 test_loss: 0.09461 \n",
      "[142/200] train_loss: 0.06295 valid_loss: 0.08116 test_loss: 0.09791 \n",
      "[143/200] train_loss: 0.06489 valid_loss: 0.07953 test_loss: 0.10113 \n",
      "[144/200] train_loss: 0.06272 valid_loss: 0.07923 test_loss: 0.09733 \n",
      "[145/200] train_loss: 0.06486 valid_loss: 0.08091 test_loss: 0.09446 \n",
      "[146/200] train_loss: 0.06244 valid_loss: 0.08209 test_loss: 0.09446 \n",
      "[147/200] train_loss: 0.06301 valid_loss: 0.07631 test_loss: 0.09251 \n",
      "[148/200] train_loss: 0.06150 valid_loss: 0.07525 test_loss: 0.09528 \n",
      "[149/200] train_loss: 0.06257 valid_loss: 0.07855 test_loss: 0.09441 \n",
      "[150/200] train_loss: 0.06290 valid_loss: 0.07807 test_loss: 0.10303 \n",
      "[151/200] train_loss: 0.06286 valid_loss: 0.07839 test_loss: 0.10212 \n",
      "[152/200] train_loss: 0.06257 valid_loss: 0.07879 test_loss: 0.09655 \n",
      "[153/200] train_loss: 0.06098 valid_loss: 0.07553 test_loss: 0.11322 \n",
      "[154/200] train_loss: 0.06449 valid_loss: 0.07974 test_loss: 0.09223 \n",
      "[155/200] train_loss: 0.06243 valid_loss: 0.07562 test_loss: 0.09186 \n",
      "[156/200] train_loss: 0.06247 valid_loss: 0.07655 test_loss: 0.11143 \n",
      "[157/200] train_loss: 0.06250 valid_loss: 0.07656 test_loss: 0.09704 \n",
      "[158/200] train_loss: 0.06188 valid_loss: 0.07711 test_loss: 0.09522 \n",
      "[159/200] train_loss: 0.06164 valid_loss: 0.07915 test_loss: 0.09760 \n",
      "[160/200] train_loss: 0.06105 valid_loss: 0.07928 test_loss: 0.09747 \n",
      "[161/200] train_loss: 0.06132 valid_loss: 0.07696 test_loss: 0.10647 \n",
      "[162/200] train_loss: 0.06069 valid_loss: 0.07602 test_loss: 0.09704 \n",
      "[163/200] train_loss: 0.06213 valid_loss: 0.07961 test_loss: 0.10358 \n",
      "[164/200] train_loss: 0.06268 valid_loss: 0.07483 test_loss: 0.09224 \n",
      "[165/200] train_loss: 0.06123 valid_loss: 0.07684 test_loss: 0.09158 \n",
      "[166/200] train_loss: 0.06072 valid_loss: 0.07742 test_loss: 0.09855 \n",
      "[167/200] train_loss: 0.06108 valid_loss: 0.07902 test_loss: 0.09871 \n",
      "[168/200] train_loss: 0.05886 valid_loss: 0.07875 test_loss: 0.09185 \n",
      "[169/200] train_loss: 0.06159 valid_loss: 0.07563 test_loss: 0.09334 \n",
      "[170/200] train_loss: 0.06033 valid_loss: 0.08006 test_loss: 0.09645 \n",
      "[171/200] train_loss: 0.05934 valid_loss: 0.08086 test_loss: 0.09808 \n",
      "[172/200] train_loss: 0.06010 valid_loss: 0.07743 test_loss: 0.09891 \n",
      "[173/200] train_loss: 0.06082 valid_loss: 0.08416 test_loss: 0.10843 \n",
      "[174/200] train_loss: 0.06043 valid_loss: 0.08178 test_loss: 0.10237 \n",
      "[175/200] train_loss: 0.06145 valid_loss: 0.08482 test_loss: 0.11268 \n",
      "[176/200] train_loss: 0.05832 valid_loss: 0.08279 test_loss: 0.10773 \n",
      "[177/200] train_loss: 0.05991 valid_loss: 0.07906 test_loss: 0.09694 \n",
      "[178/200] train_loss: 0.06027 valid_loss: 0.08451 test_loss: 0.10220 \n",
      "[179/200] train_loss: 0.05938 valid_loss: 0.07952 test_loss: 0.10217 \n",
      "[180/200] train_loss: 0.05961 valid_loss: 0.08003 test_loss: 0.10162 \n",
      "[181/200] train_loss: 0.06051 valid_loss: 0.07881 test_loss: 0.10116 \n",
      "[182/200] train_loss: 0.06066 valid_loss: 0.08334 test_loss: 0.10389 \n",
      "[183/200] train_loss: 0.05798 valid_loss: 0.08420 test_loss: 0.09772 \n",
      "[184/200] train_loss: 0.05881 valid_loss: 0.08066 test_loss: 0.09914 \n",
      "[185/200] train_loss: 0.05900 valid_loss: 0.08287 test_loss: 0.10519 \n",
      "[186/200] train_loss: 0.05877 valid_loss: 0.08421 test_loss: 0.10246 \n",
      "[187/200] train_loss: 0.05832 valid_loss: 0.08126 test_loss: 0.10508 \n",
      "[188/200] train_loss: 0.05757 valid_loss: 0.07784 test_loss: 0.10095 \n",
      "[189/200] train_loss: 0.05940 valid_loss: 0.07857 test_loss: 0.09558 \n",
      "[190/200] train_loss: 0.06007 valid_loss: 0.07910 test_loss: 0.09771 \n",
      "[191/200] train_loss: 0.05931 valid_loss: 0.07769 test_loss: 0.09460 \n",
      "[192/200] train_loss: 0.05946 valid_loss: 0.07878 test_loss: 0.09949 \n",
      "[193/200] train_loss: 0.05886 valid_loss: 0.08219 test_loss: 0.10478 \n",
      "[194/200] train_loss: 0.05807 valid_loss: 0.07864 test_loss: 0.09362 \n",
      "[195/200] train_loss: 0.05865 valid_loss: 0.08293 test_loss: 0.10163 \n",
      "[196/200] train_loss: 0.05927 valid_loss: 0.08789 test_loss: 0.11189 \n",
      "[197/200] train_loss: 0.05901 valid_loss: 0.07682 test_loss: 0.09358 \n",
      "[198/200] train_loss: 0.05923 valid_loss: 0.07905 test_loss: 0.10304 \n",
      "[199/200] train_loss: 0.05889 valid_loss: 0.08308 test_loss: 0.10354 \n",
      "[200/200] train_loss: 0.05715 valid_loss: 0.08541 test_loss: 0.09997 \n",
      "TRAINING MODEL 2\n",
      "[  1/200] train_loss: 0.38904 valid_loss: 0.27992 test_loss: 0.24348 \n",
      "验证损失减少 (inf --> 0.243481). 正在保存模型...\n",
      "[  2/200] train_loss: 0.22099 valid_loss: 0.21086 test_loss: 0.16493 \n",
      "验证损失减少 (0.279923 --> 0.164927). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17049 valid_loss: 0.17018 test_loss: 0.13499 \n",
      "验证损失减少 (0.210861 --> 0.134993). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14761 valid_loss: 0.15492 test_loss: 0.12596 \n",
      "验证损失减少 (0.170184 --> 0.125958). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13883 valid_loss: 0.14355 test_loss: 0.12296 \n",
      "验证损失减少 (0.154924 --> 0.122959). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12851 valid_loss: 0.13256 test_loss: 0.11515 \n",
      "验证损失减少 (0.143547 --> 0.115154). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12478 valid_loss: 0.13307 test_loss: 0.11930 \n",
      "验证损失减少 (0.132555 --> 0.119298). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11913 valid_loss: 0.12575 test_loss: 0.11281 \n",
      "验证损失减少 (0.133065 --> 0.112814). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11801 valid_loss: 0.12067 test_loss: 0.10874 \n",
      "验证损失减少 (0.125753 --> 0.108740). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11196 valid_loss: 0.11768 test_loss: 0.11030 \n",
      "验证损失减少 (0.120672 --> 0.110303). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11035 valid_loss: 0.11495 test_loss: 0.10949 \n",
      "验证损失减少 (0.117676 --> 0.109485). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.11224 valid_loss: 0.11156 test_loss: 0.10211 \n",
      "验证损失减少 (0.114950 --> 0.102107). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10709 valid_loss: 0.11039 test_loss: 0.10283 \n",
      "验证损失减少 (0.111561 --> 0.102830). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10641 valid_loss: 0.11108 test_loss: 0.10481 \n",
      "验证损失减少 (0.110390 --> 0.104809). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10162 valid_loss: 0.10947 test_loss: 0.10585 \n",
      "验证损失减少 (0.111081 --> 0.105855). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10011 valid_loss: 0.10825 test_loss: 0.10757 \n",
      "验证损失减少 (0.109474 --> 0.107568). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.09906 valid_loss: 0.10200 test_loss: 0.09776 \n",
      "验证损失减少 (0.108250 --> 0.097758). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.09921 valid_loss: 0.10154 test_loss: 0.09954 \n",
      "验证损失减少 (0.102002 --> 0.099543). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09828 valid_loss: 0.10528 test_loss: 0.09628 \n",
      "验证损失减少 (0.101543 --> 0.096282). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09637 valid_loss: 0.10149 test_loss: 0.12901 \n",
      "[ 21/200] train_loss: 0.09484 valid_loss: 0.09979 test_loss: 0.09984 \n",
      "验证损失减少 (0.105282 --> 0.099842). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09481 valid_loss: 0.09762 test_loss: 0.09988 \n",
      "[ 23/200] train_loss: 0.09374 valid_loss: 0.09771 test_loss: 0.10536 \n",
      "[ 24/200] train_loss: 0.09258 valid_loss: 0.09884 test_loss: 0.10191 \n",
      "[ 25/200] train_loss: 0.09118 valid_loss: 0.09440 test_loss: 0.17952 \n",
      "[ 26/200] train_loss: 0.08852 valid_loss: 0.09400 test_loss: 0.10016 \n",
      "[ 27/200] train_loss: 0.09176 valid_loss: 0.09208 test_loss: 0.16279 \n",
      "[ 28/200] train_loss: 0.09005 valid_loss: 0.09031 test_loss: 0.14139 \n",
      "[ 29/200] train_loss: 0.08789 valid_loss: 0.09216 test_loss: 0.09376 \n",
      "验证损失减少 (0.099785 --> 0.093757). 正在保存模型...\n",
      "[ 30/200] train_loss: 0.08742 valid_loss: 0.09417 test_loss: 0.10199 \n",
      "[ 31/200] train_loss: 0.08655 valid_loss: 0.09208 test_loss: 0.11071 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 32/200] train_loss: 0.08688 valid_loss: 0.09030 test_loss: 0.12934 \n",
      "[ 33/200] train_loss: 0.08599 valid_loss: 0.09230 test_loss: 0.10672 \n",
      "[ 34/200] train_loss: 0.08650 valid_loss: 0.09034 test_loss: 0.10634 \n",
      "[ 35/200] train_loss: 0.08439 valid_loss: 0.09323 test_loss: 0.10694 \n",
      "[ 36/200] train_loss: 0.08223 valid_loss: 0.08967 test_loss: 0.11081 \n",
      "[ 37/200] train_loss: 0.08422 valid_loss: 0.08919 test_loss: 0.10310 \n",
      "[ 38/200] train_loss: 0.08219 valid_loss: 0.08852 test_loss: 0.10201 \n",
      "[ 39/200] train_loss: 0.08309 valid_loss: 0.08779 test_loss: 0.11586 \n",
      "[ 40/200] train_loss: 0.08222 valid_loss: 0.08670 test_loss: 0.11558 \n",
      "[ 41/200] train_loss: 0.07968 valid_loss: 0.08834 test_loss: 0.09330 \n",
      "[ 42/200] train_loss: 0.08116 valid_loss: 0.08632 test_loss: 0.38823 \n",
      "[ 43/200] train_loss: 0.08184 valid_loss: 0.08556 test_loss: 0.09317 \n",
      "[ 44/200] train_loss: 0.08182 valid_loss: 0.08586 test_loss: 0.20875 \n",
      "[ 45/200] train_loss: 0.08033 valid_loss: 0.08606 test_loss: 0.10887 \n",
      "[ 46/200] train_loss: 0.07815 valid_loss: 0.08668 test_loss: 0.09845 \n",
      "[ 47/200] train_loss: 0.07755 valid_loss: 0.08662 test_loss: 0.26438 \n",
      "[ 48/200] train_loss: 0.07770 valid_loss: 0.08525 test_loss: 0.10112 \n",
      "[ 49/200] train_loss: 0.07933 valid_loss: 0.08396 test_loss: 0.09885 \n",
      "[ 50/200] train_loss: 0.08075 valid_loss: 0.08456 test_loss: 0.09194 \n",
      "验证损失减少 (0.092163 --> 0.091944). 正在保存模型...\n",
      "[ 51/200] train_loss: 0.07793 valid_loss: 0.08485 test_loss: 0.09504 \n",
      "[ 52/200] train_loss: 0.07669 valid_loss: 0.08158 test_loss: 0.12988 \n",
      "[ 53/200] train_loss: 0.07746 valid_loss: 0.08268 test_loss: 0.22010 \n",
      "[ 54/200] train_loss: 0.07608 valid_loss: 0.08464 test_loss: 0.10034 \n",
      "[ 55/200] train_loss: 0.07830 valid_loss: 0.08379 test_loss: 0.32089 \n",
      "[ 56/200] train_loss: 0.07638 valid_loss: 0.08524 test_loss: 0.09893 \n",
      "[ 57/200] train_loss: 0.07651 valid_loss: 0.08586 test_loss: 0.09485 \n",
      "[ 58/200] train_loss: 0.07646 valid_loss: 0.08458 test_loss: 0.08976 \n",
      "[ 59/200] train_loss: 0.07535 valid_loss: 0.08579 test_loss: 0.08977 \n",
      "[ 60/200] train_loss: 0.07336 valid_loss: 0.08476 test_loss: 0.09501 \n",
      "[ 61/200] train_loss: 0.07432 valid_loss: 0.08490 test_loss: 0.09238 \n",
      "[ 62/200] train_loss: 0.07340 valid_loss: 0.08530 test_loss: 0.09509 \n",
      "[ 63/200] train_loss: 0.07113 valid_loss: 0.08436 test_loss: 0.09471 \n",
      "[ 64/200] train_loss: 0.07413 valid_loss: 0.08143 test_loss: 0.26199 \n",
      "[ 65/200] train_loss: 0.07421 valid_loss: 0.08193 test_loss: 0.48052 \n",
      "[ 66/200] train_loss: 0.07321 valid_loss: 0.08183 test_loss: 0.11325 \n",
      "[ 67/200] train_loss: 0.07519 valid_loss: 0.08473 test_loss: 0.16550 \n",
      "[ 68/200] train_loss: 0.07255 valid_loss: 0.08153 test_loss: 0.09677 \n",
      "[ 69/200] train_loss: 0.07410 valid_loss: 0.08308 test_loss: 0.09405 \n",
      "[ 70/200] train_loss: 0.07209 valid_loss: 0.08055 test_loss: 0.28886 \n",
      "[ 71/200] train_loss: 0.07106 valid_loss: 0.08041 test_loss: 0.14296 \n",
      "[ 72/200] train_loss: 0.07033 valid_loss: 0.08067 test_loss: 0.12220 \n",
      "[ 73/200] train_loss: 0.07149 valid_loss: 0.08332 test_loss: 0.12272 \n",
      "[ 74/200] train_loss: 0.07145 valid_loss: 0.08167 test_loss: 0.09170 \n",
      "[ 75/200] train_loss: 0.07083 valid_loss: 0.08170 test_loss: 0.09937 \n",
      "[ 76/200] train_loss: 0.06968 valid_loss: 0.08222 test_loss: 0.09250 \n",
      "[ 77/200] train_loss: 0.07323 valid_loss: 0.08307 test_loss: 0.13021 \n",
      "[ 78/200] train_loss: 0.07006 valid_loss: 0.08637 test_loss: 0.11877 \n",
      "[ 79/200] train_loss: 0.07135 valid_loss: 0.08324 test_loss: 0.15434 \n",
      "[ 80/200] train_loss: 0.06813 valid_loss: 0.08236 test_loss: 0.08857 \n",
      "[ 81/200] train_loss: 0.06726 valid_loss: 0.08093 test_loss: 0.09288 \n",
      "[ 82/200] train_loss: 0.07026 valid_loss: 0.08113 test_loss: 0.10045 \n",
      "[ 83/200] train_loss: 0.07159 valid_loss: 0.07936 test_loss: 0.13926 \n",
      "[ 84/200] train_loss: 0.06931 valid_loss: 0.07918 test_loss: 0.25980 \n",
      "[ 85/200] train_loss: 0.07032 valid_loss: 0.08242 test_loss: 0.08911 \n",
      "[ 86/200] train_loss: 0.06768 valid_loss: 0.08019 test_loss: 0.10262 \n",
      "[ 87/200] train_loss: 0.06787 valid_loss: 0.07985 test_loss: 0.09477 \n",
      "[ 88/200] train_loss: 0.06970 valid_loss: 0.07914 test_loss: 0.11844 \n",
      "[ 89/200] train_loss: 0.06778 valid_loss: 0.08156 test_loss: 0.09033 \n",
      "[ 90/200] train_loss: 0.06887 valid_loss: 0.07861 test_loss: 0.10207 \n",
      "[ 91/200] train_loss: 0.06887 valid_loss: 0.07852 test_loss: 0.09111 \n",
      "[ 92/200] train_loss: 0.06746 valid_loss: 0.07928 test_loss: 0.48775 \n",
      "[ 93/200] train_loss: 0.06709 valid_loss: 0.07933 test_loss: 0.10042 \n",
      "[ 94/200] train_loss: 0.07105 valid_loss: 0.08045 test_loss: 0.30759 \n",
      "[ 95/200] train_loss: 0.06658 valid_loss: 0.07944 test_loss: 0.10233 \n",
      "[ 96/200] train_loss: 0.06723 valid_loss: 0.08000 test_loss: 0.12706 \n",
      "[ 97/200] train_loss: 0.06598 valid_loss: 0.07849 test_loss: 0.14234 \n",
      "[ 98/200] train_loss: 0.06787 valid_loss: 0.07872 test_loss: 0.17238 \n",
      "[ 99/200] train_loss: 0.06452 valid_loss: 0.07821 test_loss: 0.09030 \n",
      "[100/200] train_loss: 0.06653 valid_loss: 0.08211 test_loss: 0.09157 \n",
      "[101/200] train_loss: 0.06559 valid_loss: 0.07998 test_loss: 0.09164 \n",
      "[102/200] train_loss: 0.06612 valid_loss: 0.07929 test_loss: 0.09306 \n",
      "[103/200] train_loss: 0.06562 valid_loss: 0.07838 test_loss: 0.08753 \n",
      "[104/200] train_loss: 0.06484 valid_loss: 0.07967 test_loss: 0.09084 \n",
      "[105/200] train_loss: 0.06426 valid_loss: 0.08029 test_loss: 0.11966 \n",
      "[106/200] train_loss: 0.06620 valid_loss: 0.08181 test_loss: 0.13499 \n",
      "[107/200] train_loss: 0.06550 valid_loss: 0.08085 test_loss: 0.09135 \n",
      "[108/200] train_loss: 0.06545 valid_loss: 0.08068 test_loss: 0.09266 \n",
      "[109/200] train_loss: 0.06742 valid_loss: 0.07822 test_loss: 0.09434 \n",
      "[110/200] train_loss: 0.06691 valid_loss: 0.07934 test_loss: 0.09519 \n",
      "[111/200] train_loss: 0.06526 valid_loss: 0.07844 test_loss: 0.09203 \n",
      "[112/200] train_loss: 0.06461 valid_loss: 0.07915 test_loss: 0.09291 \n",
      "[113/200] train_loss: 0.06369 valid_loss: 0.07972 test_loss: 0.09146 \n",
      "[114/200] train_loss: 0.06439 valid_loss: 0.08140 test_loss: 0.09000 \n",
      "[115/200] train_loss: 0.06393 valid_loss: 0.08231 test_loss: 0.09154 \n",
      "[116/200] train_loss: 0.06344 valid_loss: 0.08054 test_loss: 0.09320 \n",
      "[117/200] train_loss: 0.06404 valid_loss: 0.08272 test_loss: 0.09039 \n",
      "[118/200] train_loss: 0.06366 valid_loss: 0.08349 test_loss: 0.08907 \n",
      "[119/200] train_loss: 0.06242 valid_loss: 0.08291 test_loss: 0.09029 \n",
      "[120/200] train_loss: 0.06488 valid_loss: 0.08184 test_loss: 0.09378 \n",
      "[121/200] train_loss: 0.06335 valid_loss: 0.07744 test_loss: 0.09168 \n",
      "[122/200] train_loss: 0.06243 valid_loss: 0.07997 test_loss: 0.19439 \n",
      "[123/200] train_loss: 0.06310 valid_loss: 0.07709 test_loss: 0.09043 \n",
      "[124/200] train_loss: 0.06200 valid_loss: 0.07879 test_loss: 0.08791 \n",
      "[125/200] train_loss: 0.06292 valid_loss: 0.07816 test_loss: 0.09090 \n",
      "[126/200] train_loss: 0.06112 valid_loss: 0.07776 test_loss: 0.09193 \n",
      "[127/200] train_loss: 0.06463 valid_loss: 0.07878 test_loss: 0.15474 \n",
      "[128/200] train_loss: 0.06281 valid_loss: 0.07725 test_loss: 0.20544 \n",
      "[129/200] train_loss: 0.06276 valid_loss: 0.08110 test_loss: 0.09472 \n",
      "[130/200] train_loss: 0.06313 valid_loss: 0.07896 test_loss: 0.09204 \n",
      "[131/200] train_loss: 0.06100 valid_loss: 0.08006 test_loss: 0.09187 \n",
      "[132/200] train_loss: 0.06035 valid_loss: 0.07825 test_loss: 0.09757 \n",
      "[133/200] train_loss: 0.06232 valid_loss: 0.08128 test_loss: 0.09220 \n",
      "[134/200] train_loss: 0.06148 valid_loss: 0.08129 test_loss: 0.09216 \n",
      "[135/200] train_loss: 0.06066 valid_loss: 0.07979 test_loss: 0.08960 \n",
      "[136/200] train_loss: 0.06156 valid_loss: 0.07824 test_loss: 0.10017 \n",
      "[137/200] train_loss: 0.05960 valid_loss: 0.08184 test_loss: 0.10330 \n",
      "[138/200] train_loss: 0.06150 valid_loss: 0.07773 test_loss: 0.09295 \n",
      "[139/200] train_loss: 0.06023 valid_loss: 0.08139 test_loss: 0.09623 \n",
      "[140/200] train_loss: 0.05943 valid_loss: 0.07717 test_loss: 0.21272 \n",
      "[141/200] train_loss: 0.05983 valid_loss: 0.07856 test_loss: 0.10271 \n",
      "[142/200] train_loss: 0.06219 valid_loss: 0.07702 test_loss: 0.11851 \n",
      "[143/200] train_loss: 0.05989 valid_loss: 0.07812 test_loss: 0.09853 \n",
      "[144/200] train_loss: 0.05978 valid_loss: 0.07786 test_loss: 0.16117 \n",
      "[145/200] train_loss: 0.05882 valid_loss: 0.07681 test_loss: 0.24318 \n",
      "[146/200] train_loss: 0.05942 valid_loss: 0.07599 test_loss: 0.11021 \n",
      "[147/200] train_loss: 0.06059 valid_loss: 0.07742 test_loss: 0.09482 \n",
      "[148/200] train_loss: 0.05883 valid_loss: 0.07355 test_loss: 0.20773 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[149/200] train_loss: 0.06068 valid_loss: 0.07547 test_loss: 0.13416 \n",
      "[150/200] train_loss: 0.05769 valid_loss: 0.07572 test_loss: 0.10930 \n",
      "[151/200] train_loss: 0.06018 valid_loss: 0.07623 test_loss: 0.18003 \n",
      "[152/200] train_loss: 0.05955 valid_loss: 0.07750 test_loss: 0.09277 \n",
      "[153/200] train_loss: 0.05916 valid_loss: 0.07813 test_loss: 0.09323 \n",
      "[154/200] train_loss: 0.05837 valid_loss: 0.08118 test_loss: 0.09208 \n",
      "[155/200] train_loss: 0.05917 valid_loss: 0.08030 test_loss: 0.25319 \n",
      "[156/200] train_loss: 0.05867 valid_loss: 0.07559 test_loss: 0.09200 \n",
      "[157/200] train_loss: 0.05989 valid_loss: 0.07882 test_loss: 0.08833 \n",
      "[158/200] train_loss: 0.05829 valid_loss: 0.07513 test_loss: 0.28588 \n",
      "[159/200] train_loss: 0.05785 valid_loss: 0.07693 test_loss: 0.10127 \n",
      "[160/200] train_loss: 0.05946 valid_loss: 0.07679 test_loss: 0.08853 \n",
      "[161/200] train_loss: 0.05935 valid_loss: 0.07627 test_loss: 0.11449 \n",
      "[162/200] train_loss: 0.05769 valid_loss: 0.08064 test_loss: 0.09227 \n",
      "[163/200] train_loss: 0.05916 valid_loss: 0.07625 test_loss: 0.09308 \n",
      "[164/200] train_loss: 0.05594 valid_loss: 0.07789 test_loss: 0.08954 \n",
      "[165/200] train_loss: 0.05736 valid_loss: 0.07937 test_loss: 0.09595 \n",
      "[166/200] train_loss: 0.05907 valid_loss: 0.07721 test_loss: 0.09172 \n",
      "[167/200] train_loss: 0.05810 valid_loss: 0.07587 test_loss: 0.08742 \n",
      "[168/200] train_loss: 0.05715 valid_loss: 0.07702 test_loss: 0.08908 \n",
      "[169/200] train_loss: 0.05642 valid_loss: 0.08008 test_loss: 0.08947 \n",
      "[170/200] train_loss: 0.05734 valid_loss: 0.07721 test_loss: 0.08869 \n",
      "[171/200] train_loss: 0.05815 valid_loss: 0.07649 test_loss: 0.08935 \n",
      "[172/200] train_loss: 0.05621 valid_loss: 0.07826 test_loss: 0.09262 \n",
      "[173/200] train_loss: 0.05629 valid_loss: 0.07923 test_loss: 0.09317 \n",
      "[174/200] train_loss: 0.05780 valid_loss: 0.07906 test_loss: 0.08688 \n",
      "[175/200] train_loss: 0.05561 valid_loss: 0.07664 test_loss: 0.08873 \n",
      "[176/200] train_loss: 0.05778 valid_loss: 0.07995 test_loss: 0.09481 \n",
      "[177/200] train_loss: 0.05792 valid_loss: 0.07707 test_loss: 0.09128 \n",
      "[178/200] train_loss: 0.05672 valid_loss: 0.07852 test_loss: 0.09295 \n",
      "[179/200] train_loss: 0.05785 valid_loss: 0.07437 test_loss: 0.09164 \n",
      "[180/200] train_loss: 0.05735 valid_loss: 0.07330 test_loss: 0.09225 \n",
      "[181/200] train_loss: 0.05520 valid_loss: 0.07915 test_loss: 0.09300 \n",
      "[182/200] train_loss: 0.05804 valid_loss: 0.07721 test_loss: 0.09008 \n",
      "[183/200] train_loss: 0.05567 valid_loss: 0.07755 test_loss: 0.09052 \n",
      "[184/200] train_loss: 0.05593 valid_loss: 0.07701 test_loss: 0.09554 \n",
      "[185/200] train_loss: 0.05524 valid_loss: 0.07557 test_loss: 0.09256 \n",
      "[186/200] train_loss: 0.05539 valid_loss: 0.07616 test_loss: 0.09287 \n",
      "[187/200] train_loss: 0.05586 valid_loss: 0.08137 test_loss: 0.09494 \n",
      "[188/200] train_loss: 0.05611 valid_loss: 0.08185 test_loss: 0.09476 \n",
      "[189/200] train_loss: 0.05497 valid_loss: 0.07615 test_loss: 0.09015 \n",
      "[190/200] train_loss: 0.05601 valid_loss: 0.08001 test_loss: 0.09229 \n",
      "[191/200] train_loss: 0.05589 valid_loss: 0.08194 test_loss: 0.09220 \n",
      "[192/200] train_loss: 0.05679 valid_loss: 0.08348 test_loss: 0.09471 \n",
      "[193/200] train_loss: 0.05572 valid_loss: 0.08498 test_loss: 0.09781 \n",
      "[194/200] train_loss: 0.05510 valid_loss: 0.08283 test_loss: 0.09845 \n",
      "[195/200] train_loss: 0.05509 valid_loss: 0.08629 test_loss: 0.10095 \n",
      "[196/200] train_loss: 0.05727 valid_loss: 0.08426 test_loss: 0.09813 \n",
      "[197/200] train_loss: 0.05489 valid_loss: 0.08324 test_loss: 0.09505 \n",
      "[198/200] train_loss: 0.05699 valid_loss: 0.07887 test_loss: 0.09507 \n",
      "[199/200] train_loss: 0.05689 valid_loss: 0.07845 test_loss: 0.09040 \n",
      "[200/200] train_loss: 0.05430 valid_loss: 0.07839 test_loss: 0.09379 \n",
      "TRAINING MODEL 3\n",
      "[  1/200] train_loss: 0.36026 valid_loss: 0.27420 test_loss: 0.23371 \n",
      "验证损失减少 (inf --> 0.233709). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20407 valid_loss: 0.18578 test_loss: 0.14789 \n",
      "验证损失减少 (0.274205 --> 0.147887). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16142 valid_loss: 0.15856 test_loss: 0.12711 \n",
      "验证损失减少 (0.185779 --> 0.127110). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14057 valid_loss: 0.14456 test_loss: 0.11892 \n",
      "验证损失减少 (0.158565 --> 0.118916). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13504 valid_loss: 0.14105 test_loss: 0.11999 \n",
      "验证损失减少 (0.144562 --> 0.119988). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12619 valid_loss: 0.13042 test_loss: 0.11983 \n",
      "验证损失减少 (0.141054 --> 0.119830). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12110 valid_loss: 0.13358 test_loss: 0.11478 \n",
      "验证损失减少 (0.130418 --> 0.114782). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11952 valid_loss: 0.12366 test_loss: 0.10990 \n",
      "验证损失减少 (0.133583 --> 0.109898). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11510 valid_loss: 0.12244 test_loss: 0.10974 \n",
      "验证损失减少 (0.123657 --> 0.109741). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11149 valid_loss: 0.11801 test_loss: 0.11098 \n",
      "验证损失减少 (0.122440 --> 0.110983). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11031 valid_loss: 0.11596 test_loss: 0.10648 \n",
      "验证损失减少 (0.118012 --> 0.106481). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10917 valid_loss: 0.11557 test_loss: 0.10940 \n",
      "验证损失减少 (0.115959 --> 0.109397). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10829 valid_loss: 0.10835 test_loss: 0.11491 \n",
      "验证损失减少 (0.115567 --> 0.114908). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10394 valid_loss: 0.11007 test_loss: 0.11203 \n",
      "[ 15/200] train_loss: 0.10282 valid_loss: 0.10595 test_loss: 0.10947 \n",
      "[ 16/200] train_loss: 0.10334 valid_loss: 0.10572 test_loss: 0.10106 \n",
      "验证损失减少 (0.108347 --> 0.101061). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10010 valid_loss: 0.10760 test_loss: 0.10501 \n",
      "验证损失减少 (0.105715 --> 0.105011). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.09860 valid_loss: 0.10374 test_loss: 0.10332 \n",
      "验证损失减少 (0.107598 --> 0.103315). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09725 valid_loss: 0.10281 test_loss: 0.10295 \n",
      "验证损失减少 (0.103739 --> 0.102955). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09643 valid_loss: 0.09982 test_loss: 0.10116 \n",
      "验证损失减少 (0.102812 --> 0.101162). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09597 valid_loss: 0.10247 test_loss: 0.10649 \n",
      "[ 22/200] train_loss: 0.09232 valid_loss: 0.10081 test_loss: 0.10279 \n",
      "[ 23/200] train_loss: 0.09445 valid_loss: 0.09786 test_loss: 0.09830 \n",
      "验证损失减少 (0.099825 --> 0.098302). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.09394 valid_loss: 0.09805 test_loss: 0.10011 \n",
      "[ 25/200] train_loss: 0.09373 valid_loss: 0.09872 test_loss: 0.09857 \n",
      "[ 26/200] train_loss: 0.09224 valid_loss: 0.09654 test_loss: 0.09970 \n",
      "[ 27/200] train_loss: 0.09286 valid_loss: 0.09435 test_loss: 0.10392 \n",
      "[ 28/200] train_loss: 0.08962 valid_loss: 0.09545 test_loss: 0.10178 \n",
      "[ 29/200] train_loss: 0.08962 valid_loss: 0.09716 test_loss: 0.10639 \n",
      "[ 30/200] train_loss: 0.08734 valid_loss: 0.09939 test_loss: 0.10664 \n",
      "[ 31/200] train_loss: 0.08768 valid_loss: 0.09296 test_loss: 0.09647 \n",
      "验证损失减少 (0.097857 --> 0.096470). 正在保存模型...\n",
      "[ 32/200] train_loss: 0.08834 valid_loss: 0.09279 test_loss: 0.09869 \n",
      "[ 33/200] train_loss: 0.08640 valid_loss: 0.09555 test_loss: 0.11879 \n",
      "[ 34/200] train_loss: 0.08678 valid_loss: 0.09297 test_loss: 0.12059 \n",
      "[ 35/200] train_loss: 0.08766 valid_loss: 0.09068 test_loss: 0.10117 \n",
      "[ 36/200] train_loss: 0.08640 valid_loss: 0.09175 test_loss: 0.10730 \n",
      "[ 37/200] train_loss: 0.08549 valid_loss: 0.09208 test_loss: 0.10296 \n",
      "[ 38/200] train_loss: 0.08415 valid_loss: 0.09138 test_loss: 0.10207 \n",
      "[ 39/200] train_loss: 0.08377 valid_loss: 0.09233 test_loss: 0.11697 \n",
      "[ 40/200] train_loss: 0.08354 valid_loss: 0.09064 test_loss: 0.12602 \n",
      "[ 41/200] train_loss: 0.08230 valid_loss: 0.09109 test_loss: 0.09891 \n",
      "[ 42/200] train_loss: 0.08366 valid_loss: 0.08924 test_loss: 0.09441 \n",
      "[ 43/200] train_loss: 0.08187 valid_loss: 0.09017 test_loss: 0.09865 \n",
      "[ 44/200] train_loss: 0.08346 valid_loss: 0.08822 test_loss: 0.13645 \n",
      "[ 45/200] train_loss: 0.08191 valid_loss: 0.08925 test_loss: 0.09713 \n",
      "[ 46/200] train_loss: 0.08205 valid_loss: 0.09003 test_loss: 0.09850 \n",
      "[ 47/200] train_loss: 0.08195 valid_loss: 0.08930 test_loss: 0.10015 \n",
      "[ 48/200] train_loss: 0.07833 valid_loss: 0.08812 test_loss: 0.09689 \n",
      "[ 49/200] train_loss: 0.07933 valid_loss: 0.08738 test_loss: 0.09606 \n",
      "[ 50/200] train_loss: 0.08003 valid_loss: 0.08778 test_loss: 0.24589 \n",
      "[ 51/200] train_loss: 0.08080 valid_loss: 0.08867 test_loss: 0.09500 \n",
      "[ 52/200] train_loss: 0.08069 valid_loss: 0.08438 test_loss: 0.22916 \n",
      "[ 53/200] train_loss: 0.07911 valid_loss: 0.08537 test_loss: 0.20755 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 54/200] train_loss: 0.07879 valid_loss: 0.08766 test_loss: 0.17525 \n",
      "[ 55/200] train_loss: 0.07776 valid_loss: 0.08658 test_loss: 0.13379 \n",
      "[ 56/200] train_loss: 0.07805 valid_loss: 0.08403 test_loss: 0.21735 \n",
      "[ 57/200] train_loss: 0.07677 valid_loss: 0.08633 test_loss: 0.10363 \n",
      "[ 58/200] train_loss: 0.07667 valid_loss: 0.08540 test_loss: 0.23581 \n",
      "[ 59/200] train_loss: 0.07591 valid_loss: 0.08751 test_loss: 0.19418 \n",
      "[ 60/200] train_loss: 0.07610 valid_loss: 0.08445 test_loss: 0.30742 \n",
      "[ 61/200] train_loss: 0.07597 valid_loss: 0.08457 test_loss: 0.10869 \n",
      "[ 62/200] train_loss: 0.07831 valid_loss: 0.08371 test_loss: 0.09267 \n",
      "验证损失减少 (0.092961 --> 0.092670). 正在保存模型...\n",
      "[ 63/200] train_loss: 0.07564 valid_loss: 0.08483 test_loss: 0.09257 \n",
      "[ 64/200] train_loss: 0.07550 valid_loss: 0.08416 test_loss: 0.09022 \n",
      "[ 65/200] train_loss: 0.07637 valid_loss: 0.08463 test_loss: 0.09008 \n",
      "[ 66/200] train_loss: 0.07576 valid_loss: 0.08442 test_loss: 0.10997 \n",
      "[ 67/200] train_loss: 0.07463 valid_loss: 0.08347 test_loss: 0.09096 \n",
      "[ 68/200] train_loss: 0.07396 valid_loss: 0.08571 test_loss: 0.09012 \n",
      "[ 69/200] train_loss: 0.07476 valid_loss: 0.08632 test_loss: 0.15302 \n",
      "[ 70/200] train_loss: 0.07349 valid_loss: 0.08535 test_loss: 0.20152 \n",
      "[ 71/200] train_loss: 0.07499 valid_loss: 0.08363 test_loss: 0.11822 \n",
      "[ 72/200] train_loss: 0.07433 valid_loss: 0.08297 test_loss: 0.18308 \n",
      "[ 73/200] train_loss: 0.07387 valid_loss: 0.08398 test_loss: 0.11726 \n",
      "[ 74/200] train_loss: 0.07214 valid_loss: 0.08267 test_loss: 0.10310 \n",
      "[ 75/200] train_loss: 0.07320 valid_loss: 0.08355 test_loss: 0.09793 \n",
      "[ 76/200] train_loss: 0.07493 valid_loss: 0.08422 test_loss: 0.08884 \n",
      "[ 77/200] train_loss: 0.07339 valid_loss: 0.08304 test_loss: 0.10768 \n",
      "[ 78/200] train_loss: 0.07113 valid_loss: 0.08280 test_loss: 0.10595 \n",
      "[ 79/200] train_loss: 0.07207 valid_loss: 0.08355 test_loss: 0.14588 \n",
      "[ 80/200] train_loss: 0.07290 valid_loss: 0.08269 test_loss: 0.39919 \n",
      "[ 81/200] train_loss: 0.07163 valid_loss: 0.08196 test_loss: 0.19695 \n",
      "[ 82/200] train_loss: 0.07214 valid_loss: 0.08285 test_loss: 0.09243 \n",
      "[ 83/200] train_loss: 0.07225 valid_loss: 0.08281 test_loss: 0.09994 \n",
      "[ 84/200] train_loss: 0.06955 valid_loss: 0.08273 test_loss: 0.09416 \n",
      "[ 85/200] train_loss: 0.07141 valid_loss: 0.08265 test_loss: 0.09439 \n",
      "[ 86/200] train_loss: 0.06932 valid_loss: 0.08203 test_loss: 0.11579 \n",
      "[ 87/200] train_loss: 0.06783 valid_loss: 0.08197 test_loss: 0.09006 \n",
      "[ 88/200] train_loss: 0.07125 valid_loss: 0.08168 test_loss: 0.15770 \n",
      "[ 89/200] train_loss: 0.06946 valid_loss: 0.08097 test_loss: 0.27015 \n",
      "[ 90/200] train_loss: 0.07003 valid_loss: 0.08207 test_loss: 0.49767 \n",
      "[ 91/200] train_loss: 0.07007 valid_loss: 0.07998 test_loss: 0.33603 \n",
      "[ 92/200] train_loss: 0.06831 valid_loss: 0.08149 test_loss: 0.09968 \n",
      "[ 93/200] train_loss: 0.06789 valid_loss: 0.07955 test_loss: 0.18691 \n",
      "[ 94/200] train_loss: 0.06951 valid_loss: 0.08038 test_loss: 0.16807 \n",
      "[ 95/200] train_loss: 0.06957 valid_loss: 0.08136 test_loss: 0.32816 \n",
      "[ 96/200] train_loss: 0.06719 valid_loss: 0.07899 test_loss: 0.09736 \n",
      "[ 97/200] train_loss: 0.06791 valid_loss: 0.07927 test_loss: 0.26199 \n",
      "[ 98/200] train_loss: 0.06903 valid_loss: 0.08225 test_loss: 0.21617 \n",
      "[ 99/200] train_loss: 0.06598 valid_loss: 0.08082 test_loss: 0.17697 \n",
      "[100/200] train_loss: 0.06672 valid_loss: 0.08334 test_loss: 0.09147 \n",
      "[101/200] train_loss: 0.06803 valid_loss: 0.08122 test_loss: 0.09209 \n",
      "[102/200] train_loss: 0.06778 valid_loss: 0.08203 test_loss: 0.09054 \n",
      "[103/200] train_loss: 0.06813 valid_loss: 0.08071 test_loss: 0.09007 \n",
      "[104/200] train_loss: 0.06760 valid_loss: 0.08043 test_loss: 0.09288 \n",
      "[105/200] train_loss: 0.06919 valid_loss: 0.08015 test_loss: 0.09418 \n",
      "[106/200] train_loss: 0.06882 valid_loss: 0.07997 test_loss: 0.09523 \n",
      "[107/200] train_loss: 0.06843 valid_loss: 0.08022 test_loss: 0.09674 \n",
      "[108/200] train_loss: 0.06598 valid_loss: 0.07889 test_loss: 0.09038 \n",
      "[109/200] train_loss: 0.06576 valid_loss: 0.08223 test_loss: 0.09410 \n",
      "[110/200] train_loss: 0.06476 valid_loss: 0.08120 test_loss: 0.09243 \n",
      "[111/200] train_loss: 0.06770 valid_loss: 0.08202 test_loss: 0.09115 \n",
      "[112/200] train_loss: 0.06615 valid_loss: 0.08046 test_loss: 0.08820 \n",
      "[113/200] train_loss: 0.06572 valid_loss: 0.07918 test_loss: 0.14595 \n",
      "[114/200] train_loss: 0.06566 valid_loss: 0.07842 test_loss: 0.09024 \n",
      "[115/200] train_loss: 0.06363 valid_loss: 0.08113 test_loss: 0.09181 \n",
      "[116/200] train_loss: 0.06430 valid_loss: 0.07951 test_loss: 0.17714 \n",
      "[117/200] train_loss: 0.06536 valid_loss: 0.07956 test_loss: 0.08899 \n",
      "[118/200] train_loss: 0.06405 valid_loss: 0.07735 test_loss: 0.09104 \n",
      "[119/200] train_loss: 0.06587 valid_loss: 0.08015 test_loss: 0.08921 \n",
      "[120/200] train_loss: 0.06466 valid_loss: 0.07911 test_loss: 0.09150 \n",
      "[121/200] train_loss: 0.06201 valid_loss: 0.07718 test_loss: 0.08716 \n",
      "[122/200] train_loss: 0.06380 valid_loss: 0.07945 test_loss: 0.09162 \n",
      "[123/200] train_loss: 0.06578 valid_loss: 0.07953 test_loss: 0.09273 \n",
      "[124/200] train_loss: 0.06516 valid_loss: 0.07932 test_loss: 0.09373 \n",
      "[125/200] train_loss: 0.06395 valid_loss: 0.07816 test_loss: 0.08738 \n",
      "[126/200] train_loss: 0.06293 valid_loss: 0.07892 test_loss: 0.09521 \n",
      "[127/200] train_loss: 0.06313 valid_loss: 0.07799 test_loss: 0.08831 \n",
      "[128/200] train_loss: 0.06516 valid_loss: 0.07855 test_loss: 0.32934 \n",
      "[129/200] train_loss: 0.06281 valid_loss: 0.07962 test_loss: 0.09124 \n",
      "[130/200] train_loss: 0.06351 valid_loss: 0.07995 test_loss: 0.11913 \n",
      "[131/200] train_loss: 0.06191 valid_loss: 0.07829 test_loss: 0.20333 \n",
      "[132/200] train_loss: 0.06358 valid_loss: 0.08079 test_loss: 0.10910 \n",
      "[133/200] train_loss: 0.06422 valid_loss: 0.07878 test_loss: 0.13114 \n",
      "[134/200] train_loss: 0.06214 valid_loss: 0.07886 test_loss: 0.14659 \n",
      "[135/200] train_loss: 0.06213 valid_loss: 0.07741 test_loss: 0.12198 \n",
      "[136/200] train_loss: 0.06365 valid_loss: 0.07771 test_loss: 0.31989 \n",
      "[137/200] train_loss: 0.06169 valid_loss: 0.07746 test_loss: 0.27619 \n",
      "[138/200] train_loss: 0.06494 valid_loss: 0.07769 test_loss: 0.58491 \n",
      "[139/200] train_loss: 0.06216 valid_loss: 0.07950 test_loss: 0.25636 \n",
      "[140/200] train_loss: 0.06256 valid_loss: 0.07707 test_loss: 0.13587 \n",
      "[141/200] train_loss: 0.06233 valid_loss: 0.07733 test_loss: 0.09696 \n",
      "[142/200] train_loss: 0.06063 valid_loss: 0.07629 test_loss: 0.08974 \n",
      "[143/200] train_loss: 0.06124 valid_loss: 0.07903 test_loss: 0.15857 \n",
      "[144/200] train_loss: 0.06126 valid_loss: 0.07808 test_loss: 0.10829 \n",
      "[145/200] train_loss: 0.06047 valid_loss: 0.07999 test_loss: 0.45133 \n",
      "[146/200] train_loss: 0.06102 valid_loss: 0.07824 test_loss: 0.25709 \n",
      "[147/200] train_loss: 0.05996 valid_loss: 0.07870 test_loss: 0.12236 \n",
      "[148/200] train_loss: 0.05996 valid_loss: 0.07959 test_loss: 0.22717 \n",
      "[149/200] train_loss: 0.06317 valid_loss: 0.08070 test_loss: 0.10228 \n",
      "[150/200] train_loss: 0.06042 valid_loss: 0.07839 test_loss: 0.11619 \n",
      "[151/200] train_loss: 0.06195 valid_loss: 0.07755 test_loss: 0.08685 \n",
      "[152/200] train_loss: 0.06206 valid_loss: 0.07769 test_loss: 0.09829 \n",
      "[153/200] train_loss: 0.06036 valid_loss: 0.07923 test_loss: 0.09165 \n",
      "[154/200] train_loss: 0.05926 valid_loss: 0.07771 test_loss: 0.09182 \n",
      "[155/200] train_loss: 0.06165 valid_loss: 0.07678 test_loss: 0.09573 \n",
      "[156/200] train_loss: 0.06080 valid_loss: 0.07700 test_loss: 0.09327 \n",
      "[157/200] train_loss: 0.06015 valid_loss: 0.07878 test_loss: 0.14431 \n",
      "[158/200] train_loss: 0.06060 valid_loss: 0.07818 test_loss: 0.09317 \n",
      "[159/200] train_loss: 0.05864 valid_loss: 0.07657 test_loss: 0.13275 \n",
      "[160/200] train_loss: 0.05933 valid_loss: 0.07706 test_loss: 0.10013 \n",
      "[161/200] train_loss: 0.05962 valid_loss: 0.07741 test_loss: 0.09175 \n",
      "[162/200] train_loss: 0.06029 valid_loss: 0.07806 test_loss: 0.09067 \n",
      "[163/200] train_loss: 0.05797 valid_loss: 0.07858 test_loss: 0.10474 \n",
      "[164/200] train_loss: 0.05983 valid_loss: 0.07936 test_loss: 0.09060 \n",
      "[165/200] train_loss: 0.05946 valid_loss: 0.07943 test_loss: 0.09018 \n",
      "[166/200] train_loss: 0.05861 valid_loss: 0.08003 test_loss: 0.18048 \n",
      "[167/200] train_loss: 0.06002 valid_loss: 0.07870 test_loss: 0.12596 \n",
      "[168/200] train_loss: 0.06062 valid_loss: 0.07749 test_loss: 0.09587 \n",
      "[169/200] train_loss: 0.05765 valid_loss: 0.07641 test_loss: 0.43726 \n",
      "[170/200] train_loss: 0.05854 valid_loss: 0.07769 test_loss: 0.09391 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[171/200] train_loss: 0.05864 valid_loss: 0.07654 test_loss: 0.65782 \n",
      "[172/200] train_loss: 0.05843 valid_loss: 0.07749 test_loss: 0.12812 \n",
      "[173/200] train_loss: 0.05875 valid_loss: 0.07832 test_loss: 0.09085 \n",
      "[174/200] train_loss: 0.05953 valid_loss: 0.07746 test_loss: 0.09365 \n",
      "[175/200] train_loss: 0.05771 valid_loss: 0.07864 test_loss: 0.09762 \n",
      "[176/200] train_loss: 0.05845 valid_loss: 0.07787 test_loss: 0.09001 \n",
      "[177/200] train_loss: 0.05747 valid_loss: 0.07932 test_loss: 0.52696 \n",
      "[178/200] train_loss: 0.05612 valid_loss: 0.07952 test_loss: 0.09317 \n",
      "[179/200] train_loss: 0.05788 valid_loss: 0.07769 test_loss: 0.09500 \n",
      "[180/200] train_loss: 0.05857 valid_loss: 0.07984 test_loss: 0.09533 \n",
      "[181/200] train_loss: 0.05808 valid_loss: 0.07794 test_loss: 0.09726 \n",
      "[182/200] train_loss: 0.05789 valid_loss: 0.07823 test_loss: 0.09327 \n",
      "[183/200] train_loss: 0.05836 valid_loss: 0.07869 test_loss: 0.09114 \n",
      "[184/200] train_loss: 0.05671 valid_loss: 0.07944 test_loss: 0.09131 \n",
      "[185/200] train_loss: 0.05946 valid_loss: 0.07798 test_loss: 0.08977 \n",
      "[186/200] train_loss: 0.05715 valid_loss: 0.07922 test_loss: 0.10459 \n",
      "[187/200] train_loss: 0.05859 valid_loss: 0.07659 test_loss: 0.09042 \n",
      "[188/200] train_loss: 0.05700 valid_loss: 0.07632 test_loss: 0.09511 \n",
      "[189/200] train_loss: 0.05615 valid_loss: 0.07821 test_loss: 0.10003 \n",
      "[190/200] train_loss: 0.05605 valid_loss: 0.07669 test_loss: 0.10980 \n",
      "[191/200] train_loss: 0.05682 valid_loss: 0.07666 test_loss: 0.12551 \n",
      "[192/200] train_loss: 0.05541 valid_loss: 0.07811 test_loss: 0.15211 \n",
      "[193/200] train_loss: 0.05707 valid_loss: 0.07788 test_loss: 0.15772 \n",
      "[194/200] train_loss: 0.05751 valid_loss: 0.07699 test_loss: 0.10604 \n",
      "[195/200] train_loss: 0.05669 valid_loss: 0.07757 test_loss: 0.09806 \n",
      "[196/200] train_loss: 0.05593 valid_loss: 0.07580 test_loss: 0.09150 \n",
      "[197/200] train_loss: 0.05593 valid_loss: 0.07806 test_loss: 0.10269 \n",
      "[198/200] train_loss: 0.05685 valid_loss: 0.07944 test_loss: 0.09459 \n",
      "[199/200] train_loss: 0.05627 valid_loss: 0.07797 test_loss: 0.12390 \n",
      "[200/200] train_loss: 0.05528 valid_loss: 0.07908 test_loss: 0.11186 \n",
      "TRAINING MODEL 4\n",
      "[  1/200] train_loss: 0.41071 valid_loss: 0.29408 test_loss: 0.26497 \n",
      "验证损失减少 (inf --> 0.264973). 正在保存模型...\n",
      "[  2/200] train_loss: 0.23209 valid_loss: 0.20573 test_loss: 0.16062 \n",
      "验证损失减少 (0.294084 --> 0.160615). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17285 valid_loss: 0.16867 test_loss: 0.13237 \n",
      "验证损失减少 (0.205733 --> 0.132371). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14907 valid_loss: 0.15530 test_loss: 0.12137 \n",
      "验证损失减少 (0.168668 --> 0.121365). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13677 valid_loss: 0.13774 test_loss: 0.11437 \n",
      "验证损失减少 (0.155305 --> 0.114374). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13090 valid_loss: 0.13003 test_loss: 0.11152 \n",
      "验证损失减少 (0.137736 --> 0.111524). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12285 valid_loss: 0.12644 test_loss: 0.10944 \n",
      "验证损失减少 (0.130031 --> 0.109445). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11893 valid_loss: 0.12470 test_loss: 0.11278 \n",
      "验证损失减少 (0.126437 --> 0.112780). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11754 valid_loss: 0.11833 test_loss: 0.10688 \n",
      "验证损失减少 (0.124704 --> 0.106876). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11198 valid_loss: 0.11467 test_loss: 0.10564 \n",
      "验证损失减少 (0.118330 --> 0.105639). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.10889 valid_loss: 0.11903 test_loss: 0.10887 \n",
      "验证损失减少 (0.114672 --> 0.108868). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10850 valid_loss: 0.11256 test_loss: 0.10149 \n",
      "验证损失减少 (0.119030 --> 0.101490). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10761 valid_loss: 0.11228 test_loss: 0.10450 \n",
      "验证损失减少 (0.112561 --> 0.104500). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10657 valid_loss: 0.11044 test_loss: 0.10624 \n",
      "验证损失减少 (0.112276 --> 0.106244). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10448 valid_loss: 0.10904 test_loss: 0.10271 \n",
      "验证损失减少 (0.110442 --> 0.102710). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10219 valid_loss: 0.10690 test_loss: 0.10194 \n",
      "验证损失减少 (0.109041 --> 0.101942). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.09953 valid_loss: 0.10517 test_loss: 0.10701 \n",
      "[ 18/200] train_loss: 0.09893 valid_loss: 0.10383 test_loss: 0.11193 \n",
      "[ 19/200] train_loss: 0.10014 valid_loss: 0.10059 test_loss: 0.09919 \n",
      "验证损失减少 (0.106898 --> 0.099194). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09759 valid_loss: 0.10758 test_loss: 0.10579 \n",
      "[ 21/200] train_loss: 0.09779 valid_loss: 0.09851 test_loss: 0.10379 \n",
      "[ 22/200] train_loss: 0.09551 valid_loss: 0.10022 test_loss: 0.10064 \n",
      "[ 23/200] train_loss: 0.09277 valid_loss: 0.10079 test_loss: 0.09702 \n",
      "验证损失减少 (0.100590 --> 0.097021). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.09410 valid_loss: 0.09869 test_loss: 0.11683 \n",
      "[ 25/200] train_loss: 0.09387 valid_loss: 0.09873 test_loss: 0.11170 \n",
      "[ 26/200] train_loss: 0.09321 valid_loss: 0.09845 test_loss: 0.09605 \n",
      "验证损失减少 (0.100786 --> 0.096048). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.09159 valid_loss: 0.10118 test_loss: 0.10435 \n",
      "[ 28/200] train_loss: 0.09073 valid_loss: 0.09601 test_loss: 0.09775 \n",
      "验证损失减少 (0.098451 --> 0.097747). 正在保存模型...\n",
      "[ 29/200] train_loss: 0.09036 valid_loss: 0.09372 test_loss: 0.09493 \n",
      "验证损失减少 (0.096010 --> 0.094927). 正在保存模型...\n",
      "[ 30/200] train_loss: 0.08997 valid_loss: 0.09440 test_loss: 0.16718 \n",
      "[ 31/200] train_loss: 0.08856 valid_loss: 0.09401 test_loss: 0.09457 \n",
      "[ 32/200] train_loss: 0.08986 valid_loss: 0.09588 test_loss: 0.09852 \n",
      "[ 33/200] train_loss: 0.08866 valid_loss: 0.09313 test_loss: 0.09756 \n",
      "[ 34/200] train_loss: 0.08859 valid_loss: 0.09425 test_loss: 0.09481 \n",
      "[ 35/200] train_loss: 0.08690 valid_loss: 0.09483 test_loss: 0.09859 \n",
      "[ 36/200] train_loss: 0.08533 valid_loss: 0.09136 test_loss: 0.10168 \n",
      "[ 37/200] train_loss: 0.08651 valid_loss: 0.09117 test_loss: 0.13536 \n",
      "[ 38/200] train_loss: 0.08515 valid_loss: 0.09394 test_loss: 0.12447 \n",
      "[ 39/200] train_loss: 0.08480 valid_loss: 0.09047 test_loss: 0.09780 \n",
      "[ 40/200] train_loss: 0.08492 valid_loss: 0.09157 test_loss: 0.09526 \n",
      "[ 41/200] train_loss: 0.08356 valid_loss: 0.08970 test_loss: 0.09764 \n",
      "[ 42/200] train_loss: 0.08261 valid_loss: 0.08983 test_loss: 0.11568 \n",
      "[ 43/200] train_loss: 0.08382 valid_loss: 0.08921 test_loss: 0.11326 \n",
      "[ 44/200] train_loss: 0.08095 valid_loss: 0.08811 test_loss: 0.09927 \n",
      "[ 45/200] train_loss: 0.08282 valid_loss: 0.09074 test_loss: 0.10769 \n",
      "[ 46/200] train_loss: 0.08292 valid_loss: 0.08837 test_loss: 0.16353 \n",
      "[ 47/200] train_loss: 0.08349 valid_loss: 0.08844 test_loss: 0.21069 \n",
      "[ 48/200] train_loss: 0.08311 valid_loss: 0.08870 test_loss: 0.09363 \n",
      "验证损失减少 (0.093716 --> 0.093627). 正在保存模型...\n",
      "[ 49/200] train_loss: 0.08027 valid_loss: 0.08836 test_loss: 0.09209 \n",
      "[ 50/200] train_loss: 0.07962 valid_loss: 0.08977 test_loss: 0.09498 \n",
      "[ 51/200] train_loss: 0.08235 valid_loss: 0.08861 test_loss: 0.10193 \n",
      "[ 52/200] train_loss: 0.07949 valid_loss: 0.08697 test_loss: 0.09249 \n",
      "[ 53/200] train_loss: 0.07789 valid_loss: 0.08865 test_loss: 0.17427 \n",
      "[ 54/200] train_loss: 0.08001 valid_loss: 0.08687 test_loss: 0.17398 \n",
      "[ 55/200] train_loss: 0.07812 valid_loss: 0.08542 test_loss: 0.15920 \n",
      "[ 56/200] train_loss: 0.07837 valid_loss: 0.08436 test_loss: 0.15309 \n",
      "[ 57/200] train_loss: 0.08014 valid_loss: 0.08834 test_loss: 0.09420 \n",
      "[ 58/200] train_loss: 0.07837 valid_loss: 0.08422 test_loss: 0.09468 \n",
      "[ 59/200] train_loss: 0.07387 valid_loss: 0.08455 test_loss: 0.09432 \n",
      "[ 60/200] train_loss: 0.07986 valid_loss: 0.08747 test_loss: 0.20728 \n",
      "[ 61/200] train_loss: 0.07662 valid_loss: 0.08572 test_loss: 0.14094 \n",
      "[ 62/200] train_loss: 0.07965 valid_loss: 0.08464 test_loss: 0.09377 \n",
      "[ 63/200] train_loss: 0.07849 valid_loss: 0.08284 test_loss: 0.26513 \n",
      "[ 64/200] train_loss: 0.07749 valid_loss: 0.08505 test_loss: 0.09346 \n",
      "[ 65/200] train_loss: 0.07580 valid_loss: 0.08450 test_loss: 0.14656 \n",
      "[ 66/200] train_loss: 0.07532 valid_loss: 0.08269 test_loss: 0.11303 \n",
      "[ 67/200] train_loss: 0.07400 valid_loss: 0.08276 test_loss: 0.09225 \n",
      "[ 68/200] train_loss: 0.07610 valid_loss: 0.08379 test_loss: 0.09604 \n",
      "[ 69/200] train_loss: 0.07290 valid_loss: 0.08348 test_loss: 0.09309 \n",
      "[ 70/200] train_loss: 0.07322 valid_loss: 0.08317 test_loss: 0.12187 \n",
      "[ 71/200] train_loss: 0.07386 valid_loss: 0.08237 test_loss: 0.09468 \n",
      "[ 72/200] train_loss: 0.07514 valid_loss: 0.08355 test_loss: 0.09274 \n",
      "[ 73/200] train_loss: 0.07391 valid_loss: 0.08328 test_loss: 0.10731 \n",
      "[ 74/200] train_loss: 0.07287 valid_loss: 0.08307 test_loss: 0.15793 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 75/200] train_loss: 0.07173 valid_loss: 0.08364 test_loss: 0.09541 \n",
      "[ 76/200] train_loss: 0.07431 valid_loss: 0.08319 test_loss: 0.09325 \n",
      "[ 77/200] train_loss: 0.07476 valid_loss: 0.08253 test_loss: 0.09230 \n",
      "[ 78/200] train_loss: 0.07290 valid_loss: 0.08015 test_loss: 0.09973 \n",
      "[ 79/200] train_loss: 0.07167 valid_loss: 0.08368 test_loss: 0.12292 \n",
      "[ 80/200] train_loss: 0.07355 valid_loss: 0.08130 test_loss: 0.10203 \n",
      "[ 81/200] train_loss: 0.07296 valid_loss: 0.08305 test_loss: 0.21157 \n",
      "[ 82/200] train_loss: 0.07225 valid_loss: 0.08326 test_loss: 0.10400 \n",
      "[ 83/200] train_loss: 0.07151 valid_loss: 0.07997 test_loss: 0.09189 \n",
      "[ 84/200] train_loss: 0.07209 valid_loss: 0.08314 test_loss: 0.28450 \n",
      "[ 85/200] train_loss: 0.07132 valid_loss: 0.08115 test_loss: 0.08985 \n",
      "[ 86/200] train_loss: 0.07028 valid_loss: 0.08060 test_loss: 0.15216 \n",
      "[ 87/200] train_loss: 0.07137 valid_loss: 0.08037 test_loss: 0.13794 \n",
      "[ 88/200] train_loss: 0.07065 valid_loss: 0.08080 test_loss: 0.11905 \n",
      "[ 89/200] train_loss: 0.07168 valid_loss: 0.08168 test_loss: 0.09554 \n",
      "[ 90/200] train_loss: 0.06944 valid_loss: 0.08038 test_loss: 0.09146 \n",
      "[ 91/200] train_loss: 0.06938 valid_loss: 0.08186 test_loss: 0.09443 \n",
      "[ 92/200] train_loss: 0.07040 valid_loss: 0.07974 test_loss: 0.09220 \n",
      "[ 93/200] train_loss: 0.07066 valid_loss: 0.08145 test_loss: 0.12576 \n",
      "[ 94/200] train_loss: 0.07057 valid_loss: 0.08019 test_loss: 0.09173 \n",
      "[ 95/200] train_loss: 0.07016 valid_loss: 0.07950 test_loss: 0.09887 \n",
      "[ 96/200] train_loss: 0.06945 valid_loss: 0.07992 test_loss: 0.09645 \n",
      "[ 97/200] train_loss: 0.06887 valid_loss: 0.08053 test_loss: 0.09523 \n",
      "[ 98/200] train_loss: 0.07024 valid_loss: 0.08074 test_loss: 0.09448 \n",
      "[ 99/200] train_loss: 0.06881 valid_loss: 0.08175 test_loss: 0.09392 \n",
      "[100/200] train_loss: 0.06808 valid_loss: 0.08177 test_loss: 0.09431 \n",
      "[101/200] train_loss: 0.06704 valid_loss: 0.08045 test_loss: 0.09253 \n",
      "[102/200] train_loss: 0.06618 valid_loss: 0.08068 test_loss: 0.09511 \n",
      "[103/200] train_loss: 0.06824 valid_loss: 0.07965 test_loss: 0.09404 \n",
      "[104/200] train_loss: 0.06667 valid_loss: 0.08081 test_loss: 0.09469 \n",
      "[105/200] train_loss: 0.06706 valid_loss: 0.07960 test_loss: 0.09461 \n",
      "[106/200] train_loss: 0.06845 valid_loss: 0.08063 test_loss: 0.09464 \n",
      "[107/200] train_loss: 0.06734 valid_loss: 0.07862 test_loss: 0.09549 \n",
      "[108/200] train_loss: 0.06647 valid_loss: 0.08012 test_loss: 0.11231 \n",
      "[109/200] train_loss: 0.06542 valid_loss: 0.08170 test_loss: 0.09524 \n",
      "[110/200] train_loss: 0.06667 valid_loss: 0.07950 test_loss: 0.30191 \n",
      "[111/200] train_loss: 0.06539 valid_loss: 0.07830 test_loss: 0.33619 \n",
      "[112/200] train_loss: 0.06502 valid_loss: 0.08028 test_loss: 0.31482 \n",
      "[113/200] train_loss: 0.06431 valid_loss: 0.07940 test_loss: 0.10982 \n",
      "[114/200] train_loss: 0.06741 valid_loss: 0.07750 test_loss: 0.12428 \n",
      "[115/200] train_loss: 0.06794 valid_loss: 0.07786 test_loss: 0.09483 \n",
      "[116/200] train_loss: 0.06515 valid_loss: 0.07812 test_loss: 0.09594 \n",
      "[117/200] train_loss: 0.06551 valid_loss: 0.07793 test_loss: 0.09052 \n",
      "[118/200] train_loss: 0.06590 valid_loss: 0.07809 test_loss: 0.10852 \n",
      "[119/200] train_loss: 0.06703 valid_loss: 0.07846 test_loss: 0.09585 \n",
      "[120/200] train_loss: 0.06603 valid_loss: 0.07883 test_loss: 0.10491 \n",
      "[121/200] train_loss: 0.06520 valid_loss: 0.07850 test_loss: 0.24222 \n",
      "[122/200] train_loss: 0.06507 valid_loss: 0.08052 test_loss: 0.12347 \n",
      "[123/200] train_loss: 0.06512 valid_loss: 0.07766 test_loss: 0.11103 \n",
      "[124/200] train_loss: 0.06570 valid_loss: 0.07975 test_loss: 0.09517 \n",
      "[125/200] train_loss: 0.06374 valid_loss: 0.07859 test_loss: 0.09113 \n",
      "[126/200] train_loss: 0.06643 valid_loss: 0.07878 test_loss: 0.09577 \n",
      "[127/200] train_loss: 0.06430 valid_loss: 0.07662 test_loss: 0.09454 \n",
      "[128/200] train_loss: 0.06340 valid_loss: 0.07845 test_loss: 0.09371 \n",
      "[129/200] train_loss: 0.06570 valid_loss: 0.07784 test_loss: 0.10141 \n",
      "[130/200] train_loss: 0.06481 valid_loss: 0.07874 test_loss: 0.24694 \n",
      "[131/200] train_loss: 0.06398 valid_loss: 0.07747 test_loss: 0.09563 \n",
      "[132/200] train_loss: 0.06481 valid_loss: 0.07918 test_loss: 0.17600 \n",
      "[133/200] train_loss: 0.06402 valid_loss: 0.07834 test_loss: 0.10530 \n",
      "[134/200] train_loss: 0.06157 valid_loss: 0.07824 test_loss: 0.09051 \n",
      "[135/200] train_loss: 0.06429 valid_loss: 0.07690 test_loss: 0.10909 \n",
      "[136/200] train_loss: 0.06251 valid_loss: 0.07882 test_loss: 0.09461 \n",
      "[137/200] train_loss: 0.06388 valid_loss: 0.08154 test_loss: 0.09666 \n",
      "[138/200] train_loss: 0.06181 valid_loss: 0.07861 test_loss: 0.09556 \n",
      "[139/200] train_loss: 0.06256 valid_loss: 0.07652 test_loss: 0.15086 \n",
      "[140/200] train_loss: 0.06240 valid_loss: 0.07815 test_loss: 0.09117 \n",
      "[141/200] train_loss: 0.06232 valid_loss: 0.07893 test_loss: 0.09498 \n",
      "[142/200] train_loss: 0.06298 valid_loss: 0.07712 test_loss: 0.09409 \n",
      "[143/200] train_loss: 0.06264 valid_loss: 0.07742 test_loss: 0.09206 \n",
      "[144/200] train_loss: 0.06209 valid_loss: 0.07636 test_loss: 0.16073 \n",
      "[145/200] train_loss: 0.06243 valid_loss: 0.07706 test_loss: 0.19825 \n",
      "[146/200] train_loss: 0.06127 valid_loss: 0.07742 test_loss: 0.09597 \n",
      "[147/200] train_loss: 0.06103 valid_loss: 0.07748 test_loss: 0.09505 \n",
      "[148/200] train_loss: 0.06142 valid_loss: 0.07684 test_loss: 0.09483 \n",
      "[149/200] train_loss: 0.06223 valid_loss: 0.07807 test_loss: 0.55933 \n",
      "[150/200] train_loss: 0.06130 valid_loss: 0.07814 test_loss: 0.09286 \n",
      "[151/200] train_loss: 0.06153 valid_loss: 0.07800 test_loss: 0.09349 \n",
      "[152/200] train_loss: 0.06069 valid_loss: 0.07634 test_loss: 0.09655 \n",
      "[153/200] train_loss: 0.06074 valid_loss: 0.07550 test_loss: 0.09242 \n",
      "[154/200] train_loss: 0.06080 valid_loss: 0.07817 test_loss: 0.09411 \n",
      "[155/200] train_loss: 0.06114 valid_loss: 0.07771 test_loss: 0.10097 \n",
      "[156/200] train_loss: 0.06045 valid_loss: 0.07749 test_loss: 0.19406 \n",
      "[157/200] train_loss: 0.05983 valid_loss: 0.07762 test_loss: 0.62098 \n",
      "[158/200] train_loss: 0.06125 valid_loss: 0.07565 test_loss: 0.12602 \n",
      "[159/200] train_loss: 0.06041 valid_loss: 0.07740 test_loss: 0.09253 \n",
      "[160/200] train_loss: 0.06062 valid_loss: 0.07754 test_loss: 0.17462 \n",
      "[161/200] train_loss: 0.05908 valid_loss: 0.07765 test_loss: 0.30683 \n",
      "[162/200] train_loss: 0.05859 valid_loss: 0.07547 test_loss: 0.09826 \n",
      "[163/200] train_loss: 0.06165 valid_loss: 0.07941 test_loss: 0.10255 \n",
      "[164/200] train_loss: 0.06087 valid_loss: 0.07580 test_loss: 0.09056 \n",
      "[165/200] train_loss: 0.05918 valid_loss: 0.07634 test_loss: 0.09075 \n",
      "[166/200] train_loss: 0.06205 valid_loss: 0.07710 test_loss: 0.09660 \n",
      "[167/200] train_loss: 0.05913 valid_loss: 0.07707 test_loss: 0.09572 \n",
      "[168/200] train_loss: 0.06006 valid_loss: 0.07584 test_loss: 0.34148 \n",
      "[169/200] train_loss: 0.05813 valid_loss: 0.07687 test_loss: 0.09666 \n",
      "[170/200] train_loss: 0.05793 valid_loss: 0.07646 test_loss: 0.09382 \n",
      "[171/200] train_loss: 0.05730 valid_loss: 0.07779 test_loss: 0.09306 \n",
      "[172/200] train_loss: 0.05888 valid_loss: 0.07987 test_loss: 0.09732 \n",
      "[173/200] train_loss: 0.05882 valid_loss: 0.07573 test_loss: 0.08984 \n",
      "[174/200] train_loss: 0.06057 valid_loss: 0.07514 test_loss: 0.11466 \n",
      "[175/200] train_loss: 0.05757 valid_loss: 0.08467 test_loss: 0.10871 \n",
      "[176/200] train_loss: 0.05804 valid_loss: 0.07864 test_loss: 0.09233 \n",
      "[177/200] train_loss: 0.06003 valid_loss: 0.07561 test_loss: 0.09156 \n",
      "[178/200] train_loss: 0.05876 valid_loss: 0.07958 test_loss: 0.09807 \n",
      "[179/200] train_loss: 0.05947 valid_loss: 0.07759 test_loss: 0.09680 \n",
      "[180/200] train_loss: 0.05888 valid_loss: 0.07771 test_loss: 0.10294 \n",
      "[181/200] train_loss: 0.05864 valid_loss: 0.07718 test_loss: 0.09212 \n",
      "[182/200] train_loss: 0.05807 valid_loss: 0.07965 test_loss: 0.09557 \n",
      "[183/200] train_loss: 0.05897 valid_loss: 0.07745 test_loss: 0.09570 \n",
      "[184/200] train_loss: 0.05765 valid_loss: 0.07749 test_loss: 0.09511 \n",
      "[185/200] train_loss: 0.05874 valid_loss: 0.07784 test_loss: 0.09783 \n",
      "[186/200] train_loss: 0.05614 valid_loss: 0.07728 test_loss: 0.09377 \n",
      "[187/200] train_loss: 0.05641 valid_loss: 0.07780 test_loss: 0.09942 \n",
      "[188/200] train_loss: 0.05694 valid_loss: 0.07848 test_loss: 0.09529 \n",
      "[189/200] train_loss: 0.05555 valid_loss: 0.07698 test_loss: 0.09261 \n",
      "[190/200] train_loss: 0.05703 valid_loss: 0.07636 test_loss: 0.09943 \n",
      "[191/200] train_loss: 0.05584 valid_loss: 0.07638 test_loss: 0.09257 \n",
      "[192/200] train_loss: 0.05557 valid_loss: 0.07607 test_loss: 0.11917 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[193/200] train_loss: 0.05688 valid_loss: 0.07709 test_loss: 0.09679 \n",
      "[194/200] train_loss: 0.05661 valid_loss: 0.07744 test_loss: 0.09808 \n",
      "[195/200] train_loss: 0.05553 valid_loss: 0.07599 test_loss: 0.09313 \n",
      "[196/200] train_loss: 0.05744 valid_loss: 0.07606 test_loss: 0.09558 \n",
      "[197/200] train_loss: 0.05562 valid_loss: 0.07824 test_loss: 0.09673 \n",
      "[198/200] train_loss: 0.05684 valid_loss: 0.08061 test_loss: 0.09667 \n",
      "[199/200] train_loss: 0.05700 valid_loss: 0.07683 test_loss: 0.09719 \n",
      "[200/200] train_loss: 0.05687 valid_loss: 0.07619 test_loss: 0.09430 \n",
      "TRAINING MODEL 5\n",
      "[  1/200] train_loss: 0.36712 valid_loss: 0.26435 test_loss: 0.23319 \n",
      "验证损失减少 (inf --> 0.233187). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20855 valid_loss: 0.19354 test_loss: 0.14839 \n",
      "验证损失减少 (0.264352 --> 0.148394). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16295 valid_loss: 0.15904 test_loss: 0.12844 \n",
      "验证损失减少 (0.193537 --> 0.128441). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14715 valid_loss: 0.15295 test_loss: 0.12506 \n",
      "验证损失减少 (0.159040 --> 0.125065). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13448 valid_loss: 0.13601 test_loss: 0.11630 \n",
      "验证损失减少 (0.152954 --> 0.116295). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12688 valid_loss: 0.13079 test_loss: 0.11300 \n",
      "验证损失减少 (0.136012 --> 0.113004). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12402 valid_loss: 0.12767 test_loss: 0.11427 \n",
      "验证损失减少 (0.130795 --> 0.114271). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11737 valid_loss: 0.12755 test_loss: 0.11606 \n",
      "验证损失减少 (0.127667 --> 0.116057). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11579 valid_loss: 0.11912 test_loss: 0.10907 \n",
      "验证损失减少 (0.127549 --> 0.109073). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11324 valid_loss: 0.11791 test_loss: 0.14539 \n",
      "[ 11/200] train_loss: 0.11021 valid_loss: 0.11472 test_loss: 0.10609 \n",
      "验证损失减少 (0.119123 --> 0.106091). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10810 valid_loss: 0.11301 test_loss: 0.10530 \n",
      "验证损失减少 (0.114717 --> 0.105304). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10601 valid_loss: 0.11120 test_loss: 0.11992 \n",
      "[ 14/200] train_loss: 0.10504 valid_loss: 0.10838 test_loss: 0.12073 \n",
      "[ 15/200] train_loss: 0.10465 valid_loss: 0.10699 test_loss: 0.10811 \n",
      "验证损失减少 (0.113011 --> 0.108114). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10128 valid_loss: 0.11009 test_loss: 0.10486 \n",
      "验证损失减少 (0.106993 --> 0.104863). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.09909 valid_loss: 0.10625 test_loss: 0.10063 \n",
      "验证损失减少 (0.110090 --> 0.100633). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.09871 valid_loss: 0.10174 test_loss: 0.10258 \n",
      "验证损失减少 (0.106250 --> 0.102583). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09838 valid_loss: 0.10358 test_loss: 0.10198 \n",
      "[ 20/200] train_loss: 0.09897 valid_loss: 0.10135 test_loss: 0.10268 \n",
      "[ 21/200] train_loss: 0.09354 valid_loss: 0.09812 test_loss: 0.12313 \n",
      "[ 22/200] train_loss: 0.09580 valid_loss: 0.09852 test_loss: 0.11622 \n",
      "[ 23/200] train_loss: 0.09496 valid_loss: 0.09705 test_loss: 0.10588 \n",
      "[ 24/200] train_loss: 0.09441 valid_loss: 0.09754 test_loss: 0.10065 \n",
      "验证损失减少 (0.101743 --> 0.100650). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09103 valid_loss: 0.09752 test_loss: 0.09892 \n",
      "[ 26/200] train_loss: 0.09430 valid_loss: 0.09906 test_loss: 0.10286 \n",
      "[ 27/200] train_loss: 0.09126 valid_loss: 0.09573 test_loss: 0.15637 \n",
      "[ 28/200] train_loss: 0.09218 valid_loss: 0.09623 test_loss: 0.13566 \n",
      "[ 29/200] train_loss: 0.09093 valid_loss: 0.09297 test_loss: 0.13347 \n",
      "[ 30/200] train_loss: 0.08763 valid_loss: 0.09378 test_loss: 0.11800 \n",
      "[ 31/200] train_loss: 0.08984 valid_loss: 0.09381 test_loss: 0.38944 \n",
      "[ 32/200] train_loss: 0.08710 valid_loss: 0.09159 test_loss: 0.14743 \n",
      "[ 33/200] train_loss: 0.08783 valid_loss: 0.09324 test_loss: 0.09350 \n",
      "验证损失减少 (0.097543 --> 0.093504). 正在保存模型...\n",
      "[ 34/200] train_loss: 0.08552 valid_loss: 0.09369 test_loss: 0.10184 \n",
      "[ 35/200] train_loss: 0.08706 valid_loss: 0.09266 test_loss: 0.09272 \n",
      "验证损失减少 (0.093243 --> 0.092723). 正在保存模型...\n",
      "[ 36/200] train_loss: 0.08456 valid_loss: 0.09180 test_loss: 0.09186 \n",
      "验证损失减少 (0.092662 --> 0.091859). 正在保存模型...\n",
      "[ 37/200] train_loss: 0.08504 valid_loss: 0.09098 test_loss: 0.11510 \n",
      "[ 38/200] train_loss: 0.08435 valid_loss: 0.09069 test_loss: 0.17246 \n",
      "[ 39/200] train_loss: 0.08351 valid_loss: 0.09117 test_loss: 0.09476 \n",
      "[ 40/200] train_loss: 0.08439 valid_loss: 0.09109 test_loss: 0.09441 \n",
      "[ 41/200] train_loss: 0.08269 valid_loss: 0.08816 test_loss: 0.10668 \n",
      "[ 42/200] train_loss: 0.08159 valid_loss: 0.08923 test_loss: 0.11245 \n",
      "[ 43/200] train_loss: 0.08256 valid_loss: 0.08896 test_loss: 0.12041 \n",
      "[ 44/200] train_loss: 0.08041 valid_loss: 0.09006 test_loss: 0.09324 \n",
      "[ 45/200] train_loss: 0.08263 valid_loss: 0.08853 test_loss: 0.09467 \n",
      "[ 46/200] train_loss: 0.07922 valid_loss: 0.08681 test_loss: 0.09115 \n",
      "验证损失减少 (0.091797 --> 0.091149). 正在保存模型...\n",
      "[ 47/200] train_loss: 0.08100 valid_loss: 0.08687 test_loss: 0.09628 \n",
      "[ 48/200] train_loss: 0.07979 valid_loss: 0.08789 test_loss: 0.09592 \n",
      "[ 49/200] train_loss: 0.08110 valid_loss: 0.08798 test_loss: 0.11811 \n",
      "[ 50/200] train_loss: 0.08077 valid_loss: 0.08824 test_loss: 0.28517 \n",
      "[ 51/200] train_loss: 0.08003 valid_loss: 0.08883 test_loss: 0.09415 \n",
      "[ 52/200] train_loss: 0.07823 valid_loss: 0.08590 test_loss: 0.09131 \n",
      "[ 53/200] train_loss: 0.07938 valid_loss: 0.09100 test_loss: 0.10086 \n",
      "[ 54/200] train_loss: 0.07807 valid_loss: 0.08739 test_loss: 0.09767 \n",
      "[ 55/200] train_loss: 0.07643 valid_loss: 0.08703 test_loss: 0.09406 \n",
      "[ 56/200] train_loss: 0.07884 valid_loss: 0.08567 test_loss: 0.09208 \n",
      "[ 57/200] train_loss: 0.07726 valid_loss: 0.08686 test_loss: 0.09361 \n",
      "[ 58/200] train_loss: 0.07658 valid_loss: 0.08542 test_loss: 0.09207 \n",
      "[ 59/200] train_loss: 0.07796 valid_loss: 0.08573 test_loss: 0.09090 \n",
      "[ 60/200] train_loss: 0.07642 valid_loss: 0.08600 test_loss: 0.09691 \n",
      "[ 61/200] train_loss: 0.07468 valid_loss: 0.08586 test_loss: 0.10492 \n",
      "[ 62/200] train_loss: 0.07494 valid_loss: 0.08358 test_loss: 0.11655 \n",
      "[ 63/200] train_loss: 0.07314 valid_loss: 0.08676 test_loss: 0.09494 \n",
      "[ 64/200] train_loss: 0.07571 valid_loss: 0.08568 test_loss: 0.09357 \n",
      "[ 65/200] train_loss: 0.07607 valid_loss: 0.08675 test_loss: 0.09671 \n",
      "[ 66/200] train_loss: 0.07526 valid_loss: 0.08794 test_loss: 0.09617 \n",
      "[ 67/200] train_loss: 0.07434 valid_loss: 0.08875 test_loss: 0.09756 \n",
      "[ 68/200] train_loss: 0.07235 valid_loss: 0.08682 test_loss: 0.09654 \n",
      "[ 69/200] train_loss: 0.07347 valid_loss: 0.08584 test_loss: 0.09610 \n",
      "[ 70/200] train_loss: 0.07350 valid_loss: 0.08427 test_loss: 0.09159 \n",
      "[ 71/200] train_loss: 0.07354 valid_loss: 0.08366 test_loss: 0.09345 \n",
      "[ 72/200] train_loss: 0.07363 valid_loss: 0.08413 test_loss: 0.09214 \n",
      "[ 73/200] train_loss: 0.07325 valid_loss: 0.08514 test_loss: 0.09306 \n",
      "[ 74/200] train_loss: 0.07337 valid_loss: 0.08363 test_loss: 0.09511 \n",
      "[ 75/200] train_loss: 0.07159 valid_loss: 0.08231 test_loss: 0.17025 \n",
      "[ 76/200] train_loss: 0.07212 valid_loss: 0.08297 test_loss: 0.09236 \n",
      "[ 77/200] train_loss: 0.07216 valid_loss: 0.08439 test_loss: 0.09809 \n",
      "[ 78/200] train_loss: 0.07075 valid_loss: 0.08475 test_loss: 0.09620 \n",
      "[ 79/200] train_loss: 0.07112 valid_loss: 0.08458 test_loss: 0.09627 \n",
      "[ 80/200] train_loss: 0.07014 valid_loss: 0.08284 test_loss: 0.09286 \n",
      "[ 81/200] train_loss: 0.07179 valid_loss: 0.08293 test_loss: 0.09241 \n",
      "[ 82/200] train_loss: 0.07208 valid_loss: 0.08614 test_loss: 0.09139 \n",
      "[ 83/200] train_loss: 0.07110 valid_loss: 0.08197 test_loss: 0.18448 \n",
      "[ 84/200] train_loss: 0.07107 valid_loss: 0.08315 test_loss: 0.09513 \n",
      "[ 85/200] train_loss: 0.07133 valid_loss: 0.08296 test_loss: 0.08870 \n",
      "[ 86/200] train_loss: 0.07031 valid_loss: 0.08347 test_loss: 0.09079 \n",
      "[ 87/200] train_loss: 0.07083 valid_loss: 0.08320 test_loss: 0.09283 \n",
      "[ 88/200] train_loss: 0.07094 valid_loss: 0.08545 test_loss: 0.09656 \n",
      "[ 89/200] train_loss: 0.07016 valid_loss: 0.08452 test_loss: 0.09622 \n",
      "[ 90/200] train_loss: 0.06995 valid_loss: 0.08260 test_loss: 0.09467 \n",
      "[ 91/200] train_loss: 0.06863 valid_loss: 0.08240 test_loss: 0.09270 \n",
      "[ 92/200] train_loss: 0.06694 valid_loss: 0.08215 test_loss: 0.09642 \n",
      "[ 93/200] train_loss: 0.06863 valid_loss: 0.08331 test_loss: 0.09533 \n",
      "[ 94/200] train_loss: 0.07051 valid_loss: 0.08037 test_loss: 0.11056 \n",
      "[ 95/200] train_loss: 0.06802 valid_loss: 0.08097 test_loss: 0.09317 \n",
      "[ 96/200] train_loss: 0.06885 valid_loss: 0.08276 test_loss: 0.09306 \n",
      "[ 97/200] train_loss: 0.06707 valid_loss: 0.08350 test_loss: 0.09332 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 98/200] train_loss: 0.06789 valid_loss: 0.08193 test_loss: 0.11529 \n",
      "[ 99/200] train_loss: 0.06813 valid_loss: 0.08220 test_loss: 0.33796 \n",
      "[100/200] train_loss: 0.06730 valid_loss: 0.08033 test_loss: 0.14208 \n",
      "[101/200] train_loss: 0.06649 valid_loss: 0.08124 test_loss: 0.09526 \n",
      "[102/200] train_loss: 0.06770 valid_loss: 0.08057 test_loss: 0.09391 \n",
      "[103/200] train_loss: 0.06859 valid_loss: 0.08459 test_loss: 0.09753 \n",
      "[104/200] train_loss: 0.06762 valid_loss: 0.08043 test_loss: 0.09193 \n",
      "[105/200] train_loss: 0.06611 valid_loss: 0.08447 test_loss: 0.09227 \n",
      "[106/200] train_loss: 0.06447 valid_loss: 0.08225 test_loss: 0.09069 \n",
      "[107/200] train_loss: 0.06632 valid_loss: 0.08331 test_loss: 0.09654 \n",
      "[108/200] train_loss: 0.06630 valid_loss: 0.08123 test_loss: 0.09629 \n",
      "[109/200] train_loss: 0.06651 valid_loss: 0.08049 test_loss: 0.09028 \n",
      "[110/200] train_loss: 0.06643 valid_loss: 0.08190 test_loss: 0.14143 \n",
      "[111/200] train_loss: 0.06613 valid_loss: 0.08421 test_loss: 0.09691 \n",
      "[112/200] train_loss: 0.06619 valid_loss: 0.08084 test_loss: 0.09522 \n",
      "[113/200] train_loss: 0.06473 valid_loss: 0.08101 test_loss: 0.09208 \n",
      "[114/200] train_loss: 0.06623 valid_loss: 0.08085 test_loss: 0.09466 \n",
      "[115/200] train_loss: 0.06559 valid_loss: 0.07963 test_loss: 0.09172 \n",
      "[116/200] train_loss: 0.06466 valid_loss: 0.08099 test_loss: 0.09040 \n",
      "[117/200] train_loss: 0.06370 valid_loss: 0.08128 test_loss: 0.09412 \n",
      "[118/200] train_loss: 0.06746 valid_loss: 0.08177 test_loss: 0.09265 \n",
      "[119/200] train_loss: 0.06562 valid_loss: 0.08387 test_loss: 0.09385 \n",
      "[120/200] train_loss: 0.06618 valid_loss: 0.08180 test_loss: 0.09655 \n",
      "[121/200] train_loss: 0.06443 valid_loss: 0.08190 test_loss: 0.10131 \n",
      "[122/200] train_loss: 0.06371 valid_loss: 0.08167 test_loss: 0.09542 \n",
      "[123/200] train_loss: 0.06503 valid_loss: 0.08215 test_loss: 0.09414 \n",
      "[124/200] train_loss: 0.06244 valid_loss: 0.08347 test_loss: 0.09894 \n",
      "[125/200] train_loss: 0.06400 valid_loss: 0.07823 test_loss: 0.08974 \n",
      "[126/200] train_loss: 0.06508 valid_loss: 0.08275 test_loss: 0.10978 \n",
      "[127/200] train_loss: 0.06364 valid_loss: 0.08139 test_loss: 0.09983 \n",
      "[128/200] train_loss: 0.06441 valid_loss: 0.08048 test_loss: 0.09554 \n",
      "[129/200] train_loss: 0.06364 valid_loss: 0.08060 test_loss: 0.09509 \n",
      "[130/200] train_loss: 0.06155 valid_loss: 0.08088 test_loss: 0.09904 \n",
      "[131/200] train_loss: 0.06198 valid_loss: 0.08138 test_loss: 0.09349 \n",
      "[132/200] train_loss: 0.06237 valid_loss: 0.08049 test_loss: 0.09575 \n",
      "[133/200] train_loss: 0.06294 valid_loss: 0.08013 test_loss: 0.09406 \n",
      "[134/200] train_loss: 0.06059 valid_loss: 0.08096 test_loss: 0.09716 \n",
      "[135/200] train_loss: 0.05963 valid_loss: 0.08018 test_loss: 0.12804 \n",
      "[136/200] train_loss: 0.06368 valid_loss: 0.07947 test_loss: 0.09526 \n",
      "[137/200] train_loss: 0.06128 valid_loss: 0.08063 test_loss: 0.09093 \n",
      "[138/200] train_loss: 0.06194 valid_loss: 0.08195 test_loss: 0.09611 \n",
      "[139/200] train_loss: 0.06163 valid_loss: 0.08038 test_loss: 0.09372 \n",
      "[140/200] train_loss: 0.06347 valid_loss: 0.08011 test_loss: 0.19059 \n",
      "[141/200] train_loss: 0.06134 valid_loss: 0.07806 test_loss: 0.09556 \n",
      "[142/200] train_loss: 0.06153 valid_loss: 0.08123 test_loss: 0.10197 \n",
      "[143/200] train_loss: 0.06171 valid_loss: 0.08329 test_loss: 0.09805 \n",
      "[144/200] train_loss: 0.05975 valid_loss: 0.08238 test_loss: 0.09558 \n",
      "[145/200] train_loss: 0.06106 valid_loss: 0.08133 test_loss: 0.09369 \n",
      "[146/200] train_loss: 0.06259 valid_loss: 0.08138 test_loss: 0.09712 \n",
      "[147/200] train_loss: 0.06176 valid_loss: 0.08402 test_loss: 0.09871 \n",
      "[148/200] train_loss: 0.06155 valid_loss: 0.08156 test_loss: 0.09463 \n",
      "[149/200] train_loss: 0.06137 valid_loss: 0.08369 test_loss: 0.09737 \n",
      "[150/200] train_loss: 0.06101 valid_loss: 0.07886 test_loss: 0.12175 \n",
      "[151/200] train_loss: 0.06173 valid_loss: 0.07759 test_loss: 0.09476 \n",
      "[152/200] train_loss: 0.06091 valid_loss: 0.07918 test_loss: 0.09458 \n",
      "[153/200] train_loss: 0.06073 valid_loss: 0.07871 test_loss: 0.09291 \n",
      "[154/200] train_loss: 0.06060 valid_loss: 0.07863 test_loss: 0.09582 \n",
      "[155/200] train_loss: 0.05894 valid_loss: 0.08096 test_loss: 0.09772 \n",
      "[156/200] train_loss: 0.05985 valid_loss: 0.08260 test_loss: 0.09831 \n",
      "[157/200] train_loss: 0.06041 valid_loss: 0.08069 test_loss: 0.09299 \n",
      "[158/200] train_loss: 0.05906 valid_loss: 0.08160 test_loss: 0.10163 \n",
      "[159/200] train_loss: 0.06007 valid_loss: 0.08394 test_loss: 0.09515 \n",
      "[160/200] train_loss: 0.06056 valid_loss: 0.08042 test_loss: 0.09445 \n",
      "[161/200] train_loss: 0.05969 valid_loss: 0.08226 test_loss: 0.09727 \n",
      "[162/200] train_loss: 0.06014 valid_loss: 0.08256 test_loss: 0.09773 \n",
      "[163/200] train_loss: 0.05912 valid_loss: 0.08912 test_loss: 0.10842 \n",
      "[164/200] train_loss: 0.05795 valid_loss: 0.08391 test_loss: 0.10414 \n",
      "[165/200] train_loss: 0.05857 valid_loss: 0.08254 test_loss: 0.10153 \n",
      "[166/200] train_loss: 0.05774 valid_loss: 0.08006 test_loss: 0.09261 \n",
      "[167/200] train_loss: 0.05776 valid_loss: 0.07886 test_loss: 0.10001 \n",
      "[168/200] train_loss: 0.05833 valid_loss: 0.07907 test_loss: 0.09918 \n",
      "[169/200] train_loss: 0.06062 valid_loss: 0.07774 test_loss: 0.09479 \n",
      "[170/200] train_loss: 0.05886 valid_loss: 0.08130 test_loss: 0.09880 \n",
      "[171/200] train_loss: 0.06058 valid_loss: 0.07919 test_loss: 0.09433 \n",
      "[172/200] train_loss: 0.05678 valid_loss: 0.07799 test_loss: 0.09550 \n",
      "[173/200] train_loss: 0.05955 valid_loss: 0.07872 test_loss: 0.10254 \n",
      "[174/200] train_loss: 0.05799 valid_loss: 0.08010 test_loss: 0.09818 \n",
      "[175/200] train_loss: 0.05808 valid_loss: 0.07880 test_loss: 0.09657 \n",
      "[176/200] train_loss: 0.05828 valid_loss: 0.07943 test_loss: 0.09737 \n",
      "[177/200] train_loss: 0.05892 valid_loss: 0.07936 test_loss: 0.09455 \n",
      "[178/200] train_loss: 0.05723 valid_loss: 0.07807 test_loss: 0.09647 \n",
      "[179/200] train_loss: 0.05710 valid_loss: 0.07891 test_loss: 0.09809 \n",
      "[180/200] train_loss: 0.05655 valid_loss: 0.07684 test_loss: 0.14257 \n",
      "[181/200] train_loss: 0.05662 valid_loss: 0.07703 test_loss: 0.09780 \n",
      "[182/200] train_loss: 0.05795 valid_loss: 0.08006 test_loss: 0.09701 \n",
      "[183/200] train_loss: 0.05683 valid_loss: 0.07812 test_loss: 0.09096 \n",
      "[184/200] train_loss: 0.05731 valid_loss: 0.08202 test_loss: 0.09611 \n",
      "[185/200] train_loss: 0.05728 valid_loss: 0.08401 test_loss: 0.10396 \n",
      "[186/200] train_loss: 0.05598 valid_loss: 0.08060 test_loss: 0.09726 \n",
      "[187/200] train_loss: 0.05859 valid_loss: 0.08436 test_loss: 0.10312 \n",
      "[188/200] train_loss: 0.05838 valid_loss: 0.08234 test_loss: 0.10706 \n",
      "[189/200] train_loss: 0.05630 valid_loss: 0.07934 test_loss: 0.09500 \n",
      "[190/200] train_loss: 0.05573 valid_loss: 0.08246 test_loss: 0.09569 \n",
      "[191/200] train_loss: 0.05719 valid_loss: 0.07918 test_loss: 0.09621 \n",
      "[192/200] train_loss: 0.05592 valid_loss: 0.07868 test_loss: 0.09694 \n",
      "[193/200] train_loss: 0.05666 valid_loss: 0.07955 test_loss: 0.09584 \n",
      "[194/200] train_loss: 0.05564 valid_loss: 0.08077 test_loss: 0.09162 \n",
      "[195/200] train_loss: 0.05626 valid_loss: 0.07928 test_loss: 0.09939 \n",
      "[196/200] train_loss: 0.05686 valid_loss: 0.07871 test_loss: 0.09269 \n",
      "[197/200] train_loss: 0.05605 valid_loss: 0.07862 test_loss: 0.11277 \n",
      "[198/200] train_loss: 0.05656 valid_loss: 0.08010 test_loss: 0.10093 \n",
      "[199/200] train_loss: 0.05783 valid_loss: 0.07844 test_loss: 0.26571 \n",
      "[200/200] train_loss: 0.05633 valid_loss: 0.08080 test_loss: 0.09709 \n",
      "TRAINING MODEL 6\n",
      "[  1/200] train_loss: 0.39124 valid_loss: 0.27907 test_loss: 0.26775 \n",
      "验证损失减少 (inf --> 0.267753). 正在保存模型...\n",
      "[  2/200] train_loss: 0.22341 valid_loss: 0.20165 test_loss: 0.15995 \n",
      "验证损失减少 (0.279073 --> 0.159953). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16939 valid_loss: 0.16632 test_loss: 0.13204 \n",
      "验证损失减少 (0.201646 --> 0.132035). 正在保存模型...\n",
      "[  4/200] train_loss: 0.15228 valid_loss: 0.15469 test_loss: 0.13403 \n",
      "验证损失减少 (0.166322 --> 0.134027). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13776 valid_loss: 0.13815 test_loss: 0.11989 \n",
      "验证损失减少 (0.154691 --> 0.119890). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13046 valid_loss: 0.13407 test_loss: 0.11488 \n",
      "验证损失减少 (0.138152 --> 0.114883). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12562 valid_loss: 0.13258 test_loss: 0.11751 \n",
      "验证损失减少 (0.134071 --> 0.117511). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12192 valid_loss: 0.12752 test_loss: 0.11713 \n",
      "验证损失减少 (0.132580 --> 0.117129). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11843 valid_loss: 0.12067 test_loss: 0.11077 \n",
      "验证损失减少 (0.127524 --> 0.110769). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10/200] train_loss: 0.11383 valid_loss: 0.11767 test_loss: 0.10769 \n",
      "验证损失减少 (0.120675 --> 0.107685). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11296 valid_loss: 0.11563 test_loss: 0.10902 \n",
      "验证损失减少 (0.117665 --> 0.109017). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10977 valid_loss: 0.11951 test_loss: 0.11073 \n",
      "验证损失减少 (0.115629 --> 0.110733). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10640 valid_loss: 0.11149 test_loss: 0.10512 \n",
      "验证损失减少 (0.119512 --> 0.105122). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10450 valid_loss: 0.11237 test_loss: 0.10525 \n",
      "验证损失减少 (0.111493 --> 0.105248). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10360 valid_loss: 0.10682 test_loss: 0.10961 \n",
      "验证损失减少 (0.112369 --> 0.109611). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10286 valid_loss: 0.10313 test_loss: 0.12707 \n",
      "[ 17/200] train_loss: 0.10370 valid_loss: 0.10455 test_loss: 0.10500 \n",
      "验证损失减少 (0.106825 --> 0.104999). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.10114 valid_loss: 0.10463 test_loss: 0.12324 \n",
      "[ 19/200] train_loss: 0.09941 valid_loss: 0.10469 test_loss: 0.10218 \n",
      "验证损失减少 (0.104550 --> 0.102183). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09960 valid_loss: 0.10517 test_loss: 0.10532 \n",
      "[ 21/200] train_loss: 0.09465 valid_loss: 0.10501 test_loss: 0.10555 \n",
      "[ 22/200] train_loss: 0.09868 valid_loss: 0.10176 test_loss: 0.10413 \n",
      "验证损失减少 (0.104689 --> 0.104127). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09497 valid_loss: 0.09926 test_loss: 0.11231 \n",
      "[ 24/200] train_loss: 0.09168 valid_loss: 0.09729 test_loss: 0.10980 \n",
      "[ 25/200] train_loss: 0.09191 valid_loss: 0.09827 test_loss: 0.10085 \n",
      "验证损失减少 (0.101757 --> 0.100854). 正在保存模型...\n",
      "[ 26/200] train_loss: 0.09361 valid_loss: 0.09734 test_loss: 0.09728 \n",
      "验证损失减少 (0.098270 --> 0.097275). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.09000 valid_loss: 0.09435 test_loss: 0.10038 \n",
      "[ 28/200] train_loss: 0.09143 valid_loss: 0.09753 test_loss: 0.10542 \n",
      "[ 29/200] train_loss: 0.09088 valid_loss: 0.09580 test_loss: 0.09886 \n",
      "[ 30/200] train_loss: 0.08871 valid_loss: 0.10064 test_loss: 0.10798 \n",
      "[ 31/200] train_loss: 0.08777 valid_loss: 0.09570 test_loss: 0.09882 \n",
      "[ 32/200] train_loss: 0.08844 valid_loss: 0.09405 test_loss: 0.10661 \n",
      "[ 33/200] train_loss: 0.08729 valid_loss: 0.09275 test_loss: 0.10978 \n",
      "[ 34/200] train_loss: 0.08583 valid_loss: 0.09094 test_loss: 0.09763 \n",
      "[ 35/200] train_loss: 0.08752 valid_loss: 0.09065 test_loss: 0.21311 \n",
      "[ 36/200] train_loss: 0.08781 valid_loss: 0.08951 test_loss: 0.12656 \n",
      "[ 37/200] train_loss: 0.08415 valid_loss: 0.09367 test_loss: 0.09754 \n",
      "[ 38/200] train_loss: 0.08454 valid_loss: 0.09049 test_loss: 0.11249 \n",
      "[ 39/200] train_loss: 0.08356 valid_loss: 0.09049 test_loss: 0.09739 \n",
      "[ 40/200] train_loss: 0.08477 valid_loss: 0.08784 test_loss: 0.14478 \n",
      "[ 41/200] train_loss: 0.08135 valid_loss: 0.08791 test_loss: 0.18497 \n",
      "[ 42/200] train_loss: 0.08352 valid_loss: 0.09139 test_loss: 0.10252 \n",
      "[ 43/200] train_loss: 0.08238 valid_loss: 0.08971 test_loss: 0.16332 \n",
      "[ 44/200] train_loss: 0.08101 valid_loss: 0.08791 test_loss: 0.18750 \n",
      "[ 45/200] train_loss: 0.08069 valid_loss: 0.08887 test_loss: 0.10045 \n",
      "[ 46/200] train_loss: 0.08029 valid_loss: 0.08611 test_loss: 0.16327 \n",
      "[ 47/200] train_loss: 0.08058 valid_loss: 0.08696 test_loss: 0.16422 \n",
      "[ 48/200] train_loss: 0.07671 valid_loss: 0.08580 test_loss: 0.11501 \n",
      "[ 49/200] train_loss: 0.08029 valid_loss: 0.08555 test_loss: 0.15241 \n",
      "[ 50/200] train_loss: 0.07800 valid_loss: 0.08583 test_loss: 0.16524 \n",
      "[ 51/200] train_loss: 0.08118 valid_loss: 0.08683 test_loss: 0.09732 \n",
      "验证损失减少 (0.097343 --> 0.097317). 正在保存模型...\n",
      "[ 52/200] train_loss: 0.07994 valid_loss: 0.08606 test_loss: 0.10340 \n",
      "[ 53/200] train_loss: 0.07899 valid_loss: 0.08603 test_loss: 0.09393 \n",
      "[ 54/200] train_loss: 0.07712 valid_loss: 0.08581 test_loss: 0.09307 \n",
      "[ 55/200] train_loss: 0.07973 valid_loss: 0.08561 test_loss: 0.11514 \n",
      "[ 56/200] train_loss: 0.07875 valid_loss: 0.08567 test_loss: 0.11622 \n",
      "[ 57/200] train_loss: 0.07691 valid_loss: 0.08401 test_loss: 0.12074 \n",
      "[ 58/200] train_loss: 0.07636 valid_loss: 0.08544 test_loss: 0.09372 \n",
      "[ 59/200] train_loss: 0.07403 valid_loss: 0.08512 test_loss: 0.22100 \n",
      "[ 60/200] train_loss: 0.07549 valid_loss: 0.08312 test_loss: 0.09225 \n",
      "[ 61/200] train_loss: 0.07593 valid_loss: 0.08622 test_loss: 0.09400 \n",
      "[ 62/200] train_loss: 0.07683 valid_loss: 0.08354 test_loss: 0.10790 \n",
      "[ 63/200] train_loss: 0.07471 valid_loss: 0.08186 test_loss: 0.09417 \n",
      "[ 64/200] train_loss: 0.07475 valid_loss: 0.08443 test_loss: 0.09249 \n",
      "[ 65/200] train_loss: 0.07458 valid_loss: 0.08336 test_loss: 0.09436 \n",
      "[ 66/200] train_loss: 0.07375 valid_loss: 0.08303 test_loss: 0.09888 \n",
      "[ 67/200] train_loss: 0.07389 valid_loss: 0.08221 test_loss: 0.09311 \n",
      "[ 68/200] train_loss: 0.07409 valid_loss: 0.08326 test_loss: 0.38090 \n",
      "[ 69/200] train_loss: 0.07344 valid_loss: 0.08394 test_loss: 0.09728 \n",
      "[ 70/200] train_loss: 0.07165 valid_loss: 0.08208 test_loss: 0.15867 \n",
      "[ 71/200] train_loss: 0.07291 valid_loss: 0.08193 test_loss: 0.20716 \n",
      "[ 72/200] train_loss: 0.07384 valid_loss: 0.08209 test_loss: 0.14408 \n",
      "[ 73/200] train_loss: 0.07242 valid_loss: 0.08132 test_loss: 0.10485 \n",
      "[ 74/200] train_loss: 0.07003 valid_loss: 0.08158 test_loss: 0.10628 \n",
      "[ 75/200] train_loss: 0.07192 valid_loss: 0.08171 test_loss: 0.08878 \n",
      "[ 76/200] train_loss: 0.07014 valid_loss: 0.08275 test_loss: 0.10900 \n",
      "[ 77/200] train_loss: 0.07223 valid_loss: 0.08155 test_loss: 0.09233 \n",
      "[ 78/200] train_loss: 0.07210 valid_loss: 0.08159 test_loss: 0.09270 \n",
      "[ 79/200] train_loss: 0.07332 valid_loss: 0.08138 test_loss: 0.30459 \n",
      "[ 80/200] train_loss: 0.07116 valid_loss: 0.07960 test_loss: 0.09437 \n",
      "[ 81/200] train_loss: 0.07032 valid_loss: 0.08274 test_loss: 0.16937 \n",
      "[ 82/200] train_loss: 0.07010 valid_loss: 0.07992 test_loss: 0.33535 \n",
      "[ 83/200] train_loss: 0.07156 valid_loss: 0.08094 test_loss: 0.16856 \n",
      "[ 84/200] train_loss: 0.06965 valid_loss: 0.08123 test_loss: 0.09252 \n",
      "[ 85/200] train_loss: 0.07001 valid_loss: 0.08107 test_loss: 0.10084 \n",
      "[ 86/200] train_loss: 0.06833 valid_loss: 0.08162 test_loss: 0.09637 \n",
      "[ 87/200] train_loss: 0.06878 valid_loss: 0.08055 test_loss: 0.10943 \n",
      "[ 88/200] train_loss: 0.07037 valid_loss: 0.08059 test_loss: 0.08968 \n",
      "[ 89/200] train_loss: 0.06833 valid_loss: 0.08035 test_loss: 0.09283 \n",
      "[ 90/200] train_loss: 0.06903 valid_loss: 0.07930 test_loss: 0.09819 \n",
      "[ 91/200] train_loss: 0.06816 valid_loss: 0.07984 test_loss: 0.09298 \n",
      "[ 92/200] train_loss: 0.06859 valid_loss: 0.07902 test_loss: 0.09275 \n",
      "[ 93/200] train_loss: 0.06807 valid_loss: 0.08035 test_loss: 0.09297 \n",
      "[ 94/200] train_loss: 0.06788 valid_loss: 0.08035 test_loss: 0.09532 \n",
      "[ 95/200] train_loss: 0.06784 valid_loss: 0.07961 test_loss: 0.14494 \n",
      "[ 96/200] train_loss: 0.06835 valid_loss: 0.08135 test_loss: 0.09346 \n",
      "[ 97/200] train_loss: 0.06757 valid_loss: 0.07927 test_loss: 0.09038 \n",
      "[ 98/200] train_loss: 0.06747 valid_loss: 0.07993 test_loss: 0.09183 \n",
      "[ 99/200] train_loss: 0.06802 valid_loss: 0.07986 test_loss: 0.09316 \n",
      "[100/200] train_loss: 0.06832 valid_loss: 0.07881 test_loss: 0.09075 \n",
      "[101/200] train_loss: 0.06739 valid_loss: 0.07969 test_loss: 0.20343 \n",
      "[102/200] train_loss: 0.06537 valid_loss: 0.07732 test_loss: 0.09868 \n",
      "[103/200] train_loss: 0.06414 valid_loss: 0.08076 test_loss: 0.08944 \n",
      "[104/200] train_loss: 0.06687 valid_loss: 0.07799 test_loss: 0.09276 \n",
      "[105/200] train_loss: 0.06666 valid_loss: 0.07844 test_loss: 0.09809 \n",
      "[106/200] train_loss: 0.06377 valid_loss: 0.08057 test_loss: 0.10680 \n",
      "[107/200] train_loss: 0.06776 valid_loss: 0.07965 test_loss: 0.09187 \n",
      "[108/200] train_loss: 0.06748 valid_loss: 0.07829 test_loss: 0.12557 \n",
      "[109/200] train_loss: 0.06621 valid_loss: 0.08258 test_loss: 0.09601 \n",
      "[110/200] train_loss: 0.06431 valid_loss: 0.07822 test_loss: 0.09072 \n",
      "[111/200] train_loss: 0.06461 valid_loss: 0.08004 test_loss: 0.09531 \n",
      "[112/200] train_loss: 0.06427 valid_loss: 0.07891 test_loss: 0.09122 \n",
      "[113/200] train_loss: 0.06473 valid_loss: 0.08223 test_loss: 0.10077 \n",
      "[114/200] train_loss: 0.06619 valid_loss: 0.07853 test_loss: 0.09378 \n",
      "[115/200] train_loss: 0.06425 valid_loss: 0.07955 test_loss: 0.09100 \n",
      "[116/200] train_loss: 0.06422 valid_loss: 0.07978 test_loss: 0.09419 \n",
      "[117/200] train_loss: 0.06398 valid_loss: 0.07834 test_loss: 0.09291 \n",
      "[118/200] train_loss: 0.06513 valid_loss: 0.07777 test_loss: 0.09113 \n",
      "[119/200] train_loss: 0.06336 valid_loss: 0.07888 test_loss: 0.09895 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[120/200] train_loss: 0.06282 valid_loss: 0.07736 test_loss: 0.09100 \n",
      "[121/200] train_loss: 0.06045 valid_loss: 0.07886 test_loss: 0.09163 \n",
      "[122/200] train_loss: 0.06304 valid_loss: 0.07826 test_loss: 0.09194 \n",
      "[123/200] train_loss: 0.06449 valid_loss: 0.07667 test_loss: 0.09842 \n",
      "[124/200] train_loss: 0.06253 valid_loss: 0.07667 test_loss: 0.09398 \n",
      "[125/200] train_loss: 0.06342 valid_loss: 0.07742 test_loss: 0.08854 \n",
      "[126/200] train_loss: 0.06476 valid_loss: 0.07737 test_loss: 0.09479 \n",
      "[127/200] train_loss: 0.06199 valid_loss: 0.07742 test_loss: 0.11588 \n",
      "[128/200] train_loss: 0.06293 valid_loss: 0.07696 test_loss: 0.09363 \n",
      "[129/200] train_loss: 0.06230 valid_loss: 0.07787 test_loss: 0.09408 \n",
      "[130/200] train_loss: 0.06349 valid_loss: 0.07655 test_loss: 0.10703 \n",
      "[131/200] train_loss: 0.06323 valid_loss: 0.07608 test_loss: 0.10819 \n",
      "[132/200] train_loss: 0.06258 valid_loss: 0.07582 test_loss: 0.09094 \n",
      "[133/200] train_loss: 0.06251 valid_loss: 0.07695 test_loss: 0.14418 \n",
      "[134/200] train_loss: 0.06161 valid_loss: 0.07924 test_loss: 0.09393 \n",
      "[135/200] train_loss: 0.06162 valid_loss: 0.07866 test_loss: 0.09501 \n",
      "[136/200] train_loss: 0.06148 valid_loss: 0.07876 test_loss: 0.09594 \n",
      "[137/200] train_loss: 0.06247 valid_loss: 0.07643 test_loss: 0.08867 \n",
      "[138/200] train_loss: 0.05949 valid_loss: 0.07832 test_loss: 0.08916 \n",
      "[139/200] train_loss: 0.06212 valid_loss: 0.07641 test_loss: 0.09466 \n",
      "[140/200] train_loss: 0.06116 valid_loss: 0.07644 test_loss: 0.09729 \n",
      "[141/200] train_loss: 0.06080 valid_loss: 0.07684 test_loss: 0.09510 \n",
      "[142/200] train_loss: 0.06040 valid_loss: 0.07684 test_loss: 0.09336 \n",
      "[143/200] train_loss: 0.06041 valid_loss: 0.07692 test_loss: 0.09239 \n",
      "[144/200] train_loss: 0.06027 valid_loss: 0.07797 test_loss: 0.09704 \n",
      "[145/200] train_loss: 0.06033 valid_loss: 0.07998 test_loss: 0.10076 \n",
      "[146/200] train_loss: 0.06103 valid_loss: 0.07793 test_loss: 0.09431 \n",
      "[147/200] train_loss: 0.06102 valid_loss: 0.07702 test_loss: 0.09052 \n",
      "[148/200] train_loss: 0.06278 valid_loss: 0.07648 test_loss: 0.17342 \n",
      "[149/200] train_loss: 0.06010 valid_loss: 0.07946 test_loss: 0.11285 \n",
      "[150/200] train_loss: 0.06087 valid_loss: 0.07825 test_loss: 0.09773 \n",
      "[151/200] train_loss: 0.06045 valid_loss: 0.07710 test_loss: 0.09607 \n",
      "[152/200] train_loss: 0.05969 valid_loss: 0.07638 test_loss: 0.09536 \n",
      "[153/200] train_loss: 0.06091 valid_loss: 0.07860 test_loss: 0.09935 \n",
      "[154/200] train_loss: 0.06085 valid_loss: 0.07846 test_loss: 0.09352 \n",
      "[155/200] train_loss: 0.05838 valid_loss: 0.07800 test_loss: 0.11442 \n",
      "[156/200] train_loss: 0.05889 valid_loss: 0.07775 test_loss: 0.09430 \n",
      "[157/200] train_loss: 0.05996 valid_loss: 0.07719 test_loss: 0.21130 \n",
      "[158/200] train_loss: 0.05963 valid_loss: 0.08014 test_loss: 0.09582 \n",
      "[159/200] train_loss: 0.05834 valid_loss: 0.07705 test_loss: 0.10114 \n",
      "[160/200] train_loss: 0.05894 valid_loss: 0.07779 test_loss: 0.12170 \n",
      "[161/200] train_loss: 0.05948 valid_loss: 0.07736 test_loss: 0.09590 \n",
      "[162/200] train_loss: 0.05774 valid_loss: 0.07653 test_loss: 0.11195 \n",
      "[163/200] train_loss: 0.05821 valid_loss: 0.07660 test_loss: 0.09470 \n",
      "[164/200] train_loss: 0.05918 valid_loss: 0.07763 test_loss: 0.14431 \n",
      "[165/200] train_loss: 0.05872 valid_loss: 0.07784 test_loss: 0.09708 \n",
      "[166/200] train_loss: 0.05770 valid_loss: 0.07894 test_loss: 0.12212 \n",
      "[167/200] train_loss: 0.05679 valid_loss: 0.08004 test_loss: 0.09212 \n",
      "[168/200] train_loss: 0.05796 valid_loss: 0.07836 test_loss: 0.10013 \n",
      "[169/200] train_loss: 0.05776 valid_loss: 0.08064 test_loss: 0.30996 \n",
      "[170/200] train_loss: 0.05630 valid_loss: 0.07649 test_loss: 0.14097 \n",
      "[171/200] train_loss: 0.05637 valid_loss: 0.07669 test_loss: 0.11991 \n",
      "[172/200] train_loss: 0.05632 valid_loss: 0.07633 test_loss: 0.26963 \n",
      "[173/200] train_loss: 0.05804 valid_loss: 0.07703 test_loss: 0.10390 \n",
      "[174/200] train_loss: 0.05626 valid_loss: 0.07829 test_loss: 0.09604 \n",
      "[175/200] train_loss: 0.05848 valid_loss: 0.07647 test_loss: 0.10054 \n",
      "[176/200] train_loss: 0.05677 valid_loss: 0.07698 test_loss: 0.17417 \n",
      "[177/200] train_loss: 0.05665 valid_loss: 0.07870 test_loss: 0.45513 \n",
      "[178/200] train_loss: 0.05674 valid_loss: 0.07743 test_loss: 0.15580 \n",
      "[179/200] train_loss: 0.05760 valid_loss: 0.07856 test_loss: 0.09839 \n",
      "[180/200] train_loss: 0.05629 valid_loss: 0.07554 test_loss: 0.12942 \n",
      "[181/200] train_loss: 0.05540 valid_loss: 0.07656 test_loss: 0.17744 \n",
      "[182/200] train_loss: 0.05760 valid_loss: 0.07678 test_loss: 0.12788 \n",
      "[183/200] train_loss: 0.05852 valid_loss: 0.07778 test_loss: 0.09318 \n",
      "[184/200] train_loss: 0.05752 valid_loss: 0.07802 test_loss: 0.50779 \n",
      "[185/200] train_loss: 0.05787 valid_loss: 0.07618 test_loss: 0.41855 \n",
      "[186/200] train_loss: 0.05668 valid_loss: 0.07748 test_loss: 0.29792 \n",
      "[187/200] train_loss: 0.05655 valid_loss: 0.07591 test_loss: 0.36280 \n",
      "[188/200] train_loss: 0.05632 valid_loss: 0.07767 test_loss: 0.11335 \n",
      "[189/200] train_loss: 0.05830 valid_loss: 0.07492 test_loss: 0.09682 \n",
      "[190/200] train_loss: 0.05532 valid_loss: 0.07610 test_loss: 0.09897 \n",
      "[191/200] train_loss: 0.05532 valid_loss: 0.07673 test_loss: 0.24117 \n",
      "[192/200] train_loss: 0.05568 valid_loss: 0.07666 test_loss: 0.09451 \n",
      "[193/200] train_loss: 0.05604 valid_loss: 0.07892 test_loss: 0.09491 \n",
      "[194/200] train_loss: 0.05607 valid_loss: 0.07716 test_loss: 0.10146 \n",
      "[195/200] train_loss: 0.05535 valid_loss: 0.07709 test_loss: 0.09889 \n",
      "[196/200] train_loss: 0.05512 valid_loss: 0.07547 test_loss: 0.09505 \n",
      "[197/200] train_loss: 0.05574 valid_loss: 0.07614 test_loss: 0.09447 \n",
      "[198/200] train_loss: 0.05525 valid_loss: 0.07620 test_loss: 0.09617 \n",
      "[199/200] train_loss: 0.05590 valid_loss: 0.07609 test_loss: 0.16427 \n",
      "[200/200] train_loss: 0.05407 valid_loss: 0.07748 test_loss: 0.10287 \n",
      "TRAINING MODEL 7\n",
      "[  1/200] train_loss: 0.37845 valid_loss: 0.27382 test_loss: 0.25301 \n",
      "验证损失减少 (inf --> 0.253012). 正在保存模型...\n",
      "[  2/200] train_loss: 0.21928 valid_loss: 0.19708 test_loss: 0.15845 \n",
      "验证损失减少 (0.273819 --> 0.158448). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16847 valid_loss: 0.17318 test_loss: 0.13936 \n",
      "验证损失减少 (0.197078 --> 0.139363). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14947 valid_loss: 0.15126 test_loss: 0.12452 \n",
      "验证损失减少 (0.173179 --> 0.124521). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13918 valid_loss: 0.14015 test_loss: 0.11744 \n",
      "验证损失减少 (0.151255 --> 0.117435). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13065 valid_loss: 0.13236 test_loss: 0.11530 \n",
      "验证损失减少 (0.140153 --> 0.115301). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12327 valid_loss: 0.12669 test_loss: 0.10976 \n",
      "验证损失减少 (0.132360 --> 0.109764). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12126 valid_loss: 0.12504 test_loss: 0.11139 \n",
      "验证损失减少 (0.126686 --> 0.111392). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11936 valid_loss: 0.12305 test_loss: 0.11054 \n",
      "验证损失减少 (0.125042 --> 0.110543). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11619 valid_loss: 0.12144 test_loss: 0.10935 \n",
      "验证损失减少 (0.123052 --> 0.109353). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11203 valid_loss: 0.11816 test_loss: 0.10453 \n",
      "验证损失减少 (0.121444 --> 0.104528). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10991 valid_loss: 0.11571 test_loss: 0.10841 \n",
      "验证损失减少 (0.118161 --> 0.108408). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10930 valid_loss: 0.11295 test_loss: 0.11070 \n",
      "验证损失减少 (0.115711 --> 0.110700). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.11106 valid_loss: 0.11350 test_loss: 0.11880 \n",
      "[ 15/200] train_loss: 0.10551 valid_loss: 0.10939 test_loss: 0.11571 \n",
      "[ 16/200] train_loss: 0.10172 valid_loss: 0.10806 test_loss: 0.13577 \n",
      "[ 17/200] train_loss: 0.10323 valid_loss: 0.10819 test_loss: 0.11304 \n",
      "[ 18/200] train_loss: 0.10030 valid_loss: 0.10752 test_loss: 0.10759 \n",
      "验证损失减少 (0.112953 --> 0.107586). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.10211 valid_loss: 0.10652 test_loss: 0.11088 \n",
      "[ 20/200] train_loss: 0.09826 valid_loss: 0.10299 test_loss: 0.09962 \n",
      "验证损失减少 (0.107523 --> 0.099616). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09694 valid_loss: 0.10176 test_loss: 0.10267 \n",
      "验证损失减少 (0.102988 --> 0.102673). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09649 valid_loss: 0.09817 test_loss: 0.30234 \n",
      "[ 23/200] train_loss: 0.09861 valid_loss: 0.09897 test_loss: 0.13884 \n",
      "[ 24/200] train_loss: 0.09406 valid_loss: 0.09916 test_loss: 0.10519 \n",
      "[ 25/200] train_loss: 0.09560 valid_loss: 0.10073 test_loss: 0.12707 \n",
      "[ 26/200] train_loss: 0.09347 valid_loss: 0.09692 test_loss: 0.18440 \n",
      "[ 27/200] train_loss: 0.09491 valid_loss: 0.09694 test_loss: 0.10163 \n",
      "验证损失减少 (0.101763 --> 0.101633). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28/200] train_loss: 0.09303 valid_loss: 0.09596 test_loss: 0.12505 \n",
      "[ 29/200] train_loss: 0.09075 valid_loss: 0.09493 test_loss: 0.24764 \n",
      "[ 30/200] train_loss: 0.09072 valid_loss: 0.09628 test_loss: 0.20177 \n",
      "[ 31/200] train_loss: 0.09107 valid_loss: 0.09522 test_loss: 0.09825 \n",
      "[ 32/200] train_loss: 0.08993 valid_loss: 0.09330 test_loss: 0.10021 \n",
      "[ 33/200] train_loss: 0.08690 valid_loss: 0.09329 test_loss: 0.13684 \n",
      "[ 34/200] train_loss: 0.08823 valid_loss: 0.09673 test_loss: 0.09508 \n",
      "验证损失减少 (0.096935 --> 0.095079). 正在保存模型...\n",
      "[ 35/200] train_loss: 0.08637 valid_loss: 0.09304 test_loss: 0.09673 \n",
      "[ 36/200] train_loss: 0.08817 valid_loss: 0.09170 test_loss: 0.09619 \n",
      "验证损失减少 (0.096727 --> 0.096186). 正在保存模型...\n",
      "[ 37/200] train_loss: 0.08487 valid_loss: 0.09028 test_loss: 0.11018 \n",
      "[ 38/200] train_loss: 0.08766 valid_loss: 0.09333 test_loss: 0.26110 \n",
      "[ 39/200] train_loss: 0.08476 valid_loss: 0.09142 test_loss: 0.34748 \n",
      "[ 40/200] train_loss: 0.08458 valid_loss: 0.09148 test_loss: 0.13047 \n",
      "[ 41/200] train_loss: 0.08593 valid_loss: 0.09174 test_loss: 0.09809 \n",
      "[ 42/200] train_loss: 0.08474 valid_loss: 0.09134 test_loss: 0.10234 \n",
      "[ 43/200] train_loss: 0.08249 valid_loss: 0.08987 test_loss: 0.14869 \n",
      "[ 44/200] train_loss: 0.08293 valid_loss: 0.08939 test_loss: 0.09839 \n",
      "[ 45/200] train_loss: 0.08072 valid_loss: 0.08910 test_loss: 0.10598 \n",
      "[ 46/200] train_loss: 0.08205 valid_loss: 0.09007 test_loss: 0.09996 \n",
      "[ 47/200] train_loss: 0.08333 valid_loss: 0.09133 test_loss: 0.09593 \n",
      "[ 48/200] train_loss: 0.08211 valid_loss: 0.08887 test_loss: 0.10842 \n",
      "[ 49/200] train_loss: 0.08187 valid_loss: 0.08798 test_loss: 0.09476 \n",
      "[ 50/200] train_loss: 0.08016 valid_loss: 0.08741 test_loss: 0.11570 \n",
      "[ 51/200] train_loss: 0.08086 valid_loss: 0.08706 test_loss: 0.12470 \n",
      "[ 52/200] train_loss: 0.07919 valid_loss: 0.08651 test_loss: 0.10298 \n",
      "[ 53/200] train_loss: 0.07943 valid_loss: 0.08868 test_loss: 0.09418 \n",
      "[ 54/200] train_loss: 0.08030 valid_loss: 0.08433 test_loss: 0.32521 \n",
      "[ 55/200] train_loss: 0.07787 valid_loss: 0.08494 test_loss: 0.18501 \n",
      "[ 56/200] train_loss: 0.07883 valid_loss: 0.08567 test_loss: 0.29449 \n",
      "[ 57/200] train_loss: 0.07779 valid_loss: 0.08603 test_loss: 0.09517 \n",
      "[ 58/200] train_loss: 0.08092 valid_loss: 0.08589 test_loss: 0.09350 \n",
      "[ 59/200] train_loss: 0.07582 valid_loss: 0.08589 test_loss: 0.10087 \n",
      "[ 60/200] train_loss: 0.07559 valid_loss: 0.08538 test_loss: 0.09410 \n",
      "[ 61/200] train_loss: 0.07647 valid_loss: 0.08740 test_loss: 0.08848 \n",
      "验证损失减少 (0.091695 --> 0.088478). 正在保存模型...\n",
      "[ 62/200] train_loss: 0.07712 valid_loss: 0.08417 test_loss: 0.10218 \n",
      "[ 63/200] train_loss: 0.07769 valid_loss: 0.08587 test_loss: 0.09153 \n",
      "[ 64/200] train_loss: 0.07631 valid_loss: 0.08653 test_loss: 0.09313 \n",
      "[ 65/200] train_loss: 0.07545 valid_loss: 0.08473 test_loss: 0.09165 \n",
      "[ 66/200] train_loss: 0.07738 valid_loss: 0.08821 test_loss: 0.09448 \n",
      "[ 67/200] train_loss: 0.07768 valid_loss: 0.08410 test_loss: 0.09264 \n",
      "[ 68/200] train_loss: 0.07565 valid_loss: 0.08350 test_loss: 0.09516 \n",
      "[ 69/200] train_loss: 0.07575 valid_loss: 0.08540 test_loss: 0.08831 \n",
      "[ 70/200] train_loss: 0.07431 valid_loss: 0.08525 test_loss: 0.09346 \n",
      "[ 71/200] train_loss: 0.07457 valid_loss: 0.08392 test_loss: 0.09150 \n",
      "[ 72/200] train_loss: 0.07298 valid_loss: 0.08297 test_loss: 0.10176 \n",
      "[ 73/200] train_loss: 0.07375 valid_loss: 0.08448 test_loss: 0.09844 \n",
      "[ 74/200] train_loss: 0.07339 valid_loss: 0.08309 test_loss: 0.09152 \n",
      "[ 75/200] train_loss: 0.07347 valid_loss: 0.08274 test_loss: 0.09092 \n",
      "[ 76/200] train_loss: 0.07536 valid_loss: 0.08217 test_loss: 0.09609 \n",
      "[ 77/200] train_loss: 0.07260 valid_loss: 0.08363 test_loss: 0.09436 \n",
      "[ 78/200] train_loss: 0.07225 valid_loss: 0.08572 test_loss: 0.09639 \n",
      "[ 79/200] train_loss: 0.07339 valid_loss: 0.08390 test_loss: 0.09146 \n",
      "[ 80/200] train_loss: 0.07130 valid_loss: 0.08277 test_loss: 0.11087 \n",
      "[ 81/200] train_loss: 0.07198 valid_loss: 0.08384 test_loss: 0.09495 \n",
      "[ 82/200] train_loss: 0.07185 valid_loss: 0.08202 test_loss: 0.09409 \n",
      "[ 83/200] train_loss: 0.07250 valid_loss: 0.08465 test_loss: 0.11403 \n",
      "[ 84/200] train_loss: 0.07346 valid_loss: 0.08249 test_loss: 0.09607 \n",
      "[ 85/200] train_loss: 0.07148 valid_loss: 0.08026 test_loss: 0.09286 \n",
      "[ 86/200] train_loss: 0.07087 valid_loss: 0.08150 test_loss: 0.15578 \n",
      "[ 87/200] train_loss: 0.06932 valid_loss: 0.08134 test_loss: 0.10411 \n",
      "[ 88/200] train_loss: 0.07091 valid_loss: 0.08001 test_loss: 0.14634 \n",
      "[ 89/200] train_loss: 0.07037 valid_loss: 0.08334 test_loss: 0.09657 \n",
      "[ 90/200] train_loss: 0.06927 valid_loss: 0.08409 test_loss: 0.16657 \n",
      "[ 91/200] train_loss: 0.07027 valid_loss: 0.08116 test_loss: 0.08907 \n",
      "[ 92/200] train_loss: 0.06860 valid_loss: 0.08084 test_loss: 0.08978 \n",
      "[ 93/200] train_loss: 0.06928 valid_loss: 0.08190 test_loss: 0.09162 \n",
      "[ 94/200] train_loss: 0.06839 valid_loss: 0.07849 test_loss: 0.09466 \n",
      "[ 95/200] train_loss: 0.06953 valid_loss: 0.08043 test_loss: 0.09252 \n",
      "[ 96/200] train_loss: 0.06879 valid_loss: 0.07967 test_loss: 0.09298 \n",
      "[ 97/200] train_loss: 0.06909 valid_loss: 0.08042 test_loss: 0.09147 \n",
      "[ 98/200] train_loss: 0.06853 valid_loss: 0.08152 test_loss: 0.09308 \n",
      "[ 99/200] train_loss: 0.06828 valid_loss: 0.08202 test_loss: 0.09315 \n",
      "[100/200] train_loss: 0.06837 valid_loss: 0.07922 test_loss: 0.09535 \n",
      "[101/200] train_loss: 0.06820 valid_loss: 0.08243 test_loss: 0.09352 \n",
      "[102/200] train_loss: 0.06808 valid_loss: 0.08158 test_loss: 0.09183 \n",
      "[103/200] train_loss: 0.06801 valid_loss: 0.08167 test_loss: 0.09460 \n",
      "[104/200] train_loss: 0.06755 valid_loss: 0.08332 test_loss: 0.10227 \n",
      "[105/200] train_loss: 0.06667 valid_loss: 0.07868 test_loss: 0.35355 \n",
      "[106/200] train_loss: 0.06807 valid_loss: 0.07951 test_loss: 0.09311 \n",
      "[107/200] train_loss: 0.07026 valid_loss: 0.08226 test_loss: 0.09242 \n",
      "[108/200] train_loss: 0.06849 valid_loss: 0.07888 test_loss: 0.09825 \n",
      "[109/200] train_loss: 0.06681 valid_loss: 0.08002 test_loss: 0.12694 \n",
      "[110/200] train_loss: 0.06696 valid_loss: 0.07975 test_loss: 0.09192 \n",
      "[111/200] train_loss: 0.06737 valid_loss: 0.07894 test_loss: 0.09360 \n",
      "[112/200] train_loss: 0.06617 valid_loss: 0.08062 test_loss: 0.09086 \n",
      "[113/200] train_loss: 0.06688 valid_loss: 0.08019 test_loss: 0.09260 \n",
      "[114/200] train_loss: 0.06755 valid_loss: 0.07785 test_loss: 0.10532 \n",
      "[115/200] train_loss: 0.06537 valid_loss: 0.07842 test_loss: 0.13430 \n",
      "[116/200] train_loss: 0.06514 valid_loss: 0.07716 test_loss: 0.09404 \n",
      "[117/200] train_loss: 0.06468 valid_loss: 0.07877 test_loss: 0.08804 \n",
      "[118/200] train_loss: 0.06587 valid_loss: 0.08456 test_loss: 0.10438 \n",
      "[119/200] train_loss: 0.06487 valid_loss: 0.08283 test_loss: 0.09603 \n",
      "[120/200] train_loss: 0.06544 valid_loss: 0.07926 test_loss: 0.12034 \n",
      "[121/200] train_loss: 0.06385 valid_loss: 0.07815 test_loss: 0.09237 \n",
      "[122/200] train_loss: 0.06634 valid_loss: 0.08137 test_loss: 0.09247 \n",
      "[123/200] train_loss: 0.06711 valid_loss: 0.07914 test_loss: 0.09015 \n",
      "[124/200] train_loss: 0.06501 valid_loss: 0.08154 test_loss: 0.09154 \n",
      "[125/200] train_loss: 0.06527 valid_loss: 0.07875 test_loss: 0.08919 \n",
      "[126/200] train_loss: 0.06506 valid_loss: 0.07741 test_loss: 0.09041 \n",
      "[127/200] train_loss: 0.06576 valid_loss: 0.07741 test_loss: 0.09003 \n",
      "[128/200] train_loss: 0.06458 valid_loss: 0.07849 test_loss: 0.08936 \n",
      "[129/200] train_loss: 0.06295 valid_loss: 0.08038 test_loss: 0.09628 \n",
      "[130/200] train_loss: 0.06293 valid_loss: 0.07783 test_loss: 0.32390 \n",
      "[131/200] train_loss: 0.06301 valid_loss: 0.07668 test_loss: 0.16715 \n",
      "[132/200] train_loss: 0.06393 valid_loss: 0.07922 test_loss: 0.09231 \n",
      "[133/200] train_loss: 0.06590 valid_loss: 0.07972 test_loss: 0.09103 \n",
      "[134/200] train_loss: 0.06488 valid_loss: 0.07741 test_loss: 0.09756 \n",
      "[135/200] train_loss: 0.06376 valid_loss: 0.07668 test_loss: 0.08859 \n",
      "[136/200] train_loss: 0.06624 valid_loss: 0.07883 test_loss: 0.09152 \n",
      "[137/200] train_loss: 0.06486 valid_loss: 0.07811 test_loss: 0.09524 \n",
      "[138/200] train_loss: 0.06276 valid_loss: 0.07653 test_loss: 0.09448 \n",
      "[139/200] train_loss: 0.06160 valid_loss: 0.07761 test_loss: 0.09089 \n",
      "[140/200] train_loss: 0.06288 valid_loss: 0.07572 test_loss: 0.30113 \n",
      "[141/200] train_loss: 0.06474 valid_loss: 0.07679 test_loss: 0.09038 \n",
      "[142/200] train_loss: 0.06418 valid_loss: 0.07673 test_loss: 0.09080 \n",
      "[143/200] train_loss: 0.06332 valid_loss: 0.07688 test_loss: 0.13818 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[144/200] train_loss: 0.06171 valid_loss: 0.07807 test_loss: 0.09424 \n",
      "[145/200] train_loss: 0.06153 valid_loss: 0.07574 test_loss: 0.11639 \n",
      "[146/200] train_loss: 0.06183 valid_loss: 0.07757 test_loss: 0.09580 \n",
      "[147/200] train_loss: 0.06220 valid_loss: 0.07739 test_loss: 0.09531 \n",
      "[148/200] train_loss: 0.06231 valid_loss: 0.07648 test_loss: 0.09671 \n",
      "[149/200] train_loss: 0.06272 valid_loss: 0.07786 test_loss: 0.09638 \n",
      "[150/200] train_loss: 0.06245 valid_loss: 0.07638 test_loss: 0.09243 \n",
      "[151/200] train_loss: 0.06241 valid_loss: 0.07662 test_loss: 0.09712 \n",
      "[152/200] train_loss: 0.06194 valid_loss: 0.07678 test_loss: 0.10976 \n",
      "[153/200] train_loss: 0.06220 valid_loss: 0.07757 test_loss: 0.09207 \n",
      "[154/200] train_loss: 0.06154 valid_loss: 0.07744 test_loss: 0.09453 \n",
      "[155/200] train_loss: 0.06272 valid_loss: 0.07610 test_loss: 0.09233 \n",
      "[156/200] train_loss: 0.05998 valid_loss: 0.07736 test_loss: 0.09659 \n",
      "[157/200] train_loss: 0.06246 valid_loss: 0.07860 test_loss: 0.10498 \n",
      "[158/200] train_loss: 0.06113 valid_loss: 0.07588 test_loss: 0.48398 \n",
      "[159/200] train_loss: 0.06076 valid_loss: 0.07655 test_loss: 0.09081 \n",
      "[160/200] train_loss: 0.06265 valid_loss: 0.07844 test_loss: 0.10576 \n",
      "[161/200] train_loss: 0.06133 valid_loss: 0.07696 test_loss: 0.11568 \n",
      "[162/200] train_loss: 0.05994 valid_loss: 0.07659 test_loss: 0.14518 \n",
      "[163/200] train_loss: 0.06130 valid_loss: 0.07550 test_loss: 0.27078 \n",
      "[164/200] train_loss: 0.06132 valid_loss: 0.07653 test_loss: 0.09144 \n",
      "[165/200] train_loss: 0.06126 valid_loss: 0.07681 test_loss: 0.09178 \n",
      "[166/200] train_loss: 0.05873 valid_loss: 0.07654 test_loss: 0.09109 \n",
      "[167/200] train_loss: 0.06053 valid_loss: 0.07766 test_loss: 0.11675 \n",
      "[168/200] train_loss: 0.06026 valid_loss: 0.07540 test_loss: 0.09395 \n",
      "[169/200] train_loss: 0.06108 valid_loss: 0.07798 test_loss: 0.09713 \n",
      "[170/200] train_loss: 0.05940 valid_loss: 0.07829 test_loss: 0.09429 \n",
      "[171/200] train_loss: 0.05944 valid_loss: 0.08016 test_loss: 0.09650 \n",
      "[172/200] train_loss: 0.05959 valid_loss: 0.07671 test_loss: 0.09687 \n",
      "[173/200] train_loss: 0.05973 valid_loss: 0.07859 test_loss: 0.09459 \n",
      "[174/200] train_loss: 0.06078 valid_loss: 0.07704 test_loss: 0.09235 \n",
      "[175/200] train_loss: 0.05960 valid_loss: 0.07673 test_loss: 0.09029 \n",
      "[176/200] train_loss: 0.05766 valid_loss: 0.07780 test_loss: 0.09195 \n",
      "[177/200] train_loss: 0.06010 valid_loss: 0.07771 test_loss: 0.09533 \n",
      "[178/200] train_loss: 0.05846 valid_loss: 0.07608 test_loss: 0.09366 \n",
      "[179/200] train_loss: 0.05941 valid_loss: 0.07576 test_loss: 0.14078 \n",
      "[180/200] train_loss: 0.05849 valid_loss: 0.07511 test_loss: 0.16658 \n",
      "[181/200] train_loss: 0.05903 valid_loss: 0.07552 test_loss: 0.10711 \n",
      "[182/200] train_loss: 0.05812 valid_loss: 0.07776 test_loss: 0.10744 \n",
      "[183/200] train_loss: 0.05806 valid_loss: 0.07664 test_loss: 0.09145 \n",
      "[184/200] train_loss: 0.05904 valid_loss: 0.07678 test_loss: 0.09354 \n",
      "[185/200] train_loss: 0.05770 valid_loss: 0.07526 test_loss: 0.09282 \n",
      "[186/200] train_loss: 0.05801 valid_loss: 0.07690 test_loss: 0.09447 \n",
      "[187/200] train_loss: 0.05708 valid_loss: 0.07535 test_loss: 0.09248 \n",
      "[188/200] train_loss: 0.05871 valid_loss: 0.07812 test_loss: 0.09638 \n",
      "[189/200] train_loss: 0.05614 valid_loss: 0.07726 test_loss: 0.10021 \n",
      "[190/200] train_loss: 0.05957 valid_loss: 0.07826 test_loss: 0.08981 \n",
      "[191/200] train_loss: 0.05724 valid_loss: 0.07550 test_loss: 0.15557 \n",
      "[192/200] train_loss: 0.05690 valid_loss: 0.07452 test_loss: 0.09574 \n",
      "[193/200] train_loss: 0.05751 valid_loss: 0.07585 test_loss: 0.09078 \n",
      "[194/200] train_loss: 0.05602 valid_loss: 0.07772 test_loss: 0.09123 \n",
      "[195/200] train_loss: 0.05784 valid_loss: 0.07581 test_loss: 0.08888 \n",
      "[196/200] train_loss: 0.05901 valid_loss: 0.07879 test_loss: 0.09140 \n",
      "[197/200] train_loss: 0.05875 valid_loss: 0.07663 test_loss: 0.08913 \n",
      "[198/200] train_loss: 0.05592 valid_loss: 0.07693 test_loss: 0.09258 \n",
      "[199/200] train_loss: 0.05817 valid_loss: 0.07612 test_loss: 0.09110 \n",
      "[200/200] train_loss: 0.05653 valid_loss: 0.07631 test_loss: 0.08958 \n",
      "TRAINING MODEL 8\n",
      "[  1/200] train_loss: 0.39466 valid_loss: 0.28392 test_loss: 0.24863 \n",
      "验证损失减少 (inf --> 0.248634). 正在保存模型...\n",
      "[  2/200] train_loss: 0.21558 valid_loss: 0.20539 test_loss: 0.16107 \n",
      "验证损失减少 (0.283915 --> 0.161075). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17041 valid_loss: 0.16819 test_loss: 0.13116 \n",
      "验证损失减少 (0.205394 --> 0.131160). 正在保存模型...\n",
      "[  4/200] train_loss: 0.15063 valid_loss: 0.15244 test_loss: 0.11899 \n",
      "验证损失减少 (0.168185 --> 0.118986). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13839 valid_loss: 0.14205 test_loss: 0.12103 \n",
      "验证损失减少 (0.152442 --> 0.121028). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12967 valid_loss: 0.13437 test_loss: 0.11230 \n",
      "验证损失减少 (0.142052 --> 0.112296). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12236 valid_loss: 0.12913 test_loss: 0.10965 \n",
      "验证损失减少 (0.134373 --> 0.109647). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12021 valid_loss: 0.12603 test_loss: 0.11020 \n",
      "验证损失减少 (0.129125 --> 0.110200). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11708 valid_loss: 0.12168 test_loss: 0.11142 \n",
      "验证损失减少 (0.126026 --> 0.111415). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11383 valid_loss: 0.11941 test_loss: 0.11088 \n",
      "验证损失减少 (0.121675 --> 0.110876). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11049 valid_loss: 0.11824 test_loss: 0.10867 \n",
      "验证损失减少 (0.119411 --> 0.108674). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10951 valid_loss: 0.11392 test_loss: 0.10365 \n",
      "验证损失减少 (0.118243 --> 0.103650). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10995 valid_loss: 0.11209 test_loss: 0.10500 \n",
      "验证损失减少 (0.113919 --> 0.105002). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10678 valid_loss: 0.11056 test_loss: 0.10893 \n",
      "验证损失减少 (0.112093 --> 0.108932). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10307 valid_loss: 0.11041 test_loss: 0.10284 \n",
      "验证损失减少 (0.110564 --> 0.102839). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10178 valid_loss: 0.10581 test_loss: 0.11772 \n",
      "[ 17/200] train_loss: 0.10159 valid_loss: 0.10535 test_loss: 0.10508 \n",
      "验证损失减少 (0.110415 --> 0.105076). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.09929 valid_loss: 0.10228 test_loss: 0.10778 \n",
      "[ 19/200] train_loss: 0.09730 valid_loss: 0.10197 test_loss: 0.11853 \n",
      "[ 20/200] train_loss: 0.09908 valid_loss: 0.10199 test_loss: 0.10412 \n",
      "验证损失减少 (0.105351 --> 0.104115). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09568 valid_loss: 0.10330 test_loss: 0.10434 \n",
      "[ 22/200] train_loss: 0.09501 valid_loss: 0.09998 test_loss: 0.11591 \n",
      "[ 23/200] train_loss: 0.09432 valid_loss: 0.09981 test_loss: 0.09932 \n",
      "验证损失减少 (0.101994 --> 0.099320). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.09468 valid_loss: 0.09817 test_loss: 0.09976 \n",
      "验证损失减少 (0.099813 --> 0.099757). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09321 valid_loss: 0.09572 test_loss: 0.16573 \n",
      "[ 26/200] train_loss: 0.09001 valid_loss: 0.09495 test_loss: 0.14525 \n",
      "[ 27/200] train_loss: 0.09169 valid_loss: 0.09493 test_loss: 0.10158 \n",
      "[ 28/200] train_loss: 0.08972 valid_loss: 0.09293 test_loss: 0.15508 \n",
      "[ 29/200] train_loss: 0.08804 valid_loss: 0.09409 test_loss: 0.11155 \n",
      "[ 30/200] train_loss: 0.08875 valid_loss: 0.09246 test_loss: 0.15705 \n",
      "[ 31/200] train_loss: 0.08873 valid_loss: 0.09312 test_loss: 0.15794 \n",
      "[ 32/200] train_loss: 0.08755 valid_loss: 0.09239 test_loss: 0.10183 \n",
      "[ 33/200] train_loss: 0.08482 valid_loss: 0.09010 test_loss: 0.21316 \n",
      "[ 34/200] train_loss: 0.08584 valid_loss: 0.09028 test_loss: 0.13220 \n",
      "[ 35/200] train_loss: 0.08626 valid_loss: 0.09213 test_loss: 0.09504 \n",
      "验证损失减少 (0.098172 --> 0.095043). 正在保存模型...\n",
      "[ 36/200] train_loss: 0.08574 valid_loss: 0.09006 test_loss: 0.11172 \n",
      "[ 37/200] train_loss: 0.08524 valid_loss: 0.08934 test_loss: 0.09753 \n",
      "[ 38/200] train_loss: 0.08236 valid_loss: 0.08857 test_loss: 0.10091 \n",
      "[ 39/200] train_loss: 0.08232 valid_loss: 0.08782 test_loss: 0.22586 \n",
      "[ 40/200] train_loss: 0.08181 valid_loss: 0.08949 test_loss: 0.09352 \n",
      "[ 41/200] train_loss: 0.08274 valid_loss: 0.08980 test_loss: 0.09319 \n",
      "[ 42/200] train_loss: 0.08255 valid_loss: 0.08684 test_loss: 0.09428 \n",
      "[ 43/200] train_loss: 0.08404 valid_loss: 0.08914 test_loss: 0.09517 \n",
      "[ 44/200] train_loss: 0.08120 valid_loss: 0.08610 test_loss: 0.11602 \n",
      "[ 45/200] train_loss: 0.08199 valid_loss: 0.08733 test_loss: 0.19745 \n",
      "[ 46/200] train_loss: 0.07940 valid_loss: 0.08605 test_loss: 0.09368 \n",
      "[ 47/200] train_loss: 0.07855 valid_loss: 0.08580 test_loss: 0.10476 \n",
      "[ 48/200] train_loss: 0.07869 valid_loss: 0.08603 test_loss: 0.09520 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 49/200] train_loss: 0.07979 valid_loss: 0.08768 test_loss: 0.11488 \n",
      "[ 50/200] train_loss: 0.07949 valid_loss: 0.08601 test_loss: 0.09342 \n",
      "[ 51/200] train_loss: 0.08028 valid_loss: 0.08665 test_loss: 0.09342 \n",
      "[ 52/200] train_loss: 0.07790 valid_loss: 0.08504 test_loss: 0.11551 \n",
      "[ 53/200] train_loss: 0.07813 valid_loss: 0.08468 test_loss: 0.14211 \n",
      "[ 54/200] train_loss: 0.07792 valid_loss: 0.08447 test_loss: 0.12326 \n",
      "[ 55/200] train_loss: 0.07783 valid_loss: 0.08524 test_loss: 0.09108 \n",
      "验证损失减少 (0.092133 --> 0.091076). 正在保存模型...\n",
      "[ 56/200] train_loss: 0.07803 valid_loss: 0.08333 test_loss: 0.14594 \n",
      "[ 57/200] train_loss: 0.07696 valid_loss: 0.08364 test_loss: 0.11412 \n",
      "[ 58/200] train_loss: 0.07786 valid_loss: 0.08363 test_loss: 0.09581 \n",
      "[ 59/200] train_loss: 0.07578 valid_loss: 0.08317 test_loss: 0.15726 \n",
      "[ 60/200] train_loss: 0.07668 valid_loss: 0.08405 test_loss: 0.09078 \n",
      "[ 61/200] train_loss: 0.07434 valid_loss: 0.08394 test_loss: 0.10532 \n",
      "[ 62/200] train_loss: 0.07609 valid_loss: 0.08261 test_loss: 0.09202 \n",
      "[ 63/200] train_loss: 0.07348 valid_loss: 0.08230 test_loss: 0.09489 \n",
      "[ 64/200] train_loss: 0.07752 valid_loss: 0.08164 test_loss: 0.09086 \n",
      "[ 65/200] train_loss: 0.07491 valid_loss: 0.08213 test_loss: 0.08680 \n",
      "[ 66/200] train_loss: 0.07423 valid_loss: 0.08155 test_loss: 0.08952 \n",
      "[ 67/200] train_loss: 0.07438 valid_loss: 0.08047 test_loss: 0.09034 \n",
      "[ 68/200] train_loss: 0.07346 valid_loss: 0.08095 test_loss: 0.09470 \n",
      "[ 69/200] train_loss: 0.07283 valid_loss: 0.08094 test_loss: 0.10568 \n",
      "[ 70/200] train_loss: 0.07219 valid_loss: 0.08094 test_loss: 0.08841 \n",
      "[ 71/200] train_loss: 0.07393 valid_loss: 0.08242 test_loss: 0.11176 \n",
      "[ 72/200] train_loss: 0.07374 valid_loss: 0.08085 test_loss: 0.10991 \n",
      "[ 73/200] train_loss: 0.07444 valid_loss: 0.08066 test_loss: 0.09165 \n",
      "[ 74/200] train_loss: 0.07156 valid_loss: 0.08013 test_loss: 0.34036 \n",
      "[ 75/200] train_loss: 0.07131 valid_loss: 0.08179 test_loss: 0.25499 \n",
      "[ 76/200] train_loss: 0.07125 valid_loss: 0.08154 test_loss: 0.09290 \n",
      "[ 77/200] train_loss: 0.07336 valid_loss: 0.08028 test_loss: 0.11184 \n",
      "[ 78/200] train_loss: 0.07127 valid_loss: 0.08038 test_loss: 0.09660 \n",
      "[ 79/200] train_loss: 0.07074 valid_loss: 0.07977 test_loss: 0.09165 \n",
      "[ 80/200] train_loss: 0.06920 valid_loss: 0.08328 test_loss: 0.09045 \n",
      "[ 81/200] train_loss: 0.07071 valid_loss: 0.08015 test_loss: 0.14406 \n",
      "[ 82/200] train_loss: 0.07119 valid_loss: 0.07972 test_loss: 0.09441 \n",
      "[ 83/200] train_loss: 0.07057 valid_loss: 0.08053 test_loss: 0.08915 \n",
      "[ 84/200] train_loss: 0.07148 valid_loss: 0.08013 test_loss: 0.09398 \n",
      "[ 85/200] train_loss: 0.06913 valid_loss: 0.07974 test_loss: 0.08989 \n",
      "[ 86/200] train_loss: 0.07065 valid_loss: 0.07961 test_loss: 0.08866 \n",
      "[ 87/200] train_loss: 0.06962 valid_loss: 0.07934 test_loss: 0.08803 \n",
      "[ 88/200] train_loss: 0.06813 valid_loss: 0.07934 test_loss: 0.08947 \n",
      "[ 89/200] train_loss: 0.06821 valid_loss: 0.07867 test_loss: 0.08968 \n",
      "[ 90/200] train_loss: 0.06798 valid_loss: 0.08069 test_loss: 0.08883 \n",
      "[ 91/200] train_loss: 0.06929 valid_loss: 0.07979 test_loss: 0.08970 \n",
      "[ 92/200] train_loss: 0.06805 valid_loss: 0.07979 test_loss: 0.12605 \n",
      "[ 93/200] train_loss: 0.06864 valid_loss: 0.07943 test_loss: 0.10021 \n",
      "[ 94/200] train_loss: 0.06859 valid_loss: 0.07716 test_loss: 0.08719 \n",
      "[ 95/200] train_loss: 0.06781 valid_loss: 0.07885 test_loss: 0.09897 \n",
      "[ 96/200] train_loss: 0.06614 valid_loss: 0.07994 test_loss: 0.09081 \n",
      "[ 97/200] train_loss: 0.06803 valid_loss: 0.08004 test_loss: 0.09350 \n",
      "[ 98/200] train_loss: 0.06688 valid_loss: 0.07929 test_loss: 0.09310 \n",
      "[ 99/200] train_loss: 0.07000 valid_loss: 0.07923 test_loss: 0.09371 \n",
      "[100/200] train_loss: 0.06611 valid_loss: 0.07753 test_loss: 0.09112 \n",
      "[101/200] train_loss: 0.06687 valid_loss: 0.08067 test_loss: 0.12147 \n",
      "[102/200] train_loss: 0.06462 valid_loss: 0.08045 test_loss: 0.09092 \n",
      "[103/200] train_loss: 0.06623 valid_loss: 0.07893 test_loss: 0.09026 \n",
      "[104/200] train_loss: 0.06660 valid_loss: 0.08151 test_loss: 0.09985 \n",
      "[105/200] train_loss: 0.06776 valid_loss: 0.07972 test_loss: 0.09739 \n",
      "[106/200] train_loss: 0.06606 valid_loss: 0.08054 test_loss: 0.09288 \n",
      "[107/200] train_loss: 0.06478 valid_loss: 0.07745 test_loss: 0.11266 \n",
      "[108/200] train_loss: 0.06476 valid_loss: 0.07959 test_loss: 0.10080 \n",
      "[109/200] train_loss: 0.06695 valid_loss: 0.07955 test_loss: 0.14166 \n",
      "[110/200] train_loss: 0.06529 valid_loss: 0.07737 test_loss: 0.09610 \n",
      "[111/200] train_loss: 0.06779 valid_loss: 0.07862 test_loss: 0.30363 \n",
      "[112/200] train_loss: 0.06640 valid_loss: 0.07891 test_loss: 0.09522 \n",
      "[113/200] train_loss: 0.06476 valid_loss: 0.07838 test_loss: 0.11230 \n",
      "[114/200] train_loss: 0.06357 valid_loss: 0.08295 test_loss: 0.09853 \n",
      "[115/200] train_loss: 0.06627 valid_loss: 0.07888 test_loss: 0.09226 \n",
      "[116/200] train_loss: 0.06435 valid_loss: 0.07966 test_loss: 0.16946 \n",
      "[117/200] train_loss: 0.06294 valid_loss: 0.07870 test_loss: 0.09099 \n",
      "[118/200] train_loss: 0.06598 valid_loss: 0.07934 test_loss: 0.11175 \n",
      "[119/200] train_loss: 0.06475 valid_loss: 0.07838 test_loss: 0.09408 \n",
      "[120/200] train_loss: 0.06443 valid_loss: 0.07924 test_loss: 0.09368 \n",
      "[121/200] train_loss: 0.06281 valid_loss: 0.07887 test_loss: 0.09183 \n",
      "[122/200] train_loss: 0.06371 valid_loss: 0.07915 test_loss: 0.09497 \n",
      "[123/200] train_loss: 0.06364 valid_loss: 0.07767 test_loss: 0.09529 \n",
      "[124/200] train_loss: 0.06202 valid_loss: 0.07894 test_loss: 0.09467 \n",
      "[125/200] train_loss: 0.06391 valid_loss: 0.07964 test_loss: 0.09744 \n",
      "[126/200] train_loss: 0.06322 valid_loss: 0.07835 test_loss: 0.09518 \n",
      "[127/200] train_loss: 0.06226 valid_loss: 0.07777 test_loss: 0.09095 \n",
      "[128/200] train_loss: 0.06357 valid_loss: 0.07756 test_loss: 0.12566 \n",
      "[129/200] train_loss: 0.06300 valid_loss: 0.07872 test_loss: 0.16544 \n",
      "[130/200] train_loss: 0.06188 valid_loss: 0.07899 test_loss: 0.09323 \n",
      "[131/200] train_loss: 0.06165 valid_loss: 0.08216 test_loss: 0.10959 \n",
      "[132/200] train_loss: 0.06283 valid_loss: 0.07715 test_loss: 0.09343 \n",
      "[133/200] train_loss: 0.06126 valid_loss: 0.07665 test_loss: 0.09140 \n",
      "[134/200] train_loss: 0.06213 valid_loss: 0.07902 test_loss: 0.09912 \n",
      "[135/200] train_loss: 0.06334 valid_loss: 0.07650 test_loss: 0.09875 \n",
      "[136/200] train_loss: 0.06107 valid_loss: 0.07739 test_loss: 0.09789 \n",
      "[137/200] train_loss: 0.06210 valid_loss: 0.07826 test_loss: 0.09362 \n",
      "[138/200] train_loss: 0.06330 valid_loss: 0.07764 test_loss: 0.09367 \n",
      "[139/200] train_loss: 0.05939 valid_loss: 0.07768 test_loss: 0.09574 \n",
      "[140/200] train_loss: 0.06196 valid_loss: 0.07968 test_loss: 0.09518 \n",
      "[141/200] train_loss: 0.06197 valid_loss: 0.08120 test_loss: 0.09630 \n",
      "[142/200] train_loss: 0.06077 valid_loss: 0.07939 test_loss: 0.09201 \n",
      "[143/200] train_loss: 0.06215 valid_loss: 0.08141 test_loss: 0.10109 \n",
      "[144/200] train_loss: 0.06150 valid_loss: 0.07689 test_loss: 0.09528 \n",
      "[145/200] train_loss: 0.06112 valid_loss: 0.07587 test_loss: 0.09536 \n",
      "[146/200] train_loss: 0.05934 valid_loss: 0.07535 test_loss: 0.09442 \n",
      "[147/200] train_loss: 0.06022 valid_loss: 0.07527 test_loss: 0.10942 \n",
      "[148/200] train_loss: 0.06152 valid_loss: 0.07770 test_loss: 0.09606 \n",
      "[149/200] train_loss: 0.06008 valid_loss: 0.07534 test_loss: 0.09223 \n",
      "[150/200] train_loss: 0.06080 valid_loss: 0.07657 test_loss: 0.09954 \n",
      "[151/200] train_loss: 0.06017 valid_loss: 0.07692 test_loss: 0.09375 \n",
      "[152/200] train_loss: 0.05983 valid_loss: 0.07939 test_loss: 0.09981 \n",
      "[153/200] train_loss: 0.05927 valid_loss: 0.07678 test_loss: 0.10077 \n",
      "[154/200] train_loss: 0.05969 valid_loss: 0.07748 test_loss: 0.09656 \n",
      "[155/200] train_loss: 0.06097 valid_loss: 0.07673 test_loss: 0.09743 \n",
      "[156/200] train_loss: 0.06139 valid_loss: 0.07478 test_loss: 0.09866 \n",
      "[157/200] train_loss: 0.05961 valid_loss: 0.07640 test_loss: 0.09635 \n",
      "[158/200] train_loss: 0.05962 valid_loss: 0.07611 test_loss: 0.10675 \n",
      "[159/200] train_loss: 0.05819 valid_loss: 0.07738 test_loss: 0.10255 \n",
      "[160/200] train_loss: 0.05901 valid_loss: 0.07504 test_loss: 0.11137 \n",
      "[161/200] train_loss: 0.05951 valid_loss: 0.07603 test_loss: 0.09792 \n",
      "[162/200] train_loss: 0.06033 valid_loss: 0.07789 test_loss: 0.09892 \n",
      "[163/200] train_loss: 0.05827 valid_loss: 0.07482 test_loss: 0.09290 \n",
      "[164/200] train_loss: 0.05884 valid_loss: 0.07518 test_loss: 0.09550 \n",
      "[165/200] train_loss: 0.05883 valid_loss: 0.07507 test_loss: 0.09550 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[166/200] train_loss: 0.05734 valid_loss: 0.07601 test_loss: 0.09419 \n",
      "[167/200] train_loss: 0.05776 valid_loss: 0.07586 test_loss: 0.09401 \n",
      "[168/200] train_loss: 0.05862 valid_loss: 0.07807 test_loss: 0.09934 \n",
      "[169/200] train_loss: 0.05863 valid_loss: 0.07491 test_loss: 0.09056 \n",
      "[170/200] train_loss: 0.05838 valid_loss: 0.07475 test_loss: 0.09172 \n",
      "[171/200] train_loss: 0.05903 valid_loss: 0.07486 test_loss: 0.10636 \n",
      "[172/200] train_loss: 0.05699 valid_loss: 0.07676 test_loss: 0.09505 \n",
      "[173/200] train_loss: 0.05804 valid_loss: 0.08190 test_loss: 0.10408 \n",
      "[174/200] train_loss: 0.05735 valid_loss: 0.07644 test_loss: 0.09865 \n",
      "[175/200] train_loss: 0.05832 valid_loss: 0.07652 test_loss: 0.09947 \n",
      "[176/200] train_loss: 0.05660 valid_loss: 0.07786 test_loss: 0.13359 \n",
      "[177/200] train_loss: 0.05736 valid_loss: 0.07777 test_loss: 0.09877 \n",
      "[178/200] train_loss: 0.05846 valid_loss: 0.07940 test_loss: 0.10872 \n",
      "[179/200] train_loss: 0.05759 valid_loss: 0.07604 test_loss: 0.10562 \n",
      "[180/200] train_loss: 0.05650 valid_loss: 0.07616 test_loss: 0.28219 \n",
      "[181/200] train_loss: 0.05843 valid_loss: 0.07536 test_loss: 0.09678 \n",
      "[182/200] train_loss: 0.05626 valid_loss: 0.07635 test_loss: 0.09634 \n",
      "[183/200] train_loss: 0.05743 valid_loss: 0.07551 test_loss: 0.09275 \n",
      "[184/200] train_loss: 0.05736 valid_loss: 0.07611 test_loss: 0.09644 \n",
      "[185/200] train_loss: 0.05746 valid_loss: 0.07719 test_loss: 0.09539 \n",
      "[186/200] train_loss: 0.05620 valid_loss: 0.07558 test_loss: 0.09551 \n",
      "[187/200] train_loss: 0.05501 valid_loss: 0.07633 test_loss: 0.09760 \n",
      "[188/200] train_loss: 0.05622 valid_loss: 0.07701 test_loss: 0.09640 \n",
      "[189/200] train_loss: 0.05763 valid_loss: 0.07576 test_loss: 0.11117 \n",
      "[190/200] train_loss: 0.05642 valid_loss: 0.07653 test_loss: 0.12304 \n",
      "[191/200] train_loss: 0.05543 valid_loss: 0.07761 test_loss: 0.10033 \n",
      "[192/200] train_loss: 0.05496 valid_loss: 0.07791 test_loss: 0.09697 \n",
      "[193/200] train_loss: 0.05594 valid_loss: 0.07619 test_loss: 0.10087 \n",
      "[194/200] train_loss: 0.05500 valid_loss: 0.07573 test_loss: 0.09575 \n",
      "[195/200] train_loss: 0.05414 valid_loss: 0.07577 test_loss: 0.09455 \n",
      "[196/200] train_loss: 0.05607 valid_loss: 0.07565 test_loss: 0.09496 \n",
      "[197/200] train_loss: 0.05744 valid_loss: 0.07592 test_loss: 0.09785 \n",
      "[198/200] train_loss: 0.05595 valid_loss: 0.07518 test_loss: 0.09455 \n",
      "[199/200] train_loss: 0.05493 valid_loss: 0.07653 test_loss: 0.09511 \n",
      "[200/200] train_loss: 0.05621 valid_loss: 0.07759 test_loss: 0.09619 \n",
      "TRAINING MODEL 9\n",
      "[  1/200] train_loss: 0.36305 valid_loss: 0.25533 test_loss: 0.23332 \n",
      "验证损失减少 (inf --> 0.233319). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20450 valid_loss: 0.18933 test_loss: 0.14417 \n",
      "验证损失减少 (0.255325 --> 0.144173). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16594 valid_loss: 0.16601 test_loss: 0.13303 \n",
      "验证损失减少 (0.189332 --> 0.133028). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14582 valid_loss: 0.15057 test_loss: 0.12877 \n",
      "验证损失减少 (0.166005 --> 0.128775). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13530 valid_loss: 0.13677 test_loss: 0.11397 \n",
      "验证损失减少 (0.150570 --> 0.113969). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12814 valid_loss: 0.12982 test_loss: 0.11239 \n",
      "验证损失减少 (0.136765 --> 0.112394). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12067 valid_loss: 0.12616 test_loss: 0.11104 \n",
      "验证损失减少 (0.129824 --> 0.111040). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11984 valid_loss: 0.12189 test_loss: 0.11116 \n",
      "验证损失减少 (0.126158 --> 0.111162). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11591 valid_loss: 0.11819 test_loss: 0.10285 \n",
      "验证损失减少 (0.121894 --> 0.102854). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11431 valid_loss: 0.12013 test_loss: 0.11166 \n",
      "验证损失减少 (0.118189 --> 0.111659). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11186 valid_loss: 0.11436 test_loss: 0.10496 \n",
      "验证损失减少 (0.120130 --> 0.104964). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10929 valid_loss: 0.11375 test_loss: 0.10049 \n",
      "验证损失减少 (0.114359 --> 0.100493). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10638 valid_loss: 0.11176 test_loss: 0.10563 \n",
      "验证损失减少 (0.113746 --> 0.105628). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10764 valid_loss: 0.10832 test_loss: 0.09964 \n",
      "验证损失减少 (0.111763 --> 0.099644). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10554 valid_loss: 0.10740 test_loss: 0.10094 \n",
      "验证损失减少 (0.108318 --> 0.100943). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10307 valid_loss: 0.10415 test_loss: 0.20526 \n",
      "[ 17/200] train_loss: 0.09988 valid_loss: 0.10908 test_loss: 0.10853 \n",
      "[ 18/200] train_loss: 0.10012 valid_loss: 0.10249 test_loss: 0.10032 \n",
      "验证损失减少 (0.107401 --> 0.100319). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09764 valid_loss: 0.10127 test_loss: 0.09933 \n",
      "验证损失减少 (0.102491 --> 0.099334). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09557 valid_loss: 0.10032 test_loss: 0.11671 \n",
      "[ 21/200] train_loss: 0.09496 valid_loss: 0.09881 test_loss: 0.13737 \n",
      "[ 22/200] train_loss: 0.09347 valid_loss: 0.09866 test_loss: 0.10056 \n",
      "验证损失减少 (0.101269 --> 0.100559). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09384 valid_loss: 0.09806 test_loss: 0.11433 \n",
      "[ 24/200] train_loss: 0.09213 valid_loss: 0.09462 test_loss: 0.14577 \n",
      "[ 25/200] train_loss: 0.09391 valid_loss: 0.09859 test_loss: 0.09893 \n",
      "[ 26/200] train_loss: 0.09092 valid_loss: 0.09524 test_loss: 0.10897 \n",
      "[ 27/200] train_loss: 0.08856 valid_loss: 0.09618 test_loss: 0.09982 \n",
      "[ 28/200] train_loss: 0.08941 valid_loss: 0.09394 test_loss: 0.09375 \n",
      "验证损失减少 (0.098661 --> 0.093754). 正在保存模型...\n",
      "[ 29/200] train_loss: 0.08913 valid_loss: 0.09309 test_loss: 0.17896 \n",
      "[ 30/200] train_loss: 0.08861 valid_loss: 0.09315 test_loss: 0.10140 \n",
      "[ 31/200] train_loss: 0.08760 valid_loss: 0.09144 test_loss: 0.09370 \n",
      "验证损失减少 (0.093939 --> 0.093702). 正在保存模型...\n",
      "[ 32/200] train_loss: 0.08543 valid_loss: 0.09106 test_loss: 0.10272 \n",
      "[ 33/200] train_loss: 0.08500 valid_loss: 0.09029 test_loss: 0.10602 \n",
      "[ 34/200] train_loss: 0.08561 valid_loss: 0.09140 test_loss: 0.09703 \n",
      "[ 35/200] train_loss: 0.08640 valid_loss: 0.08995 test_loss: 0.19645 \n",
      "[ 36/200] train_loss: 0.08378 valid_loss: 0.08953 test_loss: 0.31176 \n",
      "[ 37/200] train_loss: 0.08668 valid_loss: 0.08781 test_loss: 0.25197 \n",
      "[ 38/200] train_loss: 0.08315 valid_loss: 0.08839 test_loss: 0.11049 \n",
      "[ 39/200] train_loss: 0.08231 valid_loss: 0.08817 test_loss: 0.10176 \n",
      "[ 40/200] train_loss: 0.08158 valid_loss: 0.08851 test_loss: 0.09511 \n",
      "[ 41/200] train_loss: 0.08115 valid_loss: 0.09048 test_loss: 0.09471 \n",
      "[ 42/200] train_loss: 0.08130 valid_loss: 0.09096 test_loss: 0.09681 \n",
      "[ 43/200] train_loss: 0.08268 valid_loss: 0.08860 test_loss: 0.09566 \n",
      "[ 44/200] train_loss: 0.08054 valid_loss: 0.08698 test_loss: 0.09409 \n",
      "[ 45/200] train_loss: 0.07999 valid_loss: 0.08630 test_loss: 0.19335 \n",
      "[ 46/200] train_loss: 0.08144 valid_loss: 0.08472 test_loss: 0.16587 \n",
      "[ 47/200] train_loss: 0.08021 valid_loss: 0.08454 test_loss: 0.28271 \n",
      "[ 48/200] train_loss: 0.08078 valid_loss: 0.08369 test_loss: 0.20768 \n",
      "[ 49/200] train_loss: 0.07973 valid_loss: 0.08729 test_loss: 0.13090 \n",
      "[ 50/200] train_loss: 0.07977 valid_loss: 0.08241 test_loss: 0.11450 \n",
      "[ 51/200] train_loss: 0.07637 valid_loss: 0.08219 test_loss: 0.09184 \n",
      "[ 52/200] train_loss: 0.07673 valid_loss: 0.08603 test_loss: 0.09233 \n",
      "[ 53/200] train_loss: 0.07663 valid_loss: 0.08401 test_loss: 0.12365 \n",
      "[ 54/200] train_loss: 0.07693 valid_loss: 0.08439 test_loss: 0.12101 \n",
      "[ 55/200] train_loss: 0.07597 valid_loss: 0.08498 test_loss: 0.09226 \n",
      "[ 56/200] train_loss: 0.07808 valid_loss: 0.08430 test_loss: 0.09358 \n",
      "[ 57/200] train_loss: 0.07494 valid_loss: 0.08348 test_loss: 0.10033 \n",
      "[ 58/200] train_loss: 0.07585 valid_loss: 0.08453 test_loss: 0.15660 \n",
      "[ 59/200] train_loss: 0.07499 valid_loss: 0.08363 test_loss: 0.09563 \n",
      "[ 60/200] train_loss: 0.07487 valid_loss: 0.08307 test_loss: 0.09582 \n",
      "[ 61/200] train_loss: 0.07543 valid_loss: 0.08405 test_loss: 0.09407 \n",
      "[ 62/200] train_loss: 0.07403 valid_loss: 0.08371 test_loss: 0.09274 \n",
      "[ 63/200] train_loss: 0.07102 valid_loss: 0.08362 test_loss: 0.10342 \n",
      "[ 64/200] train_loss: 0.07323 valid_loss: 0.08318 test_loss: 0.08992 \n",
      "验证损失减少 (0.091436 --> 0.089918). 正在保存模型...\n",
      "[ 65/200] train_loss: 0.07408 valid_loss: 0.08164 test_loss: 0.31859 \n",
      "[ 66/200] train_loss: 0.07430 valid_loss: 0.08138 test_loss: 0.13394 \n",
      "[ 67/200] train_loss: 0.07185 valid_loss: 0.08217 test_loss: 0.09194 \n",
      "[ 68/200] train_loss: 0.07436 valid_loss: 0.08290 test_loss: 0.10447 \n",
      "[ 69/200] train_loss: 0.07378 valid_loss: 0.08253 test_loss: 0.09424 \n",
      "[ 70/200] train_loss: 0.07140 valid_loss: 0.08187 test_loss: 0.09913 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 71/200] train_loss: 0.07207 valid_loss: 0.08104 test_loss: 0.10396 \n",
      "[ 72/200] train_loss: 0.07175 valid_loss: 0.08099 test_loss: 0.09855 \n",
      "[ 73/200] train_loss: 0.07180 valid_loss: 0.08106 test_loss: 0.09737 \n",
      "[ 74/200] train_loss: 0.06960 valid_loss: 0.08007 test_loss: 0.15289 \n",
      "[ 75/200] train_loss: 0.07198 valid_loss: 0.07982 test_loss: 0.20610 \n",
      "[ 76/200] train_loss: 0.07097 valid_loss: 0.08324 test_loss: 0.09420 \n",
      "[ 77/200] train_loss: 0.07047 valid_loss: 0.07975 test_loss: 0.34350 \n",
      "[ 78/200] train_loss: 0.07236 valid_loss: 0.07953 test_loss: 0.14298 \n",
      "[ 79/200] train_loss: 0.07133 valid_loss: 0.08226 test_loss: 0.09493 \n",
      "[ 80/200] train_loss: 0.07056 valid_loss: 0.08144 test_loss: 0.08870 \n",
      "[ 81/200] train_loss: 0.07208 valid_loss: 0.08145 test_loss: 0.24509 \n",
      "[ 82/200] train_loss: 0.06950 valid_loss: 0.07880 test_loss: 0.12199 \n",
      "[ 83/200] train_loss: 0.07007 valid_loss: 0.08112 test_loss: 0.09396 \n",
      "[ 84/200] train_loss: 0.06911 valid_loss: 0.07883 test_loss: 0.39456 \n",
      "[ 85/200] train_loss: 0.06984 valid_loss: 0.07989 test_loss: 0.09451 \n",
      "[ 86/200] train_loss: 0.06700 valid_loss: 0.08003 test_loss: 0.10641 \n",
      "[ 87/200] train_loss: 0.06759 valid_loss: 0.07917 test_loss: 0.21734 \n",
      "[ 88/200] train_loss: 0.07073 valid_loss: 0.07839 test_loss: 0.16365 \n",
      "[ 89/200] train_loss: 0.06896 valid_loss: 0.07855 test_loss: 0.11375 \n",
      "[ 90/200] train_loss: 0.06867 valid_loss: 0.07961 test_loss: 0.09404 \n",
      "[ 91/200] train_loss: 0.06716 valid_loss: 0.07821 test_loss: 0.08958 \n",
      "[ 92/200] train_loss: 0.06682 valid_loss: 0.08024 test_loss: 0.09431 \n",
      "[ 93/200] train_loss: 0.06848 valid_loss: 0.07956 test_loss: 0.39625 \n",
      "[ 94/200] train_loss: 0.06721 valid_loss: 0.07852 test_loss: 0.14604 \n",
      "[ 95/200] train_loss: 0.06668 valid_loss: 0.07875 test_loss: 0.09654 \n",
      "[ 96/200] train_loss: 0.06563 valid_loss: 0.07862 test_loss: 0.09683 \n",
      "[ 97/200] train_loss: 0.06816 valid_loss: 0.07814 test_loss: 0.10175 \n",
      "[ 98/200] train_loss: 0.06729 valid_loss: 0.08187 test_loss: 0.09696 \n",
      "[ 99/200] train_loss: 0.06659 valid_loss: 0.07933 test_loss: 0.23241 \n",
      "[100/200] train_loss: 0.06678 valid_loss: 0.07856 test_loss: 0.52999 \n",
      "[101/200] train_loss: 0.06707 valid_loss: 0.07746 test_loss: 0.09111 \n",
      "[102/200] train_loss: 0.06519 valid_loss: 0.07985 test_loss: 0.09153 \n",
      "[103/200] train_loss: 0.06569 valid_loss: 0.07723 test_loss: 0.10583 \n",
      "[104/200] train_loss: 0.06599 valid_loss: 0.07781 test_loss: 0.16746 \n",
      "[105/200] train_loss: 0.06569 valid_loss: 0.07713 test_loss: 0.09728 \n",
      "[106/200] train_loss: 0.06488 valid_loss: 0.07712 test_loss: 0.09154 \n",
      "[107/200] train_loss: 0.06635 valid_loss: 0.07884 test_loss: 0.09518 \n",
      "[108/200] train_loss: 0.06595 valid_loss: 0.07840 test_loss: 0.24764 \n",
      "[109/200] train_loss: 0.06573 valid_loss: 0.07975 test_loss: 0.68621 \n",
      "[110/200] train_loss: 0.06530 valid_loss: 0.07913 test_loss: 0.09354 \n",
      "[111/200] train_loss: 0.06424 valid_loss: 0.07615 test_loss: 0.15735 \n",
      "[112/200] train_loss: 0.06461 valid_loss: 0.07715 test_loss: 0.13391 \n",
      "[113/200] train_loss: 0.06436 valid_loss: 0.07713 test_loss: 0.11467 \n",
      "[114/200] train_loss: 0.06478 valid_loss: 0.07599 test_loss: 0.12032 \n",
      "[115/200] train_loss: 0.06458 valid_loss: 0.07589 test_loss: 0.15746 \n",
      "[116/200] train_loss: 0.06271 valid_loss: 0.07809 test_loss: 0.09215 \n",
      "[117/200] train_loss: 0.06316 valid_loss: 0.07775 test_loss: 0.09557 \n",
      "[118/200] train_loss: 0.06285 valid_loss: 0.07609 test_loss: 0.08952 \n",
      "[119/200] train_loss: 0.06463 valid_loss: 0.07725 test_loss: 0.09795 \n",
      "[120/200] train_loss: 0.06359 valid_loss: 0.07640 test_loss: 0.09369 \n",
      "[121/200] train_loss: 0.06339 valid_loss: 0.08109 test_loss: 0.09862 \n",
      "[122/200] train_loss: 0.06385 valid_loss: 0.07964 test_loss: 0.09662 \n",
      "[123/200] train_loss: 0.06322 valid_loss: 0.07870 test_loss: 0.09351 \n",
      "[124/200] train_loss: 0.06256 valid_loss: 0.07844 test_loss: 0.09800 \n",
      "[125/200] train_loss: 0.06309 valid_loss: 0.07725 test_loss: 0.09744 \n",
      "[126/200] train_loss: 0.06266 valid_loss: 0.07825 test_loss: 0.09168 \n",
      "[127/200] train_loss: 0.06317 valid_loss: 0.07834 test_loss: 0.09342 \n",
      "[128/200] train_loss: 0.05993 valid_loss: 0.07768 test_loss: 0.61139 \n",
      "[129/200] train_loss: 0.06159 valid_loss: 0.07573 test_loss: 0.09334 \n",
      "[130/200] train_loss: 0.06286 valid_loss: 0.07981 test_loss: 0.09463 \n",
      "[131/200] train_loss: 0.06266 valid_loss: 0.07631 test_loss: 0.09040 \n",
      "[132/200] train_loss: 0.06144 valid_loss: 0.07859 test_loss: 0.09337 \n",
      "[133/200] train_loss: 0.06157 valid_loss: 0.07849 test_loss: 0.09603 \n",
      "[134/200] train_loss: 0.06185 valid_loss: 0.07683 test_loss: 0.08862 \n",
      "[135/200] train_loss: 0.06146 valid_loss: 0.07819 test_loss: 0.26526 \n",
      "[136/200] train_loss: 0.06169 valid_loss: 0.07477 test_loss: 0.09790 \n",
      "[137/200] train_loss: 0.06208 valid_loss: 0.07701 test_loss: 0.09522 \n",
      "[138/200] train_loss: 0.05996 valid_loss: 0.07692 test_loss: 0.09185 \n",
      "[139/200] train_loss: 0.06008 valid_loss: 0.07473 test_loss: 0.09533 \n",
      "[140/200] train_loss: 0.06112 valid_loss: 0.07414 test_loss: 0.10479 \n",
      "[141/200] train_loss: 0.06001 valid_loss: 0.07559 test_loss: 0.09254 \n",
      "[142/200] train_loss: 0.05992 valid_loss: 0.07625 test_loss: 0.13769 \n",
      "[143/200] train_loss: 0.06001 valid_loss: 0.07564 test_loss: 0.09043 \n",
      "[144/200] train_loss: 0.05831 valid_loss: 0.07467 test_loss: 0.09286 \n",
      "[145/200] train_loss: 0.05942 valid_loss: 0.07504 test_loss: 0.09188 \n",
      "[146/200] train_loss: 0.06092 valid_loss: 0.07553 test_loss: 0.09611 \n",
      "[147/200] train_loss: 0.06021 valid_loss: 0.07624 test_loss: 0.10527 \n",
      "[148/200] train_loss: 0.06096 valid_loss: 0.07655 test_loss: 0.09811 \n",
      "[149/200] train_loss: 0.05927 valid_loss: 0.07550 test_loss: 0.17734 \n",
      "[150/200] train_loss: 0.05908 valid_loss: 0.07427 test_loss: 0.09440 \n",
      "[151/200] train_loss: 0.05744 valid_loss: 0.07670 test_loss: 0.09567 \n",
      "[152/200] train_loss: 0.06059 valid_loss: 0.07531 test_loss: 0.11887 \n",
      "[153/200] train_loss: 0.06021 valid_loss: 0.07546 test_loss: 0.10112 \n",
      "[154/200] train_loss: 0.05881 valid_loss: 0.07501 test_loss: 0.09906 \n",
      "[155/200] train_loss: 0.05854 valid_loss: 0.07492 test_loss: 0.09511 \n",
      "[156/200] train_loss: 0.05893 valid_loss: 0.07633 test_loss: 0.13354 \n",
      "[157/200] train_loss: 0.05978 valid_loss: 0.07731 test_loss: 0.10154 \n",
      "[158/200] train_loss: 0.05764 valid_loss: 0.07685 test_loss: 0.09441 \n",
      "[159/200] train_loss: 0.05893 valid_loss: 0.07581 test_loss: 0.10544 \n",
      "[160/200] train_loss: 0.05894 valid_loss: 0.07543 test_loss: 0.09513 \n",
      "[161/200] train_loss: 0.05914 valid_loss: 0.07503 test_loss: 0.09675 \n",
      "[162/200] train_loss: 0.05781 valid_loss: 0.07636 test_loss: 0.39411 \n",
      "[163/200] train_loss: 0.05837 valid_loss: 0.07652 test_loss: 0.09245 \n",
      "[164/200] train_loss: 0.05865 valid_loss: 0.07565 test_loss: 0.09199 \n",
      "[165/200] train_loss: 0.05995 valid_loss: 0.07580 test_loss: 0.09426 \n",
      "[166/200] train_loss: 0.05817 valid_loss: 0.07575 test_loss: 0.09834 \n",
      "[167/200] train_loss: 0.05999 valid_loss: 0.07423 test_loss: 0.09293 \n",
      "[168/200] train_loss: 0.05775 valid_loss: 0.07569 test_loss: 0.25185 \n",
      "[169/200] train_loss: 0.05851 valid_loss: 0.07450 test_loss: 0.15060 \n",
      "[170/200] train_loss: 0.05724 valid_loss: 0.07549 test_loss: 0.09331 \n",
      "[171/200] train_loss: 0.05644 valid_loss: 0.07466 test_loss: 0.09398 \n",
      "[172/200] train_loss: 0.05756 valid_loss: 0.07542 test_loss: 0.09395 \n",
      "[173/200] train_loss: 0.05640 valid_loss: 0.07546 test_loss: 0.09249 \n",
      "[174/200] train_loss: 0.05576 valid_loss: 0.07445 test_loss: 0.09408 \n",
      "[175/200] train_loss: 0.05718 valid_loss: 0.07584 test_loss: 0.09612 \n",
      "[176/200] train_loss: 0.05678 valid_loss: 0.07574 test_loss: 0.09858 \n",
      "[177/200] train_loss: 0.05705 valid_loss: 0.07469 test_loss: 0.09328 \n",
      "[178/200] train_loss: 0.05579 valid_loss: 0.07410 test_loss: 0.09027 \n",
      "[179/200] train_loss: 0.05612 valid_loss: 0.07439 test_loss: 0.09237 \n",
      "[180/200] train_loss: 0.05712 valid_loss: 0.07533 test_loss: 0.09851 \n",
      "[181/200] train_loss: 0.05684 valid_loss: 0.07521 test_loss: 0.10064 \n",
      "[182/200] train_loss: 0.05455 valid_loss: 0.07632 test_loss: 0.10438 \n",
      "[183/200] train_loss: 0.05844 valid_loss: 0.07535 test_loss: 0.09552 \n",
      "[184/200] train_loss: 0.05740 valid_loss: 0.07556 test_loss: 0.10419 \n",
      "[185/200] train_loss: 0.05705 valid_loss: 0.07479 test_loss: 0.12995 \n",
      "[186/200] train_loss: 0.05677 valid_loss: 0.07561 test_loss: 0.09712 \n",
      "[187/200] train_loss: 0.05726 valid_loss: 0.07489 test_loss: 0.09430 \n",
      "[188/200] train_loss: 0.05495 valid_loss: 0.07402 test_loss: 0.09559 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189/200] train_loss: 0.05370 valid_loss: 0.07538 test_loss: 0.09146 \n",
      "[190/200] train_loss: 0.05421 valid_loss: 0.07525 test_loss: 0.26893 \n",
      "[191/200] train_loss: 0.05426 valid_loss: 0.07482 test_loss: 0.24912 \n",
      "[192/200] train_loss: 0.05490 valid_loss: 0.07491 test_loss: 0.09294 \n",
      "[193/200] train_loss: 0.05521 valid_loss: 0.07980 test_loss: 0.28706 \n",
      "[194/200] train_loss: 0.05615 valid_loss: 0.07473 test_loss: 0.09284 \n",
      "[195/200] train_loss: 0.05508 valid_loss: 0.07582 test_loss: 0.09871 \n",
      "[196/200] train_loss: 0.05686 valid_loss: 0.07531 test_loss: 0.09107 \n",
      "[197/200] train_loss: 0.05597 valid_loss: 0.07662 test_loss: 0.12326 \n",
      "[198/200] train_loss: 0.05566 valid_loss: 0.07666 test_loss: 0.12063 \n",
      "[199/200] train_loss: 0.05516 valid_loss: 0.07607 test_loss: 0.12254 \n",
      "[200/200] train_loss: 0.05511 valid_loss: 0.07511 test_loss: 0.10077 \n",
      "TRAINING MODEL 10\n",
      "[  1/200] train_loss: 0.35567 valid_loss: 0.27361 test_loss: 0.24952 \n",
      "验证损失减少 (inf --> 0.249524). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20776 valid_loss: 0.19649 test_loss: 0.15774 \n",
      "验证损失减少 (0.273611 --> 0.157739). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16755 valid_loss: 0.16246 test_loss: 0.13087 \n",
      "验证损失减少 (0.196495 --> 0.130874). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14640 valid_loss: 0.14779 test_loss: 0.12242 \n",
      "验证损失减少 (0.162457 --> 0.122424). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13480 valid_loss: 0.13951 test_loss: 0.12129 \n",
      "验证损失减少 (0.147788 --> 0.121287). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13040 valid_loss: 0.13351 test_loss: 0.11678 \n",
      "验证损失减少 (0.139506 --> 0.116785). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12598 valid_loss: 0.12863 test_loss: 0.11633 \n",
      "验证损失减少 (0.133513 --> 0.116326). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12146 valid_loss: 0.11964 test_loss: 0.10945 \n",
      "验证损失减少 (0.128632 --> 0.109451). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11227 valid_loss: 0.11880 test_loss: 0.10448 \n",
      "验证损失减少 (0.119637 --> 0.104479). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11153 valid_loss: 0.12089 test_loss: 0.11617 \n",
      "验证损失减少 (0.118800 --> 0.116171). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.10820 valid_loss: 0.11571 test_loss: 0.10610 \n",
      "验证损失减少 (0.120894 --> 0.106104). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10977 valid_loss: 0.11198 test_loss: 0.10459 \n",
      "验证损失减少 (0.115708 --> 0.104589). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10653 valid_loss: 0.11159 test_loss: 0.10603 \n",
      "验证损失减少 (0.111981 --> 0.106029). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10590 valid_loss: 0.11147 test_loss: 0.10180 \n",
      "验证损失减少 (0.111587 --> 0.101804). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10309 valid_loss: 0.10688 test_loss: 0.10451 \n",
      "验证损失减少 (0.111468 --> 0.104508). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10208 valid_loss: 0.10448 test_loss: 0.09866 \n",
      "验证损失减少 (0.106877 --> 0.098665). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.09882 valid_loss: 0.10667 test_loss: 0.13480 \n",
      "[ 18/200] train_loss: 0.09856 valid_loss: 0.10175 test_loss: 0.10152 \n",
      "验证损失减少 (0.104479 --> 0.101518). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09826 valid_loss: 0.10269 test_loss: 0.16479 \n",
      "[ 20/200] train_loss: 0.09527 valid_loss: 0.09760 test_loss: 0.10848 \n",
      "[ 21/200] train_loss: 0.09390 valid_loss: 0.09910 test_loss: 0.10478 \n",
      "[ 22/200] train_loss: 0.09453 valid_loss: 0.09779 test_loss: 0.10147 \n",
      "验证损失减少 (0.101751 --> 0.101474). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09235 valid_loss: 0.09832 test_loss: 0.33662 \n",
      "[ 24/200] train_loss: 0.09003 valid_loss: 0.09789 test_loss: 0.09593 \n",
      "验证损失减少 (0.097787 --> 0.095932). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09091 valid_loss: 0.10219 test_loss: 0.10287 \n",
      "[ 26/200] train_loss: 0.09175 valid_loss: 0.09653 test_loss: 0.09339 \n",
      "验证损失减少 (0.097892 --> 0.093392). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.08734 valid_loss: 0.09720 test_loss: 0.09929 \n",
      "[ 28/200] train_loss: 0.08545 valid_loss: 0.09523 test_loss: 0.09438 \n",
      "验证损失减少 (0.096532 --> 0.094380). 正在保存模型...\n",
      "[ 29/200] train_loss: 0.08871 valid_loss: 0.09538 test_loss: 0.09763 \n",
      "[ 30/200] train_loss: 0.08869 valid_loss: 0.09216 test_loss: 0.09238 \n",
      "验证损失减少 (0.095228 --> 0.092375). 正在保存模型...\n",
      "[ 31/200] train_loss: 0.08715 valid_loss: 0.09530 test_loss: 0.09949 \n",
      "[ 32/200] train_loss: 0.08622 valid_loss: 0.09031 test_loss: 0.09624 \n",
      "[ 33/200] train_loss: 0.08754 valid_loss: 0.08995 test_loss: 0.09140 \n",
      "验证损失减少 (0.092156 --> 0.091398). 正在保存模型...\n",
      "[ 34/200] train_loss: 0.08516 valid_loss: 0.09039 test_loss: 0.08948 \n",
      "验证损失减少 (0.089955 --> 0.089476). 正在保存模型...\n",
      "[ 35/200] train_loss: 0.08493 valid_loss: 0.09004 test_loss: 0.09129 \n",
      "[ 36/200] train_loss: 0.08341 valid_loss: 0.08841 test_loss: 0.31696 \n",
      "[ 37/200] train_loss: 0.08301 valid_loss: 0.08794 test_loss: 0.09853 \n",
      "[ 38/200] train_loss: 0.08256 valid_loss: 0.08992 test_loss: 0.11021 \n",
      "[ 39/200] train_loss: 0.08207 valid_loss: 0.08855 test_loss: 0.22289 \n",
      "[ 40/200] train_loss: 0.08162 valid_loss: 0.08697 test_loss: 0.09988 \n",
      "[ 41/200] train_loss: 0.08152 valid_loss: 0.08538 test_loss: 0.09602 \n",
      "[ 42/200] train_loss: 0.07942 valid_loss: 0.08624 test_loss: 0.09140 \n",
      "[ 43/200] train_loss: 0.08069 valid_loss: 0.08723 test_loss: 0.09034 \n",
      "验证损失减少 (0.090394 --> 0.090338). 正在保存模型...\n",
      "[ 44/200] train_loss: 0.07975 valid_loss: 0.08565 test_loss: 0.09079 \n",
      "[ 45/200] train_loss: 0.07979 valid_loss: 0.08761 test_loss: 0.09304 \n",
      "[ 46/200] train_loss: 0.08218 valid_loss: 0.08654 test_loss: 0.15814 \n",
      "[ 47/200] train_loss: 0.08052 valid_loss: 0.08614 test_loss: 0.10592 \n",
      "[ 48/200] train_loss: 0.08006 valid_loss: 0.08702 test_loss: 0.09552 \n",
      "[ 49/200] train_loss: 0.07907 valid_loss: 0.08562 test_loss: 0.08876 \n",
      "[ 50/200] train_loss: 0.07679 valid_loss: 0.08732 test_loss: 0.09422 \n",
      "[ 51/200] train_loss: 0.07590 valid_loss: 0.08426 test_loss: 0.12979 \n",
      "[ 52/200] train_loss: 0.07672 valid_loss: 0.08314 test_loss: 0.08680 \n",
      "验证损失减少 (0.087226 --> 0.086796). 正在保存模型...\n",
      "[ 53/200] train_loss: 0.07852 valid_loss: 0.08467 test_loss: 0.09187 \n",
      "[ 54/200] train_loss: 0.07510 valid_loss: 0.08388 test_loss: 0.12676 \n",
      "[ 55/200] train_loss: 0.07673 valid_loss: 0.08222 test_loss: 0.08778 \n",
      "[ 56/200] train_loss: 0.07643 valid_loss: 0.08348 test_loss: 0.09011 \n",
      "[ 57/200] train_loss: 0.07415 valid_loss: 0.08474 test_loss: 0.09210 \n",
      "[ 58/200] train_loss: 0.07643 valid_loss: 0.08445 test_loss: 0.36995 \n",
      "[ 59/200] train_loss: 0.07423 valid_loss: 0.08259 test_loss: 0.08712 \n",
      "[ 60/200] train_loss: 0.07387 valid_loss: 0.08470 test_loss: 0.08914 \n",
      "[ 61/200] train_loss: 0.07486 valid_loss: 0.08241 test_loss: 0.23149 \n",
      "[ 62/200] train_loss: 0.07457 valid_loss: 0.08249 test_loss: 0.14522 \n",
      "[ 63/200] train_loss: 0.07181 valid_loss: 0.08126 test_loss: 0.09000 \n",
      "[ 64/200] train_loss: 0.07409 valid_loss: 0.08212 test_loss: 0.18536 \n",
      "[ 65/200] train_loss: 0.07400 valid_loss: 0.08126 test_loss: 0.08742 \n",
      "[ 66/200] train_loss: 0.07402 valid_loss: 0.07990 test_loss: 0.08876 \n",
      "[ 67/200] train_loss: 0.07405 valid_loss: 0.08119 test_loss: 0.14637 \n",
      "[ 68/200] train_loss: 0.07302 valid_loss: 0.08455 test_loss: 0.09305 \n",
      "[ 69/200] train_loss: 0.07347 valid_loss: 0.08045 test_loss: 0.09676 \n",
      "[ 70/200] train_loss: 0.07043 valid_loss: 0.07886 test_loss: 0.09095 \n",
      "[ 71/200] train_loss: 0.07359 valid_loss: 0.08132 test_loss: 0.08769 \n",
      "[ 72/200] train_loss: 0.07087 valid_loss: 0.08057 test_loss: 0.10401 \n",
      "[ 73/200] train_loss: 0.06974 valid_loss: 0.08019 test_loss: 0.19661 \n",
      "[ 74/200] train_loss: 0.07225 valid_loss: 0.08010 test_loss: 0.08888 \n",
      "[ 75/200] train_loss: 0.07201 valid_loss: 0.08142 test_loss: 0.09218 \n",
      "[ 76/200] train_loss: 0.06676 valid_loss: 0.08550 test_loss: 0.09311 \n",
      "[ 77/200] train_loss: 0.07227 valid_loss: 0.08049 test_loss: 0.08802 \n",
      "[ 78/200] train_loss: 0.07069 valid_loss: 0.08011 test_loss: 0.08650 \n",
      "[ 79/200] train_loss: 0.07064 valid_loss: 0.08023 test_loss: 0.08695 \n",
      "[ 80/200] train_loss: 0.07122 valid_loss: 0.08028 test_loss: 0.08796 \n",
      "[ 81/200] train_loss: 0.07024 valid_loss: 0.08063 test_loss: 0.09131 \n",
      "[ 82/200] train_loss: 0.06871 valid_loss: 0.07992 test_loss: 0.08898 \n",
      "[ 83/200] train_loss: 0.06971 valid_loss: 0.07948 test_loss: 0.13129 \n",
      "[ 84/200] train_loss: 0.06993 valid_loss: 0.07920 test_loss: 0.08869 \n",
      "[ 85/200] train_loss: 0.06842 valid_loss: 0.08032 test_loss: 0.09337 \n",
      "[ 86/200] train_loss: 0.07037 valid_loss: 0.08174 test_loss: 0.09232 \n",
      "[ 87/200] train_loss: 0.06986 valid_loss: 0.07917 test_loss: 0.09496 \n",
      "[ 88/200] train_loss: 0.06842 valid_loss: 0.08303 test_loss: 0.09127 \n",
      "[ 89/200] train_loss: 0.06675 valid_loss: 0.08023 test_loss: 0.08987 \n",
      "[ 90/200] train_loss: 0.06953 valid_loss: 0.07938 test_loss: 0.09761 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 91/200] train_loss: 0.06985 valid_loss: 0.07893 test_loss: 0.11409 \n",
      "[ 92/200] train_loss: 0.06800 valid_loss: 0.07835 test_loss: 0.09355 \n",
      "[ 93/200] train_loss: 0.06670 valid_loss: 0.07953 test_loss: 0.08968 \n",
      "[ 94/200] train_loss: 0.06792 valid_loss: 0.08130 test_loss: 0.09326 \n",
      "[ 95/200] train_loss: 0.06869 valid_loss: 0.07826 test_loss: 0.10356 \n",
      "[ 96/200] train_loss: 0.06735 valid_loss: 0.07933 test_loss: 0.09498 \n",
      "[ 97/200] train_loss: 0.06713 valid_loss: 0.07922 test_loss: 0.08926 \n",
      "[ 98/200] train_loss: 0.06607 valid_loss: 0.07952 test_loss: 0.08907 \n",
      "[ 99/200] train_loss: 0.06374 valid_loss: 0.07934 test_loss: 0.08844 \n",
      "[100/200] train_loss: 0.06494 valid_loss: 0.08069 test_loss: 0.08858 \n",
      "[101/200] train_loss: 0.06609 valid_loss: 0.07819 test_loss: 0.09082 \n",
      "[102/200] train_loss: 0.06420 valid_loss: 0.07964 test_loss: 0.09426 \n",
      "[103/200] train_loss: 0.06627 valid_loss: 0.07801 test_loss: 0.08928 \n",
      "[104/200] train_loss: 0.06530 valid_loss: 0.07745 test_loss: 0.09296 \n",
      "[105/200] train_loss: 0.06558 valid_loss: 0.07831 test_loss: 0.10087 \n",
      "[106/200] train_loss: 0.06383 valid_loss: 0.07783 test_loss: 0.08873 \n",
      "[107/200] train_loss: 0.06448 valid_loss: 0.07646 test_loss: 0.09034 \n",
      "[108/200] train_loss: 0.06749 valid_loss: 0.07833 test_loss: 0.08959 \n",
      "[109/200] train_loss: 0.06378 valid_loss: 0.08018 test_loss: 0.08772 \n",
      "[110/200] train_loss: 0.06451 valid_loss: 0.07798 test_loss: 0.09113 \n",
      "[111/200] train_loss: 0.06389 valid_loss: 0.07929 test_loss: 0.08901 \n",
      "[112/200] train_loss: 0.06492 valid_loss: 0.07962 test_loss: 0.08956 \n",
      "[113/200] train_loss: 0.06449 valid_loss: 0.07930 test_loss: 0.08729 \n",
      "[114/200] train_loss: 0.06445 valid_loss: 0.07909 test_loss: 0.08794 \n",
      "[115/200] train_loss: 0.06334 valid_loss: 0.07738 test_loss: 0.09229 \n",
      "[116/200] train_loss: 0.06364 valid_loss: 0.08150 test_loss: 0.10096 \n",
      "[117/200] train_loss: 0.06307 valid_loss: 0.07825 test_loss: 0.09071 \n",
      "[118/200] train_loss: 0.06419 valid_loss: 0.07878 test_loss: 0.08995 \n",
      "[119/200] train_loss: 0.06245 valid_loss: 0.07789 test_loss: 0.09132 \n",
      "[120/200] train_loss: 0.06376 valid_loss: 0.08164 test_loss: 0.08858 \n",
      "[121/200] train_loss: 0.06254 valid_loss: 0.07806 test_loss: 0.09134 \n",
      "[122/200] train_loss: 0.06263 valid_loss: 0.08507 test_loss: 0.11057 \n",
      "[123/200] train_loss: 0.06274 valid_loss: 0.08091 test_loss: 0.09878 \n",
      "[124/200] train_loss: 0.06266 valid_loss: 0.07635 test_loss: 0.08837 \n",
      "[125/200] train_loss: 0.06333 valid_loss: 0.07956 test_loss: 0.09396 \n",
      "[126/200] train_loss: 0.06071 valid_loss: 0.07914 test_loss: 0.08825 \n",
      "[127/200] train_loss: 0.06204 valid_loss: 0.07750 test_loss: 0.09147 \n",
      "[128/200] train_loss: 0.06074 valid_loss: 0.07917 test_loss: 0.09357 \n",
      "[129/200] train_loss: 0.06119 valid_loss: 0.07756 test_loss: 0.09437 \n",
      "[130/200] train_loss: 0.06323 valid_loss: 0.07639 test_loss: 0.09177 \n",
      "[131/200] train_loss: 0.06163 valid_loss: 0.07654 test_loss: 0.09319 \n",
      "[132/200] train_loss: 0.06175 valid_loss: 0.08006 test_loss: 0.09900 \n",
      "[133/200] train_loss: 0.06224 valid_loss: 0.07910 test_loss: 0.09086 \n",
      "[134/200] train_loss: 0.06108 valid_loss: 0.08071 test_loss: 0.09526 \n",
      "[135/200] train_loss: 0.06155 valid_loss: 0.08127 test_loss: 0.09834 \n",
      "[136/200] train_loss: 0.06103 valid_loss: 0.07703 test_loss: 0.09214 \n",
      "[137/200] train_loss: 0.05996 valid_loss: 0.07765 test_loss: 0.08941 \n",
      "[138/200] train_loss: 0.06235 valid_loss: 0.07826 test_loss: 0.09277 \n",
      "[139/200] train_loss: 0.06260 valid_loss: 0.07730 test_loss: 0.09329 \n",
      "[140/200] train_loss: 0.06187 valid_loss: 0.07681 test_loss: 0.09427 \n",
      "[141/200] train_loss: 0.05914 valid_loss: 0.07695 test_loss: 0.09187 \n",
      "[142/200] train_loss: 0.06150 valid_loss: 0.07822 test_loss: 0.09448 \n",
      "[143/200] train_loss: 0.06027 valid_loss: 0.07829 test_loss: 0.09530 \n",
      "[144/200] train_loss: 0.06070 valid_loss: 0.07765 test_loss: 0.09191 \n",
      "[145/200] train_loss: 0.06040 valid_loss: 0.08482 test_loss: 0.09532 \n",
      "[146/200] train_loss: 0.05982 valid_loss: 0.07564 test_loss: 0.09114 \n",
      "[147/200] train_loss: 0.06014 valid_loss: 0.07953 test_loss: 0.09544 \n",
      "[148/200] train_loss: 0.05890 valid_loss: 0.07743 test_loss: 0.09423 \n",
      "[149/200] train_loss: 0.05878 valid_loss: 0.08429 test_loss: 0.10734 \n",
      "[150/200] train_loss: 0.05972 valid_loss: 0.07721 test_loss: 0.09263 \n",
      "[151/200] train_loss: 0.05907 valid_loss: 0.07584 test_loss: 0.08961 \n",
      "[152/200] train_loss: 0.06102 valid_loss: 0.07770 test_loss: 0.08931 \n",
      "[153/200] train_loss: 0.05874 valid_loss: 0.07856 test_loss: 0.08988 \n",
      "[154/200] train_loss: 0.05845 valid_loss: 0.07569 test_loss: 0.09002 \n",
      "[155/200] train_loss: 0.05894 valid_loss: 0.07513 test_loss: 0.08998 \n",
      "[156/200] train_loss: 0.05899 valid_loss: 0.07672 test_loss: 0.08945 \n",
      "[157/200] train_loss: 0.05806 valid_loss: 0.07915 test_loss: 0.09673 \n",
      "[158/200] train_loss: 0.05938 valid_loss: 0.07920 test_loss: 0.09863 \n",
      "[159/200] train_loss: 0.05876 valid_loss: 0.07754 test_loss: 0.09446 \n",
      "[160/200] train_loss: 0.05893 valid_loss: 0.07949 test_loss: 0.09547 \n",
      "[161/200] train_loss: 0.05850 valid_loss: 0.07753 test_loss: 0.50098 \n",
      "[162/200] train_loss: 0.05829 valid_loss: 0.07661 test_loss: 0.10361 \n",
      "[163/200] train_loss: 0.05856 valid_loss: 0.07674 test_loss: 0.09469 \n",
      "[164/200] train_loss: 0.05758 valid_loss: 0.07790 test_loss: 0.09459 \n",
      "[165/200] train_loss: 0.05845 valid_loss: 0.07647 test_loss: 0.09652 \n",
      "[166/200] train_loss: 0.05850 valid_loss: 0.07614 test_loss: 0.09193 \n",
      "[167/200] train_loss: 0.05570 valid_loss: 0.07582 test_loss: 0.09215 \n",
      "[168/200] train_loss: 0.05583 valid_loss: 0.07782 test_loss: 0.09569 \n",
      "[169/200] train_loss: 0.05718 valid_loss: 0.07939 test_loss: 0.09639 \n",
      "[170/200] train_loss: 0.05644 valid_loss: 0.07804 test_loss: 0.09599 \n",
      "[171/200] train_loss: 0.05755 valid_loss: 0.07581 test_loss: 0.09319 \n",
      "[172/200] train_loss: 0.05716 valid_loss: 0.07806 test_loss: 0.09812 \n",
      "[173/200] train_loss: 0.05655 valid_loss: 0.07712 test_loss: 0.10000 \n",
      "[174/200] train_loss: 0.05810 valid_loss: 0.07528 test_loss: 0.09461 \n",
      "[175/200] train_loss: 0.05668 valid_loss: 0.07673 test_loss: 0.09798 \n",
      "[176/200] train_loss: 0.05589 valid_loss: 0.07751 test_loss: 0.08886 \n",
      "[177/200] train_loss: 0.05574 valid_loss: 0.07890 test_loss: 0.09508 \n",
      "[178/200] train_loss: 0.05688 valid_loss: 0.08209 test_loss: 0.10087 \n",
      "[179/200] train_loss: 0.05623 valid_loss: 0.08189 test_loss: 0.10332 \n",
      "[180/200] train_loss: 0.05625 valid_loss: 0.08187 test_loss: 0.10202 \n",
      "[181/200] train_loss: 0.05681 valid_loss: 0.09456 test_loss: 0.11185 \n",
      "[182/200] train_loss: 0.05632 valid_loss: 0.08214 test_loss: 0.09409 \n",
      "[183/200] train_loss: 0.05618 valid_loss: 0.08086 test_loss: 0.09603 \n",
      "[184/200] train_loss: 0.05781 valid_loss: 0.07770 test_loss: 0.09353 \n",
      "[185/200] train_loss: 0.05488 valid_loss: 0.08141 test_loss: 0.09826 \n",
      "[186/200] train_loss: 0.05743 valid_loss: 0.07725 test_loss: 0.09142 \n",
      "[187/200] train_loss: 0.05469 valid_loss: 0.07609 test_loss: 0.09182 \n",
      "[188/200] train_loss: 0.05662 valid_loss: 0.07812 test_loss: 0.09295 \n",
      "[189/200] train_loss: 0.05492 valid_loss: 0.08133 test_loss: 0.09518 \n",
      "[190/200] train_loss: 0.05551 valid_loss: 0.07735 test_loss: 0.09135 \n",
      "[191/200] train_loss: 0.05575 valid_loss: 0.07812 test_loss: 0.09712 \n",
      "[192/200] train_loss: 0.05583 valid_loss: 0.07751 test_loss: 0.11727 \n",
      "[193/200] train_loss: 0.05482 valid_loss: 0.07878 test_loss: 0.09428 \n",
      "[194/200] train_loss: 0.05512 valid_loss: 0.07504 test_loss: 0.09059 \n",
      "[195/200] train_loss: 0.05646 valid_loss: 0.07636 test_loss: 0.09334 \n",
      "[196/200] train_loss: 0.05662 valid_loss: 0.08485 test_loss: 0.09835 \n",
      "[197/200] train_loss: 0.05524 valid_loss: 0.07563 test_loss: 0.09003 \n",
      "[198/200] train_loss: 0.05539 valid_loss: 0.07749 test_loss: 0.09603 \n",
      "[199/200] train_loss: 0.05557 valid_loss: 0.07661 test_loss: 0.09236 \n",
      "[200/200] train_loss: 0.05537 valid_loss: 0.08644 test_loss: 0.10150 \n",
      "TRAINING MODEL 11\n",
      "[  1/200] train_loss: 0.45982 valid_loss: 0.30704 test_loss: 0.28190 \n",
      "验证损失减少 (inf --> 0.281903). 正在保存模型...\n",
      "[  2/200] train_loss: 0.23242 valid_loss: 0.21206 test_loss: 0.16546 \n",
      "验证损失减少 (0.307036 --> 0.165456). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17576 valid_loss: 0.16880 test_loss: 0.13508 \n",
      "验证损失减少 (0.212059 --> 0.135078). 正在保存模型...\n",
      "[  4/200] train_loss: 0.15243 valid_loss: 0.15912 test_loss: 0.12776 \n",
      "验证损失减少 (0.168801 --> 0.127756). 正在保存模型...\n",
      "[  5/200] train_loss: 0.14020 valid_loss: 0.14334 test_loss: 0.12032 \n",
      "验证损失减少 (0.159121 --> 0.120324). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6/200] train_loss: 0.13267 valid_loss: 0.13436 test_loss: 0.11698 \n",
      "验证损失减少 (0.143338 --> 0.116977). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12388 valid_loss: 0.13084 test_loss: 0.11754 \n",
      "验证损失减少 (0.134363 --> 0.117543). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12382 valid_loss: 0.12585 test_loss: 0.13395 \n",
      "[  9/200] train_loss: 0.11728 valid_loss: 0.12486 test_loss: 0.11038 \n",
      "验证损失减少 (0.130842 --> 0.110382). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11803 valid_loss: 0.11953 test_loss: 0.12034 \n",
      "验证损失减少 (0.124858 --> 0.120345). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11363 valid_loss: 0.12014 test_loss: 0.11113 \n",
      "验证损失减少 (0.119534 --> 0.111130). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.11268 valid_loss: 0.11867 test_loss: 0.11326 \n",
      "验证损失减少 (0.120135 --> 0.113255). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.11068 valid_loss: 0.11747 test_loss: 0.11358 \n",
      "验证损失减少 (0.118669 --> 0.113578). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10688 valid_loss: 0.11364 test_loss: 0.11092 \n",
      "验证损失减少 (0.117467 --> 0.110918). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10608 valid_loss: 0.11308 test_loss: 0.10469 \n",
      "验证损失减少 (0.113644 --> 0.104690). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10355 valid_loss: 0.11064 test_loss: 0.10401 \n",
      "验证损失减少 (0.113075 --> 0.104008). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10465 valid_loss: 0.11066 test_loss: 0.10576 \n",
      "验证损失减少 (0.110637 --> 0.105758). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.10298 valid_loss: 0.10891 test_loss: 0.10329 \n",
      "验证损失减少 (0.110655 --> 0.103291). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.10076 valid_loss: 0.10863 test_loss: 0.10591 \n",
      "验证损失减少 (0.108908 --> 0.105906). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09833 valid_loss: 0.10508 test_loss: 0.10326 \n",
      "验证损失减少 (0.108625 --> 0.103258). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09972 valid_loss: 0.10558 test_loss: 0.10067 \n",
      "验证损失减少 (0.105084 --> 0.100668). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09671 valid_loss: 0.10389 test_loss: 0.09927 \n",
      "验证损失减少 (0.105582 --> 0.099272). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09660 valid_loss: 0.10182 test_loss: 0.09904 \n",
      "验证损失减少 (0.103894 --> 0.099036). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.09334 valid_loss: 0.10546 test_loss: 0.09865 \n",
      "验证损失减少 (0.101818 --> 0.098655). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09481 valid_loss: 0.10015 test_loss: 0.09736 \n",
      "验证损失减少 (0.105456 --> 0.097361). 正在保存模型...\n",
      "[ 26/200] train_loss: 0.09186 valid_loss: 0.09784 test_loss: 0.10285 \n",
      "[ 27/200] train_loss: 0.09212 valid_loss: 0.09956 test_loss: 0.09597 \n",
      "验证损失减少 (0.100152 --> 0.095970). 正在保存模型...\n",
      "[ 28/200] train_loss: 0.09252 valid_loss: 0.10149 test_loss: 0.10395 \n",
      "[ 29/200] train_loss: 0.09207 valid_loss: 0.09777 test_loss: 0.10011 \n",
      "[ 30/200] train_loss: 0.09063 valid_loss: 0.09609 test_loss: 0.09487 \n",
      "验证损失减少 (0.099556 --> 0.094875). 正在保存模型...\n",
      "[ 31/200] train_loss: 0.09323 valid_loss: 0.09662 test_loss: 0.10096 \n",
      "[ 32/200] train_loss: 0.08961 valid_loss: 0.09816 test_loss: 0.10186 \n",
      "[ 33/200] train_loss: 0.09018 valid_loss: 0.09619 test_loss: 0.09857 \n",
      "[ 34/200] train_loss: 0.08706 valid_loss: 0.09518 test_loss: 0.09968 \n",
      "[ 35/200] train_loss: 0.08737 valid_loss: 0.09547 test_loss: 0.09986 \n",
      "[ 36/200] train_loss: 0.08604 valid_loss: 0.09482 test_loss: 0.12139 \n",
      "[ 37/200] train_loss: 0.08633 valid_loss: 0.09353 test_loss: 0.09997 \n",
      "[ 38/200] train_loss: 0.08511 valid_loss: 0.09304 test_loss: 0.09451 \n",
      "验证损失减少 (0.096090 --> 0.094509). 正在保存模型...\n",
      "[ 39/200] train_loss: 0.08630 valid_loss: 0.09163 test_loss: 0.15011 \n",
      "[ 40/200] train_loss: 0.08311 valid_loss: 0.09098 test_loss: 0.26151 \n",
      "[ 41/200] train_loss: 0.08615 valid_loss: 0.09031 test_loss: 0.17308 \n",
      "[ 42/200] train_loss: 0.08620 valid_loss: 0.09108 test_loss: 0.09746 \n",
      "[ 43/200] train_loss: 0.08346 valid_loss: 0.09019 test_loss: 0.09978 \n",
      "[ 44/200] train_loss: 0.08275 valid_loss: 0.09106 test_loss: 0.09531 \n",
      "[ 45/200] train_loss: 0.08207 valid_loss: 0.08990 test_loss: 0.10503 \n",
      "[ 46/200] train_loss: 0.08296 valid_loss: 0.09154 test_loss: 0.09654 \n",
      "[ 47/200] train_loss: 0.07944 valid_loss: 0.08990 test_loss: 0.10485 \n",
      "[ 48/200] train_loss: 0.08298 valid_loss: 0.08970 test_loss: 0.09082 \n",
      "验证损失减少 (0.093045 --> 0.090819). 正在保存模型...\n",
      "[ 49/200] train_loss: 0.07831 valid_loss: 0.08939 test_loss: 0.09210 \n",
      "[ 50/200] train_loss: 0.08071 valid_loss: 0.08780 test_loss: 0.10041 \n",
      "[ 51/200] train_loss: 0.08078 valid_loss: 0.08716 test_loss: 0.10312 \n",
      "[ 52/200] train_loss: 0.07939 valid_loss: 0.08926 test_loss: 0.09089 \n",
      "[ 53/200] train_loss: 0.08007 valid_loss: 0.09086 test_loss: 0.09402 \n",
      "[ 54/200] train_loss: 0.07973 valid_loss: 0.08805 test_loss: 0.08986 \n",
      "[ 55/200] train_loss: 0.07791 valid_loss: 0.08896 test_loss: 0.09580 \n",
      "[ 56/200] train_loss: 0.07867 valid_loss: 0.08640 test_loss: 0.09197 \n",
      "[ 57/200] train_loss: 0.07957 valid_loss: 0.08613 test_loss: 0.10345 \n",
      "[ 58/200] train_loss: 0.07735 valid_loss: 0.08865 test_loss: 0.09248 \n",
      "[ 59/200] train_loss: 0.07803 valid_loss: 0.08762 test_loss: 0.09310 \n",
      "[ 60/200] train_loss: 0.07743 valid_loss: 0.08718 test_loss: 0.09333 \n",
      "[ 61/200] train_loss: 0.07731 valid_loss: 0.08654 test_loss: 0.09934 \n",
      "[ 62/200] train_loss: 0.07544 valid_loss: 0.08632 test_loss: 0.09242 \n",
      "[ 63/200] train_loss: 0.07691 valid_loss: 0.08688 test_loss: 0.09394 \n",
      "[ 64/200] train_loss: 0.07691 valid_loss: 0.08601 test_loss: 0.09306 \n",
      "[ 65/200] train_loss: 0.07477 valid_loss: 0.08528 test_loss: 0.10253 \n",
      "[ 66/200] train_loss: 0.07544 valid_loss: 0.08773 test_loss: 0.09484 \n",
      "[ 67/200] train_loss: 0.07425 valid_loss: 0.08641 test_loss: 0.09869 \n",
      "[ 68/200] train_loss: 0.07706 valid_loss: 0.08455 test_loss: 0.12180 \n",
      "[ 69/200] train_loss: 0.07472 valid_loss: 0.08646 test_loss: 0.10438 \n",
      "[ 70/200] train_loss: 0.07484 valid_loss: 0.08395 test_loss: 0.09388 \n",
      "[ 71/200] train_loss: 0.07491 valid_loss: 0.08531 test_loss: 0.09115 \n",
      "[ 72/200] train_loss: 0.07396 valid_loss: 0.08378 test_loss: 0.08923 \n",
      "验证损失减少 (0.089702 --> 0.089230). 正在保存模型...\n",
      "[ 73/200] train_loss: 0.07330 valid_loss: 0.08308 test_loss: 0.08793 \n",
      "[ 74/200] train_loss: 0.07347 valid_loss: 0.08394 test_loss: 0.11628 \n",
      "[ 75/200] train_loss: 0.07381 valid_loss: 0.08297 test_loss: 0.09364 \n",
      "[ 76/200] train_loss: 0.07214 valid_loss: 0.08438 test_loss: 0.09064 \n",
      "[ 77/200] train_loss: 0.07359 valid_loss: 0.08662 test_loss: 0.09632 \n",
      "[ 78/200] train_loss: 0.07434 valid_loss: 0.08544 test_loss: 0.08838 \n",
      "[ 79/200] train_loss: 0.07167 valid_loss: 0.08459 test_loss: 0.08813 \n",
      "[ 80/200] train_loss: 0.07230 valid_loss: 0.08376 test_loss: 0.08799 \n",
      "[ 81/200] train_loss: 0.07190 valid_loss: 0.08258 test_loss: 0.09031 \n",
      "[ 82/200] train_loss: 0.07119 valid_loss: 0.08309 test_loss: 0.09046 \n",
      "[ 83/200] train_loss: 0.07230 valid_loss: 0.08285 test_loss: 0.10397 \n",
      "[ 84/200] train_loss: 0.07147 valid_loss: 0.08246 test_loss: 0.09149 \n",
      "[ 85/200] train_loss: 0.06985 valid_loss: 0.08401 test_loss: 0.09272 \n",
      "[ 86/200] train_loss: 0.07334 valid_loss: 0.08196 test_loss: 0.08723 \n",
      "[ 87/200] train_loss: 0.07003 valid_loss: 0.08416 test_loss: 0.12389 \n",
      "[ 88/200] train_loss: 0.07070 valid_loss: 0.08274 test_loss: 0.09546 \n",
      "[ 89/200] train_loss: 0.07171 valid_loss: 0.08178 test_loss: 0.09204 \n",
      "[ 90/200] train_loss: 0.07023 valid_loss: 0.08154 test_loss: 0.10215 \n",
      "[ 91/200] train_loss: 0.07121 valid_loss: 0.08397 test_loss: 0.09282 \n",
      "[ 92/200] train_loss: 0.06807 valid_loss: 0.08180 test_loss: 0.16326 \n",
      "[ 93/200] train_loss: 0.07036 valid_loss: 0.08227 test_loss: 0.08670 \n",
      "[ 94/200] train_loss: 0.07048 valid_loss: 0.08112 test_loss: 0.08874 \n",
      "[ 95/200] train_loss: 0.06802 valid_loss: 0.08152 test_loss: 0.08524 \n",
      "[ 96/200] train_loss: 0.06964 valid_loss: 0.08056 test_loss: 0.08938 \n",
      "[ 97/200] train_loss: 0.06980 valid_loss: 0.08158 test_loss: 0.09202 \n",
      "[ 98/200] train_loss: 0.06743 valid_loss: 0.08332 test_loss: 0.09162 \n",
      "[ 99/200] train_loss: 0.06740 valid_loss: 0.08231 test_loss: 0.08733 \n",
      "[100/200] train_loss: 0.06582 valid_loss: 0.08070 test_loss: 0.08747 \n",
      "[101/200] train_loss: 0.06818 valid_loss: 0.08236 test_loss: 0.08897 \n",
      "[102/200] train_loss: 0.06763 valid_loss: 0.08077 test_loss: 0.23806 \n",
      "[103/200] train_loss: 0.06831 valid_loss: 0.08259 test_loss: 0.08821 \n",
      "[104/200] train_loss: 0.06820 valid_loss: 0.08263 test_loss: 0.08866 \n",
      "[105/200] train_loss: 0.06811 valid_loss: 0.08225 test_loss: 0.09591 \n",
      "[106/200] train_loss: 0.06884 valid_loss: 0.08234 test_loss: 0.08980 \n",
      "[107/200] train_loss: 0.06579 valid_loss: 0.08140 test_loss: 0.19229 \n",
      "[108/200] train_loss: 0.06538 valid_loss: 0.08552 test_loss: 0.09599 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109/200] train_loss: 0.06664 valid_loss: 0.08038 test_loss: 0.08807 \n",
      "[110/200] train_loss: 0.06831 valid_loss: 0.08339 test_loss: 0.09134 \n",
      "[111/200] train_loss: 0.06666 valid_loss: 0.08235 test_loss: 0.08807 \n",
      "[112/200] train_loss: 0.06615 valid_loss: 0.08080 test_loss: 0.09329 \n",
      "[113/200] train_loss: 0.06744 valid_loss: 0.08869 test_loss: 0.10572 \n",
      "[114/200] train_loss: 0.06771 valid_loss: 0.08710 test_loss: 0.09998 \n",
      "[115/200] train_loss: 0.06603 valid_loss: 0.08427 test_loss: 0.09627 \n",
      "[116/200] train_loss: 0.06792 valid_loss: 0.08728 test_loss: 0.09718 \n",
      "[117/200] train_loss: 0.06656 valid_loss: 0.08190 test_loss: 0.09022 \n",
      "[118/200] train_loss: 0.06449 valid_loss: 0.08346 test_loss: 0.09081 \n",
      "[119/200] train_loss: 0.06266 valid_loss: 0.08125 test_loss: 0.09113 \n",
      "[120/200] train_loss: 0.06531 valid_loss: 0.08945 test_loss: 0.10254 \n",
      "[121/200] train_loss: 0.06611 valid_loss: 0.08279 test_loss: 0.10074 \n",
      "[122/200] train_loss: 0.06410 valid_loss: 0.07995 test_loss: 0.12463 \n",
      "[123/200] train_loss: 0.06510 valid_loss: 0.08289 test_loss: 0.09420 \n",
      "[124/200] train_loss: 0.06477 valid_loss: 0.08162 test_loss: 0.10874 \n",
      "[125/200] train_loss: 0.06263 valid_loss: 0.08318 test_loss: 0.09059 \n",
      "[126/200] train_loss: 0.06377 valid_loss: 0.08430 test_loss: 0.09008 \n",
      "[127/200] train_loss: 0.06509 valid_loss: 0.08751 test_loss: 0.10193 \n",
      "[128/200] train_loss: 0.06439 valid_loss: 0.07958 test_loss: 0.09284 \n",
      "[129/200] train_loss: 0.06441 valid_loss: 0.09367 test_loss: 0.11436 \n",
      "[130/200] train_loss: 0.06243 valid_loss: 0.08614 test_loss: 0.09691 \n",
      "[131/200] train_loss: 0.06296 valid_loss: 0.08746 test_loss: 0.10472 \n",
      "[132/200] train_loss: 0.06332 valid_loss: 0.07938 test_loss: 0.08686 \n",
      "[133/200] train_loss: 0.06530 valid_loss: 0.08036 test_loss: 0.09343 \n",
      "[134/200] train_loss: 0.06295 valid_loss: 0.08050 test_loss: 0.08791 \n",
      "[135/200] train_loss: 0.06334 valid_loss: 0.08122 test_loss: 0.09032 \n",
      "[136/200] train_loss: 0.06285 valid_loss: 0.08128 test_loss: 0.09362 \n",
      "[137/200] train_loss: 0.06334 valid_loss: 0.07923 test_loss: 0.34455 \n",
      "[138/200] train_loss: 0.06333 valid_loss: 0.07963 test_loss: 0.19065 \n",
      "[139/200] train_loss: 0.06172 valid_loss: 0.07980 test_loss: 0.09577 \n",
      "[140/200] train_loss: 0.06286 valid_loss: 0.07886 test_loss: 0.13782 \n",
      "[141/200] train_loss: 0.06403 valid_loss: 0.07812 test_loss: 0.15138 \n",
      "[142/200] train_loss: 0.06361 valid_loss: 0.07978 test_loss: 0.09253 \n",
      "[143/200] train_loss: 0.05991 valid_loss: 0.07751 test_loss: 0.15304 \n",
      "[144/200] train_loss: 0.05966 valid_loss: 0.07738 test_loss: 0.21421 \n",
      "[145/200] train_loss: 0.06256 valid_loss: 0.07799 test_loss: 0.12343 \n",
      "[146/200] train_loss: 0.06134 valid_loss: 0.07732 test_loss: 0.08697 \n",
      "[147/200] train_loss: 0.06219 valid_loss: 0.07803 test_loss: 0.10854 \n",
      "[148/200] train_loss: 0.06246 valid_loss: 0.07958 test_loss: 0.10338 \n",
      "[149/200] train_loss: 0.05985 valid_loss: 0.07961 test_loss: 0.11466 \n",
      "[150/200] train_loss: 0.06133 valid_loss: 0.07877 test_loss: 0.13477 \n",
      "[151/200] train_loss: 0.06085 valid_loss: 0.07941 test_loss: 0.09430 \n",
      "[152/200] train_loss: 0.06145 valid_loss: 0.07680 test_loss: 0.26757 \n",
      "[153/200] train_loss: 0.05958 valid_loss: 0.07774 test_loss: 0.09254 \n",
      "[154/200] train_loss: 0.06088 valid_loss: 0.08053 test_loss: 0.19008 \n",
      "[155/200] train_loss: 0.05992 valid_loss: 0.07875 test_loss: 0.09405 \n",
      "[156/200] train_loss: 0.06157 valid_loss: 0.07674 test_loss: 0.09852 \n",
      "[157/200] train_loss: 0.06162 valid_loss: 0.07842 test_loss: 0.08840 \n",
      "[158/200] train_loss: 0.05908 valid_loss: 0.07830 test_loss: 0.08867 \n",
      "[159/200] train_loss: 0.06166 valid_loss: 0.08113 test_loss: 0.16737 \n",
      "[160/200] train_loss: 0.05856 valid_loss: 0.07857 test_loss: 0.09433 \n",
      "[161/200] train_loss: 0.05999 valid_loss: 0.07804 test_loss: 0.09121 \n",
      "[162/200] train_loss: 0.06028 valid_loss: 0.07934 test_loss: 0.09191 \n",
      "[163/200] train_loss: 0.06122 valid_loss: 0.08238 test_loss: 0.09071 \n",
      "[164/200] train_loss: 0.06009 valid_loss: 0.08106 test_loss: 0.09277 \n",
      "[165/200] train_loss: 0.05883 valid_loss: 0.08177 test_loss: 0.09361 \n",
      "[166/200] train_loss: 0.05969 valid_loss: 0.07659 test_loss: 0.08904 \n",
      "[167/200] train_loss: 0.05834 valid_loss: 0.07822 test_loss: 0.09071 \n",
      "[168/200] train_loss: 0.05900 valid_loss: 0.07671 test_loss: 0.12881 \n",
      "[169/200] train_loss: 0.06007 valid_loss: 0.08071 test_loss: 0.09955 \n",
      "[170/200] train_loss: 0.05908 valid_loss: 0.07810 test_loss: 0.09446 \n",
      "[171/200] train_loss: 0.05887 valid_loss: 0.07801 test_loss: 0.09404 \n",
      "[172/200] train_loss: 0.05778 valid_loss: 0.07782 test_loss: 0.12717 \n",
      "[173/200] train_loss: 0.05922 valid_loss: 0.07763 test_loss: 0.09274 \n",
      "[174/200] train_loss: 0.05940 valid_loss: 0.07755 test_loss: 0.09583 \n",
      "[175/200] train_loss: 0.05738 valid_loss: 0.07644 test_loss: 0.08860 \n",
      "[176/200] train_loss: 0.05813 valid_loss: 0.07718 test_loss: 0.48913 \n",
      "[177/200] train_loss: 0.05917 valid_loss: 0.07656 test_loss: 0.08921 \n",
      "[178/200] train_loss: 0.05789 valid_loss: 0.07812 test_loss: 0.08950 \n",
      "[179/200] train_loss: 0.05815 valid_loss: 0.07821 test_loss: 0.09030 \n",
      "[180/200] train_loss: 0.05687 valid_loss: 0.07792 test_loss: 0.33867 \n",
      "[181/200] train_loss: 0.05824 valid_loss: 0.07836 test_loss: 0.19117 \n",
      "[182/200] train_loss: 0.05637 valid_loss: 0.07752 test_loss: 0.17193 \n",
      "[183/200] train_loss: 0.06026 valid_loss: 0.07797 test_loss: 0.09533 \n",
      "[184/200] train_loss: 0.05772 valid_loss: 0.07853 test_loss: 0.09123 \n",
      "[185/200] train_loss: 0.05764 valid_loss: 0.07678 test_loss: 0.08913 \n",
      "[186/200] train_loss: 0.05759 valid_loss: 0.07945 test_loss: 0.09298 \n",
      "[187/200] train_loss: 0.05798 valid_loss: 0.07557 test_loss: 0.13328 \n",
      "[188/200] train_loss: 0.05770 valid_loss: 0.07678 test_loss: 0.09132 \n",
      "[189/200] train_loss: 0.05541 valid_loss: 0.07734 test_loss: 0.09345 \n",
      "[190/200] train_loss: 0.05627 valid_loss: 0.07827 test_loss: 0.09263 \n",
      "[191/200] train_loss: 0.05675 valid_loss: 0.07833 test_loss: 0.09793 \n",
      "[192/200] train_loss: 0.05627 valid_loss: 0.08079 test_loss: 0.09397 \n",
      "[193/200] train_loss: 0.05513 valid_loss: 0.08498 test_loss: 0.09517 \n",
      "[194/200] train_loss: 0.05799 valid_loss: 0.08276 test_loss: 0.09365 \n",
      "[195/200] train_loss: 0.05625 valid_loss: 0.07916 test_loss: 0.09308 \n",
      "[196/200] train_loss: 0.05852 valid_loss: 0.08391 test_loss: 0.09507 \n",
      "[197/200] train_loss: 0.05523 valid_loss: 0.08115 test_loss: 0.09437 \n",
      "[198/200] train_loss: 0.05655 valid_loss: 0.08623 test_loss: 0.09556 \n",
      "[199/200] train_loss: 0.05673 valid_loss: 0.07745 test_loss: 0.09333 \n",
      "[200/200] train_loss: 0.05688 valid_loss: 0.08245 test_loss: 0.11454 \n",
      "TRAINING MODEL 12\n",
      "[  1/200] train_loss: 0.36926 valid_loss: 0.27948 test_loss: 0.25577 \n",
      "验证损失减少 (inf --> 0.255771). 正在保存模型...\n",
      "[  2/200] train_loss: 0.23034 valid_loss: 0.20944 test_loss: 0.16861 \n",
      "验证损失减少 (0.279483 --> 0.168610). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17393 valid_loss: 0.17379 test_loss: 0.13666 \n",
      "验证损失减少 (0.209444 --> 0.136661). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14833 valid_loss: 0.14679 test_loss: 0.12078 \n",
      "验证损失减少 (0.173793 --> 0.120779). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13877 valid_loss: 0.13947 test_loss: 0.11900 \n",
      "验证损失减少 (0.146786 --> 0.119000). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12991 valid_loss: 0.13372 test_loss: 0.11195 \n",
      "验证损失减少 (0.139474 --> 0.111950). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12244 valid_loss: 0.12767 test_loss: 0.11144 \n",
      "验证损失减少 (0.133721 --> 0.111436). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12188 valid_loss: 0.12284 test_loss: 0.10750 \n",
      "验证损失减少 (0.127666 --> 0.107504). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11594 valid_loss: 0.11988 test_loss: 0.10561 \n",
      "验证损失减少 (0.122835 --> 0.105608). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11211 valid_loss: 0.12762 test_loss: 0.12635 \n",
      "[ 11/200] train_loss: 0.11096 valid_loss: 0.11382 test_loss: 0.10480 \n",
      "验证损失减少 (0.119885 --> 0.104803). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10722 valid_loss: 0.11076 test_loss: 0.10493 \n",
      "验证损失减少 (0.113816 --> 0.104926). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10560 valid_loss: 0.11049 test_loss: 0.10307 \n",
      "验证损失减少 (0.110760 --> 0.103072). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10606 valid_loss: 0.10757 test_loss: 0.10149 \n",
      "验证损失减少 (0.110490 --> 0.101494). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10265 valid_loss: 0.10639 test_loss: 0.10113 \n",
      "验证损失减少 (0.107571 --> 0.101128). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10143 valid_loss: 0.10174 test_loss: 0.10396 \n",
      "验证损失减少 (0.106386 --> 0.103965). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17/200] train_loss: 0.09782 valid_loss: 0.10159 test_loss: 0.11011 \n",
      "[ 18/200] train_loss: 0.09767 valid_loss: 0.10179 test_loss: 0.09889 \n",
      "验证损失减少 (0.101740 --> 0.098893). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09677 valid_loss: 0.10107 test_loss: 0.10368 \n",
      "[ 20/200] train_loss: 0.09599 valid_loss: 0.10112 test_loss: 0.10052 \n",
      "验证损失减少 (0.101787 --> 0.100516). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09639 valid_loss: 0.09803 test_loss: 0.10479 \n",
      "[ 22/200] train_loss: 0.09403 valid_loss: 0.09865 test_loss: 0.10358 \n",
      "[ 23/200] train_loss: 0.09225 valid_loss: 0.09811 test_loss: 0.09894 \n",
      "验证损失减少 (0.101123 --> 0.098945). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.09031 valid_loss: 0.09458 test_loss: 0.12873 \n",
      "[ 25/200] train_loss: 0.09274 valid_loss: 0.09585 test_loss: 0.09915 \n",
      "[ 26/200] train_loss: 0.09057 valid_loss: 0.09329 test_loss: 0.14296 \n",
      "[ 27/200] train_loss: 0.09045 valid_loss: 0.09360 test_loss: 0.10642 \n",
      "[ 28/200] train_loss: 0.08858 valid_loss: 0.09350 test_loss: 0.09950 \n",
      "[ 29/200] train_loss: 0.08789 valid_loss: 0.09440 test_loss: 0.10548 \n",
      "[ 30/200] train_loss: 0.08703 valid_loss: 0.09005 test_loss: 0.09762 \n",
      "验证损失减少 (0.098111 --> 0.097618). 正在保存模型...\n",
      "[ 31/200] train_loss: 0.08781 valid_loss: 0.09167 test_loss: 0.09449 \n",
      "[ 32/200] train_loss: 0.08527 valid_loss: 0.08985 test_loss: 0.11482 \n",
      "[ 33/200] train_loss: 0.08489 valid_loss: 0.09012 test_loss: 0.09788 \n",
      "[ 34/200] train_loss: 0.08436 valid_loss: 0.08847 test_loss: 0.22414 \n",
      "[ 35/200] train_loss: 0.08561 valid_loss: 0.09081 test_loss: 0.34073 \n",
      "[ 36/200] train_loss: 0.08277 valid_loss: 0.08659 test_loss: 0.13687 \n",
      "[ 37/200] train_loss: 0.08184 valid_loss: 0.08871 test_loss: 0.10120 \n",
      "[ 38/200] train_loss: 0.08224 valid_loss: 0.08983 test_loss: 0.09726 \n",
      "[ 39/200] train_loss: 0.08138 valid_loss: 0.08874 test_loss: 0.10153 \n",
      "[ 40/200] train_loss: 0.08214 valid_loss: 0.08924 test_loss: 0.09485 \n",
      "[ 41/200] train_loss: 0.08129 valid_loss: 0.08894 test_loss: 0.19897 \n",
      "[ 42/200] train_loss: 0.08338 valid_loss: 0.08672 test_loss: 0.10322 \n",
      "[ 43/200] train_loss: 0.07922 valid_loss: 0.08648 test_loss: 0.37073 \n",
      "[ 44/200] train_loss: 0.07705 valid_loss: 0.08560 test_loss: 0.10026 \n",
      "[ 45/200] train_loss: 0.08004 valid_loss: 0.08695 test_loss: 0.10622 \n",
      "[ 46/200] train_loss: 0.07739 valid_loss: 0.08419 test_loss: 0.10876 \n",
      "[ 47/200] train_loss: 0.07726 valid_loss: 0.08459 test_loss: 0.09649 \n",
      "[ 48/200] train_loss: 0.07754 valid_loss: 0.08330 test_loss: 0.10025 \n",
      "[ 49/200] train_loss: 0.07580 valid_loss: 0.08373 test_loss: 0.18393 \n",
      "[ 50/200] train_loss: 0.07775 valid_loss: 0.08502 test_loss: 0.37741 \n",
      "[ 51/200] train_loss: 0.07592 valid_loss: 0.08341 test_loss: 0.09272 \n",
      "[ 52/200] train_loss: 0.07722 valid_loss: 0.08666 test_loss: 0.10091 \n",
      "[ 53/200] train_loss: 0.07585 valid_loss: 0.08433 test_loss: 0.28341 \n",
      "[ 54/200] train_loss: 0.07797 valid_loss: 0.08271 test_loss: 0.18640 \n",
      "[ 55/200] train_loss: 0.07561 valid_loss: 0.08420 test_loss: 0.21851 \n",
      "[ 56/200] train_loss: 0.07687 valid_loss: 0.08271 test_loss: 0.09470 \n",
      "[ 57/200] train_loss: 0.07647 valid_loss: 0.08315 test_loss: 0.09408 \n",
      "[ 58/200] train_loss: 0.07456 valid_loss: 0.08378 test_loss: 0.09780 \n",
      "[ 59/200] train_loss: 0.07289 valid_loss: 0.08424 test_loss: 0.09522 \n",
      "[ 60/200] train_loss: 0.07322 valid_loss: 0.08298 test_loss: 0.09392 \n",
      "[ 61/200] train_loss: 0.07493 valid_loss: 0.08327 test_loss: 0.09569 \n",
      "[ 62/200] train_loss: 0.07476 valid_loss: 0.08278 test_loss: 0.09179 \n",
      "[ 63/200] train_loss: 0.07348 valid_loss: 0.08279 test_loss: 0.09476 \n",
      "[ 64/200] train_loss: 0.07425 valid_loss: 0.08120 test_loss: 0.09523 \n",
      "[ 65/200] train_loss: 0.07418 valid_loss: 0.08148 test_loss: 0.09454 \n",
      "[ 66/200] train_loss: 0.07443 valid_loss: 0.08260 test_loss: 0.10715 \n",
      "[ 67/200] train_loss: 0.07385 valid_loss: 0.08144 test_loss: 0.16492 \n",
      "[ 68/200] train_loss: 0.07238 valid_loss: 0.08133 test_loss: 0.33531 \n",
      "[ 69/200] train_loss: 0.07128 valid_loss: 0.08102 test_loss: 0.09840 \n",
      "[ 70/200] train_loss: 0.07107 valid_loss: 0.08021 test_loss: 0.09916 \n",
      "[ 71/200] train_loss: 0.07038 valid_loss: 0.08003 test_loss: 0.10301 \n",
      "[ 72/200] train_loss: 0.07070 valid_loss: 0.07960 test_loss: 0.11133 \n",
      "[ 73/200] train_loss: 0.07001 valid_loss: 0.07879 test_loss: 0.09628 \n",
      "[ 74/200] train_loss: 0.07271 valid_loss: 0.08222 test_loss: 0.09606 \n",
      "[ 75/200] train_loss: 0.06931 valid_loss: 0.08133 test_loss: 0.12217 \n",
      "[ 76/200] train_loss: 0.06995 valid_loss: 0.07977 test_loss: 0.09874 \n",
      "[ 77/200] train_loss: 0.06933 valid_loss: 0.08082 test_loss: 0.09477 \n",
      "[ 78/200] train_loss: 0.07014 valid_loss: 0.08135 test_loss: 0.09905 \n",
      "[ 79/200] train_loss: 0.07097 valid_loss: 0.07866 test_loss: 0.13491 \n",
      "[ 80/200] train_loss: 0.06954 valid_loss: 0.07996 test_loss: 0.09881 \n",
      "[ 81/200] train_loss: 0.07064 valid_loss: 0.08071 test_loss: 0.09446 \n",
      "[ 82/200] train_loss: 0.07072 valid_loss: 0.08065 test_loss: 0.09400 \n",
      "[ 83/200] train_loss: 0.06881 valid_loss: 0.07953 test_loss: 0.09249 \n",
      "[ 84/200] train_loss: 0.06912 valid_loss: 0.07860 test_loss: 0.13409 \n",
      "[ 85/200] train_loss: 0.07055 valid_loss: 0.08042 test_loss: 0.09749 \n",
      "[ 86/200] train_loss: 0.06919 valid_loss: 0.08181 test_loss: 0.09342 \n",
      "[ 87/200] train_loss: 0.06803 valid_loss: 0.07910 test_loss: 0.09924 \n",
      "[ 88/200] train_loss: 0.06863 valid_loss: 0.07905 test_loss: 0.09511 \n",
      "[ 89/200] train_loss: 0.06573 valid_loss: 0.07912 test_loss: 0.09372 \n",
      "[ 90/200] train_loss: 0.06770 valid_loss: 0.07933 test_loss: 0.09294 \n",
      "[ 91/200] train_loss: 0.06889 valid_loss: 0.08025 test_loss: 0.09094 \n",
      "[ 92/200] train_loss: 0.06711 valid_loss: 0.07757 test_loss: 0.09363 \n",
      "[ 93/200] train_loss: 0.06732 valid_loss: 0.07789 test_loss: 0.10009 \n",
      "[ 94/200] train_loss: 0.06713 valid_loss: 0.07810 test_loss: 0.09175 \n",
      "[ 95/200] train_loss: 0.06693 valid_loss: 0.07942 test_loss: 0.09559 \n",
      "[ 96/200] train_loss: 0.06673 valid_loss: 0.07717 test_loss: 0.08860 \n",
      "验证损失减少 (0.090055 --> 0.088601). 正在保存模型...\n",
      "[ 97/200] train_loss: 0.06680 valid_loss: 0.07789 test_loss: 0.09419 \n",
      "[ 98/200] train_loss: 0.06756 valid_loss: 0.07801 test_loss: 0.11205 \n",
      "[ 99/200] train_loss: 0.06745 valid_loss: 0.08068 test_loss: 0.09769 \n",
      "[100/200] train_loss: 0.06415 valid_loss: 0.07983 test_loss: 0.09193 \n",
      "[101/200] train_loss: 0.06705 valid_loss: 0.07675 test_loss: 0.10031 \n",
      "[102/200] train_loss: 0.06559 valid_loss: 0.07922 test_loss: 0.52707 \n",
      "[103/200] train_loss: 0.06608 valid_loss: 0.07734 test_loss: 0.10176 \n",
      "[104/200] train_loss: 0.06522 valid_loss: 0.07987 test_loss: 0.09291 \n",
      "[105/200] train_loss: 0.06491 valid_loss: 0.07648 test_loss: 0.55834 \n",
      "[106/200] train_loss: 0.06670 valid_loss: 0.08039 test_loss: 0.14120 \n",
      "[107/200] train_loss: 0.06446 valid_loss: 0.07770 test_loss: 0.11335 \n",
      "[108/200] train_loss: 0.06508 valid_loss: 0.07787 test_loss: 0.28281 \n",
      "[109/200] train_loss: 0.06407 valid_loss: 0.07714 test_loss: 0.11246 \n",
      "[110/200] train_loss: 0.06699 valid_loss: 0.07661 test_loss: 0.54650 \n",
      "[111/200] train_loss: 0.06364 valid_loss: 0.07825 test_loss: 0.73044 \n",
      "[112/200] train_loss: 0.06225 valid_loss: 0.07657 test_loss: 0.57050 \n",
      "[113/200] train_loss: 0.06394 valid_loss: 0.07706 test_loss: 0.12428 \n",
      "[114/200] train_loss: 0.06260 valid_loss: 0.07802 test_loss: 0.09926 \n",
      "[115/200] train_loss: 0.06531 valid_loss: 0.07650 test_loss: 0.14891 \n",
      "[116/200] train_loss: 0.06403 valid_loss: 0.07689 test_loss: 0.10368 \n",
      "[117/200] train_loss: 0.06260 valid_loss: 0.08051 test_loss: 0.09946 \n",
      "[118/200] train_loss: 0.06356 valid_loss: 0.07755 test_loss: 0.09898 \n",
      "[119/200] train_loss: 0.06374 valid_loss: 0.07753 test_loss: 0.29684 \n",
      "[120/200] train_loss: 0.06331 valid_loss: 0.07786 test_loss: 0.14814 \n",
      "[121/200] train_loss: 0.06257 valid_loss: 0.07652 test_loss: 0.09384 \n",
      "[122/200] train_loss: 0.06208 valid_loss: 0.07835 test_loss: 0.12474 \n",
      "[123/200] train_loss: 0.06237 valid_loss: 0.07824 test_loss: 0.10027 \n",
      "[124/200] train_loss: 0.06375 valid_loss: 0.07678 test_loss: 0.09790 \n",
      "[125/200] train_loss: 0.06098 valid_loss: 0.07652 test_loss: 0.10532 \n",
      "[126/200] train_loss: 0.06265 valid_loss: 0.07633 test_loss: 0.09364 \n",
      "[127/200] train_loss: 0.06250 valid_loss: 0.07744 test_loss: 0.10332 \n",
      "[128/200] train_loss: 0.06227 valid_loss: 0.07945 test_loss: 0.09659 \n",
      "[129/200] train_loss: 0.06324 valid_loss: 0.07721 test_loss: 0.09852 \n",
      "[130/200] train_loss: 0.06017 valid_loss: 0.07768 test_loss: 0.13593 \n",
      "[131/200] train_loss: 0.06177 valid_loss: 0.07670 test_loss: 0.09300 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132/200] train_loss: 0.06164 valid_loss: 0.07939 test_loss: 0.09732 \n",
      "[133/200] train_loss: 0.06273 valid_loss: 0.07765 test_loss: 0.11607 \n",
      "[134/200] train_loss: 0.06087 valid_loss: 0.07790 test_loss: 0.20524 \n",
      "[135/200] train_loss: 0.06075 valid_loss: 0.07864 test_loss: 0.11338 \n",
      "[136/200] train_loss: 0.05971 valid_loss: 0.07885 test_loss: 0.09866 \n",
      "[137/200] train_loss: 0.06103 valid_loss: 0.07685 test_loss: 0.09346 \n",
      "[138/200] train_loss: 0.06036 valid_loss: 0.07770 test_loss: 0.10199 \n",
      "[139/200] train_loss: 0.06117 valid_loss: 0.07804 test_loss: 0.10404 \n",
      "[140/200] train_loss: 0.06174 valid_loss: 0.07693 test_loss: 0.09764 \n",
      "[141/200] train_loss: 0.06046 valid_loss: 0.07973 test_loss: 0.09533 \n",
      "[142/200] train_loss: 0.06166 valid_loss: 0.07905 test_loss: 0.10005 \n",
      "[143/200] train_loss: 0.05958 valid_loss: 0.07972 test_loss: 0.11788 \n",
      "[144/200] train_loss: 0.06006 valid_loss: 0.07545 test_loss: 0.11325 \n",
      "[145/200] train_loss: 0.05885 valid_loss: 0.07904 test_loss: 0.09700 \n",
      "[146/200] train_loss: 0.06172 valid_loss: 0.07867 test_loss: 0.14302 \n",
      "[147/200] train_loss: 0.06059 valid_loss: 0.07924 test_loss: 0.19166 \n",
      "[148/200] train_loss: 0.05918 valid_loss: 0.07858 test_loss: 0.17417 \n",
      "[149/200] train_loss: 0.05899 valid_loss: 0.07764 test_loss: 0.11146 \n",
      "[150/200] train_loss: 0.05907 valid_loss: 0.07892 test_loss: 0.13844 \n",
      "[151/200] train_loss: 0.05880 valid_loss: 0.07979 test_loss: 0.09867 \n",
      "[152/200] train_loss: 0.05854 valid_loss: 0.07874 test_loss: 0.09707 \n",
      "[153/200] train_loss: 0.05962 valid_loss: 0.07951 test_loss: 0.09620 \n",
      "[154/200] train_loss: 0.06029 valid_loss: 0.07794 test_loss: 0.09447 \n",
      "[155/200] train_loss: 0.05896 valid_loss: 0.07623 test_loss: 0.11056 \n",
      "[156/200] train_loss: 0.05900 valid_loss: 0.07640 test_loss: 0.09702 \n",
      "[157/200] train_loss: 0.05875 valid_loss: 0.07689 test_loss: 0.12962 \n",
      "[158/200] train_loss: 0.05763 valid_loss: 0.07859 test_loss: 0.09752 \n",
      "[159/200] train_loss: 0.05738 valid_loss: 0.07824 test_loss: 0.09868 \n",
      "[160/200] train_loss: 0.05881 valid_loss: 0.07851 test_loss: 0.09782 \n",
      "[161/200] train_loss: 0.05661 valid_loss: 0.07565 test_loss: 0.09774 \n",
      "[162/200] train_loss: 0.05789 valid_loss: 0.07652 test_loss: 0.10194 \n",
      "[163/200] train_loss: 0.05854 valid_loss: 0.07641 test_loss: 0.09717 \n",
      "[164/200] train_loss: 0.05808 valid_loss: 0.07634 test_loss: 0.12684 \n",
      "[165/200] train_loss: 0.05816 valid_loss: 0.07754 test_loss: 0.09865 \n",
      "[166/200] train_loss: 0.05744 valid_loss: 0.07559 test_loss: 0.17272 \n",
      "[167/200] train_loss: 0.05750 valid_loss: 0.07756 test_loss: 0.09817 \n",
      "[168/200] train_loss: 0.05604 valid_loss: 0.07720 test_loss: 0.15451 \n",
      "[169/200] train_loss: 0.05904 valid_loss: 0.07691 test_loss: 0.11172 \n",
      "[170/200] train_loss: 0.05773 valid_loss: 0.07806 test_loss: 0.17207 \n",
      "[171/200] train_loss: 0.05757 valid_loss: 0.07834 test_loss: 0.10710 \n",
      "[172/200] train_loss: 0.05852 valid_loss: 0.07686 test_loss: 0.10572 \n",
      "[173/200] train_loss: 0.05568 valid_loss: 0.07681 test_loss: 0.13502 \n",
      "[174/200] train_loss: 0.05709 valid_loss: 0.07699 test_loss: 0.10733 \n",
      "[175/200] train_loss: 0.05627 valid_loss: 0.07800 test_loss: 0.11550 \n",
      "[176/200] train_loss: 0.05718 valid_loss: 0.07625 test_loss: 0.09492 \n",
      "[177/200] train_loss: 0.05544 valid_loss: 0.07976 test_loss: 0.10085 \n",
      "[178/200] train_loss: 0.05566 valid_loss: 0.07654 test_loss: 0.09347 \n",
      "[179/200] train_loss: 0.05646 valid_loss: 0.07818 test_loss: 0.09841 \n",
      "[180/200] train_loss: 0.05541 valid_loss: 0.07705 test_loss: 0.09531 \n",
      "[181/200] train_loss: 0.05739 valid_loss: 0.08017 test_loss: 0.09830 \n",
      "[182/200] train_loss: 0.05717 valid_loss: 0.07649 test_loss: 0.09920 \n",
      "[183/200] train_loss: 0.05640 valid_loss: 0.07592 test_loss: 0.13964 \n",
      "[184/200] train_loss: 0.05728 valid_loss: 0.07614 test_loss: 0.17447 \n",
      "[185/200] train_loss: 0.05780 valid_loss: 0.07669 test_loss: 0.11054 \n",
      "[186/200] train_loss: 0.05621 valid_loss: 0.07688 test_loss: 0.10051 \n",
      "[187/200] train_loss: 0.05888 valid_loss: 0.07599 test_loss: 0.09678 \n",
      "[188/200] train_loss: 0.05525 valid_loss: 0.07740 test_loss: 0.09478 \n",
      "[189/200] train_loss: 0.05570 valid_loss: 0.07637 test_loss: 0.09728 \n",
      "[190/200] train_loss: 0.05766 valid_loss: 0.07705 test_loss: 0.29353 \n",
      "[191/200] train_loss: 0.05613 valid_loss: 0.07826 test_loss: 0.51622 \n",
      "[192/200] train_loss: 0.05625 valid_loss: 0.07734 test_loss: 0.09986 \n",
      "[193/200] train_loss: 0.05535 valid_loss: 0.07567 test_loss: 0.09777 \n",
      "[194/200] train_loss: 0.05632 valid_loss: 0.07541 test_loss: 0.09749 \n",
      "[195/200] train_loss: 0.05708 valid_loss: 0.07865 test_loss: 0.09526 \n",
      "[196/200] train_loss: 0.05467 valid_loss: 0.07716 test_loss: 0.09637 \n",
      "[197/200] train_loss: 0.05513 valid_loss: 0.07760 test_loss: 0.09309 \n",
      "[198/200] train_loss: 0.05534 valid_loss: 0.07766 test_loss: 0.10429 \n",
      "[199/200] train_loss: 0.05608 valid_loss: 0.07675 test_loss: 0.09527 \n",
      "[200/200] train_loss: 0.05472 valid_loss: 0.07590 test_loss: 0.09748 \n",
      "TRAINING MODEL 13\n",
      "[  1/200] train_loss: 0.35368 valid_loss: 0.27546 test_loss: 0.24822 \n",
      "验证损失减少 (inf --> 0.248218). 正在保存模型...\n",
      "[  2/200] train_loss: 0.21648 valid_loss: 0.19936 test_loss: 0.15384 \n",
      "验证损失减少 (0.275461 --> 0.153842). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16453 valid_loss: 0.16985 test_loss: 0.13284 \n",
      "验证损失减少 (0.199360 --> 0.132845). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14578 valid_loss: 0.14717 test_loss: 0.11887 \n",
      "验证损失减少 (0.169846 --> 0.118868). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13596 valid_loss: 0.13837 test_loss: 0.11569 \n",
      "验证损失减少 (0.147175 --> 0.115692). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12729 valid_loss: 0.13476 test_loss: 0.11802 \n",
      "验证损失减少 (0.138369 --> 0.118021). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12473 valid_loss: 0.12799 test_loss: 0.11546 \n",
      "验证损失减少 (0.134763 --> 0.115459). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12072 valid_loss: 0.12435 test_loss: 0.10780 \n",
      "验证损失减少 (0.127988 --> 0.107798). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11356 valid_loss: 0.12746 test_loss: 0.11196 \n",
      "验证损失减少 (0.124353 --> 0.111964). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11366 valid_loss: 0.11623 test_loss: 0.10428 \n",
      "验证损失减少 (0.127456 --> 0.104281). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.10923 valid_loss: 0.11361 test_loss: 0.10321 \n",
      "验证损失减少 (0.116231 --> 0.103211). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10800 valid_loss: 0.11364 test_loss: 0.10475 \n",
      "验证损失减少 (0.113608 --> 0.104750). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10710 valid_loss: 0.10839 test_loss: 0.10154 \n",
      "验证损失减少 (0.113639 --> 0.101536). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10674 valid_loss: 0.10782 test_loss: 0.10440 \n",
      "验证损失减少 (0.108387 --> 0.104397). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10451 valid_loss: 0.10709 test_loss: 0.18150 \n",
      "[ 16/200] train_loss: 0.09906 valid_loss: 0.10746 test_loss: 0.12669 \n",
      "[ 17/200] train_loss: 0.10099 valid_loss: 0.10266 test_loss: 0.13425 \n",
      "[ 18/200] train_loss: 0.10116 valid_loss: 0.10382 test_loss: 0.10040 \n",
      "验证损失减少 (0.107824 --> 0.100399). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09924 valid_loss: 0.10297 test_loss: 0.11138 \n",
      "[ 20/200] train_loss: 0.09815 valid_loss: 0.10015 test_loss: 0.09999 \n",
      "验证损失减少 (0.103820 --> 0.099988). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09864 valid_loss: 0.09890 test_loss: 0.12127 \n",
      "[ 22/200] train_loss: 0.09546 valid_loss: 0.09985 test_loss: 0.10616 \n",
      "[ 23/200] train_loss: 0.09338 valid_loss: 0.09770 test_loss: 0.10218 \n",
      "[ 24/200] train_loss: 0.09509 valid_loss: 0.09710 test_loss: 0.09749 \n",
      "验证损失减少 (0.100148 --> 0.097493). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09115 valid_loss: 0.09524 test_loss: 0.11385 \n",
      "[ 26/200] train_loss: 0.09107 valid_loss: 0.09571 test_loss: 0.12056 \n",
      "[ 27/200] train_loss: 0.09076 valid_loss: 0.09510 test_loss: 0.11388 \n",
      "[ 28/200] train_loss: 0.09076 valid_loss: 0.09622 test_loss: 0.09752 \n",
      "[ 29/200] train_loss: 0.09029 valid_loss: 0.09206 test_loss: 0.13607 \n",
      "[ 30/200] train_loss: 0.08919 valid_loss: 0.09194 test_loss: 0.31965 \n",
      "[ 31/200] train_loss: 0.09115 valid_loss: 0.09299 test_loss: 0.09747 \n",
      "[ 32/200] train_loss: 0.09043 valid_loss: 0.09152 test_loss: 0.12087 \n",
      "[ 33/200] train_loss: 0.08735 valid_loss: 0.09090 test_loss: 0.10235 \n",
      "[ 34/200] train_loss: 0.08630 valid_loss: 0.09125 test_loss: 0.10221 \n",
      "[ 35/200] train_loss: 0.08407 valid_loss: 0.09062 test_loss: 0.16278 \n",
      "[ 36/200] train_loss: 0.08502 valid_loss: 0.09151 test_loss: 0.38623 \n",
      "[ 37/200] train_loss: 0.08534 valid_loss: 0.09078 test_loss: 0.09279 \n",
      "验证损失减少 (0.097097 --> 0.092789). 正在保存模型...\n",
      "[ 38/200] train_loss: 0.08431 valid_loss: 0.09044 test_loss: 0.21217 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 39/200] train_loss: 0.08413 valid_loss: 0.08940 test_loss: 0.21804 \n",
      "[ 40/200] train_loss: 0.08432 valid_loss: 0.08780 test_loss: 0.11714 \n",
      "[ 41/200] train_loss: 0.08496 valid_loss: 0.08722 test_loss: 0.35481 \n",
      "[ 42/200] train_loss: 0.08123 valid_loss: 0.08593 test_loss: 0.25627 \n",
      "[ 43/200] train_loss: 0.08213 valid_loss: 0.08569 test_loss: 0.20849 \n",
      "[ 44/200] train_loss: 0.08079 valid_loss: 0.08563 test_loss: 0.09386 \n",
      "[ 45/200] train_loss: 0.08081 valid_loss: 0.08505 test_loss: 0.18951 \n",
      "[ 46/200] train_loss: 0.07996 valid_loss: 0.08493 test_loss: 0.11392 \n",
      "[ 47/200] train_loss: 0.08011 valid_loss: 0.08438 test_loss: 0.09923 \n",
      "[ 48/200] train_loss: 0.07973 valid_loss: 0.08551 test_loss: 0.22741 \n",
      "[ 49/200] train_loss: 0.08023 valid_loss: 0.08536 test_loss: 0.09939 \n",
      "[ 50/200] train_loss: 0.07891 valid_loss: 0.08683 test_loss: 0.12607 \n",
      "[ 51/200] train_loss: 0.07848 valid_loss: 0.08341 test_loss: 0.21218 \n",
      "[ 52/200] train_loss: 0.07843 valid_loss: 0.08501 test_loss: 0.39848 \n",
      "[ 53/200] train_loss: 0.07741 valid_loss: 0.08367 test_loss: 0.21494 \n",
      "[ 54/200] train_loss: 0.07671 valid_loss: 0.08418 test_loss: 0.10064 \n",
      "[ 55/200] train_loss: 0.07789 valid_loss: 0.08357 test_loss: 0.11523 \n",
      "[ 56/200] train_loss: 0.07888 valid_loss: 0.08278 test_loss: 0.09212 \n",
      "[ 57/200] train_loss: 0.07737 valid_loss: 0.08344 test_loss: 0.13811 \n",
      "[ 58/200] train_loss: 0.07763 valid_loss: 0.08327 test_loss: 0.36336 \n",
      "[ 59/200] train_loss: 0.07851 valid_loss: 0.08446 test_loss: 0.13199 \n",
      "[ 60/200] train_loss: 0.07662 valid_loss: 0.08399 test_loss: 0.09885 \n",
      "[ 61/200] train_loss: 0.07747 valid_loss: 0.08282 test_loss: 0.09255 \n",
      "[ 62/200] train_loss: 0.07659 valid_loss: 0.08296 test_loss: 0.18365 \n",
      "[ 63/200] train_loss: 0.07484 valid_loss: 0.08252 test_loss: 0.09780 \n",
      "[ 64/200] train_loss: 0.07274 valid_loss: 0.08212 test_loss: 0.10356 \n",
      "[ 65/200] train_loss: 0.07220 valid_loss: 0.08313 test_loss: 0.45657 \n",
      "[ 66/200] train_loss: 0.07545 valid_loss: 0.08273 test_loss: 0.09005 \n",
      "验证损失减少 (0.090776 --> 0.090047). 正在保存模型...\n",
      "[ 67/200] train_loss: 0.07458 valid_loss: 0.08121 test_loss: 0.16475 \n",
      "[ 68/200] train_loss: 0.07431 valid_loss: 0.08225 test_loss: 0.09391 \n",
      "[ 69/200] train_loss: 0.07389 valid_loss: 0.08313 test_loss: 0.11372 \n",
      "[ 70/200] train_loss: 0.07329 valid_loss: 0.08142 test_loss: 0.09280 \n",
      "[ 71/200] train_loss: 0.07287 valid_loss: 0.07976 test_loss: 0.09399 \n",
      "[ 72/200] train_loss: 0.07204 valid_loss: 0.07972 test_loss: 0.08699 \n",
      "[ 73/200] train_loss: 0.07196 valid_loss: 0.08038 test_loss: 0.10996 \n",
      "[ 74/200] train_loss: 0.07245 valid_loss: 0.07952 test_loss: 0.13199 \n",
      "[ 75/200] train_loss: 0.07112 valid_loss: 0.08133 test_loss: 0.09118 \n",
      "[ 76/200] train_loss: 0.07338 valid_loss: 0.08025 test_loss: 0.09793 \n",
      "[ 77/200] train_loss: 0.07149 valid_loss: 0.08039 test_loss: 0.09434 \n",
      "[ 78/200] train_loss: 0.07038 valid_loss: 0.07902 test_loss: 0.25325 \n",
      "[ 79/200] train_loss: 0.06967 valid_loss: 0.07877 test_loss: 0.29335 \n",
      "[ 80/200] train_loss: 0.07283 valid_loss: 0.07899 test_loss: 0.09225 \n",
      "[ 81/200] train_loss: 0.06922 valid_loss: 0.07908 test_loss: 0.09270 \n",
      "[ 82/200] train_loss: 0.07114 valid_loss: 0.08082 test_loss: 0.09356 \n",
      "[ 83/200] train_loss: 0.07013 valid_loss: 0.07979 test_loss: 0.08968 \n",
      "[ 84/200] train_loss: 0.07077 valid_loss: 0.07956 test_loss: 0.09152 \n",
      "[ 85/200] train_loss: 0.07151 valid_loss: 0.08144 test_loss: 0.15607 \n",
      "[ 86/200] train_loss: 0.06937 valid_loss: 0.08127 test_loss: 0.09687 \n",
      "[ 87/200] train_loss: 0.07035 valid_loss: 0.07789 test_loss: 0.11042 \n",
      "[ 88/200] train_loss: 0.07003 valid_loss: 0.08076 test_loss: 0.13778 \n",
      "[ 89/200] train_loss: 0.06928 valid_loss: 0.07814 test_loss: 0.09181 \n",
      "[ 90/200] train_loss: 0.06869 valid_loss: 0.07773 test_loss: 0.12812 \n",
      "[ 91/200] train_loss: 0.06917 valid_loss: 0.07863 test_loss: 0.09088 \n",
      "[ 92/200] train_loss: 0.06668 valid_loss: 0.07754 test_loss: 0.09123 \n",
      "[ 93/200] train_loss: 0.06875 valid_loss: 0.07950 test_loss: 0.08942 \n",
      "[ 94/200] train_loss: 0.06777 valid_loss: 0.07792 test_loss: 0.11323 \n",
      "[ 95/200] train_loss: 0.06852 valid_loss: 0.07727 test_loss: 0.09869 \n",
      "[ 96/200] train_loss: 0.06822 valid_loss: 0.07844 test_loss: 0.09187 \n",
      "[ 97/200] train_loss: 0.06761 valid_loss: 0.07980 test_loss: 0.09458 \n",
      "[ 98/200] train_loss: 0.06974 valid_loss: 0.07807 test_loss: 0.36930 \n",
      "[ 99/200] train_loss: 0.06925 valid_loss: 0.07801 test_loss: 0.22832 \n",
      "[100/200] train_loss: 0.06849 valid_loss: 0.08135 test_loss: 0.09544 \n",
      "[101/200] train_loss: 0.06719 valid_loss: 0.08047 test_loss: 0.34102 \n",
      "[102/200] train_loss: 0.06630 valid_loss: 0.07821 test_loss: 0.10516 \n",
      "[103/200] train_loss: 0.06498 valid_loss: 0.07735 test_loss: 0.09046 \n",
      "[104/200] train_loss: 0.06730 valid_loss: 0.07740 test_loss: 0.19798 \n",
      "[105/200] train_loss: 0.06634 valid_loss: 0.07910 test_loss: 0.09988 \n",
      "[106/200] train_loss: 0.06510 valid_loss: 0.07669 test_loss: 0.47248 \n",
      "[107/200] train_loss: 0.06557 valid_loss: 0.07752 test_loss: 0.16118 \n",
      "[108/200] train_loss: 0.06570 valid_loss: 0.07860 test_loss: 0.21124 \n",
      "[109/200] train_loss: 0.06694 valid_loss: 0.07794 test_loss: 0.35562 \n",
      "[110/200] train_loss: 0.06571 valid_loss: 0.07795 test_loss: 0.13390 \n",
      "[111/200] train_loss: 0.06726 valid_loss: 0.07828 test_loss: 0.24719 \n",
      "[112/200] train_loss: 0.06507 valid_loss: 0.07728 test_loss: 0.09029 \n",
      "[113/200] train_loss: 0.06399 valid_loss: 0.07732 test_loss: 0.09264 \n",
      "[114/200] train_loss: 0.06513 valid_loss: 0.07644 test_loss: 0.09330 \n",
      "[115/200] train_loss: 0.06524 valid_loss: 0.07869 test_loss: 0.10313 \n",
      "[116/200] train_loss: 0.06465 valid_loss: 0.07865 test_loss: 0.09725 \n",
      "[117/200] train_loss: 0.06481 valid_loss: 0.07572 test_loss: 0.09861 \n",
      "[118/200] train_loss: 0.06500 valid_loss: 0.07829 test_loss: 0.10942 \n",
      "[119/200] train_loss: 0.06570 valid_loss: 0.07702 test_loss: 0.55033 \n",
      "[120/200] train_loss: 0.06450 valid_loss: 0.07694 test_loss: 0.21093 \n",
      "[121/200] train_loss: 0.06406 valid_loss: 0.07867 test_loss: 0.09228 \n",
      "[122/200] train_loss: 0.06318 valid_loss: 0.07636 test_loss: 0.09834 \n",
      "[123/200] train_loss: 0.06527 valid_loss: 0.08242 test_loss: 0.10180 \n",
      "[124/200] train_loss: 0.06296 valid_loss: 0.07824 test_loss: 0.09744 \n",
      "[125/200] train_loss: 0.06446 valid_loss: 0.07751 test_loss: 0.09477 \n",
      "[126/200] train_loss: 0.06258 valid_loss: 0.07562 test_loss: 0.10625 \n",
      "[127/200] train_loss: 0.06398 valid_loss: 0.07645 test_loss: 0.09276 \n",
      "[128/200] train_loss: 0.06252 valid_loss: 0.07698 test_loss: 0.09618 \n",
      "[129/200] train_loss: 0.06306 valid_loss: 0.07759 test_loss: 0.09115 \n",
      "[130/200] train_loss: 0.06179 valid_loss: 0.07779 test_loss: 0.09312 \n",
      "[131/200] train_loss: 0.06222 valid_loss: 0.07781 test_loss: 0.09123 \n",
      "[132/200] train_loss: 0.06269 valid_loss: 0.07952 test_loss: 0.09367 \n",
      "[133/200] train_loss: 0.06135 valid_loss: 0.07761 test_loss: 0.09068 \n",
      "[134/200] train_loss: 0.06269 valid_loss: 0.07766 test_loss: 0.12926 \n",
      "[135/200] train_loss: 0.06375 valid_loss: 0.07861 test_loss: 0.09643 \n",
      "[136/200] train_loss: 0.06309 valid_loss: 0.07685 test_loss: 0.12498 \n",
      "[137/200] train_loss: 0.06225 valid_loss: 0.07709 test_loss: 0.09205 \n",
      "[138/200] train_loss: 0.06225 valid_loss: 0.07643 test_loss: 0.09475 \n",
      "[139/200] train_loss: 0.06233 valid_loss: 0.07713 test_loss: 0.26587 \n",
      "[140/200] train_loss: 0.06263 valid_loss: 0.07531 test_loss: 0.08976 \n",
      "[141/200] train_loss: 0.06004 valid_loss: 0.07683 test_loss: 0.17011 \n",
      "[142/200] train_loss: 0.05957 valid_loss: 0.07781 test_loss: 0.09530 \n",
      "[143/200] train_loss: 0.06119 valid_loss: 0.07678 test_loss: 0.09474 \n",
      "[144/200] train_loss: 0.05974 valid_loss: 0.08028 test_loss: 0.09931 \n",
      "[145/200] train_loss: 0.06119 valid_loss: 0.07621 test_loss: 0.22586 \n",
      "[146/200] train_loss: 0.06152 valid_loss: 0.07581 test_loss: 0.09436 \n",
      "[147/200] train_loss: 0.06066 valid_loss: 0.07866 test_loss: 0.09647 \n",
      "[148/200] train_loss: 0.06040 valid_loss: 0.07662 test_loss: 0.09256 \n",
      "[149/200] train_loss: 0.06246 valid_loss: 0.07536 test_loss: 0.09522 \n",
      "[150/200] train_loss: 0.05923 valid_loss: 0.07549 test_loss: 0.09263 \n",
      "[151/200] train_loss: 0.06069 valid_loss: 0.07724 test_loss: 0.09238 \n",
      "[152/200] train_loss: 0.05968 valid_loss: 0.07707 test_loss: 0.09360 \n",
      "[153/200] train_loss: 0.06181 valid_loss: 0.08175 test_loss: 0.09621 \n",
      "[154/200] train_loss: 0.06010 valid_loss: 0.08107 test_loss: 0.10300 \n",
      "[155/200] train_loss: 0.05969 valid_loss: 0.07672 test_loss: 0.09570 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156/200] train_loss: 0.05976 valid_loss: 0.07734 test_loss: 0.09651 \n",
      "[157/200] train_loss: 0.05857 valid_loss: 0.08060 test_loss: 0.09331 \n",
      "[158/200] train_loss: 0.06023 valid_loss: 0.07749 test_loss: 0.09667 \n",
      "[159/200] train_loss: 0.06028 valid_loss: 0.08719 test_loss: 0.11084 \n",
      "[160/200] train_loss: 0.05871 valid_loss: 0.08439 test_loss: 0.09840 \n",
      "[161/200] train_loss: 0.05949 valid_loss: 0.07551 test_loss: 0.09425 \n",
      "[162/200] train_loss: 0.05978 valid_loss: 0.07835 test_loss: 0.09214 \n",
      "[163/200] train_loss: 0.05896 valid_loss: 0.07677 test_loss: 0.09528 \n",
      "[164/200] train_loss: 0.06042 valid_loss: 0.07696 test_loss: 0.09978 \n",
      "[165/200] train_loss: 0.05863 valid_loss: 0.08262 test_loss: 0.09831 \n",
      "[166/200] train_loss: 0.06128 valid_loss: 0.07633 test_loss: 0.09425 \n",
      "[167/200] train_loss: 0.05853 valid_loss: 0.07797 test_loss: 0.09541 \n",
      "[168/200] train_loss: 0.05824 valid_loss: 0.07711 test_loss: 0.10998 \n",
      "[169/200] train_loss: 0.05823 valid_loss: 0.07767 test_loss: 0.09562 \n",
      "[170/200] train_loss: 0.05765 valid_loss: 0.07574 test_loss: 0.09666 \n",
      "[171/200] train_loss: 0.05743 valid_loss: 0.07794 test_loss: 0.09690 \n",
      "[172/200] train_loss: 0.05818 valid_loss: 0.07504 test_loss: 0.09578 \n",
      "[173/200] train_loss: 0.05824 valid_loss: 0.07563 test_loss: 0.09223 \n",
      "[174/200] train_loss: 0.05970 valid_loss: 0.08524 test_loss: 0.09772 \n",
      "[175/200] train_loss: 0.05686 valid_loss: 0.09302 test_loss: 0.11161 \n",
      "[176/200] train_loss: 0.05791 valid_loss: 0.09087 test_loss: 0.12235 \n",
      "[177/200] train_loss: 0.05549 valid_loss: 0.07719 test_loss: 0.09652 \n",
      "[178/200] train_loss: 0.05675 valid_loss: 0.07688 test_loss: 0.09445 \n",
      "[179/200] train_loss: 0.05827 valid_loss: 0.07704 test_loss: 0.09494 \n",
      "[180/200] train_loss: 0.05834 valid_loss: 0.08076 test_loss: 0.10005 \n",
      "[181/200] train_loss: 0.05738 valid_loss: 0.07704 test_loss: 0.09447 \n",
      "[182/200] train_loss: 0.05545 valid_loss: 0.08071 test_loss: 0.09855 \n",
      "[183/200] train_loss: 0.05756 valid_loss: 0.07511 test_loss: 0.09573 \n",
      "[184/200] train_loss: 0.05717 valid_loss: 0.07532 test_loss: 0.09627 \n",
      "[185/200] train_loss: 0.05680 valid_loss: 0.07755 test_loss: 0.10160 \n",
      "[186/200] train_loss: 0.05675 valid_loss: 0.07415 test_loss: 0.09306 \n",
      "[187/200] train_loss: 0.05716 valid_loss: 0.07384 test_loss: 0.10937 \n",
      "[188/200] train_loss: 0.05613 valid_loss: 0.07500 test_loss: 0.09600 \n",
      "[189/200] train_loss: 0.05682 valid_loss: 0.07648 test_loss: 0.09371 \n",
      "[190/200] train_loss: 0.05702 valid_loss: 0.07749 test_loss: 0.09687 \n",
      "[191/200] train_loss: 0.05618 valid_loss: 0.07582 test_loss: 0.09164 \n",
      "[192/200] train_loss: 0.05718 valid_loss: 0.07737 test_loss: 0.09513 \n",
      "[193/200] train_loss: 0.05707 valid_loss: 0.07558 test_loss: 0.09207 \n",
      "[194/200] train_loss: 0.05530 valid_loss: 0.07687 test_loss: 0.09260 \n",
      "[195/200] train_loss: 0.05436 valid_loss: 0.07449 test_loss: 0.09137 \n",
      "[196/200] train_loss: 0.05735 valid_loss: 0.07516 test_loss: 0.09312 \n",
      "[197/200] train_loss: 0.05606 valid_loss: 0.07678 test_loss: 0.09639 \n",
      "[198/200] train_loss: 0.05625 valid_loss: 0.07515 test_loss: 0.09613 \n",
      "[199/200] train_loss: 0.05409 valid_loss: 0.07573 test_loss: 0.33903 \n",
      "[200/200] train_loss: 0.05538 valid_loss: 0.07563 test_loss: 0.12801 \n",
      "TRAINING MODEL 14\n",
      "[  1/200] train_loss: 0.34891 valid_loss: 0.25389 test_loss: 0.21648 \n",
      "验证损失减少 (inf --> 0.216483). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20183 valid_loss: 0.19117 test_loss: 0.14858 \n",
      "验证损失减少 (0.253887 --> 0.148583). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16275 valid_loss: 0.16622 test_loss: 0.13276 \n",
      "验证损失减少 (0.191168 --> 0.132764). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14774 valid_loss: 0.15526 test_loss: 0.12551 \n",
      "验证损失减少 (0.166224 --> 0.125510). 正在保存模型...\n",
      "[  5/200] train_loss: 0.14032 valid_loss: 0.14158 test_loss: 0.12087 \n",
      "验证损失减少 (0.155262 --> 0.120873). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13272 valid_loss: 0.13528 test_loss: 0.12053 \n",
      "验证损失减少 (0.141582 --> 0.120532). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12917 valid_loss: 0.12929 test_loss: 0.11548 \n",
      "验证损失减少 (0.135283 --> 0.115482). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12161 valid_loss: 0.13159 test_loss: 0.11460 \n",
      "验证损失减少 (0.129294 --> 0.114598). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11982 valid_loss: 0.12528 test_loss: 0.11300 \n",
      "验证损失减少 (0.131586 --> 0.113000). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11485 valid_loss: 0.12279 test_loss: 0.11256 \n",
      "验证损失减少 (0.125276 --> 0.112563). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11566 valid_loss: 0.11927 test_loss: 0.11155 \n",
      "验证损失减少 (0.122787 --> 0.111554). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.11360 valid_loss: 0.11674 test_loss: 0.11051 \n",
      "验证损失减少 (0.119272 --> 0.110513). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10897 valid_loss: 0.11625 test_loss: 0.10939 \n",
      "验证损失减少 (0.116738 --> 0.109394). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.11022 valid_loss: 0.11256 test_loss: 0.10665 \n",
      "验证损失减少 (0.116248 --> 0.106653). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10770 valid_loss: 0.11121 test_loss: 0.10525 \n",
      "验证损失减少 (0.112558 --> 0.105252). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10399 valid_loss: 0.11158 test_loss: 0.11893 \n",
      "[ 17/200] train_loss: 0.10403 valid_loss: 0.11226 test_loss: 0.11102 \n",
      "验证损失减少 (0.111213 --> 0.111020). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.10071 valid_loss: 0.10735 test_loss: 0.10710 \n",
      "验证损失减少 (0.112261 --> 0.107099). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.10239 valid_loss: 0.10826 test_loss: 0.10669 \n",
      "验证损失减少 (0.107353 --> 0.106691). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.10221 valid_loss: 0.10778 test_loss: 0.10389 \n",
      "验证损失减少 (0.108258 --> 0.103890). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09809 valid_loss: 0.10577 test_loss: 0.10650 \n",
      "验证损失减少 (0.107776 --> 0.106502). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09765 valid_loss: 0.10617 test_loss: 0.10292 \n",
      "验证损失减少 (0.105772 --> 0.102918). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09649 valid_loss: 0.10024 test_loss: 0.15582 \n",
      "[ 24/200] train_loss: 0.09838 valid_loss: 0.10326 test_loss: 0.10291 \n",
      "验证损失减少 (0.106167 --> 0.102906). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09652 valid_loss: 0.10212 test_loss: 0.10112 \n",
      "验证损失减少 (0.103261 --> 0.101118). 正在保存模型...\n",
      "[ 26/200] train_loss: 0.09426 valid_loss: 0.10110 test_loss: 0.10076 \n",
      "验证损失减少 (0.102124 --> 0.100755). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.09368 valid_loss: 0.10021 test_loss: 0.09892 \n",
      "验证损失减少 (0.101097 --> 0.098921). 正在保存模型...\n",
      "[ 28/200] train_loss: 0.09116 valid_loss: 0.09779 test_loss: 0.10335 \n",
      "[ 29/200] train_loss: 0.09071 valid_loss: 0.10067 test_loss: 0.10801 \n",
      "[ 30/200] train_loss: 0.09040 valid_loss: 0.09638 test_loss: 0.23341 \n",
      "[ 31/200] train_loss: 0.09187 valid_loss: 0.10042 test_loss: 0.10470 \n",
      "[ 32/200] train_loss: 0.08938 valid_loss: 0.09709 test_loss: 0.15144 \n",
      "[ 33/200] train_loss: 0.08960 valid_loss: 0.09848 test_loss: 0.10794 \n",
      "[ 34/200] train_loss: 0.08798 valid_loss: 0.09590 test_loss: 0.16084 \n",
      "[ 35/200] train_loss: 0.09085 valid_loss: 0.09542 test_loss: 0.10054 \n",
      "[ 36/200] train_loss: 0.08782 valid_loss: 0.09461 test_loss: 0.09571 \n",
      "验证损失减少 (0.100206 --> 0.095710). 正在保存模型...\n",
      "[ 37/200] train_loss: 0.08868 valid_loss: 0.09375 test_loss: 0.11236 \n",
      "[ 38/200] train_loss: 0.08637 valid_loss: 0.09365 test_loss: 0.24714 \n",
      "[ 39/200] train_loss: 0.08503 valid_loss: 0.09382 test_loss: 0.09426 \n",
      "验证损失减少 (0.094612 --> 0.094263). 正在保存模型...\n",
      "[ 40/200] train_loss: 0.08413 valid_loss: 0.09312 test_loss: 0.09408 \n",
      "[ 41/200] train_loss: 0.08457 valid_loss: 0.09332 test_loss: 0.09723 \n",
      "[ 42/200] train_loss: 0.08522 valid_loss: 0.09320 test_loss: 0.09627 \n",
      "[ 43/200] train_loss: 0.08523 valid_loss: 0.09394 test_loss: 0.10007 \n",
      "[ 44/200] train_loss: 0.08540 valid_loss: 0.09046 test_loss: 0.09940 \n",
      "[ 45/200] train_loss: 0.08456 valid_loss: 0.09248 test_loss: 0.09934 \n",
      "[ 46/200] train_loss: 0.08215 valid_loss: 0.09102 test_loss: 0.09499 \n",
      "[ 47/200] train_loss: 0.08197 valid_loss: 0.09114 test_loss: 0.09877 \n",
      "[ 48/200] train_loss: 0.08135 valid_loss: 0.09080 test_loss: 0.12010 \n",
      "[ 49/200] train_loss: 0.08100 valid_loss: 0.08963 test_loss: 0.09622 \n",
      "[ 50/200] train_loss: 0.08148 valid_loss: 0.08824 test_loss: 0.09697 \n",
      "[ 51/200] train_loss: 0.07799 valid_loss: 0.08680 test_loss: 0.13651 \n",
      "[ 52/200] train_loss: 0.08240 valid_loss: 0.08699 test_loss: 0.10524 \n",
      "[ 53/200] train_loss: 0.07955 valid_loss: 0.08755 test_loss: 0.12023 \n",
      "[ 54/200] train_loss: 0.08107 valid_loss: 0.08897 test_loss: 0.10055 \n",
      "[ 55/200] train_loss: 0.07912 valid_loss: 0.09070 test_loss: 0.09790 \n",
      "[ 56/200] train_loss: 0.07941 valid_loss: 0.08664 test_loss: 0.09476 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 57/200] train_loss: 0.07972 valid_loss: 0.08802 test_loss: 0.09752 \n",
      "[ 58/200] train_loss: 0.08008 valid_loss: 0.08812 test_loss: 0.17502 \n",
      "[ 59/200] train_loss: 0.07937 valid_loss: 0.08772 test_loss: 0.11997 \n",
      "[ 60/200] train_loss: 0.07606 valid_loss: 0.08718 test_loss: 0.09494 \n",
      "[ 61/200] train_loss: 0.07711 valid_loss: 0.08500 test_loss: 0.10289 \n",
      "[ 62/200] train_loss: 0.07663 valid_loss: 0.08629 test_loss: 0.09626 \n",
      "[ 63/200] train_loss: 0.07694 valid_loss: 0.08504 test_loss: 0.09246 \n",
      "验证损失减少 (0.093821 --> 0.092458). 正在保存模型...\n",
      "[ 64/200] train_loss: 0.07624 valid_loss: 0.08524 test_loss: 0.09265 \n",
      "[ 65/200] train_loss: 0.07771 valid_loss: 0.08450 test_loss: 0.20726 \n",
      "[ 66/200] train_loss: 0.07405 valid_loss: 0.08278 test_loss: 0.13834 \n",
      "[ 67/200] train_loss: 0.07535 valid_loss: 0.08703 test_loss: 0.44289 \n",
      "[ 68/200] train_loss: 0.07584 valid_loss: 0.08402 test_loss: 0.20051 \n",
      "[ 69/200] train_loss: 0.07456 valid_loss: 0.08513 test_loss: 0.09834 \n",
      "[ 70/200] train_loss: 0.07569 valid_loss: 0.08335 test_loss: 0.09333 \n",
      "[ 71/200] train_loss: 0.07540 valid_loss: 0.08308 test_loss: 0.09337 \n",
      "[ 72/200] train_loss: 0.07527 valid_loss: 0.08534 test_loss: 0.09363 \n",
      "[ 73/200] train_loss: 0.07281 valid_loss: 0.08530 test_loss: 0.09275 \n",
      "[ 74/200] train_loss: 0.07396 valid_loss: 0.08683 test_loss: 0.09763 \n",
      "[ 75/200] train_loss: 0.07429 valid_loss: 0.08577 test_loss: 0.09383 \n",
      "[ 76/200] train_loss: 0.07497 valid_loss: 0.08419 test_loss: 0.09215 \n",
      "[ 77/200] train_loss: 0.07248 valid_loss: 0.08395 test_loss: 0.09520 \n",
      "[ 78/200] train_loss: 0.07223 valid_loss: 0.08694 test_loss: 0.09764 \n",
      "[ 79/200] train_loss: 0.07090 valid_loss: 0.08415 test_loss: 0.10208 \n",
      "[ 80/200] train_loss: 0.07358 valid_loss: 0.08266 test_loss: 0.09441 \n",
      "[ 81/200] train_loss: 0.07351 valid_loss: 0.08602 test_loss: 0.10154 \n",
      "[ 82/200] train_loss: 0.07178 valid_loss: 0.08443 test_loss: 0.11684 \n",
      "[ 83/200] train_loss: 0.07416 valid_loss: 0.08213 test_loss: 0.10451 \n",
      "[ 84/200] train_loss: 0.07280 valid_loss: 0.08184 test_loss: 0.09565 \n",
      "[ 85/200] train_loss: 0.07039 valid_loss: 0.08262 test_loss: 0.09257 \n",
      "[ 86/200] train_loss: 0.07044 valid_loss: 0.08231 test_loss: 0.09309 \n",
      "[ 87/200] train_loss: 0.07030 valid_loss: 0.08238 test_loss: 0.09287 \n",
      "[ 88/200] train_loss: 0.06909 valid_loss: 0.08200 test_loss: 0.11078 \n",
      "[ 89/200] train_loss: 0.06931 valid_loss: 0.08201 test_loss: 0.15996 \n",
      "[ 90/200] train_loss: 0.07077 valid_loss: 0.08484 test_loss: 0.09533 \n",
      "[ 91/200] train_loss: 0.07021 valid_loss: 0.08459 test_loss: 0.09476 \n",
      "[ 92/200] train_loss: 0.07192 valid_loss: 0.08316 test_loss: 0.09371 \n",
      "[ 93/200] train_loss: 0.06897 valid_loss: 0.08255 test_loss: 0.09122 \n",
      "[ 94/200] train_loss: 0.06920 valid_loss: 0.08104 test_loss: 0.11586 \n",
      "[ 95/200] train_loss: 0.06911 valid_loss: 0.08068 test_loss: 0.09465 \n",
      "[ 96/200] train_loss: 0.06908 valid_loss: 0.08034 test_loss: 0.18197 \n",
      "[ 97/200] train_loss: 0.06748 valid_loss: 0.07918 test_loss: 0.23347 \n",
      "[ 98/200] train_loss: 0.06879 valid_loss: 0.08083 test_loss: 0.09371 \n",
      "[ 99/200] train_loss: 0.06785 valid_loss: 0.07972 test_loss: 0.16263 \n",
      "[100/200] train_loss: 0.06703 valid_loss: 0.08019 test_loss: 0.09335 \n",
      "[101/200] train_loss: 0.06877 valid_loss: 0.08039 test_loss: 0.09902 \n",
      "[102/200] train_loss: 0.06760 valid_loss: 0.08045 test_loss: 0.33278 \n",
      "[103/200] train_loss: 0.06783 valid_loss: 0.08227 test_loss: 0.09689 \n",
      "[104/200] train_loss: 0.06951 valid_loss: 0.08144 test_loss: 0.09314 \n",
      "[105/200] train_loss: 0.06612 valid_loss: 0.07879 test_loss: 0.09807 \n",
      "[106/200] train_loss: 0.06790 valid_loss: 0.07841 test_loss: 0.21569 \n",
      "[107/200] train_loss: 0.06784 valid_loss: 0.08009 test_loss: 0.49277 \n",
      "[108/200] train_loss: 0.06632 valid_loss: 0.07921 test_loss: 0.35602 \n",
      "[109/200] train_loss: 0.06718 valid_loss: 0.07924 test_loss: 0.50377 \n",
      "[110/200] train_loss: 0.06547 valid_loss: 0.07841 test_loss: 0.52818 \n",
      "[111/200] train_loss: 0.06672 valid_loss: 0.08084 test_loss: 0.42463 \n",
      "[112/200] train_loss: 0.06630 valid_loss: 0.08036 test_loss: 0.64855 \n",
      "[113/200] train_loss: 0.06645 valid_loss: 0.07883 test_loss: 0.50966 \n",
      "[114/200] train_loss: 0.06534 valid_loss: 0.07928 test_loss: 0.31758 \n",
      "[115/200] train_loss: 0.06403 valid_loss: 0.07889 test_loss: 0.41445 \n",
      "[116/200] train_loss: 0.06622 valid_loss: 0.07989 test_loss: 0.10214 \n",
      "[117/200] train_loss: 0.06675 valid_loss: 0.07884 test_loss: 0.26025 \n",
      "[118/200] train_loss: 0.06653 valid_loss: 0.07870 test_loss: 0.56460 \n",
      "[119/200] train_loss: 0.06520 valid_loss: 0.07877 test_loss: 0.26243 \n",
      "[120/200] train_loss: 0.06386 valid_loss: 0.08047 test_loss: 0.16522 \n",
      "[121/200] train_loss: 0.06464 valid_loss: 0.07848 test_loss: 0.09270 \n",
      "[122/200] train_loss: 0.06462 valid_loss: 0.07737 test_loss: 0.10719 \n",
      "[123/200] train_loss: 0.06424 valid_loss: 0.07948 test_loss: 0.09321 \n",
      "[124/200] train_loss: 0.06433 valid_loss: 0.07949 test_loss: 0.10199 \n",
      "[125/200] train_loss: 0.06434 valid_loss: 0.07945 test_loss: 0.09659 \n",
      "[126/200] train_loss: 0.06502 valid_loss: 0.08214 test_loss: 0.09581 \n",
      "[127/200] train_loss: 0.06416 valid_loss: 0.08081 test_loss: 0.09884 \n",
      "[128/200] train_loss: 0.06306 valid_loss: 0.07785 test_loss: 0.09375 \n",
      "[129/200] train_loss: 0.06510 valid_loss: 0.07945 test_loss: 0.09417 \n",
      "[130/200] train_loss: 0.06445 valid_loss: 0.08169 test_loss: 0.10200 \n",
      "[131/200] train_loss: 0.06484 valid_loss: 0.07940 test_loss: 0.09484 \n",
      "[132/200] train_loss: 0.06255 valid_loss: 0.07968 test_loss: 0.09706 \n",
      "[133/200] train_loss: 0.06361 valid_loss: 0.07825 test_loss: 0.09668 \n",
      "[134/200] train_loss: 0.06362 valid_loss: 0.07720 test_loss: 0.09208 \n",
      "[135/200] train_loss: 0.06210 valid_loss: 0.07611 test_loss: 0.08747 \n",
      "[136/200] train_loss: 0.06266 valid_loss: 0.07864 test_loss: 0.09815 \n",
      "[137/200] train_loss: 0.06439 valid_loss: 0.07926 test_loss: 0.25346 \n",
      "[138/200] train_loss: 0.06222 valid_loss: 0.07904 test_loss: 0.09268 \n",
      "[139/200] train_loss: 0.06213 valid_loss: 0.08085 test_loss: 0.09280 \n",
      "[140/200] train_loss: 0.06238 valid_loss: 0.07885 test_loss: 0.32296 \n",
      "[141/200] train_loss: 0.06191 valid_loss: 0.08056 test_loss: 0.10228 \n",
      "[142/200] train_loss: 0.06288 valid_loss: 0.07884 test_loss: 0.09951 \n",
      "[143/200] train_loss: 0.06168 valid_loss: 0.07754 test_loss: 0.09467 \n",
      "[144/200] train_loss: 0.06107 valid_loss: 0.07862 test_loss: 0.09437 \n",
      "[145/200] train_loss: 0.06146 valid_loss: 0.07684 test_loss: 0.14913 \n",
      "[146/200] train_loss: 0.06111 valid_loss: 0.07785 test_loss: 0.09695 \n",
      "[147/200] train_loss: 0.06196 valid_loss: 0.07744 test_loss: 0.13015 \n",
      "[148/200] train_loss: 0.05995 valid_loss: 0.08049 test_loss: 0.09616 \n",
      "[149/200] train_loss: 0.06291 valid_loss: 0.07963 test_loss: 0.38681 \n",
      "[150/200] train_loss: 0.06036 valid_loss: 0.07838 test_loss: 0.09226 \n",
      "[151/200] train_loss: 0.06238 valid_loss: 0.07835 test_loss: 0.09430 \n",
      "[152/200] train_loss: 0.06033 valid_loss: 0.07822 test_loss: 0.09189 \n",
      "[153/200] train_loss: 0.06235 valid_loss: 0.07884 test_loss: 0.09433 \n",
      "[154/200] train_loss: 0.05926 valid_loss: 0.07928 test_loss: 0.09365 \n",
      "[155/200] train_loss: 0.06005 valid_loss: 0.07981 test_loss: 0.10300 \n",
      "[156/200] train_loss: 0.06126 valid_loss: 0.07770 test_loss: 0.09465 \n",
      "[157/200] train_loss: 0.05947 valid_loss: 0.07583 test_loss: 0.09806 \n",
      "[158/200] train_loss: 0.06155 valid_loss: 0.07772 test_loss: 0.09543 \n",
      "[159/200] train_loss: 0.05662 valid_loss: 0.07756 test_loss: 0.09826 \n",
      "[160/200] train_loss: 0.05953 valid_loss: 0.07761 test_loss: 0.16300 \n",
      "[161/200] train_loss: 0.06076 valid_loss: 0.07774 test_loss: 0.09522 \n",
      "[162/200] train_loss: 0.05984 valid_loss: 0.07835 test_loss: 0.09622 \n",
      "[163/200] train_loss: 0.05982 valid_loss: 0.07988 test_loss: 0.09993 \n",
      "[164/200] train_loss: 0.05945 valid_loss: 0.07809 test_loss: 0.09916 \n",
      "[165/200] train_loss: 0.06050 valid_loss: 0.08051 test_loss: 0.09827 \n",
      "[166/200] train_loss: 0.05805 valid_loss: 0.07759 test_loss: 0.20070 \n",
      "[167/200] train_loss: 0.05703 valid_loss: 0.07785 test_loss: 0.09667 \n",
      "[168/200] train_loss: 0.05814 valid_loss: 0.08043 test_loss: 0.09946 \n",
      "[169/200] train_loss: 0.05972 valid_loss: 0.07869 test_loss: 0.09510 \n",
      "[170/200] train_loss: 0.05865 valid_loss: 0.07912 test_loss: 0.09251 \n",
      "[171/200] train_loss: 0.05963 valid_loss: 0.07711 test_loss: 0.13596 \n",
      "[172/200] train_loss: 0.05856 valid_loss: 0.07714 test_loss: 0.10344 \n",
      "[173/200] train_loss: 0.05831 valid_loss: 0.07779 test_loss: 0.09485 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[174/200] train_loss: 0.05890 valid_loss: 0.07712 test_loss: 0.24526 \n",
      "[175/200] train_loss: 0.05966 valid_loss: 0.07773 test_loss: 0.10030 \n",
      "[176/200] train_loss: 0.05840 valid_loss: 0.07750 test_loss: 0.09881 \n",
      "[177/200] train_loss: 0.05807 valid_loss: 0.07677 test_loss: 0.09144 \n",
      "[178/200] train_loss: 0.05859 valid_loss: 0.07810 test_loss: 0.09126 \n",
      "[179/200] train_loss: 0.05755 valid_loss: 0.08113 test_loss: 0.09305 \n",
      "[180/200] train_loss: 0.05820 valid_loss: 0.08255 test_loss: 0.09896 \n",
      "[181/200] train_loss: 0.05836 valid_loss: 0.08038 test_loss: 0.09453 \n",
      "[182/200] train_loss: 0.05884 valid_loss: 0.07998 test_loss: 0.09558 \n",
      "[183/200] train_loss: 0.05839 valid_loss: 0.07872 test_loss: 0.09574 \n",
      "[184/200] train_loss: 0.05782 valid_loss: 0.07837 test_loss: 0.09795 \n",
      "[185/200] train_loss: 0.05618 valid_loss: 0.07921 test_loss: 0.09582 \n",
      "[186/200] train_loss: 0.05784 valid_loss: 0.07714 test_loss: 0.09546 \n",
      "[187/200] train_loss: 0.05863 valid_loss: 0.08556 test_loss: 0.10659 \n",
      "[188/200] train_loss: 0.05716 valid_loss: 0.08567 test_loss: 0.10184 \n",
      "[189/200] train_loss: 0.05754 valid_loss: 0.07777 test_loss: 0.09323 \n",
      "[190/200] train_loss: 0.05686 valid_loss: 0.07998 test_loss: 0.10502 \n",
      "[191/200] train_loss: 0.05826 valid_loss: 0.08181 test_loss: 0.10400 \n",
      "[192/200] train_loss: 0.05655 valid_loss: 0.08159 test_loss: 0.10104 \n",
      "[193/200] train_loss: 0.05571 valid_loss: 0.08208 test_loss: 0.09880 \n",
      "[194/200] train_loss: 0.05662 valid_loss: 0.07801 test_loss: 0.09420 \n",
      "[195/200] train_loss: 0.05526 valid_loss: 0.07662 test_loss: 0.09347 \n",
      "[196/200] train_loss: 0.05532 valid_loss: 0.07628 test_loss: 0.09246 \n",
      "[197/200] train_loss: 0.05633 valid_loss: 0.07723 test_loss: 0.09248 \n",
      "[198/200] train_loss: 0.05578 valid_loss: 0.07625 test_loss: 0.09285 \n",
      "[199/200] train_loss: 0.05520 valid_loss: 0.07853 test_loss: 0.09313 \n",
      "[200/200] train_loss: 0.05638 valid_loss: 0.07660 test_loss: 0.09199 \n",
      "TRAINING MODEL 15\n",
      "[  1/200] train_loss: 0.40164 valid_loss: 0.28069 test_loss: 0.24640 \n",
      "验证损失减少 (inf --> 0.246402). 正在保存模型...\n",
      "[  2/200] train_loss: 0.21739 valid_loss: 0.19973 test_loss: 0.15685 \n",
      "验证损失减少 (0.280688 --> 0.156846). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17126 valid_loss: 0.16611 test_loss: 0.13927 \n",
      "验证损失减少 (0.199726 --> 0.139270). 正在保存模型...\n",
      "[  4/200] train_loss: 0.15208 valid_loss: 0.14930 test_loss: 0.12117 \n",
      "验证损失减少 (0.166105 --> 0.121170). 正在保存模型...\n",
      "[  5/200] train_loss: 0.14146 valid_loss: 0.14257 test_loss: 0.12043 \n",
      "验证损失减少 (0.149301 --> 0.120429). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13292 valid_loss: 0.13553 test_loss: 0.12007 \n",
      "验证损失减少 (0.142572 --> 0.120072). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12420 valid_loss: 0.13486 test_loss: 0.11767 \n",
      "验证损失减少 (0.135534 --> 0.117672). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12292 valid_loss: 0.13042 test_loss: 0.11830 \n",
      "验证损失减少 (0.134858 --> 0.118304). 正在保存模型...\n",
      "[  9/200] train_loss: 0.12136 valid_loss: 0.12784 test_loss: 0.11591 \n",
      "验证损失减少 (0.130421 --> 0.115909). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11893 valid_loss: 0.12538 test_loss: 0.11186 \n",
      "验证损失减少 (0.127836 --> 0.111861). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11073 valid_loss: 0.12137 test_loss: 0.11169 \n",
      "验证损失减少 (0.125384 --> 0.111693). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.11273 valid_loss: 0.11886 test_loss: 0.11295 \n",
      "验证损失减少 (0.121372 --> 0.112953). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.11078 valid_loss: 0.11753 test_loss: 0.10804 \n",
      "验证损失减少 (0.118863 --> 0.108039). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.11134 valid_loss: 0.11714 test_loss: 0.11197 \n",
      "验证损失减少 (0.117533 --> 0.111972). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10680 valid_loss: 0.11566 test_loss: 0.11230 \n",
      "验证损失减少 (0.117136 --> 0.112298). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10773 valid_loss: 0.10991 test_loss: 0.12565 \n",
      "[ 17/200] train_loss: 0.10480 valid_loss: 0.11114 test_loss: 0.10797 \n",
      "验证损失减少 (0.115659 --> 0.107971). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.10569 valid_loss: 0.10776 test_loss: 0.10795 \n",
      "验证损失减少 (0.111136 --> 0.107946). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.10458 valid_loss: 0.10645 test_loss: 0.18284 \n",
      "[ 20/200] train_loss: 0.09958 valid_loss: 0.10342 test_loss: 0.10416 \n",
      "验证损失减少 (0.107755 --> 0.104156). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.10064 valid_loss: 0.10319 test_loss: 0.11168 \n",
      "[ 22/200] train_loss: 0.09807 valid_loss: 0.10663 test_loss: 0.10560 \n",
      "[ 23/200] train_loss: 0.09799 valid_loss: 0.10156 test_loss: 0.11133 \n",
      "[ 24/200] train_loss: 0.09636 valid_loss: 0.10366 test_loss: 0.10399 \n",
      "[ 25/200] train_loss: 0.09646 valid_loss: 0.10184 test_loss: 0.11122 \n",
      "[ 26/200] train_loss: 0.09606 valid_loss: 0.10254 test_loss: 0.10033 \n",
      "验证损失减少 (0.103423 --> 0.100330). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.09466 valid_loss: 0.09888 test_loss: 0.11196 \n",
      "[ 28/200] train_loss: 0.09379 valid_loss: 0.09730 test_loss: 0.25185 \n",
      "[ 29/200] train_loss: 0.09222 valid_loss: 0.10038 test_loss: 0.12186 \n",
      "[ 30/200] train_loss: 0.09203 valid_loss: 0.09993 test_loss: 0.12733 \n",
      "[ 31/200] train_loss: 0.09323 valid_loss: 0.09586 test_loss: 0.16154 \n",
      "[ 32/200] train_loss: 0.08996 valid_loss: 0.09743 test_loss: 0.14749 \n",
      "[ 33/200] train_loss: 0.09064 valid_loss: 0.09519 test_loss: 0.28249 \n",
      "[ 34/200] train_loss: 0.08985 valid_loss: 0.09455 test_loss: 0.10266 \n",
      "[ 35/200] train_loss: 0.08957 valid_loss: 0.09311 test_loss: 0.10299 \n",
      "[ 36/200] train_loss: 0.08870 valid_loss: 0.09398 test_loss: 0.10301 \n",
      "[ 37/200] train_loss: 0.08811 valid_loss: 0.09200 test_loss: 0.18418 \n",
      "[ 38/200] train_loss: 0.08884 valid_loss: 0.09308 test_loss: 0.10625 \n",
      "[ 39/200] train_loss: 0.08344 valid_loss: 0.09115 test_loss: 0.15203 \n",
      "[ 40/200] train_loss: 0.08460 valid_loss: 0.09095 test_loss: 0.11645 \n",
      "[ 41/200] train_loss: 0.08533 valid_loss: 0.09344 test_loss: 0.09577 \n",
      "验证损失减少 (0.102544 --> 0.095767). 正在保存模型...\n",
      "[ 42/200] train_loss: 0.08499 valid_loss: 0.09246 test_loss: 0.09527 \n",
      "[ 43/200] train_loss: 0.08360 valid_loss: 0.09312 test_loss: 0.09896 \n",
      "[ 44/200] train_loss: 0.08418 valid_loss: 0.09397 test_loss: 0.09604 \n",
      "[ 45/200] train_loss: 0.08315 valid_loss: 0.09000 test_loss: 0.11878 \n",
      "[ 46/200] train_loss: 0.08055 valid_loss: 0.09030 test_loss: 0.10033 \n",
      "[ 47/200] train_loss: 0.08185 valid_loss: 0.08938 test_loss: 0.10534 \n",
      "[ 48/200] train_loss: 0.08175 valid_loss: 0.08937 test_loss: 0.10124 \n",
      "[ 49/200] train_loss: 0.08403 valid_loss: 0.09024 test_loss: 0.09753 \n",
      "[ 50/200] train_loss: 0.07932 valid_loss: 0.08963 test_loss: 0.12246 \n",
      "[ 51/200] train_loss: 0.08075 valid_loss: 0.08911 test_loss: 0.09664 \n",
      "[ 52/200] train_loss: 0.07926 valid_loss: 0.08770 test_loss: 0.09477 \n",
      "[ 53/200] train_loss: 0.08094 valid_loss: 0.08622 test_loss: 0.09902 \n",
      "[ 54/200] train_loss: 0.07939 valid_loss: 0.08862 test_loss: 0.09521 \n",
      "[ 55/200] train_loss: 0.07814 valid_loss: 0.08865 test_loss: 0.20745 \n",
      "[ 56/200] train_loss: 0.07987 valid_loss: 0.08995 test_loss: 0.12555 \n",
      "[ 57/200] train_loss: 0.07739 valid_loss: 0.08468 test_loss: 0.12753 \n",
      "[ 58/200] train_loss: 0.07791 valid_loss: 0.08733 test_loss: 0.34255 \n",
      "[ 59/200] train_loss: 0.07703 valid_loss: 0.08662 test_loss: 0.12241 \n",
      "[ 60/200] train_loss: 0.07683 valid_loss: 0.08647 test_loss: 0.12768 \n",
      "[ 61/200] train_loss: 0.07841 valid_loss: 0.08825 test_loss: 0.09993 \n",
      "[ 62/200] train_loss: 0.07692 valid_loss: 0.08844 test_loss: 0.09917 \n",
      "[ 63/200] train_loss: 0.07836 valid_loss: 0.08619 test_loss: 0.09139 \n",
      "验证损失减少 (0.093442 --> 0.091395). 正在保存模型...\n",
      "[ 64/200] train_loss: 0.07569 valid_loss: 0.08535 test_loss: 0.09212 \n",
      "[ 65/200] train_loss: 0.07495 valid_loss: 0.08629 test_loss: 0.09462 \n",
      "[ 66/200] train_loss: 0.07486 valid_loss: 0.08398 test_loss: 0.13380 \n",
      "[ 67/200] train_loss: 0.07704 valid_loss: 0.08538 test_loss: 0.09214 \n",
      "[ 68/200] train_loss: 0.07500 valid_loss: 0.08492 test_loss: 0.09735 \n",
      "[ 69/200] train_loss: 0.07559 valid_loss: 0.08633 test_loss: 0.09493 \n",
      "[ 70/200] train_loss: 0.07428 valid_loss: 0.08630 test_loss: 0.09525 \n",
      "[ 71/200] train_loss: 0.07449 valid_loss: 0.08472 test_loss: 0.10087 \n",
      "[ 72/200] train_loss: 0.07375 valid_loss: 0.08464 test_loss: 0.09711 \n",
      "[ 73/200] train_loss: 0.07423 valid_loss: 0.08387 test_loss: 0.10153 \n",
      "[ 74/200] train_loss: 0.07423 valid_loss: 0.08435 test_loss: 0.09952 \n",
      "[ 75/200] train_loss: 0.07299 valid_loss: 0.08282 test_loss: 0.09301 \n",
      "[ 76/200] train_loss: 0.07202 valid_loss: 0.08620 test_loss: 0.09374 \n",
      "[ 77/200] train_loss: 0.07158 valid_loss: 0.08245 test_loss: 0.09786 \n",
      "[ 78/200] train_loss: 0.07361 valid_loss: 0.08269 test_loss: 0.09356 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79/200] train_loss: 0.07081 valid_loss: 0.08128 test_loss: 0.09645 \n",
      "[ 80/200] train_loss: 0.07200 valid_loss: 0.08292 test_loss: 0.09126 \n",
      "[ 81/200] train_loss: 0.07228 valid_loss: 0.08369 test_loss: 0.17789 \n",
      "[ 82/200] train_loss: 0.07232 valid_loss: 0.08172 test_loss: 0.08992 \n",
      "[ 83/200] train_loss: 0.06874 valid_loss: 0.08445 test_loss: 0.09486 \n",
      "[ 84/200] train_loss: 0.07181 valid_loss: 0.08191 test_loss: 0.09573 \n",
      "[ 85/200] train_loss: 0.07017 valid_loss: 0.08303 test_loss: 0.09543 \n",
      "[ 86/200] train_loss: 0.07163 valid_loss: 0.08099 test_loss: 0.09140 \n",
      "[ 87/200] train_loss: 0.07065 valid_loss: 0.08199 test_loss: 0.09658 \n",
      "[ 88/200] train_loss: 0.07158 valid_loss: 0.08147 test_loss: 0.11485 \n",
      "[ 89/200] train_loss: 0.06990 valid_loss: 0.08346 test_loss: 0.09263 \n",
      "[ 90/200] train_loss: 0.07063 valid_loss: 0.08317 test_loss: 0.13522 \n",
      "[ 91/200] train_loss: 0.06751 valid_loss: 0.08027 test_loss: 0.09766 \n",
      "[ 92/200] train_loss: 0.06846 valid_loss: 0.07977 test_loss: 0.10158 \n",
      "[ 93/200] train_loss: 0.06815 valid_loss: 0.08032 test_loss: 0.44910 \n",
      "[ 94/200] train_loss: 0.07042 valid_loss: 0.08122 test_loss: 0.09498 \n",
      "[ 95/200] train_loss: 0.06880 valid_loss: 0.08191 test_loss: 0.09497 \n",
      "[ 96/200] train_loss: 0.06814 valid_loss: 0.08251 test_loss: 0.09394 \n",
      "[ 97/200] train_loss: 0.06923 valid_loss: 0.08129 test_loss: 0.09112 \n",
      "[ 98/200] train_loss: 0.06904 valid_loss: 0.07864 test_loss: 0.10611 \n",
      "[ 99/200] train_loss: 0.06789 valid_loss: 0.07894 test_loss: 0.09233 \n",
      "[100/200] train_loss: 0.06852 valid_loss: 0.07908 test_loss: 0.09217 \n",
      "[101/200] train_loss: 0.06951 valid_loss: 0.08266 test_loss: 0.09499 \n",
      "[102/200] train_loss: 0.06756 valid_loss: 0.08012 test_loss: 0.49167 \n",
      "[103/200] train_loss: 0.07082 valid_loss: 0.07910 test_loss: 0.23202 \n",
      "[104/200] train_loss: 0.06827 valid_loss: 0.08075 test_loss: 0.12997 \n",
      "[105/200] train_loss: 0.06635 valid_loss: 0.07982 test_loss: 0.10066 \n",
      "[106/200] train_loss: 0.06671 valid_loss: 0.07889 test_loss: 0.25555 \n",
      "[107/200] train_loss: 0.06760 valid_loss: 0.08093 test_loss: 0.36567 \n",
      "[108/200] train_loss: 0.06726 valid_loss: 0.08108 test_loss: 0.10549 \n",
      "[109/200] train_loss: 0.06428 valid_loss: 0.07946 test_loss: 0.18082 \n",
      "[110/200] train_loss: 0.06660 valid_loss: 0.07919 test_loss: 0.14344 \n",
      "[111/200] train_loss: 0.06576 valid_loss: 0.07890 test_loss: 0.11042 \n",
      "[112/200] train_loss: 0.06629 valid_loss: 0.07914 test_loss: 0.12464 \n",
      "[113/200] train_loss: 0.06601 valid_loss: 0.08006 test_loss: 0.09867 \n",
      "[114/200] train_loss: 0.06450 valid_loss: 0.08107 test_loss: 0.24224 \n",
      "[115/200] train_loss: 0.06462 valid_loss: 0.07856 test_loss: 0.10564 \n",
      "[116/200] train_loss: 0.06650 valid_loss: 0.07961 test_loss: 0.09639 \n",
      "[117/200] train_loss: 0.06556 valid_loss: 0.07980 test_loss: 0.09668 \n",
      "[118/200] train_loss: 0.06518 valid_loss: 0.07990 test_loss: 0.11820 \n",
      "[119/200] train_loss: 0.06601 valid_loss: 0.08145 test_loss: 0.09668 \n",
      "[120/200] train_loss: 0.06440 valid_loss: 0.07974 test_loss: 0.34483 \n",
      "[121/200] train_loss: 0.06559 valid_loss: 0.07863 test_loss: 0.40952 \n",
      "[122/200] train_loss: 0.06529 valid_loss: 0.07802 test_loss: 0.11053 \n",
      "[123/200] train_loss: 0.06376 valid_loss: 0.07715 test_loss: 0.14279 \n",
      "[124/200] train_loss: 0.06217 valid_loss: 0.07831 test_loss: 0.12116 \n",
      "[125/200] train_loss: 0.06456 valid_loss: 0.07765 test_loss: 0.11137 \n",
      "[126/200] train_loss: 0.06454 valid_loss: 0.07838 test_loss: 0.23212 \n",
      "[127/200] train_loss: 0.06372 valid_loss: 0.07949 test_loss: 0.10603 \n",
      "[128/200] train_loss: 0.06459 valid_loss: 0.07830 test_loss: 0.11579 \n",
      "[129/200] train_loss: 0.06337 valid_loss: 0.07794 test_loss: 0.10094 \n",
      "[130/200] train_loss: 0.06152 valid_loss: 0.07682 test_loss: 0.11513 \n",
      "[131/200] train_loss: 0.06299 valid_loss: 0.07788 test_loss: 0.10730 \n",
      "[132/200] train_loss: 0.06441 valid_loss: 0.07805 test_loss: 0.17319 \n",
      "[133/200] train_loss: 0.06315 valid_loss: 0.07910 test_loss: 0.11895 \n",
      "[134/200] train_loss: 0.06247 valid_loss: 0.07905 test_loss: 0.10498 \n",
      "[135/200] train_loss: 0.06276 valid_loss: 0.07838 test_loss: 0.34021 \n",
      "[136/200] train_loss: 0.06328 valid_loss: 0.07815 test_loss: 0.09441 \n",
      "[137/200] train_loss: 0.06224 valid_loss: 0.07901 test_loss: 0.11530 \n",
      "[138/200] train_loss: 0.06312 valid_loss: 0.07859 test_loss: 0.11614 \n",
      "[139/200] train_loss: 0.06286 valid_loss: 0.07895 test_loss: 0.12577 \n",
      "[140/200] train_loss: 0.06343 valid_loss: 0.07797 test_loss: 0.45881 \n",
      "[141/200] train_loss: 0.05971 valid_loss: 0.07838 test_loss: 0.20983 \n",
      "[142/200] train_loss: 0.06343 valid_loss: 0.07924 test_loss: 0.09707 \n",
      "[143/200] train_loss: 0.06274 valid_loss: 0.07989 test_loss: 0.10149 \n",
      "[144/200] train_loss: 0.06061 valid_loss: 0.07923 test_loss: 0.10059 \n",
      "[145/200] train_loss: 0.06104 valid_loss: 0.07847 test_loss: 0.13491 \n",
      "[146/200] train_loss: 0.06150 valid_loss: 0.07728 test_loss: 0.09736 \n",
      "[147/200] train_loss: 0.06001 valid_loss: 0.07687 test_loss: 0.09235 \n",
      "[148/200] train_loss: 0.06333 valid_loss: 0.07731 test_loss: 0.10296 \n",
      "[149/200] train_loss: 0.06265 valid_loss: 0.07712 test_loss: 0.09807 \n",
      "[150/200] train_loss: 0.05906 valid_loss: 0.07822 test_loss: 0.09548 \n",
      "[151/200] train_loss: 0.06066 valid_loss: 0.08061 test_loss: 0.09250 \n",
      "[152/200] train_loss: 0.05971 valid_loss: 0.07740 test_loss: 0.09192 \n",
      "[153/200] train_loss: 0.06145 valid_loss: 0.07864 test_loss: 0.09493 \n",
      "[154/200] train_loss: 0.06198 valid_loss: 0.07888 test_loss: 0.09313 \n",
      "[155/200] train_loss: 0.06025 valid_loss: 0.07954 test_loss: 0.09738 \n",
      "[156/200] train_loss: 0.06196 valid_loss: 0.08027 test_loss: 0.09870 \n",
      "[157/200] train_loss: 0.06034 valid_loss: 0.07830 test_loss: 0.10779 \n",
      "[158/200] train_loss: 0.05961 valid_loss: 0.07786 test_loss: 0.09366 \n",
      "[159/200] train_loss: 0.05957 valid_loss: 0.07840 test_loss: 0.10292 \n",
      "[160/200] train_loss: 0.05986 valid_loss: 0.07666 test_loss: 0.09441 \n",
      "[161/200] train_loss: 0.06042 valid_loss: 0.07714 test_loss: 0.09272 \n",
      "[162/200] train_loss: 0.05925 valid_loss: 0.07930 test_loss: 0.09807 \n",
      "[163/200] train_loss: 0.05813 valid_loss: 0.07588 test_loss: 0.09273 \n",
      "[164/200] train_loss: 0.05993 valid_loss: 0.07577 test_loss: 0.09462 \n",
      "[165/200] train_loss: 0.05875 valid_loss: 0.07753 test_loss: 0.10050 \n",
      "[166/200] train_loss: 0.05956 valid_loss: 0.07663 test_loss: 0.09301 \n",
      "[167/200] train_loss: 0.05738 valid_loss: 0.07767 test_loss: 0.09493 \n",
      "[168/200] train_loss: 0.05997 valid_loss: 0.07589 test_loss: 0.30391 \n",
      "[169/200] train_loss: 0.05820 valid_loss: 0.07774 test_loss: 0.09216 \n",
      "[170/200] train_loss: 0.05911 valid_loss: 0.07597 test_loss: 0.10071 \n",
      "[171/200] train_loss: 0.05870 valid_loss: 0.07691 test_loss: 0.16374 \n",
      "[172/200] train_loss: 0.05848 valid_loss: 0.07551 test_loss: 0.10011 \n",
      "[173/200] train_loss: 0.05946 valid_loss: 0.07652 test_loss: 0.18809 \n",
      "[174/200] train_loss: 0.05842 valid_loss: 0.07951 test_loss: 0.09406 \n",
      "[175/200] train_loss: 0.05753 valid_loss: 0.07884 test_loss: 0.09585 \n",
      "[176/200] train_loss: 0.05918 valid_loss: 0.07640 test_loss: 0.25792 \n",
      "[177/200] train_loss: 0.05849 valid_loss: 0.07886 test_loss: 0.09588 \n",
      "[178/200] train_loss: 0.05706 valid_loss: 0.07786 test_loss: 0.09556 \n",
      "[179/200] train_loss: 0.05773 valid_loss: 0.07906 test_loss: 0.09707 \n",
      "[180/200] train_loss: 0.05867 valid_loss: 0.07642 test_loss: 0.09456 \n",
      "[181/200] train_loss: 0.05624 valid_loss: 0.07732 test_loss: 0.17185 \n",
      "[182/200] train_loss: 0.05647 valid_loss: 0.07859 test_loss: 0.09446 \n",
      "[183/200] train_loss: 0.05673 valid_loss: 0.07814 test_loss: 0.09772 \n",
      "[184/200] train_loss: 0.05507 valid_loss: 0.07675 test_loss: 0.09596 \n",
      "[185/200] train_loss: 0.05569 valid_loss: 0.07827 test_loss: 0.09556 \n",
      "[186/200] train_loss: 0.05647 valid_loss: 0.07867 test_loss: 0.09743 \n",
      "[187/200] train_loss: 0.05711 valid_loss: 0.08084 test_loss: 0.09492 \n",
      "[188/200] train_loss: 0.05700 valid_loss: 0.07786 test_loss: 0.09318 \n",
      "[189/200] train_loss: 0.05593 valid_loss: 0.07936 test_loss: 0.09480 \n",
      "[190/200] train_loss: 0.05610 valid_loss: 0.07944 test_loss: 0.09409 \n",
      "[191/200] train_loss: 0.05672 valid_loss: 0.07656 test_loss: 0.09592 \n",
      "[192/200] train_loss: 0.05754 valid_loss: 0.07715 test_loss: 0.09311 \n",
      "[193/200] train_loss: 0.05747 valid_loss: 0.07816 test_loss: 0.09707 \n",
      "[194/200] train_loss: 0.05809 valid_loss: 0.07559 test_loss: 0.09572 \n",
      "[195/200] train_loss: 0.05639 valid_loss: 0.07800 test_loss: 0.09784 \n",
      "[196/200] train_loss: 0.05523 valid_loss: 0.07984 test_loss: 0.09917 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[197/200] train_loss: 0.05687 valid_loss: 0.07605 test_loss: 0.09526 \n",
      "[198/200] train_loss: 0.05672 valid_loss: 0.07838 test_loss: 0.09427 \n",
      "[199/200] train_loss: 0.05747 valid_loss: 0.07816 test_loss: 0.10124 \n",
      "[200/200] train_loss: 0.05413 valid_loss: 0.07769 test_loss: 0.09447 \n",
      "TRAINING MODEL 16\n",
      "[  1/200] train_loss: 0.35996 valid_loss: 0.26590 test_loss: 0.23438 \n",
      "验证损失减少 (inf --> 0.234385). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20778 valid_loss: 0.19375 test_loss: 0.14923 \n",
      "验证损失减少 (0.265899 --> 0.149230). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16513 valid_loss: 0.16677 test_loss: 0.13013 \n",
      "验证损失减少 (0.193754 --> 0.130129). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14338 valid_loss: 0.14503 test_loss: 0.11689 \n",
      "验证损失减少 (0.166767 --> 0.116893). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13464 valid_loss: 0.13621 test_loss: 0.11341 \n",
      "验证损失减少 (0.145030 --> 0.113409). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13100 valid_loss: 0.13335 test_loss: 0.11475 \n",
      "验证损失减少 (0.136207 --> 0.114752). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12390 valid_loss: 0.12736 test_loss: 0.11123 \n",
      "验证损失减少 (0.133349 --> 0.111231). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11813 valid_loss: 0.12375 test_loss: 0.10963 \n",
      "验证损失减少 (0.127359 --> 0.109629). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11585 valid_loss: 0.12222 test_loss: 0.11060 \n",
      "验证损失减少 (0.123747 --> 0.110605). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11295 valid_loss: 0.11800 test_loss: 0.10877 \n",
      "验证损失减少 (0.122220 --> 0.108769). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11114 valid_loss: 0.11432 test_loss: 0.10451 \n",
      "验证损失减少 (0.118003 --> 0.104505). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10933 valid_loss: 0.11083 test_loss: 0.11946 \n",
      "[ 13/200] train_loss: 0.10760 valid_loss: 0.11142 test_loss: 0.10448 \n",
      "验证损失减少 (0.114322 --> 0.104482). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10278 valid_loss: 0.11171 test_loss: 0.10616 \n",
      "验证损失减少 (0.111422 --> 0.106164). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10421 valid_loss: 0.10709 test_loss: 0.10529 \n",
      "验证损失减少 (0.111708 --> 0.105288). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10011 valid_loss: 0.10498 test_loss: 0.10174 \n",
      "验证损失减少 (0.107091 --> 0.101744). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10191 valid_loss: 0.10507 test_loss: 0.10040 \n",
      "验证损失减少 (0.104979 --> 0.100401). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.09718 valid_loss: 0.10511 test_loss: 0.10199 \n",
      "验证损失减少 (0.105071 --> 0.101986). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09745 valid_loss: 0.10252 test_loss: 0.09757 \n",
      "验证损失减少 (0.105108 --> 0.097573). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09231 valid_loss: 0.10233 test_loss: 0.09917 \n",
      "验证损失减少 (0.102524 --> 0.099166). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09351 valid_loss: 0.10020 test_loss: 0.09985 \n",
      "验证损失减少 (0.102329 --> 0.099847). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09342 valid_loss: 0.09757 test_loss: 0.18415 \n",
      "[ 23/200] train_loss: 0.09417 valid_loss: 0.09822 test_loss: 0.09874 \n",
      "验证损失减少 (0.100197 --> 0.098738). 正在保存模型...\n",
      "[ 24/200] train_loss: 0.09347 valid_loss: 0.09886 test_loss: 0.10284 \n",
      "[ 25/200] train_loss: 0.09153 valid_loss: 0.09871 test_loss: 0.09729 \n",
      "验证损失减少 (0.098222 --> 0.097288). 正在保存模型...\n",
      "[ 26/200] train_loss: 0.08843 valid_loss: 0.09506 test_loss: 0.10303 \n",
      "[ 27/200] train_loss: 0.08830 valid_loss: 0.09230 test_loss: 0.11659 \n",
      "[ 28/200] train_loss: 0.08808 valid_loss: 0.09511 test_loss: 0.09754 \n",
      "验证损失减少 (0.098712 --> 0.097537). 正在保存模型...\n",
      "[ 29/200] train_loss: 0.08885 valid_loss: 0.09551 test_loss: 0.09822 \n",
      "[ 30/200] train_loss: 0.08755 valid_loss: 0.09538 test_loss: 0.09293 \n",
      "验证损失减少 (0.095107 --> 0.092932). 正在保存模型...\n",
      "[ 31/200] train_loss: 0.08811 valid_loss: 0.09451 test_loss: 0.10301 \n",
      "[ 32/200] train_loss: 0.08397 valid_loss: 0.09322 test_loss: 0.09507 \n",
      "验证损失减少 (0.095380 --> 0.095067). 正在保存模型...\n",
      "[ 33/200] train_loss: 0.08669 valid_loss: 0.09349 test_loss: 0.09627 \n",
      "[ 34/200] train_loss: 0.08478 valid_loss: 0.09236 test_loss: 0.09520 \n",
      "[ 35/200] train_loss: 0.08586 valid_loss: 0.09106 test_loss: 0.19933 \n",
      "[ 36/200] train_loss: 0.08259 valid_loss: 0.09058 test_loss: 0.09342 \n",
      "[ 37/200] train_loss: 0.08457 valid_loss: 0.08814 test_loss: 0.11511 \n",
      "[ 38/200] train_loss: 0.08353 valid_loss: 0.09131 test_loss: 0.11145 \n",
      "[ 39/200] train_loss: 0.08294 valid_loss: 0.08658 test_loss: 0.10773 \n",
      "[ 40/200] train_loss: 0.08257 valid_loss: 0.08698 test_loss: 0.10172 \n",
      "[ 41/200] train_loss: 0.08048 valid_loss: 0.08809 test_loss: 0.08932 \n",
      "验证损失减少 (0.093217 --> 0.089319). 正在保存模型...\n",
      "[ 42/200] train_loss: 0.08166 valid_loss: 0.08711 test_loss: 0.09850 \n",
      "[ 43/200] train_loss: 0.08199 valid_loss: 0.08914 test_loss: 0.09245 \n",
      "[ 44/200] train_loss: 0.08052 valid_loss: 0.09001 test_loss: 0.09567 \n",
      "[ 45/200] train_loss: 0.07937 valid_loss: 0.08619 test_loss: 0.09138 \n",
      "[ 46/200] train_loss: 0.08139 valid_loss: 0.08824 test_loss: 0.09384 \n",
      "[ 47/200] train_loss: 0.07696 valid_loss: 0.08710 test_loss: 0.09077 \n",
      "[ 48/200] train_loss: 0.07928 valid_loss: 0.08560 test_loss: 0.09524 \n",
      "[ 49/200] train_loss: 0.07976 valid_loss: 0.08756 test_loss: 0.09174 \n",
      "[ 50/200] train_loss: 0.07828 valid_loss: 0.08614 test_loss: 0.09270 \n",
      "[ 51/200] train_loss: 0.07662 valid_loss: 0.08416 test_loss: 0.11498 \n",
      "[ 52/200] train_loss: 0.07695 valid_loss: 0.08386 test_loss: 0.13637 \n",
      "[ 53/200] train_loss: 0.07644 valid_loss: 0.08511 test_loss: 0.10443 \n",
      "[ 54/200] train_loss: 0.07715 valid_loss: 0.08779 test_loss: 0.09320 \n",
      "[ 55/200] train_loss: 0.07784 valid_loss: 0.08578 test_loss: 0.08899 \n",
      "[ 56/200] train_loss: 0.07610 valid_loss: 0.08566 test_loss: 0.09001 \n",
      "[ 57/200] train_loss: 0.07493 valid_loss: 0.08411 test_loss: 0.09264 \n",
      "[ 58/200] train_loss: 0.07505 valid_loss: 0.08616 test_loss: 0.09102 \n",
      "[ 59/200] train_loss: 0.07518 valid_loss: 0.08524 test_loss: 0.09475 \n",
      "[ 60/200] train_loss: 0.07589 valid_loss: 0.08260 test_loss: 0.18625 \n",
      "[ 61/200] train_loss: 0.07503 valid_loss: 0.08627 test_loss: 0.09016 \n",
      "[ 62/200] train_loss: 0.07682 valid_loss: 0.08570 test_loss: 0.10019 \n",
      "[ 63/200] train_loss: 0.07329 valid_loss: 0.08416 test_loss: 0.08801 \n",
      "验证损失减少 (0.088093 --> 0.088011). 正在保存模型...\n",
      "[ 64/200] train_loss: 0.07513 valid_loss: 0.08377 test_loss: 0.08643 \n",
      "[ 65/200] train_loss: 0.07503 valid_loss: 0.08235 test_loss: 0.09130 \n",
      "[ 66/200] train_loss: 0.07488 valid_loss: 0.08225 test_loss: 0.09007 \n",
      "[ 67/200] train_loss: 0.07551 valid_loss: 0.08316 test_loss: 0.09011 \n",
      "[ 68/200] train_loss: 0.07298 valid_loss: 0.08190 test_loss: 0.13611 \n",
      "[ 69/200] train_loss: 0.07261 valid_loss: 0.08371 test_loss: 0.08783 \n",
      "[ 70/200] train_loss: 0.07055 valid_loss: 0.08299 test_loss: 0.08951 \n",
      "[ 71/200] train_loss: 0.07175 valid_loss: 0.08346 test_loss: 0.08646 \n",
      "[ 72/200] train_loss: 0.07264 valid_loss: 0.08275 test_loss: 0.08698 \n",
      "[ 73/200] train_loss: 0.07176 valid_loss: 0.08356 test_loss: 0.09393 \n",
      "[ 74/200] train_loss: 0.07197 valid_loss: 0.08367 test_loss: 0.09209 \n",
      "[ 75/200] train_loss: 0.07142 valid_loss: 0.08456 test_loss: 0.09346 \n",
      "[ 76/200] train_loss: 0.07224 valid_loss: 0.08163 test_loss: 0.08809 \n",
      "[ 77/200] train_loss: 0.07199 valid_loss: 0.08458 test_loss: 0.09082 \n",
      "[ 78/200] train_loss: 0.07059 valid_loss: 0.08377 test_loss: 0.09073 \n",
      "[ 79/200] train_loss: 0.07009 valid_loss: 0.08109 test_loss: 0.08890 \n",
      "[ 80/200] train_loss: 0.06819 valid_loss: 0.08193 test_loss: 0.12936 \n",
      "[ 81/200] train_loss: 0.07211 valid_loss: 0.08133 test_loss: 0.09100 \n",
      "[ 82/200] train_loss: 0.07146 valid_loss: 0.08070 test_loss: 0.08888 \n",
      "[ 83/200] train_loss: 0.06919 valid_loss: 0.08038 test_loss: 0.08766 \n",
      "[ 84/200] train_loss: 0.06761 valid_loss: 0.08125 test_loss: 0.09091 \n",
      "[ 85/200] train_loss: 0.07152 valid_loss: 0.08049 test_loss: 0.09289 \n",
      "[ 86/200] train_loss: 0.07099 valid_loss: 0.08047 test_loss: 0.09141 \n",
      "[ 87/200] train_loss: 0.07007 valid_loss: 0.07923 test_loss: 0.09305 \n",
      "[ 88/200] train_loss: 0.06842 valid_loss: 0.07966 test_loss: 0.09003 \n",
      "[ 89/200] train_loss: 0.06970 valid_loss: 0.07816 test_loss: 0.08764 \n",
      "[ 90/200] train_loss: 0.06778 valid_loss: 0.07995 test_loss: 0.09240 \n",
      "[ 91/200] train_loss: 0.06589 valid_loss: 0.08150 test_loss: 0.09174 \n",
      "[ 92/200] train_loss: 0.06785 valid_loss: 0.07952 test_loss: 0.08964 \n",
      "[ 93/200] train_loss: 0.06575 valid_loss: 0.08095 test_loss: 0.09542 \n",
      "[ 94/200] train_loss: 0.06872 valid_loss: 0.07889 test_loss: 0.11411 \n",
      "[ 95/200] train_loss: 0.06726 valid_loss: 0.08078 test_loss: 0.09325 \n",
      "[ 96/200] train_loss: 0.06767 valid_loss: 0.08241 test_loss: 0.09176 \n",
      "[ 97/200] train_loss: 0.06652 valid_loss: 0.07881 test_loss: 0.21941 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 98/200] train_loss: 0.06785 valid_loss: 0.07892 test_loss: 0.08699 \n",
      "[ 99/200] train_loss: 0.06696 valid_loss: 0.07857 test_loss: 0.08969 \n",
      "[100/200] train_loss: 0.06665 valid_loss: 0.07905 test_loss: 0.09007 \n",
      "[101/200] train_loss: 0.06668 valid_loss: 0.07754 test_loss: 0.09161 \n",
      "[102/200] train_loss: 0.06644 valid_loss: 0.07897 test_loss: 0.12886 \n",
      "[103/200] train_loss: 0.06599 valid_loss: 0.07791 test_loss: 0.09419 \n",
      "[104/200] train_loss: 0.06568 valid_loss: 0.07779 test_loss: 0.09362 \n",
      "[105/200] train_loss: 0.06613 valid_loss: 0.07747 test_loss: 0.08784 \n",
      "[106/200] train_loss: 0.06532 valid_loss: 0.08054 test_loss: 0.09149 \n",
      "[107/200] train_loss: 0.06499 valid_loss: 0.07660 test_loss: 0.08832 \n",
      "[108/200] train_loss: 0.06536 valid_loss: 0.07898 test_loss: 0.09244 \n",
      "[109/200] train_loss: 0.06247 valid_loss: 0.07846 test_loss: 0.09095 \n",
      "[110/200] train_loss: 0.06402 valid_loss: 0.07898 test_loss: 0.08970 \n",
      "[111/200] train_loss: 0.06492 valid_loss: 0.07843 test_loss: 0.25243 \n",
      "[112/200] train_loss: 0.06256 valid_loss: 0.07888 test_loss: 0.09289 \n",
      "[113/200] train_loss: 0.06386 valid_loss: 0.07757 test_loss: 0.09173 \n",
      "[114/200] train_loss: 0.06565 valid_loss: 0.08472 test_loss: 0.09957 \n",
      "[115/200] train_loss: 0.06297 valid_loss: 0.07786 test_loss: 0.09126 \n",
      "[116/200] train_loss: 0.06525 valid_loss: 0.07826 test_loss: 0.11553 \n",
      "[117/200] train_loss: 0.06540 valid_loss: 0.07923 test_loss: 0.08851 \n",
      "[118/200] train_loss: 0.06249 valid_loss: 0.07867 test_loss: 0.09069 \n",
      "[119/200] train_loss: 0.06268 valid_loss: 0.07768 test_loss: 0.09045 \n",
      "[120/200] train_loss: 0.06555 valid_loss: 0.07958 test_loss: 0.08880 \n",
      "[121/200] train_loss: 0.06310 valid_loss: 0.07799 test_loss: 0.08778 \n",
      "[122/200] train_loss: 0.06418 valid_loss: 0.07682 test_loss: 0.08704 \n",
      "[123/200] train_loss: 0.06113 valid_loss: 0.07780 test_loss: 0.09177 \n",
      "[124/200] train_loss: 0.06304 valid_loss: 0.07919 test_loss: 0.08707 \n",
      "[125/200] train_loss: 0.06296 valid_loss: 0.07781 test_loss: 0.08796 \n",
      "[126/200] train_loss: 0.06145 valid_loss: 0.07439 test_loss: 0.09082 \n",
      "[127/200] train_loss: 0.06239 valid_loss: 0.07908 test_loss: 0.09319 \n",
      "[128/200] train_loss: 0.06114 valid_loss: 0.07737 test_loss: 0.09010 \n",
      "[129/200] train_loss: 0.06320 valid_loss: 0.07588 test_loss: 0.08532 \n",
      "[130/200] train_loss: 0.06301 valid_loss: 0.07835 test_loss: 0.09213 \n",
      "[131/200] train_loss: 0.06149 valid_loss: 0.07876 test_loss: 0.09199 \n",
      "[132/200] train_loss: 0.06107 valid_loss: 0.07662 test_loss: 0.08824 \n",
      "[133/200] train_loss: 0.06181 valid_loss: 0.07786 test_loss: 0.08812 \n",
      "[134/200] train_loss: 0.06257 valid_loss: 0.07675 test_loss: 0.08793 \n",
      "[135/200] train_loss: 0.06215 valid_loss: 0.07872 test_loss: 0.08959 \n",
      "[136/200] train_loss: 0.06052 valid_loss: 0.07763 test_loss: 0.08694 \n",
      "[137/200] train_loss: 0.06052 valid_loss: 0.07813 test_loss: 0.09399 \n",
      "[138/200] train_loss: 0.06112 valid_loss: 0.07630 test_loss: 0.09311 \n",
      "[139/200] train_loss: 0.06099 valid_loss: 0.07765 test_loss: 0.10208 \n",
      "[140/200] train_loss: 0.05961 valid_loss: 0.07701 test_loss: 0.08635 \n",
      "[141/200] train_loss: 0.06155 valid_loss: 0.07697 test_loss: 0.08821 \n",
      "[142/200] train_loss: 0.06137 valid_loss: 0.07633 test_loss: 0.09076 \n",
      "[143/200] train_loss: 0.06057 valid_loss: 0.07899 test_loss: 0.09117 \n",
      "[144/200] train_loss: 0.05943 valid_loss: 0.07799 test_loss: 0.08592 \n",
      "[145/200] train_loss: 0.05896 valid_loss: 0.07870 test_loss: 0.08933 \n",
      "[146/200] train_loss: 0.06026 valid_loss: 0.07932 test_loss: 0.08705 \n",
      "[147/200] train_loss: 0.06082 valid_loss: 0.07771 test_loss: 0.09210 \n",
      "[148/200] train_loss: 0.05842 valid_loss: 0.07630 test_loss: 0.08909 \n",
      "[149/200] train_loss: 0.05826 valid_loss: 0.07926 test_loss: 0.08842 \n",
      "[150/200] train_loss: 0.05891 valid_loss: 0.07872 test_loss: 0.09104 \n",
      "[151/200] train_loss: 0.06107 valid_loss: 0.07699 test_loss: 0.08993 \n",
      "[152/200] train_loss: 0.05926 valid_loss: 0.07656 test_loss: 0.08871 \n",
      "[153/200] train_loss: 0.05937 valid_loss: 0.07889 test_loss: 0.09420 \n",
      "[154/200] train_loss: 0.05965 valid_loss: 0.07793 test_loss: 0.09137 \n",
      "[155/200] train_loss: 0.06006 valid_loss: 0.08188 test_loss: 0.10242 \n",
      "[156/200] train_loss: 0.05997 valid_loss: 0.07584 test_loss: 0.15449 \n",
      "[157/200] train_loss: 0.05731 valid_loss: 0.07757 test_loss: 0.09275 \n",
      "[158/200] train_loss: 0.05880 valid_loss: 0.07965 test_loss: 0.09718 \n",
      "[159/200] train_loss: 0.05940 valid_loss: 0.07805 test_loss: 0.09416 \n",
      "[160/200] train_loss: 0.05828 valid_loss: 0.07692 test_loss: 0.08975 \n",
      "[161/200] train_loss: 0.05932 valid_loss: 0.08060 test_loss: 0.09337 \n",
      "[162/200] train_loss: 0.05803 valid_loss: 0.07653 test_loss: 0.09499 \n",
      "[163/200] train_loss: 0.05746 valid_loss: 0.07855 test_loss: 0.09490 \n",
      "[164/200] train_loss: 0.05979 valid_loss: 0.07443 test_loss: 0.08917 \n",
      "[165/200] train_loss: 0.05767 valid_loss: 0.07806 test_loss: 0.09129 \n",
      "[166/200] train_loss: 0.05801 valid_loss: 0.07766 test_loss: 0.08866 \n",
      "[167/200] train_loss: 0.05879 valid_loss: 0.07594 test_loss: 0.09177 \n",
      "[168/200] train_loss: 0.05666 valid_loss: 0.07673 test_loss: 0.20294 \n",
      "[169/200] train_loss: 0.05836 valid_loss: 0.07572 test_loss: 0.09035 \n",
      "[170/200] train_loss: 0.05740 valid_loss: 0.07802 test_loss: 0.09877 \n",
      "[171/200] train_loss: 0.05805 valid_loss: 0.07711 test_loss: 0.08740 \n",
      "[172/200] train_loss: 0.05654 valid_loss: 0.07824 test_loss: 0.09516 \n",
      "[173/200] train_loss: 0.05699 valid_loss: 0.07670 test_loss: 0.08928 \n",
      "[174/200] train_loss: 0.05667 valid_loss: 0.07653 test_loss: 0.08953 \n",
      "[175/200] train_loss: 0.05816 valid_loss: 0.07825 test_loss: 0.09217 \n",
      "[176/200] train_loss: 0.05693 valid_loss: 0.07689 test_loss: 0.09349 \n",
      "[177/200] train_loss: 0.05857 valid_loss: 0.07893 test_loss: 0.09065 \n",
      "[178/200] train_loss: 0.05683 valid_loss: 0.07607 test_loss: 0.34762 \n",
      "[179/200] train_loss: 0.05549 valid_loss: 0.07550 test_loss: 0.09521 \n",
      "[180/200] train_loss: 0.05709 valid_loss: 0.07861 test_loss: 0.09925 \n",
      "[181/200] train_loss: 0.05794 valid_loss: 0.08060 test_loss: 0.09044 \n",
      "[182/200] train_loss: 0.05630 valid_loss: 0.07638 test_loss: 0.09271 \n",
      "[183/200] train_loss: 0.05502 valid_loss: 0.07743 test_loss: 0.09283 \n",
      "[184/200] train_loss: 0.05574 valid_loss: 0.07674 test_loss: 0.09108 \n",
      "[185/200] train_loss: 0.05625 valid_loss: 0.07866 test_loss: 0.09508 \n",
      "[186/200] train_loss: 0.05721 valid_loss: 0.07603 test_loss: 0.09301 \n",
      "[187/200] train_loss: 0.05564 valid_loss: 0.07893 test_loss: 0.09764 \n",
      "[188/200] train_loss: 0.05574 valid_loss: 0.07661 test_loss: 0.09371 \n",
      "[189/200] train_loss: 0.05646 valid_loss: 0.07962 test_loss: 0.09951 \n",
      "[190/200] train_loss: 0.05605 valid_loss: 0.08065 test_loss: 0.09151 \n",
      "[191/200] train_loss: 0.05593 valid_loss: 0.08003 test_loss: 0.09714 \n",
      "[192/200] train_loss: 0.05557 valid_loss: 0.07709 test_loss: 0.09512 \n",
      "[193/200] train_loss: 0.05492 valid_loss: 0.07858 test_loss: 0.09244 \n",
      "[194/200] train_loss: 0.05586 valid_loss: 0.07755 test_loss: 0.09508 \n",
      "[195/200] train_loss: 0.05564 valid_loss: 0.07710 test_loss: 0.09073 \n",
      "[196/200] train_loss: 0.05475 valid_loss: 0.07860 test_loss: 0.09470 \n",
      "[197/200] train_loss: 0.05454 valid_loss: 0.07708 test_loss: 0.09240 \n",
      "[198/200] train_loss: 0.05495 valid_loss: 0.07591 test_loss: 0.09031 \n",
      "[199/200] train_loss: 0.05606 valid_loss: 0.07590 test_loss: 0.09130 \n",
      "[200/200] train_loss: 0.05527 valid_loss: 0.07712 test_loss: 0.09299 \n",
      "TRAINING MODEL 17\n",
      "[  1/200] train_loss: 0.36962 valid_loss: 0.26072 test_loss: 0.23707 \n",
      "验证损失减少 (inf --> 0.237067). 正在保存模型...\n",
      "[  2/200] train_loss: 0.20918 valid_loss: 0.19413 test_loss: 0.15062 \n",
      "验证损失减少 (0.260725 --> 0.150619). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16458 valid_loss: 0.16667 test_loss: 0.13515 \n",
      "验证损失减少 (0.194128 --> 0.135153). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14429 valid_loss: 0.14633 test_loss: 0.12005 \n",
      "验证损失减少 (0.166674 --> 0.120055). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13411 valid_loss: 0.13641 test_loss: 0.11625 \n",
      "验证损失减少 (0.146333 --> 0.116254). 正在保存模型...\n",
      "[  6/200] train_loss: 0.12998 valid_loss: 0.13103 test_loss: 0.11439 \n",
      "验证损失减少 (0.136407 --> 0.114388). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12065 valid_loss: 0.12430 test_loss: 0.10822 \n",
      "验证损失减少 (0.131030 --> 0.108216). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11706 valid_loss: 0.12392 test_loss: 0.11133 \n",
      "验证损失减少 (0.124301 --> 0.111326). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11312 valid_loss: 0.11720 test_loss: 0.10559 \n",
      "验证损失减少 (0.123922 --> 0.105594). 正在保存模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10/200] train_loss: 0.11349 valid_loss: 0.11465 test_loss: 0.10565 \n",
      "验证损失减少 (0.117200 --> 0.105653). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11127 valid_loss: 0.11352 test_loss: 0.10705 \n",
      "验证损失减少 (0.114645 --> 0.107052). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10891 valid_loss: 0.11228 test_loss: 0.10286 \n",
      "验证损失减少 (0.113518 --> 0.102863). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10686 valid_loss: 0.10829 test_loss: 0.10775 \n",
      "验证损失减少 (0.112278 --> 0.107755). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10527 valid_loss: 0.10661 test_loss: 0.21029 \n",
      "[ 15/200] train_loss: 0.10305 valid_loss: 0.10618 test_loss: 0.10614 \n",
      "验证损失减少 (0.108286 --> 0.106138). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10204 valid_loss: 0.10336 test_loss: 0.10019 \n",
      "验证损失减少 (0.106183 --> 0.100192). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10131 valid_loss: 0.10438 test_loss: 0.10469 \n",
      "[ 18/200] train_loss: 0.09923 valid_loss: 0.10268 test_loss: 0.10205 \n",
      "验证损失减少 (0.103363 --> 0.102048). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09527 valid_loss: 0.10028 test_loss: 0.09971 \n",
      "验证损失减少 (0.102684 --> 0.099714). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09696 valid_loss: 0.09891 test_loss: 0.10873 \n",
      "[ 21/200] train_loss: 0.09456 valid_loss: 0.09774 test_loss: 0.09502 \n",
      "验证损失减少 (0.100281 --> 0.095022). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09464 valid_loss: 0.09681 test_loss: 0.11508 \n",
      "[ 23/200] train_loss: 0.09337 valid_loss: 0.10024 test_loss: 0.10161 \n",
      "[ 24/200] train_loss: 0.09042 valid_loss: 0.09749 test_loss: 0.09921 \n",
      "[ 25/200] train_loss: 0.09094 valid_loss: 0.09589 test_loss: 0.10992 \n",
      "[ 26/200] train_loss: 0.09299 valid_loss: 0.09365 test_loss: 0.10130 \n",
      "[ 27/200] train_loss: 0.09154 valid_loss: 0.09531 test_loss: 0.09991 \n",
      "[ 28/200] train_loss: 0.09082 valid_loss: 0.09366 test_loss: 0.10665 \n",
      "[ 29/200] train_loss: 0.08935 valid_loss: 0.09181 test_loss: 0.10886 \n",
      "[ 30/200] train_loss: 0.08591 valid_loss: 0.09239 test_loss: 0.09462 \n",
      "验证损失减少 (0.097738 --> 0.094620). 正在保存模型...\n",
      "[ 31/200] train_loss: 0.08802 valid_loss: 0.09396 test_loss: 0.11892 \n",
      "[ 32/200] train_loss: 0.08751 valid_loss: 0.09131 test_loss: 0.09846 \n",
      "[ 33/200] train_loss: 0.08567 valid_loss: 0.09154 test_loss: 0.09670 \n",
      "[ 34/200] train_loss: 0.08818 valid_loss: 0.09460 test_loss: 0.10616 \n",
      "[ 35/200] train_loss: 0.08551 valid_loss: 0.09136 test_loss: 0.12944 \n",
      "[ 36/200] train_loss: 0.08423 valid_loss: 0.09123 test_loss: 0.09564 \n",
      "[ 37/200] train_loss: 0.08576 valid_loss: 0.08749 test_loss: 0.11400 \n",
      "[ 38/200] train_loss: 0.08493 valid_loss: 0.08916 test_loss: 0.10431 \n",
      "[ 39/200] train_loss: 0.08403 valid_loss: 0.09184 test_loss: 0.11963 \n",
      "[ 40/200] train_loss: 0.08289 valid_loss: 0.08751 test_loss: 0.20484 \n",
      "[ 41/200] train_loss: 0.08318 valid_loss: 0.08820 test_loss: 0.28331 \n",
      "[ 42/200] train_loss: 0.08372 valid_loss: 0.08864 test_loss: 0.15298 \n",
      "[ 43/200] train_loss: 0.08318 valid_loss: 0.08796 test_loss: 0.26177 \n",
      "[ 44/200] train_loss: 0.08227 valid_loss: 0.08781 test_loss: 0.12836 \n",
      "[ 45/200] train_loss: 0.08042 valid_loss: 0.08526 test_loss: 0.32847 \n",
      "[ 46/200] train_loss: 0.08144 valid_loss: 0.08657 test_loss: 0.23044 \n",
      "[ 47/200] train_loss: 0.07951 valid_loss: 0.08573 test_loss: 0.25551 \n",
      "[ 48/200] train_loss: 0.08050 valid_loss: 0.08762 test_loss: 0.22483 \n",
      "[ 49/200] train_loss: 0.07908 valid_loss: 0.08758 test_loss: 0.09297 \n",
      "[ 50/200] train_loss: 0.07805 valid_loss: 0.08619 test_loss: 0.14235 \n",
      "[ 51/200] train_loss: 0.08024 valid_loss: 0.08436 test_loss: 0.13625 \n",
      "[ 52/200] train_loss: 0.07911 valid_loss: 0.08524 test_loss: 0.16963 \n",
      "[ 53/200] train_loss: 0.07853 valid_loss: 0.08482 test_loss: 0.09720 \n",
      "[ 54/200] train_loss: 0.07781 valid_loss: 0.08257 test_loss: 0.09996 \n",
      "[ 55/200] train_loss: 0.07879 valid_loss: 0.08743 test_loss: 0.14584 \n",
      "[ 56/200] train_loss: 0.07627 valid_loss: 0.08457 test_loss: 0.13863 \n",
      "[ 57/200] train_loss: 0.07721 valid_loss: 0.08346 test_loss: 0.12174 \n",
      "[ 58/200] train_loss: 0.07597 valid_loss: 0.08490 test_loss: 0.09078 \n",
      "验证损失减少 (0.092393 --> 0.090783). 正在保存模型...\n",
      "[ 59/200] train_loss: 0.07722 valid_loss: 0.08301 test_loss: 0.16262 \n",
      "[ 60/200] train_loss: 0.07432 valid_loss: 0.08405 test_loss: 0.09543 \n",
      "[ 61/200] train_loss: 0.07436 valid_loss: 0.08191 test_loss: 0.10254 \n",
      "[ 62/200] train_loss: 0.07520 valid_loss: 0.08348 test_loss: 0.14632 \n",
      "[ 63/200] train_loss: 0.07569 valid_loss: 0.08358 test_loss: 0.10192 \n",
      "[ 64/200] train_loss: 0.07348 valid_loss: 0.08097 test_loss: 0.09845 \n",
      "[ 65/200] train_loss: 0.07496 valid_loss: 0.08469 test_loss: 0.09664 \n",
      "[ 66/200] train_loss: 0.07387 valid_loss: 0.08367 test_loss: 0.09385 \n",
      "[ 67/200] train_loss: 0.07288 valid_loss: 0.08303 test_loss: 0.08963 \n",
      "[ 68/200] train_loss: 0.07347 valid_loss: 0.08196 test_loss: 0.09387 \n",
      "[ 69/200] train_loss: 0.07392 valid_loss: 0.08376 test_loss: 0.09361 \n",
      "[ 70/200] train_loss: 0.07309 valid_loss: 0.08306 test_loss: 0.08992 \n",
      "[ 71/200] train_loss: 0.07414 valid_loss: 0.08038 test_loss: 0.20449 \n",
      "[ 72/200] train_loss: 0.07395 valid_loss: 0.08074 test_loss: 0.25339 \n",
      "[ 73/200] train_loss: 0.07160 valid_loss: 0.07990 test_loss: 0.09350 \n",
      "[ 74/200] train_loss: 0.07311 valid_loss: 0.08059 test_loss: 0.15096 \n",
      "[ 75/200] train_loss: 0.07317 valid_loss: 0.08295 test_loss: 0.09203 \n",
      "[ 76/200] train_loss: 0.07261 valid_loss: 0.08083 test_loss: 0.10533 \n",
      "[ 77/200] train_loss: 0.07005 valid_loss: 0.08095 test_loss: 0.12435 \n",
      "[ 78/200] train_loss: 0.07365 valid_loss: 0.08134 test_loss: 0.12348 \n",
      "[ 79/200] train_loss: 0.07128 valid_loss: 0.08125 test_loss: 0.09716 \n",
      "[ 80/200] train_loss: 0.06921 valid_loss: 0.07928 test_loss: 0.09666 \n",
      "[ 81/200] train_loss: 0.07083 valid_loss: 0.08111 test_loss: 0.27080 \n",
      "[ 82/200] train_loss: 0.06964 valid_loss: 0.08083 test_loss: 0.08691 \n",
      "[ 83/200] train_loss: 0.07069 valid_loss: 0.08225 test_loss: 0.09967 \n",
      "[ 84/200] train_loss: 0.07007 valid_loss: 0.08317 test_loss: 0.10345 \n",
      "[ 85/200] train_loss: 0.06988 valid_loss: 0.08319 test_loss: 0.29000 \n",
      "[ 86/200] train_loss: 0.06993 valid_loss: 0.08042 test_loss: 0.12448 \n",
      "[ 87/200] train_loss: 0.07012 valid_loss: 0.08250 test_loss: 0.09396 \n",
      "[ 88/200] train_loss: 0.06773 valid_loss: 0.08471 test_loss: 0.09260 \n",
      "[ 89/200] train_loss: 0.06927 valid_loss: 0.08061 test_loss: 0.09968 \n",
      "[ 90/200] train_loss: 0.06683 valid_loss: 0.08049 test_loss: 0.09512 \n",
      "[ 91/200] train_loss: 0.06804 valid_loss: 0.08017 test_loss: 0.11696 \n",
      "[ 92/200] train_loss: 0.06980 valid_loss: 0.08025 test_loss: 0.09793 \n",
      "[ 93/200] train_loss: 0.06980 valid_loss: 0.08120 test_loss: 0.15100 \n",
      "[ 94/200] train_loss: 0.06899 valid_loss: 0.07980 test_loss: 0.09582 \n",
      "[ 95/200] train_loss: 0.06656 valid_loss: 0.08069 test_loss: 0.08859 \n",
      "[ 96/200] train_loss: 0.06722 valid_loss: 0.07978 test_loss: 0.08962 \n",
      "[ 97/200] train_loss: 0.06799 valid_loss: 0.08241 test_loss: 0.09121 \n",
      "[ 98/200] train_loss: 0.06693 valid_loss: 0.08023 test_loss: 0.09281 \n",
      "[ 99/200] train_loss: 0.06731 valid_loss: 0.08126 test_loss: 0.11148 \n",
      "[100/200] train_loss: 0.06648 valid_loss: 0.08132 test_loss: 0.09123 \n",
      "[101/200] train_loss: 0.06774 valid_loss: 0.08119 test_loss: 0.32399 \n",
      "[102/200] train_loss: 0.06656 valid_loss: 0.07877 test_loss: 0.10277 \n",
      "[103/200] train_loss: 0.06595 valid_loss: 0.08264 test_loss: 0.09443 \n",
      "[104/200] train_loss: 0.06677 valid_loss: 0.08044 test_loss: 0.09001 \n",
      "[105/200] train_loss: 0.06426 valid_loss: 0.08012 test_loss: 0.08951 \n",
      "[106/200] train_loss: 0.06567 valid_loss: 0.08143 test_loss: 0.09525 \n",
      "[107/200] train_loss: 0.06458 valid_loss: 0.07947 test_loss: 0.09238 \n",
      "[108/200] train_loss: 0.06582 valid_loss: 0.08104 test_loss: 0.09014 \n",
      "[109/200] train_loss: 0.06607 valid_loss: 0.08336 test_loss: 0.14234 \n",
      "[110/200] train_loss: 0.06606 valid_loss: 0.07921 test_loss: 0.14222 \n",
      "[111/200] train_loss: 0.06525 valid_loss: 0.07839 test_loss: 0.09900 \n",
      "[112/200] train_loss: 0.06553 valid_loss: 0.08124 test_loss: 0.09636 \n",
      "[113/200] train_loss: 0.06437 valid_loss: 0.07951 test_loss: 0.08930 \n",
      "[114/200] train_loss: 0.06531 valid_loss: 0.07888 test_loss: 0.09867 \n",
      "[115/200] train_loss: 0.06424 valid_loss: 0.07902 test_loss: 0.09228 \n",
      "[116/200] train_loss: 0.06380 valid_loss: 0.08091 test_loss: 0.09135 \n",
      "[117/200] train_loss: 0.06554 valid_loss: 0.08152 test_loss: 0.15766 \n",
      "[118/200] train_loss: 0.06562 valid_loss: 0.08075 test_loss: 0.09316 \n",
      "[119/200] train_loss: 0.06178 valid_loss: 0.07947 test_loss: 0.43335 \n",
      "[120/200] train_loss: 0.06440 valid_loss: 0.07867 test_loss: 0.09408 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121/200] train_loss: 0.06231 valid_loss: 0.07946 test_loss: 0.09120 \n",
      "[122/200] train_loss: 0.06269 valid_loss: 0.07928 test_loss: 0.09236 \n",
      "[123/200] train_loss: 0.06397 valid_loss: 0.07991 test_loss: 0.27162 \n",
      "[124/200] train_loss: 0.06266 valid_loss: 0.07946 test_loss: 0.15938 \n",
      "[125/200] train_loss: 0.06257 valid_loss: 0.07795 test_loss: 0.09483 \n",
      "[126/200] train_loss: 0.06147 valid_loss: 0.07956 test_loss: 0.09320 \n",
      "[127/200] train_loss: 0.06193 valid_loss: 0.07924 test_loss: 0.21748 \n",
      "[128/200] train_loss: 0.06296 valid_loss: 0.07894 test_loss: 0.08865 \n",
      "[129/200] train_loss: 0.06238 valid_loss: 0.08035 test_loss: 0.09639 \n",
      "[130/200] train_loss: 0.06230 valid_loss: 0.07840 test_loss: 0.09102 \n",
      "[131/200] train_loss: 0.06376 valid_loss: 0.07856 test_loss: 0.09079 \n",
      "[132/200] train_loss: 0.06038 valid_loss: 0.08058 test_loss: 0.09136 \n",
      "[133/200] train_loss: 0.06327 valid_loss: 0.07939 test_loss: 0.09152 \n",
      "[134/200] train_loss: 0.06218 valid_loss: 0.07961 test_loss: 0.08868 \n",
      "[135/200] train_loss: 0.06191 valid_loss: 0.07988 test_loss: 0.29953 \n",
      "[136/200] train_loss: 0.06210 valid_loss: 0.07836 test_loss: 0.31086 \n",
      "[137/200] train_loss: 0.06206 valid_loss: 0.07860 test_loss: 0.09313 \n",
      "[138/200] train_loss: 0.06202 valid_loss: 0.07889 test_loss: 0.11759 \n",
      "[139/200] train_loss: 0.06126 valid_loss: 0.07811 test_loss: 0.11940 \n",
      "[140/200] train_loss: 0.06166 valid_loss: 0.08063 test_loss: 0.09329 \n",
      "[141/200] train_loss: 0.06250 valid_loss: 0.07698 test_loss: 0.09523 \n",
      "[142/200] train_loss: 0.05967 valid_loss: 0.07748 test_loss: 0.13347 \n",
      "[143/200] train_loss: 0.06148 valid_loss: 0.07808 test_loss: 0.09276 \n",
      "[144/200] train_loss: 0.06125 valid_loss: 0.07961 test_loss: 0.09424 \n",
      "[145/200] train_loss: 0.06041 valid_loss: 0.08145 test_loss: 0.09671 \n",
      "[146/200] train_loss: 0.05879 valid_loss: 0.07730 test_loss: 0.09195 \n",
      "[147/200] train_loss: 0.06196 valid_loss: 0.07803 test_loss: 0.14520 \n",
      "[148/200] train_loss: 0.06062 valid_loss: 0.07743 test_loss: 0.12900 \n",
      "[149/200] train_loss: 0.06171 valid_loss: 0.08017 test_loss: 0.18872 \n",
      "[150/200] train_loss: 0.06033 valid_loss: 0.07938 test_loss: 0.31980 \n",
      "[151/200] train_loss: 0.06071 valid_loss: 0.08101 test_loss: 0.09697 \n",
      "[152/200] train_loss: 0.05839 valid_loss: 0.07784 test_loss: 0.62175 \n",
      "[153/200] train_loss: 0.05902 valid_loss: 0.07834 test_loss: 0.28877 \n",
      "[154/200] train_loss: 0.06045 valid_loss: 0.07739 test_loss: 0.09854 \n",
      "[155/200] train_loss: 0.06016 valid_loss: 0.07794 test_loss: 0.44967 \n",
      "[156/200] train_loss: 0.06051 valid_loss: 0.07839 test_loss: 0.09128 \n",
      "[157/200] train_loss: 0.06092 valid_loss: 0.07798 test_loss: 0.25264 \n",
      "[158/200] train_loss: 0.06057 valid_loss: 0.07614 test_loss: 0.09102 \n",
      "[159/200] train_loss: 0.05910 valid_loss: 0.07733 test_loss: 0.13652 \n",
      "[160/200] train_loss: 0.06047 valid_loss: 0.07768 test_loss: 0.36086 \n",
      "[161/200] train_loss: 0.05900 valid_loss: 0.07789 test_loss: 0.38886 \n",
      "[162/200] train_loss: 0.05703 valid_loss: 0.07794 test_loss: 0.12710 \n",
      "[163/200] train_loss: 0.05823 valid_loss: 0.07600 test_loss: 0.30374 \n",
      "[164/200] train_loss: 0.05715 valid_loss: 0.07790 test_loss: 0.09713 \n",
      "[165/200] train_loss: 0.05684 valid_loss: 0.07906 test_loss: 0.09203 \n",
      "[166/200] train_loss: 0.05853 valid_loss: 0.07985 test_loss: 0.09205 \n",
      "[167/200] train_loss: 0.05984 valid_loss: 0.07917 test_loss: 0.11780 \n",
      "[168/200] train_loss: 0.05780 valid_loss: 0.08235 test_loss: 0.09496 \n",
      "[169/200] train_loss: 0.06028 valid_loss: 0.07809 test_loss: 0.09262 \n",
      "[170/200] train_loss: 0.05846 valid_loss: 0.07833 test_loss: 0.15966 \n",
      "[171/200] train_loss: 0.05791 valid_loss: 0.07787 test_loss: 0.09515 \n",
      "[172/200] train_loss: 0.05764 valid_loss: 0.08085 test_loss: 0.09182 \n",
      "[173/200] train_loss: 0.05886 valid_loss: 0.08124 test_loss: 0.12668 \n",
      "[174/200] train_loss: 0.05772 valid_loss: 0.08100 test_loss: 0.09360 \n",
      "[175/200] train_loss: 0.05603 valid_loss: 0.07973 test_loss: 0.09104 \n",
      "[176/200] train_loss: 0.05759 valid_loss: 0.08050 test_loss: 0.09101 \n",
      "[177/200] train_loss: 0.05784 valid_loss: 0.07813 test_loss: 0.09214 \n",
      "[178/200] train_loss: 0.05754 valid_loss: 0.08047 test_loss: 0.09611 \n",
      "[179/200] train_loss: 0.05803 valid_loss: 0.07903 test_loss: 0.13824 \n",
      "[180/200] train_loss: 0.05654 valid_loss: 0.07820 test_loss: 0.09498 \n",
      "[181/200] train_loss: 0.05873 valid_loss: 0.07868 test_loss: 0.16924 \n",
      "[182/200] train_loss: 0.05821 valid_loss: 0.07708 test_loss: 0.09199 \n",
      "[183/200] train_loss: 0.05716 valid_loss: 0.07604 test_loss: 0.18306 \n",
      "[184/200] train_loss: 0.05653 valid_loss: 0.08012 test_loss: 0.09514 \n",
      "[185/200] train_loss: 0.05677 valid_loss: 0.07693 test_loss: 0.18798 \n",
      "[186/200] train_loss: 0.05667 valid_loss: 0.07812 test_loss: 0.10547 \n",
      "[187/200] train_loss: 0.05813 valid_loss: 0.07733 test_loss: 0.10278 \n",
      "[188/200] train_loss: 0.05631 valid_loss: 0.07695 test_loss: 0.09265 \n",
      "[189/200] train_loss: 0.05677 valid_loss: 0.07832 test_loss: 0.09287 \n",
      "[190/200] train_loss: 0.05663 valid_loss: 0.08020 test_loss: 0.10116 \n",
      "[191/200] train_loss: 0.05506 valid_loss: 0.08178 test_loss: 0.09393 \n",
      "[192/200] train_loss: 0.05628 valid_loss: 0.07774 test_loss: 0.09265 \n",
      "[193/200] train_loss: 0.05522 valid_loss: 0.07712 test_loss: 0.16860 \n",
      "[194/200] train_loss: 0.05639 valid_loss: 0.07882 test_loss: 0.09415 \n",
      "[195/200] train_loss: 0.05615 valid_loss: 0.07686 test_loss: 0.42230 \n",
      "[196/200] train_loss: 0.05762 valid_loss: 0.08061 test_loss: 0.16950 \n",
      "[197/200] train_loss: 0.05535 valid_loss: 0.08008 test_loss: 0.09696 \n",
      "[198/200] train_loss: 0.05719 valid_loss: 0.07751 test_loss: 0.10316 \n",
      "[199/200] train_loss: 0.05721 valid_loss: 0.07747 test_loss: 0.09793 \n",
      "[200/200] train_loss: 0.05530 valid_loss: 0.07866 test_loss: 0.86809 \n",
      "TRAINING MODEL 18\n",
      "[  1/200] train_loss: 0.37835 valid_loss: 0.27076 test_loss: 0.24798 \n",
      "验证损失减少 (inf --> 0.247977). 正在保存模型...\n",
      "[  2/200] train_loss: 0.22202 valid_loss: 0.19638 test_loss: 0.16224 \n",
      "验证损失减少 (0.270762 --> 0.162242). 正在保存模型...\n",
      "[  3/200] train_loss: 0.16972 valid_loss: 0.16879 test_loss: 0.13667 \n",
      "验证损失减少 (0.196384 --> 0.136673). 正在保存模型...\n",
      "[  4/200] train_loss: 0.14858 valid_loss: 0.15229 test_loss: 0.13174 \n",
      "验证损失减少 (0.168788 --> 0.131738). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13549 valid_loss: 0.14187 test_loss: 0.11786 \n",
      "验证损失减少 (0.152289 --> 0.117859). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13199 valid_loss: 0.13224 test_loss: 0.11043 \n",
      "验证损失减少 (0.141867 --> 0.110431). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12391 valid_loss: 0.12819 test_loss: 0.11949 \n",
      "验证损失减少 (0.132239 --> 0.119488). 正在保存模型...\n",
      "[  8/200] train_loss: 0.11754 valid_loss: 0.12493 test_loss: 0.10887 \n",
      "验证损失减少 (0.128189 --> 0.108869). 正在保存模型...\n",
      "[  9/200] train_loss: 0.11736 valid_loss: 0.12153 test_loss: 0.11405 \n",
      "验证损失减少 (0.124935 --> 0.114050). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11387 valid_loss: 0.11716 test_loss: 0.10549 \n",
      "验证损失减少 (0.121526 --> 0.105490). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11087 valid_loss: 0.11374 test_loss: 0.10397 \n",
      "验证损失减少 (0.117157 --> 0.103969). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.10940 valid_loss: 0.11255 test_loss: 0.10279 \n",
      "验证损失减少 (0.113736 --> 0.102786). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.10868 valid_loss: 0.11169 test_loss: 0.10854 \n",
      "验证损失减少 (0.112545 --> 0.108539). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.10322 valid_loss: 0.10685 test_loss: 0.09954 \n",
      "验证损失减少 (0.111693 --> 0.099543). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10383 valid_loss: 0.10951 test_loss: 0.10128 \n",
      "验证损失减少 (0.106851 --> 0.101282). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10014 valid_loss: 0.10684 test_loss: 0.09994 \n",
      "验证损失减少 (0.109508 --> 0.099943). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.09773 valid_loss: 0.10607 test_loss: 0.10046 \n",
      "验证损失减少 (0.106839 --> 0.100464). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.09943 valid_loss: 0.10489 test_loss: 0.09937 \n",
      "验证损失减少 (0.106066 --> 0.099369). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.09586 valid_loss: 0.10389 test_loss: 0.10282 \n",
      "验证损失减少 (0.104894 --> 0.102819). 正在保存模型...\n",
      "[ 20/200] train_loss: 0.09675 valid_loss: 0.10201 test_loss: 0.09689 \n",
      "验证损失减少 (0.103892 --> 0.096893). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09592 valid_loss: 0.09976 test_loss: 0.09577 \n",
      "验证损失减少 (0.102014 --> 0.095774). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09675 valid_loss: 0.09790 test_loss: 0.09597 \n",
      "验证损失减少 (0.099757 --> 0.095974). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09509 valid_loss: 0.09703 test_loss: 0.09792 \n",
      "[ 24/200] train_loss: 0.09407 valid_loss: 0.09834 test_loss: 0.09818 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25/200] train_loss: 0.09213 valid_loss: 0.09629 test_loss: 0.09897 \n",
      "[ 26/200] train_loss: 0.09307 valid_loss: 0.09614 test_loss: 0.09640 \n",
      "验证损失减少 (0.097901 --> 0.096398). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.08909 valid_loss: 0.09555 test_loss: 0.10044 \n",
      "[ 28/200] train_loss: 0.08770 valid_loss: 0.09343 test_loss: 0.16290 \n",
      "[ 29/200] train_loss: 0.08729 valid_loss: 0.09306 test_loss: 0.09670 \n",
      "[ 30/200] train_loss: 0.08826 valid_loss: 0.09059 test_loss: 0.09062 \n",
      "验证损失减少 (0.096136 --> 0.090620). 正在保存模型...\n",
      "[ 31/200] train_loss: 0.08587 valid_loss: 0.09190 test_loss: 0.09654 \n",
      "[ 32/200] train_loss: 0.08668 valid_loss: 0.09309 test_loss: 0.09443 \n",
      "[ 33/200] train_loss: 0.08594 valid_loss: 0.09380 test_loss: 0.09503 \n",
      "[ 34/200] train_loss: 0.08715 valid_loss: 0.08940 test_loss: 0.17265 \n",
      "[ 35/200] train_loss: 0.08584 valid_loss: 0.09003 test_loss: 0.10752 \n",
      "[ 36/200] train_loss: 0.08505 valid_loss: 0.09170 test_loss: 0.11621 \n",
      "[ 37/200] train_loss: 0.08366 valid_loss: 0.08968 test_loss: 0.09551 \n",
      "[ 38/200] train_loss: 0.08225 valid_loss: 0.08989 test_loss: 0.09106 \n",
      "[ 39/200] train_loss: 0.08305 valid_loss: 0.09131 test_loss: 0.10031 \n",
      "[ 40/200] train_loss: 0.08192 valid_loss: 0.08990 test_loss: 0.09496 \n",
      "[ 41/200] train_loss: 0.08073 valid_loss: 0.09009 test_loss: 0.09280 \n",
      "[ 42/200] train_loss: 0.07923 valid_loss: 0.08880 test_loss: 0.10321 \n",
      "[ 43/200] train_loss: 0.08124 valid_loss: 0.08567 test_loss: 0.13998 \n",
      "[ 44/200] train_loss: 0.07936 valid_loss: 0.08677 test_loss: 0.16180 \n",
      "[ 45/200] train_loss: 0.08052 valid_loss: 0.08831 test_loss: 0.09831 \n",
      "[ 46/200] train_loss: 0.08110 valid_loss: 0.08897 test_loss: 0.09173 \n",
      "[ 47/200] train_loss: 0.08117 valid_loss: 0.08702 test_loss: 0.08718 \n",
      "验证损失减少 (0.090594 --> 0.087176). 正在保存模型...\n",
      "[ 48/200] train_loss: 0.07698 valid_loss: 0.08611 test_loss: 0.08913 \n",
      "[ 49/200] train_loss: 0.07940 valid_loss: 0.08736 test_loss: 0.09035 \n",
      "[ 50/200] train_loss: 0.07739 valid_loss: 0.08426 test_loss: 0.19412 \n",
      "[ 51/200] train_loss: 0.07831 valid_loss: 0.08708 test_loss: 0.08950 \n",
      "[ 52/200] train_loss: 0.07506 valid_loss: 0.08471 test_loss: 0.09529 \n",
      "[ 53/200] train_loss: 0.07721 valid_loss: 0.08819 test_loss: 0.09337 \n",
      "[ 54/200] train_loss: 0.07906 valid_loss: 0.08345 test_loss: 0.12406 \n",
      "[ 55/200] train_loss: 0.07616 valid_loss: 0.08572 test_loss: 0.08984 \n",
      "[ 56/200] train_loss: 0.07552 valid_loss: 0.08535 test_loss: 0.08563 \n",
      "验证损失减少 (0.087022 --> 0.085635). 正在保存模型...\n",
      "[ 57/200] train_loss: 0.07580 valid_loss: 0.08416 test_loss: 0.09133 \n",
      "[ 58/200] train_loss: 0.07567 valid_loss: 0.08424 test_loss: 0.08862 \n",
      "[ 59/200] train_loss: 0.07372 valid_loss: 0.08436 test_loss: 0.08741 \n",
      "[ 60/200] train_loss: 0.07622 valid_loss: 0.08482 test_loss: 0.10021 \n",
      "[ 61/200] train_loss: 0.07508 valid_loss: 0.08257 test_loss: 0.17165 \n",
      "[ 62/200] train_loss: 0.07490 valid_loss: 0.08238 test_loss: 0.11451 \n",
      "[ 63/200] train_loss: 0.07531 valid_loss: 0.08332 test_loss: 0.21078 \n",
      "[ 64/200] train_loss: 0.07344 valid_loss: 0.08293 test_loss: 0.09200 \n",
      "[ 65/200] train_loss: 0.07506 valid_loss: 0.08304 test_loss: 0.10044 \n",
      "[ 66/200] train_loss: 0.07189 valid_loss: 0.08364 test_loss: 0.09839 \n",
      "[ 67/200] train_loss: 0.07343 valid_loss: 0.08523 test_loss: 0.08921 \n",
      "[ 68/200] train_loss: 0.07299 valid_loss: 0.08295 test_loss: 0.12273 \n",
      "[ 69/200] train_loss: 0.07126 valid_loss: 0.08385 test_loss: 0.09211 \n",
      "[ 70/200] train_loss: 0.07403 valid_loss: 0.08235 test_loss: 0.08998 \n",
      "[ 71/200] train_loss: 0.07196 valid_loss: 0.08389 test_loss: 0.09110 \n",
      "[ 72/200] train_loss: 0.07469 valid_loss: 0.08147 test_loss: 0.41469 \n",
      "[ 73/200] train_loss: 0.07120 valid_loss: 0.08436 test_loss: 0.38063 \n",
      "[ 74/200] train_loss: 0.07174 valid_loss: 0.08218 test_loss: 0.17722 \n",
      "[ 75/200] train_loss: 0.07023 valid_loss: 0.08085 test_loss: 0.36901 \n",
      "[ 76/200] train_loss: 0.07291 valid_loss: 0.08289 test_loss: 0.09720 \n",
      "[ 77/200] train_loss: 0.07113 valid_loss: 0.08277 test_loss: 0.09373 \n",
      "[ 78/200] train_loss: 0.07156 valid_loss: 0.08099 test_loss: 0.11456 \n",
      "[ 79/200] train_loss: 0.07057 valid_loss: 0.08330 test_loss: 0.25514 \n",
      "[ 80/200] train_loss: 0.06907 valid_loss: 0.08115 test_loss: 0.10390 \n",
      "[ 81/200] train_loss: 0.07025 valid_loss: 0.08057 test_loss: 0.13528 \n",
      "[ 82/200] train_loss: 0.07107 valid_loss: 0.08262 test_loss: 0.10179 \n",
      "[ 83/200] train_loss: 0.07051 valid_loss: 0.08132 test_loss: 0.09227 \n",
      "[ 84/200] train_loss: 0.07028 valid_loss: 0.08065 test_loss: 0.08996 \n",
      "[ 85/200] train_loss: 0.06864 valid_loss: 0.08252 test_loss: 0.09455 \n",
      "[ 86/200] train_loss: 0.06905 valid_loss: 0.08055 test_loss: 0.11001 \n",
      "[ 87/200] train_loss: 0.06863 valid_loss: 0.07970 test_loss: 0.09020 \n",
      "[ 88/200] train_loss: 0.06959 valid_loss: 0.08065 test_loss: 0.11782 \n",
      "[ 89/200] train_loss: 0.06730 valid_loss: 0.08111 test_loss: 0.09261 \n",
      "[ 90/200] train_loss: 0.07005 valid_loss: 0.07929 test_loss: 0.10450 \n",
      "[ 91/200] train_loss: 0.06769 valid_loss: 0.08068 test_loss: 0.11076 \n",
      "[ 92/200] train_loss: 0.06782 valid_loss: 0.08019 test_loss: 0.18261 \n",
      "[ 93/200] train_loss: 0.06746 valid_loss: 0.08034 test_loss: 0.14549 \n",
      "[ 94/200] train_loss: 0.06961 valid_loss: 0.08080 test_loss: 0.12470 \n",
      "[ 95/200] train_loss: 0.06810 valid_loss: 0.07899 test_loss: 0.25943 \n",
      "[ 96/200] train_loss: 0.06666 valid_loss: 0.07853 test_loss: 0.11837 \n",
      "[ 97/200] train_loss: 0.06775 valid_loss: 0.08007 test_loss: 0.17654 \n",
      "[ 98/200] train_loss: 0.06725 valid_loss: 0.08033 test_loss: 0.09423 \n",
      "[ 99/200] train_loss: 0.06533 valid_loss: 0.08018 test_loss: 0.09407 \n",
      "[100/200] train_loss: 0.06662 valid_loss: 0.07937 test_loss: 0.09324 \n",
      "[101/200] train_loss: 0.06754 valid_loss: 0.08171 test_loss: 0.16303 \n",
      "[102/200] train_loss: 0.06714 valid_loss: 0.07974 test_loss: 0.09211 \n",
      "[103/200] train_loss: 0.06651 valid_loss: 0.08117 test_loss: 0.09021 \n",
      "[104/200] train_loss: 0.06719 valid_loss: 0.08098 test_loss: 0.09290 \n",
      "[105/200] train_loss: 0.06516 valid_loss: 0.07953 test_loss: 0.09109 \n",
      "[106/200] train_loss: 0.06479 valid_loss: 0.07938 test_loss: 0.24895 \n",
      "[107/200] train_loss: 0.06446 valid_loss: 0.07963 test_loss: 0.12800 \n",
      "[108/200] train_loss: 0.06717 valid_loss: 0.08109 test_loss: 0.15107 \n",
      "[109/200] train_loss: 0.06713 valid_loss: 0.08041 test_loss: 0.09923 \n",
      "[110/200] train_loss: 0.06585 valid_loss: 0.07833 test_loss: 0.30459 \n",
      "[111/200] train_loss: 0.06582 valid_loss: 0.08041 test_loss: 0.09467 \n",
      "[112/200] train_loss: 0.06517 valid_loss: 0.08024 test_loss: 0.09259 \n",
      "[113/200] train_loss: 0.06604 valid_loss: 0.07952 test_loss: 0.10294 \n",
      "[114/200] train_loss: 0.06574 valid_loss: 0.07934 test_loss: 0.18659 \n",
      "[115/200] train_loss: 0.06487 valid_loss: 0.07822 test_loss: 0.09314 \n",
      "[116/200] train_loss: 0.06284 valid_loss: 0.07915 test_loss: 0.16688 \n",
      "[117/200] train_loss: 0.06419 valid_loss: 0.08049 test_loss: 0.10957 \n",
      "[118/200] train_loss: 0.06492 valid_loss: 0.07746 test_loss: 0.16802 \n",
      "[119/200] train_loss: 0.06553 valid_loss: 0.07954 test_loss: 0.10051 \n",
      "[120/200] train_loss: 0.06473 valid_loss: 0.07917 test_loss: 0.13973 \n",
      "[121/200] train_loss: 0.06499 valid_loss: 0.07916 test_loss: 0.17927 \n",
      "[122/200] train_loss: 0.06382 valid_loss: 0.07759 test_loss: 0.09317 \n",
      "[123/200] train_loss: 0.06394 valid_loss: 0.08186 test_loss: 0.09668 \n",
      "[124/200] train_loss: 0.06209 valid_loss: 0.07860 test_loss: 0.12428 \n",
      "[125/200] train_loss: 0.06466 valid_loss: 0.07847 test_loss: 0.08966 \n",
      "[126/200] train_loss: 0.06412 valid_loss: 0.07923 test_loss: 0.08814 \n",
      "[127/200] train_loss: 0.06348 valid_loss: 0.07730 test_loss: 0.08827 \n",
      "[128/200] train_loss: 0.06331 valid_loss: 0.07904 test_loss: 0.09101 \n",
      "[129/200] train_loss: 0.06387 valid_loss: 0.07758 test_loss: 0.16006 \n",
      "[130/200] train_loss: 0.06321 valid_loss: 0.07834 test_loss: 0.09542 \n",
      "[131/200] train_loss: 0.06248 valid_loss: 0.07780 test_loss: 0.10924 \n",
      "[132/200] train_loss: 0.06213 valid_loss: 0.07929 test_loss: 0.08966 \n",
      "[133/200] train_loss: 0.06247 valid_loss: 0.07822 test_loss: 0.10191 \n",
      "[134/200] train_loss: 0.06114 valid_loss: 0.07682 test_loss: 0.09509 \n",
      "[135/200] train_loss: 0.06177 valid_loss: 0.07682 test_loss: 0.09843 \n",
      "[136/200] train_loss: 0.06040 valid_loss: 0.08056 test_loss: 0.08836 \n",
      "[137/200] train_loss: 0.06119 valid_loss: 0.07840 test_loss: 0.09125 \n",
      "[138/200] train_loss: 0.06132 valid_loss: 0.07733 test_loss: 0.09010 \n",
      "[139/200] train_loss: 0.05964 valid_loss: 0.07905 test_loss: 0.09044 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[140/200] train_loss: 0.06082 valid_loss: 0.07734 test_loss: 0.21101 \n",
      "[141/200] train_loss: 0.06088 valid_loss: 0.07758 test_loss: 0.09183 \n",
      "[142/200] train_loss: 0.06233 valid_loss: 0.07829 test_loss: 0.09120 \n",
      "[143/200] train_loss: 0.06245 valid_loss: 0.07842 test_loss: 0.09353 \n",
      "[144/200] train_loss: 0.06138 valid_loss: 0.07880 test_loss: 0.20201 \n",
      "[145/200] train_loss: 0.06026 valid_loss: 0.07791 test_loss: 0.09395 \n",
      "[146/200] train_loss: 0.06101 valid_loss: 0.07730 test_loss: 0.09189 \n",
      "[147/200] train_loss: 0.06138 valid_loss: 0.07930 test_loss: 0.09441 \n",
      "[148/200] train_loss: 0.06170 valid_loss: 0.07848 test_loss: 0.09065 \n",
      "[149/200] train_loss: 0.05976 valid_loss: 0.08022 test_loss: 0.09357 \n",
      "[150/200] train_loss: 0.06122 valid_loss: 0.07924 test_loss: 0.09290 \n",
      "[151/200] train_loss: 0.06017 valid_loss: 0.07898 test_loss: 0.09497 \n",
      "[152/200] train_loss: 0.05859 valid_loss: 0.07878 test_loss: 0.09460 \n",
      "[153/200] train_loss: 0.05795 valid_loss: 0.07989 test_loss: 0.09256 \n",
      "[154/200] train_loss: 0.06067 valid_loss: 0.07811 test_loss: 0.09268 \n",
      "[155/200] train_loss: 0.05995 valid_loss: 0.07676 test_loss: 0.14872 \n",
      "[156/200] train_loss: 0.05994 valid_loss: 0.07804 test_loss: 0.09077 \n",
      "[157/200] train_loss: 0.06056 valid_loss: 0.07726 test_loss: 0.10421 \n",
      "[158/200] train_loss: 0.05879 valid_loss: 0.08123 test_loss: 0.08953 \n",
      "[159/200] train_loss: 0.05860 valid_loss: 0.07806 test_loss: 0.09041 \n",
      "[160/200] train_loss: 0.05982 valid_loss: 0.07902 test_loss: 0.09086 \n",
      "[161/200] train_loss: 0.06104 valid_loss: 0.07925 test_loss: 0.09071 \n",
      "[162/200] train_loss: 0.05944 valid_loss: 0.07970 test_loss: 0.09375 \n",
      "[163/200] train_loss: 0.05967 valid_loss: 0.07699 test_loss: 0.13912 \n",
      "[164/200] train_loss: 0.05786 valid_loss: 0.07802 test_loss: 0.09228 \n",
      "[165/200] train_loss: 0.05926 valid_loss: 0.07769 test_loss: 0.09611 \n",
      "[166/200] train_loss: 0.05964 valid_loss: 0.07794 test_loss: 0.13850 \n",
      "[167/200] train_loss: 0.05843 valid_loss: 0.07779 test_loss: 0.09884 \n",
      "[168/200] train_loss: 0.06005 valid_loss: 0.07727 test_loss: 0.09343 \n",
      "[169/200] train_loss: 0.05766 valid_loss: 0.07849 test_loss: 0.09597 \n",
      "[170/200] train_loss: 0.05860 valid_loss: 0.07801 test_loss: 0.09335 \n",
      "[171/200] train_loss: 0.05703 valid_loss: 0.07656 test_loss: 0.09580 \n",
      "[172/200] train_loss: 0.05757 valid_loss: 0.07797 test_loss: 0.09367 \n",
      "[173/200] train_loss: 0.05863 valid_loss: 0.07720 test_loss: 0.09845 \n",
      "[174/200] train_loss: 0.05940 valid_loss: 0.07845 test_loss: 0.10148 \n",
      "[175/200] train_loss: 0.05776 valid_loss: 0.07836 test_loss: 0.11741 \n",
      "[176/200] train_loss: 0.05710 valid_loss: 0.07788 test_loss: 0.09099 \n",
      "[177/200] train_loss: 0.05819 valid_loss: 0.07811 test_loss: 0.09324 \n",
      "[178/200] train_loss: 0.05821 valid_loss: 0.07970 test_loss: 0.09567 \n",
      "[179/200] train_loss: 0.05695 valid_loss: 0.07721 test_loss: 0.09267 \n",
      "[180/200] train_loss: 0.05704 valid_loss: 0.07716 test_loss: 0.09517 \n",
      "[181/200] train_loss: 0.05684 valid_loss: 0.07721 test_loss: 0.09276 \n",
      "[182/200] train_loss: 0.05725 valid_loss: 0.07805 test_loss: 0.14953 \n",
      "[183/200] train_loss: 0.05634 valid_loss: 0.07665 test_loss: 0.09911 \n",
      "[184/200] train_loss: 0.05682 valid_loss: 0.07724 test_loss: 0.09251 \n",
      "[185/200] train_loss: 0.05677 valid_loss: 0.07708 test_loss: 0.27731 \n",
      "[186/200] train_loss: 0.05629 valid_loss: 0.07882 test_loss: 0.09412 \n",
      "[187/200] train_loss: 0.05716 valid_loss: 0.07725 test_loss: 0.09803 \n",
      "[188/200] train_loss: 0.05639 valid_loss: 0.07599 test_loss: 0.09580 \n",
      "[189/200] train_loss: 0.05682 valid_loss: 0.07586 test_loss: 0.09957 \n",
      "[190/200] train_loss: 0.05871 valid_loss: 0.07667 test_loss: 0.11030 \n",
      "[191/200] train_loss: 0.05602 valid_loss: 0.07602 test_loss: 0.10750 \n",
      "[192/200] train_loss: 0.05381 valid_loss: 0.07578 test_loss: 0.16811 \n",
      "[193/200] train_loss: 0.05641 valid_loss: 0.07472 test_loss: 0.12653 \n",
      "[194/200] train_loss: 0.05523 valid_loss: 0.07570 test_loss: 0.10536 \n",
      "[195/200] train_loss: 0.05694 valid_loss: 0.07617 test_loss: 0.09204 \n",
      "[196/200] train_loss: 0.05511 valid_loss: 0.07547 test_loss: 0.17768 \n",
      "[197/200] train_loss: 0.05516 valid_loss: 0.07793 test_loss: 0.09004 \n",
      "[198/200] train_loss: 0.05554 valid_loss: 0.07754 test_loss: 0.19351 \n",
      "[199/200] train_loss: 0.05523 valid_loss: 0.07764 test_loss: 0.09493 \n",
      "[200/200] train_loss: 0.05431 valid_loss: 0.07689 test_loss: 0.09389 \n",
      "TRAINING MODEL 19\n",
      "[  1/200] train_loss: 0.40672 valid_loss: 0.28521 test_loss: 0.24873 \n",
      "验证损失减少 (inf --> 0.248728). 正在保存模型...\n",
      "[  2/200] train_loss: 0.21983 valid_loss: 0.20057 test_loss: 0.15682 \n",
      "验证损失减少 (0.285206 --> 0.156824). 正在保存模型...\n",
      "[  3/200] train_loss: 0.17090 valid_loss: 0.16672 test_loss: 0.13046 \n",
      "验证损失减少 (0.200574 --> 0.130461). 正在保存模型...\n",
      "[  4/200] train_loss: 0.15084 valid_loss: 0.14721 test_loss: 0.12189 \n",
      "验证损失减少 (0.166724 --> 0.121893). 正在保存模型...\n",
      "[  5/200] train_loss: 0.13663 valid_loss: 0.14331 test_loss: 0.12713 \n",
      "验证损失减少 (0.147210 --> 0.127131). 正在保存模型...\n",
      "[  6/200] train_loss: 0.13045 valid_loss: 0.13400 test_loss: 0.11382 \n",
      "验证损失减少 (0.143312 --> 0.113816). 正在保存模型...\n",
      "[  7/200] train_loss: 0.12634 valid_loss: 0.12971 test_loss: 0.11552 \n",
      "验证损失减少 (0.134003 --> 0.115515). 正在保存模型...\n",
      "[  8/200] train_loss: 0.12433 valid_loss: 0.12956 test_loss: 0.13665 \n",
      "[  9/200] train_loss: 0.11775 valid_loss: 0.12452 test_loss: 0.11225 \n",
      "验证损失减少 (0.129710 --> 0.112255). 正在保存模型...\n",
      "[ 10/200] train_loss: 0.11458 valid_loss: 0.12115 test_loss: 0.11202 \n",
      "验证损失减少 (0.124524 --> 0.112022). 正在保存模型...\n",
      "[ 11/200] train_loss: 0.11652 valid_loss: 0.11906 test_loss: 0.11161 \n",
      "验证损失减少 (0.121147 --> 0.111613). 正在保存模型...\n",
      "[ 12/200] train_loss: 0.11090 valid_loss: 0.11905 test_loss: 0.10897 \n",
      "验证损失减少 (0.119063 --> 0.108973). 正在保存模型...\n",
      "[ 13/200] train_loss: 0.11086 valid_loss: 0.11520 test_loss: 0.10412 \n",
      "验证损失减少 (0.119054 --> 0.104116). 正在保存模型...\n",
      "[ 14/200] train_loss: 0.11196 valid_loss: 0.11796 test_loss: 0.11327 \n",
      "验证损失减少 (0.115200 --> 0.113271). 正在保存模型...\n",
      "[ 15/200] train_loss: 0.10775 valid_loss: 0.11113 test_loss: 0.10642 \n",
      "验证损失减少 (0.117962 --> 0.106423). 正在保存模型...\n",
      "[ 16/200] train_loss: 0.10381 valid_loss: 0.11069 test_loss: 0.10486 \n",
      "验证损失减少 (0.111131 --> 0.104860). 正在保存模型...\n",
      "[ 17/200] train_loss: 0.10304 valid_loss: 0.11454 test_loss: 0.10848 \n",
      "验证损失减少 (0.110691 --> 0.108485). 正在保存模型...\n",
      "[ 18/200] train_loss: 0.10453 valid_loss: 0.10847 test_loss: 0.10084 \n",
      "验证损失减少 (0.114540 --> 0.100845). 正在保存模型...\n",
      "[ 19/200] train_loss: 0.10097 valid_loss: 0.10700 test_loss: 0.15730 \n",
      "[ 20/200] train_loss: 0.09983 valid_loss: 0.10795 test_loss: 0.10448 \n",
      "验证损失减少 (0.108466 --> 0.104478). 正在保存模型...\n",
      "[ 21/200] train_loss: 0.09717 valid_loss: 0.10614 test_loss: 0.10470 \n",
      "验证损失减少 (0.107951 --> 0.104696). 正在保存模型...\n",
      "[ 22/200] train_loss: 0.09816 valid_loss: 0.10287 test_loss: 0.10587 \n",
      "验证损失减少 (0.106144 --> 0.105867). 正在保存模型...\n",
      "[ 23/200] train_loss: 0.09831 valid_loss: 0.09925 test_loss: 0.20311 \n",
      "[ 24/200] train_loss: 0.09243 valid_loss: 0.10257 test_loss: 0.10045 \n",
      "验证损失减少 (0.102871 --> 0.100451). 正在保存模型...\n",
      "[ 25/200] train_loss: 0.09549 valid_loss: 0.10512 test_loss: 0.09763 \n",
      "验证损失减少 (0.102572 --> 0.097627). 正在保存模型...\n",
      "[ 26/200] train_loss: 0.09517 valid_loss: 0.10356 test_loss: 0.09997 \n",
      "验证损失减少 (0.105118 --> 0.099972). 正在保存模型...\n",
      "[ 27/200] train_loss: 0.09376 valid_loss: 0.10308 test_loss: 0.12664 \n",
      "[ 28/200] train_loss: 0.09236 valid_loss: 0.10071 test_loss: 0.09777 \n",
      "验证损失减少 (0.103557 --> 0.097767). 正在保存模型...\n",
      "[ 29/200] train_loss: 0.09374 valid_loss: 0.10073 test_loss: 0.18159 \n",
      "[ 30/200] train_loss: 0.09461 valid_loss: 0.09635 test_loss: 0.23856 \n",
      "[ 31/200] train_loss: 0.08972 valid_loss: 0.09904 test_loss: 0.11472 \n",
      "[ 32/200] train_loss: 0.09046 valid_loss: 0.09515 test_loss: 0.09669 \n",
      "验证损失减少 (0.100712 --> 0.096687). 正在保存模型...\n",
      "[ 33/200] train_loss: 0.08897 valid_loss: 0.09864 test_loss: 0.09689 \n",
      "[ 34/200] train_loss: 0.09064 valid_loss: 0.09463 test_loss: 0.09333 \n",
      "验证损失减少 (0.095147 --> 0.093325). 正在保存模型...\n",
      "[ 35/200] train_loss: 0.08624 valid_loss: 0.09601 test_loss: 0.11764 \n",
      "[ 36/200] train_loss: 0.08860 valid_loss: 0.09540 test_loss: 0.11391 \n",
      "[ 37/200] train_loss: 0.08534 valid_loss: 0.09347 test_loss: 0.23080 \n",
      "[ 38/200] train_loss: 0.08642 valid_loss: 0.09166 test_loss: 0.21265 \n",
      "[ 39/200] train_loss: 0.08487 valid_loss: 0.09306 test_loss: 0.10232 \n",
      "[ 40/200] train_loss: 0.08404 valid_loss: 0.09355 test_loss: 0.09539 \n",
      "[ 41/200] train_loss: 0.08407 valid_loss: 0.09476 test_loss: 0.12524 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42/200] train_loss: 0.08577 valid_loss: 0.09184 test_loss: 0.09894 \n",
      "[ 43/200] train_loss: 0.08554 valid_loss: 0.08962 test_loss: 0.11202 \n",
      "[ 44/200] train_loss: 0.08351 valid_loss: 0.09329 test_loss: 0.38401 \n",
      "[ 45/200] train_loss: 0.08365 valid_loss: 0.09083 test_loss: 0.17441 \n",
      "[ 46/200] train_loss: 0.08304 valid_loss: 0.09117 test_loss: 0.12248 \n",
      "[ 47/200] train_loss: 0.07899 valid_loss: 0.08967 test_loss: 0.12743 \n",
      "[ 48/200] train_loss: 0.08337 valid_loss: 0.08916 test_loss: 0.11242 \n",
      "[ 49/200] train_loss: 0.08057 valid_loss: 0.08967 test_loss: 0.10286 \n",
      "[ 50/200] train_loss: 0.08073 valid_loss: 0.08977 test_loss: 0.10312 \n",
      "[ 51/200] train_loss: 0.08047 valid_loss: 0.08796 test_loss: 0.10256 \n",
      "[ 52/200] train_loss: 0.07899 valid_loss: 0.08765 test_loss: 0.09895 \n",
      "[ 53/200] train_loss: 0.08080 valid_loss: 0.09041 test_loss: 0.09358 \n",
      "验证损失减少 (0.094635 --> 0.093581). 正在保存模型...\n",
      "[ 54/200] train_loss: 0.07739 valid_loss: 0.08645 test_loss: 0.11536 \n",
      "[ 55/200] train_loss: 0.07843 valid_loss: 0.08914 test_loss: 0.09613 \n",
      "[ 56/200] train_loss: 0.07800 valid_loss: 0.09068 test_loss: 0.09237 \n",
      "[ 57/200] train_loss: 0.07863 valid_loss: 0.08757 test_loss: 0.30652 \n",
      "[ 58/200] train_loss: 0.07883 valid_loss: 0.08817 test_loss: 0.23952 \n",
      "[ 59/200] train_loss: 0.07661 valid_loss: 0.08533 test_loss: 0.30785 \n",
      "[ 60/200] train_loss: 0.07794 valid_loss: 0.08870 test_loss: 0.09642 \n",
      "[ 61/200] train_loss: 0.07668 valid_loss: 0.08603 test_loss: 0.10729 \n",
      "[ 62/200] train_loss: 0.07864 valid_loss: 0.08763 test_loss: 0.17126 \n",
      "[ 63/200] train_loss: 0.07669 valid_loss: 0.09009 test_loss: 0.09478 \n",
      "[ 64/200] train_loss: 0.07398 valid_loss: 0.08871 test_loss: 0.09750 \n",
      "[ 65/200] train_loss: 0.07553 valid_loss: 0.08698 test_loss: 0.22130 \n",
      "[ 66/200] train_loss: 0.07440 valid_loss: 0.08753 test_loss: 0.09383 \n",
      "[ 67/200] train_loss: 0.07264 valid_loss: 0.08568 test_loss: 0.09740 \n",
      "[ 68/200] train_loss: 0.07585 valid_loss: 0.08475 test_loss: 0.09574 \n",
      "[ 69/200] train_loss: 0.07540 valid_loss: 0.08492 test_loss: 0.16518 \n",
      "[ 70/200] train_loss: 0.07376 valid_loss: 0.08452 test_loss: 0.45663 \n",
      "[ 71/200] train_loss: 0.07400 valid_loss: 0.08433 test_loss: 0.10006 \n",
      "[ 72/200] train_loss: 0.07496 valid_loss: 0.08276 test_loss: 0.35489 \n",
      "[ 73/200] train_loss: 0.07413 valid_loss: 0.08459 test_loss: 0.09570 \n",
      "[ 74/200] train_loss: 0.07291 valid_loss: 0.08745 test_loss: 0.29891 \n",
      "[ 75/200] train_loss: 0.07502 valid_loss: 0.08505 test_loss: 0.09601 \n",
      "[ 76/200] train_loss: 0.07408 valid_loss: 0.08421 test_loss: 0.09634 \n",
      "[ 77/200] train_loss: 0.07152 valid_loss: 0.08543 test_loss: 0.09996 \n",
      "[ 78/200] train_loss: 0.07118 valid_loss: 0.08457 test_loss: 0.09420 \n",
      "[ 79/200] train_loss: 0.07294 valid_loss: 0.08430 test_loss: 0.12377 \n",
      "[ 80/200] train_loss: 0.07125 valid_loss: 0.08447 test_loss: 0.13059 \n",
      "[ 81/200] train_loss: 0.07510 valid_loss: 0.08437 test_loss: 0.09375 \n",
      "[ 82/200] train_loss: 0.06938 valid_loss: 0.08550 test_loss: 0.09999 \n",
      "[ 83/200] train_loss: 0.07205 valid_loss: 0.08206 test_loss: 0.15225 \n",
      "[ 84/200] train_loss: 0.06881 valid_loss: 0.08239 test_loss: 0.09020 \n",
      "验证损失减少 (0.090413 --> 0.090196). 正在保存模型...\n",
      "[ 85/200] train_loss: 0.07147 valid_loss: 0.08318 test_loss: 0.09260 \n",
      "[ 86/200] train_loss: 0.07191 valid_loss: 0.08369 test_loss: 0.15639 \n",
      "[ 87/200] train_loss: 0.07134 valid_loss: 0.08346 test_loss: 0.09949 \n",
      "[ 88/200] train_loss: 0.06901 valid_loss: 0.08189 test_loss: 0.13449 \n",
      "[ 89/200] train_loss: 0.06936 valid_loss: 0.08220 test_loss: 0.09415 \n",
      "[ 90/200] train_loss: 0.07111 valid_loss: 0.08391 test_loss: 0.09018 \n",
      "[ 91/200] train_loss: 0.07059 valid_loss: 0.08211 test_loss: 0.09584 \n",
      "[ 92/200] train_loss: 0.06907 valid_loss: 0.08212 test_loss: 0.10053 \n",
      "[ 93/200] train_loss: 0.06933 valid_loss: 0.08237 test_loss: 0.09040 \n",
      "[ 94/200] train_loss: 0.06889 valid_loss: 0.08276 test_loss: 0.08749 \n",
      "[ 95/200] train_loss: 0.06894 valid_loss: 0.08209 test_loss: 0.08883 \n",
      "[ 96/200] train_loss: 0.06959 valid_loss: 0.08085 test_loss: 0.11046 \n",
      "[ 97/200] train_loss: 0.06765 valid_loss: 0.08073 test_loss: 0.25368 \n",
      "[ 98/200] train_loss: 0.06748 valid_loss: 0.08491 test_loss: 0.16982 \n",
      "[ 99/200] train_loss: 0.06823 valid_loss: 0.08209 test_loss: 0.09444 \n",
      "[100/200] train_loss: 0.06835 valid_loss: 0.07947 test_loss: 0.10069 \n",
      "[101/200] train_loss: 0.06636 valid_loss: 0.08014 test_loss: 0.08917 \n",
      "[102/200] train_loss: 0.06804 valid_loss: 0.08239 test_loss: 0.11710 \n",
      "[103/200] train_loss: 0.06815 valid_loss: 0.08013 test_loss: 0.09233 \n",
      "[104/200] train_loss: 0.06711 valid_loss: 0.08009 test_loss: 0.09729 \n",
      "[105/200] train_loss: 0.06616 valid_loss: 0.07910 test_loss: 0.35942 \n",
      "[106/200] train_loss: 0.06653 valid_loss: 0.07944 test_loss: 0.35143 \n",
      "[107/200] train_loss: 0.06557 valid_loss: 0.07991 test_loss: 0.13807 \n",
      "[108/200] train_loss: 0.06702 valid_loss: 0.07923 test_loss: 0.12711 \n",
      "[109/200] train_loss: 0.06599 valid_loss: 0.07823 test_loss: 0.19221 \n",
      "[110/200] train_loss: 0.06685 valid_loss: 0.07986 test_loss: 0.17158 \n",
      "[111/200] train_loss: 0.06522 valid_loss: 0.07942 test_loss: 0.09853 \n",
      "[112/200] train_loss: 0.06942 valid_loss: 0.07868 test_loss: 0.18394 \n",
      "[113/200] train_loss: 0.06622 valid_loss: 0.08118 test_loss: 0.11852 \n",
      "[114/200] train_loss: 0.06454 valid_loss: 0.08100 test_loss: 0.16258 \n",
      "[115/200] train_loss: 0.06503 valid_loss: 0.07924 test_loss: 0.18766 \n",
      "[116/200] train_loss: 0.06559 valid_loss: 0.07840 test_loss: 0.37684 \n",
      "[117/200] train_loss: 0.06472 valid_loss: 0.07890 test_loss: 0.35876 \n",
      "[118/200] train_loss: 0.06418 valid_loss: 0.08023 test_loss: 0.39623 \n",
      "[119/200] train_loss: 0.06474 valid_loss: 0.07952 test_loss: 0.09625 \n",
      "[120/200] train_loss: 0.06486 valid_loss: 0.07917 test_loss: 0.09061 \n",
      "[121/200] train_loss: 0.06469 valid_loss: 0.07987 test_loss: 0.11218 \n",
      "[122/200] train_loss: 0.06376 valid_loss: 0.07730 test_loss: 0.14138 \n",
      "[123/200] train_loss: 0.06483 valid_loss: 0.08012 test_loss: 0.58909 \n",
      "[124/200] train_loss: 0.06406 valid_loss: 0.07727 test_loss: 0.10157 \n",
      "[125/200] train_loss: 0.06489 valid_loss: 0.07961 test_loss: 0.17115 \n",
      "[126/200] train_loss: 0.06416 valid_loss: 0.07868 test_loss: 0.20186 \n",
      "[127/200] train_loss: 0.06523 valid_loss: 0.08172 test_loss: 0.09368 \n",
      "[128/200] train_loss: 0.06357 valid_loss: 0.07858 test_loss: 0.16908 \n",
      "[129/200] train_loss: 0.06497 valid_loss: 0.07817 test_loss: 0.09644 \n",
      "[130/200] train_loss: 0.06161 valid_loss: 0.07879 test_loss: 0.21437 \n",
      "[131/200] train_loss: 0.06259 valid_loss: 0.07971 test_loss: 0.09599 \n",
      "[132/200] train_loss: 0.06236 valid_loss: 0.07807 test_loss: 0.08901 \n",
      "[133/200] train_loss: 0.06279 valid_loss: 0.07893 test_loss: 0.08988 \n",
      "[134/200] train_loss: 0.06331 valid_loss: 0.07696 test_loss: 0.09047 \n",
      "[135/200] train_loss: 0.06269 valid_loss: 0.07896 test_loss: 0.08775 \n",
      "[136/200] train_loss: 0.06393 valid_loss: 0.07995 test_loss: 0.09313 \n",
      "[137/200] train_loss: 0.06172 valid_loss: 0.07969 test_loss: 0.09035 \n",
      "[138/200] train_loss: 0.06271 valid_loss: 0.08116 test_loss: 0.09463 \n",
      "[139/200] train_loss: 0.06243 valid_loss: 0.08047 test_loss: 0.09215 \n",
      "[140/200] train_loss: 0.06154 valid_loss: 0.07767 test_loss: 0.09254 \n",
      "[141/200] train_loss: 0.06070 valid_loss: 0.07742 test_loss: 0.09211 \n",
      "[142/200] train_loss: 0.06345 valid_loss: 0.07972 test_loss: 0.09673 \n",
      "[143/200] train_loss: 0.06158 valid_loss: 0.07894 test_loss: 0.10628 \n",
      "[144/200] train_loss: 0.06167 valid_loss: 0.07752 test_loss: 0.09584 \n",
      "[145/200] train_loss: 0.06063 valid_loss: 0.07709 test_loss: 0.09691 \n",
      "[146/200] train_loss: 0.06260 valid_loss: 0.07760 test_loss: 0.16902 \n",
      "[147/200] train_loss: 0.06261 valid_loss: 0.07763 test_loss: 0.09190 \n",
      "[148/200] train_loss: 0.06208 valid_loss: 0.07575 test_loss: 0.10131 \n",
      "[149/200] train_loss: 0.06117 valid_loss: 0.07713 test_loss: 0.08994 \n",
      "[150/200] train_loss: 0.06155 valid_loss: 0.07677 test_loss: 0.09486 \n",
      "[151/200] train_loss: 0.06144 valid_loss: 0.07721 test_loss: 0.11115 \n",
      "[152/200] train_loss: 0.06164 valid_loss: 0.07786 test_loss: 0.09945 \n",
      "[153/200] train_loss: 0.06081 valid_loss: 0.07671 test_loss: 0.09177 \n",
      "[154/200] train_loss: 0.05992 valid_loss: 0.07622 test_loss: 0.09307 \n",
      "[155/200] train_loss: 0.06033 valid_loss: 0.07550 test_loss: 0.10408 \n",
      "[156/200] train_loss: 0.05872 valid_loss: 0.07574 test_loss: 0.09076 \n",
      "[157/200] train_loss: 0.06156 valid_loss: 0.07782 test_loss: 0.09130 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158/200] train_loss: 0.05947 valid_loss: 0.07586 test_loss: 0.09022 \n",
      "[159/200] train_loss: 0.06038 valid_loss: 0.07932 test_loss: 0.09264 \n",
      "[160/200] train_loss: 0.05915 valid_loss: 0.07868 test_loss: 0.09139 \n",
      "[161/200] train_loss: 0.06124 valid_loss: 0.07602 test_loss: 0.08998 \n",
      "[162/200] train_loss: 0.05936 valid_loss: 0.07661 test_loss: 0.09194 \n",
      "[163/200] train_loss: 0.05958 valid_loss: 0.07756 test_loss: 0.09264 \n",
      "[164/200] train_loss: 0.06007 valid_loss: 0.07541 test_loss: 0.09185 \n",
      "[165/200] train_loss: 0.06044 valid_loss: 0.07544 test_loss: 0.14273 \n",
      "[166/200] train_loss: 0.05885 valid_loss: 0.07889 test_loss: 0.09226 \n",
      "[167/200] train_loss: 0.05842 valid_loss: 0.07701 test_loss: 0.09141 \n",
      "[168/200] train_loss: 0.05987 valid_loss: 0.07659 test_loss: 0.09215 \n",
      "[169/200] train_loss: 0.05973 valid_loss: 0.07772 test_loss: 0.09280 \n",
      "[170/200] train_loss: 0.06078 valid_loss: 0.07832 test_loss: 0.15935 \n",
      "[171/200] train_loss: 0.05854 valid_loss: 0.08095 test_loss: 0.09279 \n",
      "[172/200] train_loss: 0.05939 valid_loss: 0.07774 test_loss: 0.09070 \n",
      "[173/200] train_loss: 0.05841 valid_loss: 0.07994 test_loss: 0.09248 \n",
      "[174/200] train_loss: 0.05909 valid_loss: 0.07632 test_loss: 0.09242 \n",
      "[175/200] train_loss: 0.05836 valid_loss: 0.07839 test_loss: 0.09668 \n",
      "[176/200] train_loss: 0.05896 valid_loss: 0.07915 test_loss: 0.09331 \n",
      "[177/200] train_loss: 0.05800 valid_loss: 0.07623 test_loss: 0.11096 \n",
      "[178/200] train_loss: 0.05889 valid_loss: 0.07887 test_loss: 0.09924 \n",
      "[179/200] train_loss: 0.05783 valid_loss: 0.07799 test_loss: 0.10589 \n",
      "[180/200] train_loss: 0.05861 valid_loss: 0.07863 test_loss: 0.09428 \n",
      "[181/200] train_loss: 0.05755 valid_loss: 0.07973 test_loss: 0.09047 \n",
      "[182/200] train_loss: 0.05912 valid_loss: 0.07929 test_loss: 0.09878 \n",
      "[183/200] train_loss: 0.05819 valid_loss: 0.07721 test_loss: 0.10886 \n",
      "[184/200] train_loss: 0.05690 valid_loss: 0.07671 test_loss: 0.09055 \n",
      "[185/200] train_loss: 0.05594 valid_loss: 0.07533 test_loss: 0.27210 \n",
      "[186/200] train_loss: 0.05735 valid_loss: 0.07564 test_loss: 0.22254 \n",
      "[187/200] train_loss: 0.05748 valid_loss: 0.07602 test_loss: 0.09241 \n",
      "[188/200] train_loss: 0.05667 valid_loss: 0.07761 test_loss: 0.09419 \n",
      "[189/200] train_loss: 0.05675 valid_loss: 0.07675 test_loss: 0.14270 \n",
      "[190/200] train_loss: 0.05720 valid_loss: 0.07868 test_loss: 0.10032 \n",
      "[191/200] train_loss: 0.05539 valid_loss: 0.07928 test_loss: 0.12290 \n",
      "[192/200] train_loss: 0.05673 valid_loss: 0.07628 test_loss: 0.09728 \n",
      "[193/200] train_loss: 0.05588 valid_loss: 0.07580 test_loss: 0.09446 \n",
      "[194/200] train_loss: 0.05670 valid_loss: 0.07711 test_loss: 0.09360 \n",
      "[195/200] train_loss: 0.05695 valid_loss: 0.07802 test_loss: 0.12204 \n",
      "[196/200] train_loss: 0.05562 valid_loss: 0.07861 test_loss: 0.09494 \n",
      "[197/200] train_loss: 0.05719 valid_loss: 0.07837 test_loss: 0.45390 \n",
      "[198/200] train_loss: 0.05562 valid_loss: 0.07804 test_loss: 0.09992 \n",
      "[199/200] train_loss: 0.05625 valid_loss: 0.07752 test_loss: 0.10059 \n",
      "[200/200] train_loss: 0.05641 valid_loss: 0.07896 test_loss: 0.09212 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = 200\n",
    "\n",
    "train_loader = dl_train_unseen\n",
    "valid_loader = dl_valid_unseen\n",
    "test_loader = dl_test_unseen\n",
    "\n",
    "#i = 0\n",
    "for i in range(20):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model =PTPNet(1,3,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-4)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    fn = 'UKDALE_unseen_01_10%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7abd295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x266b088d220>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, color='blue', label='train loss')\n",
    "plt.plot(valid_loss, color='green', label='valid loss')\n",
    "plt.plot(test_loss, color='red', label='test loss')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "295ecfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PTPNet(\n",
       "  (encoder1): Encoder(\n",
       "    (conv): Conv1d(1, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool1_1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (encoder2): Encoder(\n",
       "    (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool2_1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (encoder3): Encoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (pool3_1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (encoder4): Encoder(\n",
       "    (conv): Conv1d(128, 320, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (EMA): EMA(\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (agp): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (pool_h): AdaptiveAvgPool2d(output_size=(None, 1))\n",
       "    (pool_w): AdaptiveAvgPool2d(output_size=(1, None))\n",
       "    (gn): GroupNorm(2, 2, eps=1e-05, affine=True)\n",
       "    (conv1x1): Conv2d(2, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv3x3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (se): SKAttention(\n",
       "    (convs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (conv): Conv2d(16, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fcs): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (1): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (2): Linear(in_features=32, out_features=16, bias=True)\n",
       "      (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       "  (tpool1): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(320, 128, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool2): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(15,), stride=(15,), padding=(0,))\n",
       "    (conv): Conv1d(320, 128, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool3): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(15,), stride=(15,), padding=(0,))\n",
       "    (conv): Conv1d(320, 128, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool4): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(15,), stride=(15,), padding=(0,))\n",
       "    (conv): Conv1d(320, 128, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool5): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(320, 128, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sp1): StripPooling(\n",
       "    (pool1): AdaptiveAvgPool2d(output_size=20)\n",
       "    (pool2): AdaptiveAvgPool2d(output_size=12)\n",
       "    (pool3): AdaptiveAvgPool2d(output_size=(1, None))\n",
       "    (pool4): AdaptiveAvgPool2d(output_size=(None, 1))\n",
       "    (conv1_1): Sequential(\n",
       "      (0): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv1_2): Sequential(\n",
       "      (0): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_0): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2_1): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2_2): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2_3): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2_4): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv2_5): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_6): Sequential(\n",
       "      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): ConvTranspose1d(960, 32, kernel_size=(8,), stride=(8,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (activation): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PTPNet(1,3,32).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee30182",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为model的神经网络模型。该模型是一个PTPNet模型，输入维度为1，输出维度为3，隐藏层维度为32。通过调用.cuda()方法，将模型移动到GPU上进行计算。接下来，代码调用model.eval()将模型设置为评估模式，表示在进行推理时不进行梯度计算和参数更新。\n",
    "\n",
    "总结：这段代码定义了一个PTPNet模型，并将其移动到GPU上进行评估模式的设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c701e47e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_seen_01_010.pth\n",
      "./UKDALE_seen_01_011.pth\n",
      "./UKDALE_seen_01_012.pth\n",
      "./UKDALE_seen_01_013.pth\n",
      "./UKDALE_seen_01_014.pth\n",
      "./UKDALE_seen_01_015.pth\n",
      "./UKDALE_seen_01_016.pth\n",
      "./UKDALE_seen_01_017.pth\n",
      "./UKDALE_seen_01_018.pth\n",
      "./UKDALE_seen_01_019.pth\n",
      "./UKDALE_seen_01_0110.pth\n",
      "./UKDALE_seen_01_0111.pth\n",
      "./UKDALE_seen_01_0112.pth\n",
      "./UKDALE_seen_01_0113.pth\n",
      "./UKDALE_seen_01_0114.pth\n",
      "./UKDALE_seen_01_0115.pth\n",
      "./UKDALE_seen_01_0116.pth\n",
      "./UKDALE_seen_01_0117.pth\n",
      "./UKDALE_seen_01_0118.pth\n",
      "./UKDALE_seen_01_0119.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.890 (0.881, 0.896)\n",
      "Precision : 0.894 (0.883, 0.898)\n",
      "Recall    : 0.887 (0.870, 0.898)\n",
      "Accuracy  : 0.901 (0.894, 0.905)\n",
      "MCC       : 0.800 (0.785, 0.809)\n",
      "MAE       : 13.44 (12.98, 13.70)\n",
      "SAE       : -0.010 (-0.028, 0.005)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.935 (0.903, 0.958)\n",
      "Precision : 0.918 (0.840, 0.952)\n",
      "Recall    : 0.953 (0.924, 0.969)\n",
      "Accuracy  : 0.997 (0.995, 0.998)\n",
      "MCC       : 0.934 (0.903, 0.957)\n",
      "MAE       : 21.00 (20.22, 21.57)\n",
      "SAE       : 0.020 (-0.029, 0.059)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.985 (0.980, 0.988)\n",
      "Precision : 0.982 (0.969, 0.990)\n",
      "Recall    : 0.987 (0.975, 0.991)\n",
      "Accuracy  : 0.998 (0.997, 0.998)\n",
      "MCC       : 0.984 (0.979, 0.987)\n",
      "MAE       : 41.59 (41.18, 41.86)\n",
      "SAE       : -0.078 (-0.094, -0.071)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "for i in range(20):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './UKDALE_seen_01_01%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        \n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[0], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[0], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[0], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[0], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[0], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[0], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[0], sorted(scores[i]['SAE'])[18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "011f5967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_unseen_01_100.pth\n",
      "./UKDALE_unseen_01_101.pth\n",
      "./UKDALE_unseen_01_102.pth\n",
      "./UKDALE_unseen_01_103.pth\n",
      "./UKDALE_unseen_01_104.pth\n",
      "./UKDALE_unseen_01_105.pth\n",
      "./UKDALE_unseen_01_106.pth\n",
      "./UKDALE_unseen_01_107.pth\n",
      "./UKDALE_unseen_01_108.pth\n",
      "./UKDALE_unseen_01_109.pth\n",
      "./UKDALE_unseen_01_1010.pth\n",
      "./UKDALE_unseen_01_1011.pth\n",
      "./UKDALE_unseen_01_1012.pth\n",
      "./UKDALE_unseen_01_1013.pth\n",
      "./UKDALE_unseen_01_1014.pth\n",
      "./UKDALE_unseen_01_1015.pth\n",
      "./UKDALE_unseen_01_1016.pth\n",
      "./UKDALE_unseen_01_1017.pth\n",
      "./UKDALE_unseen_01_1018.pth\n",
      "./UKDALE_unseen_01_1019.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.865 (0.855, 0.872)\n",
      "Precision : 0.879 (0.866, 0.887)\n",
      "Recall    : 0.851 (0.833, 0.872)\n",
      "Accuracy  : 0.899 (0.893, 0.904)\n",
      "MCC       : 0.785 (0.770, 0.795)\n",
      "MAE       : 17.56 (17.20, 17.83)\n",
      "SAE       : -0.032 (-0.061, -0.002)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.784 (0.746, 0.805)\n",
      "Precision : 0.747 (0.709, 0.795)\n",
      "Recall    : 0.829 (0.749, 0.887)\n",
      "Accuracy  : 0.987 (0.985, 0.988)\n",
      "MCC       : 0.779 (0.738, 0.801)\n",
      "MAE       : 35.54 (32.50, 37.52)\n",
      "SAE       : 0.112 (-0.014, 0.244)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.813 (0.659, 0.885)\n",
      "Precision : 0.857 (0.615, 0.921)\n",
      "Recall    : 0.780 (0.610, 0.908)\n",
      "Accuracy  : 0.996 (0.992, 0.997)\n",
      "MCC       : 0.814 (0.656, 0.884)\n",
      "MAE       : 8.22 (7.66, 9.46)\n",
      "SAE       : -0.081 (-0.300, 0.151)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for i in range(20):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_unseen_%d.pth' %i\n",
    "    filename = './UKDALE_unseen_01_10%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        pm = ds_appliance[1][APPLIANCE[a]].sum() / ds_status[1][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[0], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[0], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[0], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[0], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[0], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[0], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[0], sorted(scores[i]['SAE'])[18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78c1af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_seen_11_11100.pth\n",
      "./UKDALE_seen_11_11101.pth\n",
      "./UKDALE_seen_11_11102.pth\n",
      "./UKDALE_seen_11_11103.pth\n",
      "./UKDALE_seen_11_11104.pth\n",
      "./UKDALE_seen_11_11105.pth\n",
      "./UKDALE_seen_11_11106.pth\n",
      "./UKDALE_seen_11_11107.pth\n",
      "./UKDALE_seen_11_11108.pth\n",
      "./UKDALE_seen_11_11109.pth\n",
      "./UKDALE_seen_11_111010.pth\n",
      "./UKDALE_seen_11_111011.pth\n",
      "./UKDALE_seen_11_111012.pth\n",
      "./UKDALE_seen_11_111013.pth\n",
      "./UKDALE_seen_11_111014.pth\n",
      "./UKDALE_seen_11_111015.pth\n",
      "./UKDALE_seen_11_111016.pth\n",
      "./UKDALE_seen_11_111017.pth\n",
      "./UKDALE_seen_11_111018.pth\n",
      "./UKDALE_seen_11_111019.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.899 (0.895, 0.902)\n",
      "Precision : 0.902 (0.896, 0.907)\n",
      "Recall    : 0.895 (0.890, 0.902)\n",
      "Accuracy  : 0.908 (0.905, 0.911)\n",
      "MCC       : 0.815 (0.809, 0.821)\n",
      "MAE       : 12.77 (12.52, 13.03)\n",
      "SAE       : -0.009 (-0.021, 0.006)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.943 (0.921, 0.966)\n",
      "Precision : 0.938 (0.896, 0.965)\n",
      "Recall    : 0.948 (0.923, 0.967)\n",
      "Accuracy  : 0.997 (0.996, 0.998)\n",
      "MCC       : 0.941 (0.919, 0.965)\n",
      "MAE       : 20.58 (20.14, 21.33)\n",
      "SAE       : -0.008 (-0.051, 0.048)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.985 (0.981, 0.988)\n",
      "Precision : 0.983 (0.974, 0.989)\n",
      "Recall    : 0.987 (0.981, 0.990)\n",
      "Accuracy  : 0.998 (0.997, 0.998)\n",
      "MCC       : 0.984 (0.979, 0.987)\n",
      "MAE       : 41.64 (41.38, 41.95)\n",
      "SAE       : -0.079 (-0.088, -0.068)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.5\n",
    "for i in range(20):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './UKDALE_seen_11_1110%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        \n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4063a",
   "metadata": {},
   "source": [
    " 这段代码主要是为了评估一个模型在多个房屋的表现。首先，它创建了一个名为scores的字典，其中包含了每个房屋的评价指标，如F1得分、精度、召回率、准确率、MCC、MAE和SAE。然后，它对20个不同的模型进行了评估，每个模型分别对应一个.pth文件。对于每个模型和每个房屋，它计算了相应的预测结果，并将这些结果与真实结果进行比较，从而计算出每个房屋的评价指标。最后，它输出了每个房屋的平均评价指标，并显示了每个指标的最小值和最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2be47a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "966ed11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc66002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.08573913574219\n",
      "654.533203125\n",
      "489.18988037109375\n"
     ]
    }
   ],
   "source": [
    "mean = {}\n",
    "\n",
    "for a in range(len(APPLIANCE)):\n",
    "    data = ds_appliance[0][APPLIANCE[a]]\n",
    "    non_zero_data = data[data != 0]\n",
    "    mean[a] = np.mean(non_zero_data)\n",
    "    print(mean[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17476822",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (20,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappliance_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 每种a值对应的F1分值折线\u001b[39;00m\n\u001b[0;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# 设置横坐标刻度为2一格\u001b[39;00m\n\u001b[0;32m     19\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\ruanjian\\Anaconda\\envs\\nilmtk-env\\lib\\site-packages\\matplotlib\\pyplot.py:2794\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2792\u001b[0m \u001b[38;5;129m@docstring\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   2793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 2794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2795\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m   2796\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ruanjian\\Anaconda\\envs\\nilmtk-env\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1665\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1436\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1664\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D\u001b[38;5;241m.\u001b[39m_alias_map)\n\u001b[1;32m-> 1665\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32mD:\\ruanjian\\Anaconda\\envs\\nilmtk-env\\lib\\site-packages\\matplotlib\\axes\\_base.py:225\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    224\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\ruanjian\\Anaconda\\envs\\nilmtk-env\\lib\\site-packages\\matplotlib\\axes\\_base.py:391\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m index_of(tup[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 391\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_xy_from_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    394\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_makeline\n",
      "File \u001b[1;32mD:\\ruanjian\\Anaconda\\envs\\nilmtk-env\\lib\\site-packages\\matplotlib\\axes\\_base.py:269\u001b[0m, in \u001b[0;36m_process_plot_var_args._xy_from_xy\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    267\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_1d(y)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2-D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (20,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化数据\n",
    "a_values = [0, 1, 2]\n",
    "f1_scores = [[] for _ in range(3)]  # 用于存储三种a值对应的F1分值\n",
    "\n",
    "# 从scores字典中提取F1分值\n",
    "for a in range(3):\n",
    "    f1_scores[a] = scores[a]['F1']\n",
    "\n",
    "# 家电设备名称\n",
    "appliance_labels = ['Fridge', 'Dish Washer', 'Washing Machine']\n",
    "\n",
    "# 绘制折线图\n",
    "plt.figure(figsize=(8, 6))\n",
    "for a in range(3):\n",
    "    plt.plot(range(20), f1_scores[a], label=appliance_labels[a])  # 每种a值对应的F1分值折线\n",
    "plt.xticks(range(0, 21, 2))  # 设置横坐标刻度为2一格\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 Score vs Iteration for Different Appliances\")\n",
    "plt.legend()\n",
    "# 设置分辨率为600ppi\n",
    "dpi = 600\n",
    "\n",
    "# 选择文件路径和文件名，保存为PNG格式\n",
    "save_path = r'D:\\NILM\\绘图\\F1折线图.png'\n",
    "\n",
    "# 导出图片\n",
    "#plt.savefig(save_path, dpi=dpi)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "302c837a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\ruanjian\\anaconda\\lib\\site-packages (8.4.0)\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)'))) - skipping\n"
     ]
    }
   ],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb38b91f",
   "metadata": {},
   "source": [
    "x_true: 包含了模型输入数据经过处理后的真实值。在该代码中，x_true 是一个一维数组，保存了经过处理后的输入数据 x 的值。\n",
    "\n",
    "p_true: 包含了模型在数据加载器上对应位置的真实标签。在该代码中，p_true 是一个一维数组，保存了真实的标签值 p。\n",
    "\n",
    "s_true: 包含了模型在数据加载器上对应位置的真实输出值。在该代码中，s_true 是一个一维数组，保存了真实的输出值 s。\n",
    "\n",
    "s_hat: 包含了模型在数据加载器上对应位置的预测输出值。在该代码中，s_hat 是一个一维数组，保存了模型对应位置的预测输出值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "231a6985",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKDALE_seen_01_0116.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "thr = 0.5\n",
    "for i in range(1):\n",
    "    filename = 'UKDALE_seen_01_01%d.pth' %(i+16)\n",
    "    #filename =r'E:\\jupyter\\TPNILM-master\\TPNILM-1108\\UKDALE_seen_11_1110%d.pth' %(i+9)\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):#1为fridge，2为dish_washer，3为washing_machine\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = s_hat+0.5\n",
    "        #scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        #scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        #scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        #scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        #scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        #scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        #scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        # 绘制折线图\n",
    "        #plt.plot(s_true[:1000], label='s_true')\n",
    "        #plt.plot(p_true[:5000], label='p_true')\n",
    "        #plt.plot(p_hat[:1000], label='p_hat')\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib\n",
    "\n",
    "        # 设置字体和大小\n",
    "        matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 中文标签\n",
    "        matplotlib.rcParams['font.size'] = 8\n",
    "        matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "        matplotlib.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "        # 假设 s_true 和 p_hat 是您的数据\n",
    "        # 假设 mean 是您使用的平均值数组\n",
    "\n",
    "        # 设置图表尺寸\n",
    "        plt.figure(figsize=(4.13, 3.15))  # 半栏A4宽度约4.13英寸\n",
    "\n",
    "        # 根据 a 的值绘制不同的设备\n",
    "        if a == 0:\n",
    "            device_name = \"Fridge\"\n",
    "            plt.plot(s_true[:600], label='t_value', linestyle='-', color='blue')  # 使用实线和正方形标记，并指定颜色为蓝色\n",
    "            plt.plot(p_hat[:600], label='p_value', linestyle='-.', color='red')  # 使用点划线和星形标记，并指定颜色为红色\n",
    "        elif a == 1:\n",
    "            device_name = \"Dish Washer\"\n",
    "            plt.plot(s_true[:], label='t_value', linestyle='-', color='blue')\n",
    "            plt.plot(p_hat[:], label='p_value', linestyle='-.',color='red')\n",
    "        elif a == 2:\n",
    "            device_name = \"Washing Machine\"\n",
    "            s_true[0:100] = 0\n",
    "            plt.plot(s_true[:8000], label='t_value', linestyle='-', color='blue')  # 使用实线和正方形标记，并指定颜色为蓝色\n",
    "            plt.plot(p_hat[:8000], label='p_value', linestyle='-.', color='red')  # 使用点划线和星形标记，并指定颜色为红色\n",
    "        else:\n",
    "            device_name = \"Appliance \" + str(a)\n",
    "\n",
    "        # 添加标题、x轴标签和y轴标签\n",
    "        plt.title(device_name)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Value\")\n",
    "\n",
    "        # 添加图例\n",
    "        plt.legend()\n",
    "\n",
    "        # 设置分辨率为600ppi\n",
    "        dpi = 600\n",
    "\n",
    "        # 选择文件路径和文件名，保存为PNG格式\n",
    "        temp_save_path = fr'D:\\NILM\\小论文\\SKNILM\\绘图\\{device_name}图(3).png'\n",
    "        final_save_path = fr'D:\\NILM\\小论文\\SKNILM\\绘图\\{device_name}图(2000).png'\n",
    "\n",
    "        # 导出图片\n",
    "        plt.savefig(temp_save_path, dpi=dpi, format='png')\n",
    "\n",
    "        # 使用Pillow转换为灰度图\n",
    "        #img = Image.open(temp_save_path)\n",
    "        #gray_img = img.convert('L')\n",
    "        #gray_img.save(final_save_path)\n",
    "        # 显示图形\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cb082db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKDALE_unseen_01_1016.pth\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 413x315 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "thr = 0.5\n",
    "for i in range(1):\n",
    "    filename = 'UKDALE_unseen_01_10%d.pth' %(i+16)\n",
    "    #filename =r'E:\\jupyter\\TPNILM-master\\TPNILM-1108\\UKDALE_seen_11_1110%d.pth' %(i+9)\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):#1为fridge，2为dish_washer，3为washing_machine\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        #计算了一个家电设备的平均功率（pm），并将其除以最大功率（MAX_POWER）进行归一化处理。\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = s_hat+0.5\n",
    "        #scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        #scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        #scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        #scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        #scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        #scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        #scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "        # 绘制折线图\n",
    "        #plt.plot(s_true[:1000], label='s_true')\n",
    "        #plt.plot(p_true[:5000], label='p_true')\n",
    "        #plt.plot(p_hat[:1000], label='p_hat')\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        import matplotlib\n",
    "\n",
    "        # 设置字体和大小\n",
    "        matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 中文标签\n",
    "        matplotlib.rcParams['font.size'] = 8\n",
    "        matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "        matplotlib.rcParams['axes.unicode_minus'] = False  # 正确显示负号\n",
    "\n",
    "        # 假设 s_true 和 p_hat 是您的数据\n",
    "        # 假设 mean 是您使用的平均值数组\n",
    "\n",
    "        # 设置图表尺寸\n",
    "        plt.figure(figsize=(4.13, 3.15))  # 半栏A4宽度约4.13英寸\n",
    "\n",
    "        # 根据 a 的值绘制不同的设备\n",
    "        if a == 0:\n",
    "            device_name = \"Fridge\"\n",
    "            plt.plot(s_true[:500], label='t_value', linestyle='-', color='blue')  # 使用实线和正方形标记，并指定颜色为蓝色\n",
    "            plt.plot(p_hat[:500], label='p_value', linestyle='-.', color='red')  # 使用点划线和星形标记，并指定颜色为红色\n",
    "        elif a == 1:\n",
    "            device_name = \"Dish Washer\"\n",
    "            plt.plot(s_true[:8000], label='t_value', linestyle='-', color='blue')\n",
    "            plt.plot(p_hat[:8000], label='p_value', linestyle='-.',color='red')\n",
    "        elif a == 2:\n",
    "            device_name = \"Washing Machine\"\n",
    "            s_true[0:100] = 0\n",
    "            plt.plot(s_true[:8000], label='t_value', linestyle='-', color='blue')  # 使用实线和正方形标记，并指定颜色为蓝色\n",
    "            plt.plot(p_hat[:8000], label='p_value', linestyle='-.', color='red')  # 使用点划线和星形标记，并指定颜色为红色\n",
    "        else:\n",
    "            device_name = \"Appliance \" + str(a)\n",
    "\n",
    "        # 添加标题、x轴标签和y轴标签\n",
    "        plt.title(device_name)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Value\")\n",
    "\n",
    "        # 添加图例\n",
    "        plt.legend()\n",
    "\n",
    "        # 设置分辨率为600ppi\n",
    "        dpi = 600\n",
    "\n",
    "        # 选择文件路径和文件名，保存为PNG格式\n",
    "        temp_save_path = fr'D:\\NILM\\小论文\\SKNILM\\绘图\\{device_name}图(unseen).png'\n",
    "        final_save_path = fr'D:\\NILM\\小论文\\SKNILM\\绘图\\{device_name}图(2000).png'\n",
    "\n",
    "        # 导出图片\n",
    "        plt.savefig(temp_save_path, dpi=dpi, format='png')\n",
    "\n",
    "        # 使用Pillow转换为灰度图\n",
    "        #img = Image.open(temp_save_path)\n",
    "        #gray_img = img.convert('L')\n",
    "        #gray_img.save(final_save_path)\n",
    "        # 显示图形\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ced20c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./UKDALE_unseen_8_11100.pth\n",
      "\n",
      "fridge\n",
      "F1 score  : 0.847 (0.847, 0.847)\n",
      "Precision : 0.818 (0.818, 0.818)\n",
      "Recall    : 0.877 (0.877, 0.877)\n",
      "Accuracy  : 0.880 (0.880, 0.880)\n",
      "MCC       : 0.750 (0.750, 0.750)\n",
      "MAE       : 19.90 (19.90, 19.90)\n",
      "SAE       : 0.072 (0.072, 0.072)\n",
      "\n",
      "dish_washer\n",
      "F1 score  : 0.776 (0.776, 0.776)\n",
      "Precision : 0.704 (0.704, 0.704)\n",
      "Recall    : 0.864 (0.864, 0.864)\n",
      "Accuracy  : 0.986 (0.986, 0.986)\n",
      "MCC       : 0.773 (0.773, 0.773)\n",
      "MAE       : 38.38 (38.38, 38.38)\n",
      "SAE       : 0.226 (0.226, 0.226)\n",
      "\n",
      "washing_machine\n",
      "F1 score  : 0.838 (0.838, 0.838)\n",
      "Precision : 0.846 (0.846, 0.846)\n",
      "Recall    : 0.830 (0.830, 0.830)\n",
      "Accuracy  : 0.996 (0.996, 0.996)\n",
      "MCC       : 0.836 (0.836, 0.836)\n",
      "MAE       : 8.33 (8.33, 8.33)\n",
      "SAE       : -0.020 (-0.020, -0.020)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "for a in range(3):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "    scores[a]['s_true'] = []\n",
    "    scores[a]['s_hat'] = []\n",
    "\n",
    "thr = 0.5\n",
    "\n",
    "for i in range(1):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_unseen_%d.pth' %i\n",
    "    filename = './UKDALE_unseen_8_1110%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(3):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        pm = ds_appliance[1][APPLIANCE[a]].sum() / ds_status[1][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['s_true'].append(s_true)\n",
    "        scores[a]['s_hat'].append(s_hat)\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "\n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[0], sorted(scores[i]['F1'])[0]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[0], sorted(scores[i]['Precision'])[0]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[0], sorted(scores[i]['Recall'])[0]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[0], sorted(scores[i]['Accuracy'])[0]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[0], sorted(scores[i]['MCC'])[0]))\n",
    "    print('MAE       : %.2f (%.2f, %.2f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[0], sorted(scores[i]['MAE'])[0]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[0], sorted(scores[i]['SAE'])[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "nilmtk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
